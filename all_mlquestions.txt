Question=  0 [Beginner] Lightest way to implement Gmail like auto suggest
-------------
Question=  1 How to explain sudden jump in precision
-------------
Question=  2 Good tutorials on spiking neurons?
-------------
Question=  3 What is the difference between biological and artificial neural networks?
selttext=  
comment no.= 0 -->  The main difference is that a biological nueron is uni directional. Basically, if we think of electricity as water, and a neuron as a bucket, biological information flow happens like this:

Imagine 4 empty buckets arranged as tightly as possible together.  Now, lifted over the center of those buckets, imagine another empty bucket suspended somehow.

Imagine you start to pour cups of water into the top bucket. In the beginning, none of that water goes into the bottom buckets. The bucket is filling, or the neuron is polarizing. 

Imagine that each bucket is set up to tip over and spill all its water at once as soon as it reaches a certain volume of water. So you keep pouring cups of water in, still nothing goes into the buckets below.

You're pouring and pouring. Suddenly, as soon as you reach the tipping point, the bucket tips over and spills all of its water into the downstream buckets. It slowly returns back to normal (I.e. Standing and empty)

Meanwhile all the downstream buckets, are only partially filled and none of their water gets to the buckets below them and so it continues.

The cups of water you poured into the first neuron represent each time the neuron was activated from upstream. The size of the buckets are usually the same. (The amount of water it takes for them to tip). But the amount of water they recieve is not always the same.

The buckets underneath cannot communicate with the buckets above usually.

So that's how biological neurons work in a really simplified fashion. Millions of buckets arranged on top of other buckets on top of other buckets.


When it comes to artificial networks backpropagation maybe sort of works like the initial evolution of the biological network in the developing mind. But it doesn't really work that way once the neurons are laid out.

Also in the animal brain, there is a network for each thing our brain can do. One for sight, within sight one for motion, one for contrast etc, one for hearing, one for words, one for singing, one for smell, etc, and they're all integrated centrally. There is a memory relay that the sensory centers connect to, it gets solidified during  dreaming, etc, there ate networks for balance, hunger, love, muscle movements, tools, actions, names, etc.

The 6 layeted cortex in humans acts like a 6 layer convergence pattern. You just throw an entire tidal wave of data at a sensor. The first layer just reads how the sensor responds without making any sense of it. The next layer detects patterns in that noise, the next detects patterns in that layer and so on and so forth.

The result at the end is often fed into a network for concepts and then vocabulary of any language. The network for vocabulary is tied with the network for mouth movement, sound etc and so forth, and that's how we create skynet. 
-------------
Question=  4 Text Mapping via Universal Taxonomy - Looking for practitioners to test use-cases
-------------
Question=  5 Using Regression to solve for multiple variables
-------------
Question=  6 Exploding linear value function approximation
-------------
Question=  7 Dimensionality in the Bayes decision rule for normally distributed classes.
-------------
Question=  8 Is there any book/lecture series/MOOC for Unsupervised learning?
selttext=  Apart from K-Means Clustering and some Gaussian stuff, I could not find much on Unsupervised Learning. Are there any set of Tutorials to get started with Unsupervised Learning? Or a book or a thesis of a PhD candidate?

P.S. I did my fair share of searching before asking.
comment no.= 0 -->  Try the MOOC on Udacity. It is great for beginners and it is really fun!
-------------
Question=  9 How to calculate equal error rate (EER) in multi-class decision?
-------------
Question=  10 What Activation Function to use for Convolutional Layers?
-------------
Question=  11 Project Help/Advice
-------------
Question=  12 General recurrent neural networks
-------------
Question=  13 I do not understand a step in neural networks, am i missing something?
selttext=  So i am a programmer, getting into AI/neural networks. Kind of started reading into it and a few tutorials. Mostly basic stuff (teach a few neurons to simulate/figure out how to be an AND/OR gate etc.)

Now i get how those neural networks work, they have input, then neurons with gain and all(?) inputs on them with a weight. Then they provide an output, which you put through another set of neurons which connects to all of the first lines of neurons, and they generate output for all your outputs.

Given this thing, and say a car (output: forward, steer left, steer right) and input: (distance to wall forward, distance to wall left of nose, distance to wall right of nose, distance to goal)

Then write a feedback loop to reward the neural network for not colliding (or punish collisions) and reward it got getting closer to the goal. (using back propagation)

Now we give it an area, set it in a simple maze, simple, but required backtracking or going away from the goal to get to the goal.

We let this neutal network run, give it random values, tons of iterations and generations, whatever way you like it.
No matter how i imagine this thing working, i can't imagine any other situation then ending up with a car, that will steer straight to the goal, and stops JUST in front of wall, and then just sits there. Doing nothing at all, because it can't for the life of him identify or work with the maze in order to go through.

Am i missing something here? Does the neural network have hidden magic i don't get. Or am i simply lacking inputs to complete the task? I can't really find any good examples online to look at either. They either do basic stuff, mayor hand holding (for the AI), or are insanely difficult to understand.
I want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described. 
comment no.= 0 -->  I think you're correct in assuming that the system you've described wouldn't really work. It's not because of any intrinsic weakness of neural networks - I think you've just chosen a poor objective function. 

You probably want to only reward reaching the goal (and optimize for doing so in as few steps as possible).
comment no.= 1 -->  I think understanding some more basic machine learning algorithms would help.

Have you done linear regression and logistic regression?

Do you know the difference between regression and classification? (All ML algorithms have essentially the same API, which allows you to plug-and-play algorithms like the ones included in sci-kit learn with ease)

Logistic regression is basically the same form as linear regression with a sigmoid on top that will make the output between 0 and 1.

Neural networks are basically layers of logistic regressions stacked up.

I've created a bunch of courses on Udemy that go through this progression:

https://www.udemy.com/data-science-linear-regression-in-python

https://www.udemy.com/data-science-logistic-regression-in-python

https://www.udemy.com/data-science-deep-learning-in-python

https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow

[Apologies if I understood you wrong and you're just trying to do reinforcement learning exclusively. Although I do have plans to do a course on that soon also. What I'm trying to get at here is that if you used a simpler linear model with the exact same inputs and outputs, it might give you some more insight into what's happening.]
comment no.= 2 -->  Try experimenting with [genetic algorithms](http://blog.otoro.net/2015/03/28/neural-slime-volleyball/) and [reinforcement learning](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html).
-------------
Question=  14 ML Options for Selling Ranking?
selttext=  I am currently experimenting with being a seller on Amazon. One of the features they have is to allow for advertising campaigns. They allow for you to 'bid' on certain keywords so your product appears when you search for that particular phrase. For a particular campaign, you can have many keywords and give them a daily budget. At the end of each day, you see how these performed through various metrics, culminating in the ROI for that keyword.

I wanted to use an algorithm to help figure out after every week which keywords were worth investing more funds into. For these types of problems, what algorithms would be best suited? Are there any services (Azure, AWS, etc) that would aid in this research?
comment no.= 0 -->  I don't want to belittle ML, but wouldn't this seem to be something that could more easily/efficiently be accomplished by just comparing things such as clickthrough/items shipped/etc. ? Just having some kind of comparison of revenue to cost would seem to do exactly what you want, and is a lot less taxing to implement!
-------------
Question=  15 Cost exploding in squared loss optimization. How to avoid it?
-------------
Question=  16 Teaching machine learning code
-------------
Question=  17 Is Donald Trump an artifact of overfitting?
selttext=  
comment no.= 0 -->  Over-regularization on noisy data,leading to regression to mean.
comment no.= 1 -->  I'm more thinking some kind of deep instability in the optimization algorithm, leading to a pathological but unphysical solution. 
comment no.= 2 -->  His unpredictability and changing opinions would point to it yes. 
comment no.= 3 -->  Does this thread belong in this subreddit?
-------------
Question=  18 Use of Option Parser in sklearn
-------------
Question=  19 Replicating Neural Style
selttext=  It seems a lot of people have been replicating the results of this paper http://arxiv.org/pdf/1508.06576v2.pdf, training a CNN to recreate an image in the style of another.

I would like to try doing this, but I'm not perfectly clear on how it's done. From what I understand, the CNN is essentially trained to map randomly generated pixels to the pixels of the desired picture? Or is it much more complicated than that?
comment no.= 0 -->  Have you checked something like this: https://github.com/jcjohnson/neural-style ?
or this: https://github.com/anishathalye/neural-style ?
-------------
Question=  20 Validation accuracy higher when feed-forward with dropout than without
selttext=  I am training with dropout. If I feed the validation set forward with dropout still on the validation error is lower. If I turn it off, the validation error is far higher. Ideas? Note that this issue isn't present for a shallow net, but it is for a very deep net.
comment no.= 0 -->  Are you dividing the weights by half ? At rest time or at validation, you need to divide the weights by half if you're using 0.5 as dropout probability.
-------------
Question=  21 Hoping for some guidance in selecting models for feature extraction
selttext=  Hi, I've been studying machine learning over the last couple of months with the hope of solving a specific problem. I'm hoping someone can help me with advice on what approach to take.

I have a large collection of labeled, connected graphs on which I would like to do unsupervised feature extraction. Ideally I want to discover higher level features present in the graphs, (similar to these features described by Prof Ng in this [video](https://youtu.be/ZmNOAtZIgIk?t=29m36s)) and be able to generate random graphs composed of only those features.

From my research, autoencoders seem to be used for similar problems. Does that sound right? Any advice or suggestions would be much appreciated. Thanks!
comment no.= 0 -->  Okay, so estimating "features" of a graph and simulating graphs can be difficult.

The best way I know about to do that is based on the "kronecker graph" idea. You suppose your graph can be generated by kronecker product from a small graph.

This actually yields excellent results, close to natural graphs. Now what you want to do is estimate the generator of your graph, and the simulate other graphs from it.

Here is the paper that introduced kronecker graphs, and how to estimate our generator.

https://cs.stanford.edu/people/jure/pubs/kronecker-jmlr10.pdf
-------------
Question=  22 How can I create a topic model with a mixture of multinomials and EM?
-------------
Question=  23 Parameter learning in LSTM
-------------
Question=  24 Question about best matlab libraries for certain classifiers and dealing with NaN values
-------------
Question=  25 Get dataset used in Learning to Execute
selttext=  Zaremba put up his Learning to Execute code. https://github.com/wojciechz/learning_to_execute
 I want to use data from this program, but I do not know torch so I don't know how to obtain the data. What lines should I add to get the data, or does anyone have a set saved anywhere?
comment no.= 0 -->  Did you read his paper? He uses the Penn Tree Bank data set. Its a common benchmark. I believe the paper explicitly states how to get it from Mikolov's website.
-------------
Question=  26 Need some direction for doing shared parameter regression.
selttext=  I have some y vs x data for a number of sub-populations in a population.  For the sake of this question, the data for each sub-population can be modelled using linear regression, y = m x + b.  There are only a handful of unique m and b values that are shared amongst the different sub-populations, and there are typically many fewer unique m's and b's than there are sub-populations.  We don't always know a priori which sub-population matches which m and b values, nor do we always know how many unique values of each there are.  For this problem it is equally as important to get the correct number of unique values as it is to determine what those values are.

I have tried two ways to do this so far:

 * The first way is to decide a priori which sub-populations will share which parameters (for our data this works some of the time).  This way is very fast, but it isn't always a great model because there is no general rule for deciding this.

 * The second way is to find m and b for all sub-populations, then find the unique values by using some clustering methods.  This way takes much more cpu time, which isn't great for our setup but isn't the end of the world; and sometimes we don't have enough data for all sub-populations to make reasonable estimates of the parameters.

Ideally there is a third way that both decides on which parameters are shared by which sub-populations and what the unique values of those parameters are.  

What I am looking for is some literature, or a specific topic to research on.  I have prior distributions for the model parameters, so bonus made up internet points to you if you can suggest a bayesian method.
comment no.= 0 -->  How about gradient descent https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
-------------
Question=  27 I would like to perform access control based on a learned confidence level - controlling a gate automatically with a license plate reader. Possible?
-------------
Question=  28 Are there databases for malicious javascript?
-------------
Question=  29 ML for Java Dev
-------------
Question=  30 Giving probability distribution as label
-------------
Question=  31 What model should I use for predicting a certain value (task completion time) ? Details inside.
selttext=  Hi All, 

I am new to this and I recently finished a beginner course on PluralSight [this](https://app.pluralsight.com/library/courses/r-understanding-machine-learning). And I recalled that at my workplace, I have a process which takes quite long time to complete and lot of other deliverables are dependent on this process to complete.

I have the historic data of that process which gives information like it's start date, end date and the start time and end time of the subprocesses that it is comprised of. How can use this data to predict the estimated end time of any fresh instance of this process? Which algorithm should I use? Any pointers are highly appreciated.
comment no.= 0 -->  Does the process itself have any input? How variable is the duration of the process?
comment no.= 1 -->  Firstly what is your baseline distribution of what you want to predict? 

What is the business value of the thing you are going to predict, and what representation would be most useful to those people? 

What would they do with the answer?
-------------
Question=  32 RNN - Vanishing or Exploding problem
selttext=  I'm trying to understand an exercise from [Hinton's course on Neural Networks](https://www.coursera.org/course/neuralnets).

[Complete exercise](http://i.imgur.com/9QpYbsn.jpg)

Basically, I need to know if it's a vanishing or exploding problem.

So what I did was first calculate ALL the partial derivatives corresponding to "chain ruling" ∂E/∂Wxy and search for some light there:

    ∂E/∂Wxy = ∂E/∂y * ∂y/∂h3 * ∂h/∂z3 * ∂z3/∂h2 * ∂h2/∂z2 * ∂z2/∂h1 * ∂h1/∂z1 * ∂z1/∂Wxh

    Calculating each component:

    ∂E/∂y = - (t3 - y) = - (0.5 - y)
    ∂y/∂h3 = Why = 1
    ∂h3/∂z3 = h3(1 - h3)
    ∂z3/∂h2 = Whh = -2
    ∂h2/∂z2 = h2(1 - h2)
    ∂z2/∂h1 = Whh = -2
    ∂h1/∂z1 = h1(1 - h1)
    ∂z1/∂Wxh = x1

    So:

    ∂E/∂Wxy = - (0.5 - y) * 1 * h3(1 - h3) * (-2) * h2(1 - h2) * (-2) * h1(1 - h1) * x1

I saw nothing there.

So I payed attention to ∂y/∂z as the exercise says, and this is what I found:

    ∂y/∂z3 = ∂y/∂h3 * ∂h3/∂z3

    ∂y/∂h3 = Why = 1
    ∂h3/∂z3 = h3(1 - h3)

    So:

    ∂y/∂z3 = 1 * h3(1 - h3)

What I see there:

 - h3 is the logistic function, output always will be between (0; 1)
 - 1 minus something between (0; 1) is something between (0; 1)
 - and something between (0; 1) multiplied by something between (0; 1) is something between (0; 1)

So in conclusion, not only it will be something between (0; 1), but it will be pretty small because the multiplication in ∂y/∂z3, and then it will be much smaller because in ∂E/∂Wxy, that logistic function appears 3 times (one for each logistic hidden unit) multiplying together, and that will shrink the whole gradient a lot, independently of the other terms.

My question is, am I correct? and when can it be an *exploding* problem? because as I see here and with that logic (that maybe is wrong), it'll ALWAYS be a vanishing problem.
comment no.= 0 -->  To my knowledge, as long as you're using the sigmoid activation function then you'll only have the vanishing gradient problem. Notice that the the derivative of the sigmoid produces outputs on the range (0, 1/4]. Every time you backpropagate you're multiplying by the derivative of the sigmoid, and since it's always a fraction you're always making it smaller.
comment no.= 1 -->  I'm gonna add this here just in case someone has a similar question.

So yes, that deduction was correctly, and could find it [on wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (yeap...):


> *Traditional activation functions such as the hyperbolic tangent function have gradients in the range (-1, 1) or [0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the "front" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly.*

I suppose that the exploding problem occurs only when other kind of activation functions are used, that allow values over 1, that multiplying together over backprop can "explode" into some really big numbers.
-------------
Question=  33 Lagrange Formulation of SVM model - Question
selttext=  I'm going through the Caltech lectures on Machine Learning and am confused by one step in the calculating the solution to the SVM model. In slide 13 (link below), why is there a minus sign in front of the summation? I thought that for constraints of the form g(x)>=0, you add a lamba * g(x) term to the lagrangian, not -lambda * g(x). Slide 9 shows that the constraint is of the form g(x)>=0.  I must be missing something small.

http://work.caltech.edu/slides/slides14.pdf
comment no.= 0 -->  That's because if you have an optimization problem
min f(x) subject to h(x) <= 0, 

the lagrangian will be
L = f + lambda * h where the lambda(i) are positive.


Here with h = - g, he used - lambda with lambda(i) >= 0 instead of +lambda with lambda(i) <= 0 for readability.
-------------
Question=  34 Comparing two models on different instances
-------------
Question=  35 Using Bayes Classifier in home automation to build an Alexa like assistant
selttext=  Hi,

I have a lot of connected devices at home, I can control everything from my phone on a interface I made, but I would like to go further. 

My goal is to be able to text or speak with my home system, like a kind of Alexa/siri ( with very basic commands to begin ).

I think machine learning can be a good idea to implement that, using a Bayes Classifier ( I found this awesome library to do that : https://github.com/NaturalNode/natural#classifiers )

My goal is to train the system with sentence in input ( "Turn on the light" for example ), and in output the action ( "light-on" for example ), and then use the trained system to detect what I want to do when I speak to my house.

Do you think that's a good way of doing this ?

Thanks a lot,
comment no.= 0 -->  Mark?
-------------
Question=  36 Examples of Restricted Boltzmann Machines in TensorFlow?
-------------
Question=  37 1D Convolution in Neural Networks
-------------
Question=  38 Short time series of unequal lengths, weights changing with every time step, how to align?
-------------
Question=  39 Would a neural network be a good choice for this problem?
selttext=  I am new to ML, and want to do a beginner project. Making an individual perceptron seems easy, but I'm wondering how straightforward it would be to scale things up to do something more complex: I have a few microphones spaced around a room that listen for a specific tone. The amplitude of the tone at each mic is measured, and then a program will guess at the sound's location in the room.

There are 3 or 4 inputs to the system: The amplitude readings. There are 2 outputs: The x, y coordinates (though maybe polar coordinates with the origin at the center of the room would be simpler to model mathematically).

After each measurement and subsequent guess, I'd feed it the right answer and it would adjust the weighting for some variables in a provided equation. This would constitute one training round.

Does this sound do-able to code from scratch? Or is there a more appropriate algorithm I should try?
comment no.= 0 -->  In an ideal room (no reverberations) you could do without a neural network. You can measure how much the volume fades per unit distance. Then at each mic you can draw a circle of where the sound would be for the volume you measure. With two circles you get 2 intersection points, with three you get one point (triangulation). Perhaps a neural net can help handle room imperfection.
comment no.= 1 -->  Do you have to use ML?

Because, 
first, you'll need data (let's at least some 100s of examples to see some pattern). So, either you have data, or either you create your own data with some simulations.

and second, I think some direct optimization approach might work a lot better if you have a model.
For Ipred(x,y, pos(micro)) the intensity predicted by your model from a source at x,y to a micro at pos(micro): 
something like minimize on {x,y} \sum over pos-micro of cost( Ipred(x, y, micro), Imeasured) might work and still be somewhat robust to noise.

The model should be very simple, like some constant over r^2.
comment no.= 2 -->  You might want to check over in /r/DSP, since they may have an approach for this kind of thing - though it may not be an ML approach.  

For well-defined problems with fields devoted to them, often an ML approach will be less efficient than a "theory-based" solution.  But even so, if you are determined to approach this with ML (and why not, it sounds fun), looking at the DSP approach will give you a benchmark to shoot for and a way to evaluate the performance of whatever you come up with.
-------------
Question=  40 Why is this NN so dumb?
-------------
Question=  41 python KeyError
selttext=  KeyError obtained when reading from a csv file. I've created an empty dict and want the most frequent values of a headed column (attribute) to populate the dict. 

Could someone please explain why this is occurring?

 

    def gain(data, attr, target_attr):
    """
    Calculates the information gain (reduction in entropy) that would
    result by splitting the data on the chosen attribute (attr).
    """
    val_freq = {}
    subset_entropy = 0.0

    # Calculate the frequency of each of the values in the target attribute
    for record in data:
        if record in val_freq[record[attr]]:
            val_freq[record[attr]] += 1.0
        else:
            val_freq[record[attr]] = 1.0
comment no.= 0 -->  You are testing if `record` is in `val_freq[record[attr]]` without being sure if `record[attr]` is in `val_freq`.

What you might want to do is to check:

    if record[attr] in val_freq:

Also you should make sure if `attr` is actually in `record`. I don't know how exactly your data looks like, but that could be another error waiting to happen.
-------------
Question=  42 Question for Tensorflow different execution path in training / testing phase
-------------
Question=  43 Backprop: Squared Error or Mean Squared Error or Cross Entropy?
-------------
Question=  44 Tensorflow Cifar-10 evaluation error: Enqueue operation was cancelled - How to fix?
-------------
Question=  45 Machine Learning for MRI/CT Scans
-------------
Question=  46 How do you guys download massive datasets?
selttext=  How are you guys getting massive datasets? Anything over 300Gigs which is the datacap by popular ISP's like comcast? 


comment no.= 0 -->  If you work at a university or company, ask your IT department. Otherwise I don't really know. From what I hear about Comcast, the chances will probably be slim, but maybe you can ask them to make an exception because you're using the data for (non-profit) research or something. 
comment no.= 1 -->  I'm not sure what datasets you're looking at, but you could always ask the uploader to split it up into separate files, and then put them back together yourself.
-------------
Question=  47 Trying to match transactions with receipts - where to start
selttext=  I'm trying to match receipts (ocr scanned) with their transaction entry from my credit card provider.

I have training data for receipts I already matched .

But where to start? And can I use an existing service API?


comment no.= 0 -->  Are you trying to automatically reconcile your statements with your receipts?
-------------
Question=  48 (X-Post from r/Java_Programming) Java-ml Clustering Distance from Centriod
-------------
Question=  49 Machine learning, Deep and wide data, how to start?
-------------
Question=  50 Let's have some fun through webcam? register here and i'll be your all night! AmazonGirl44 RHQBEU
-------------
Question=  51 Necessary Math Background?
selttext=  Considering taking a course in ML and trying to evaluate what specific math skills would be requisite.
comment no.= 0 -->  Calculus, linear algebra, probability, and statistics
comment no.= 1 -->  If this is in a university, try writing the professor and asking for a syllabus or at least what text they use.  Machine Learning is a growing field and any one class will have different requirements than another.  A lot will depend on whether it's more theoretical or applied.

Try looking at the various common algorithms (k-means, support vector machines, naive bayes, decision trees, neural networks, etc.) and read up on the math involved.  For example, naive bayes relies heavily on conditional probability and Bayes' Rule while neural networks rely on using linear algebra to manipulate matrices and tensors (thus Google's "Tensor Flow").

What you're looking for also depends on what you're trying to learn how to do.  Do you want learn about various algorithms and how to apply them using common tools (like scikit learn)?  Or do you want to know enough that you could be developing and implementing your own algorithms?
comment no.= 2 -->  I took an upper level undergrad course in ML and did well enough even though my grasp of math isn't great (I was pretty much re-learning the linear algebra as I went). The most intense linear alegebra I ended up doing is deriving the psudo-inverse of a matrix, which is super easy. Also you need calculus, but only derivatives (in fact, as I recall, mostly only partial derivatives) and they're always super simple. In this case I think the most complex thing you'll need to understand is the chain rule. Be able to do it (it's easy once you understand it). For probability you need to know bayes theorem (which every human being should know regardless), and understanding the concept of a probability distribution is good.

But the best thing you can do is talk to your professor, tell him your math background, and ask for his opinion.
-------------
Question=  52 Kmeans Clustering Library (X-post from /r/Python)
-------------
Question=  53 How to single-node ML lab? For text log classification.
selttext=  So I've spent time this week on regex filters and field extractions for Logstash to read my log files and insert the logs and extracted fields into Elasticsearch. My application is very log-noisy and I was weeding out the "normal" errors to better identify actual issues, so I've iteratively been identifying patterns of the most common remaining log entries to end up with the more rare ones.

I showed the progress to coworkers, and one asked if machine learning could do the classification for me and free me up to better interpret the meaning. Hmmm.... So a few dozen Internet pages later...

I'm wanting to install Mahout or Spark/Mlib to kick the tires, feed it some logs and see if I can figure out what to ask next. But much of the help material on installing on a cluster. I just want to set something up on a single machine and feed it up to a gigabyte of log files and see what it I can do with it.

So am I on the right track? Can Mahout or Spark/MLib run on a single machine, or should I be looking at something else?
comment no.= 0 -->  Crickets over the weekend so far. But I got Apache Spark installed and doing a couple of simple things on a single machine, and the actual steps aren't difficult at all:

- Have Java with JAVA_HOME set appropriately
- [Download Spark](http://spark.apache.org/downloads.html) precompiled with Hadoop client
- Untar the Spark tarball into a directory
- Run things in the bin folder

On Windows there were some errors even though there are cmd/bat versions of the commands in bin. I think I need extra libraries. But on a bare Ubuntu 14.04 container plus Java 8 and Spark it's running with no extra steps so far.

[This page ](http://spark.apache.org/docs/latest/) has some example commands.

[This section showing language classification of tweets](https://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html) (YouTube presentation included) is where I'm going to start my tinkering. It demonstrates tokenizing and classifying tweets into clusters that end up being more or less language collections, but I think this can do what I've been trying to manually do: classify log entry types into clusters, and then I can focus on the small clusters as rare log entry types.

I think my steps are going to be:

- Use my existing Logstash field extractions against a couple of non-problem days' logs
- Store that in some intermediate data store...the tweet exercise uses SQL; with my relatively small data set I'll see if I can use text dumps or just pull it back out of Elasticsearch. If those fail, MongoDb?
- Featurize the log text. I may omit the timestamp and thread pool info; or try both with and without. I'll probably start with the [HashingTF](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf) method as the example uses, but the [Word2Vec](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec) method looks worth trying out for this purpose to my untrained eye.
- Do [K-Means clustering](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means) against the featurized log data
- ...
- Profit
- Well, actually then I'll see what size each cluster is and look for small-cluster or unmatched cluster (if that's a thing) against the rest of the log data.
-------------
Question=  54 choose steps to apply various image transform tools to images
-------------
Question=  55 Videos: introduction to Azure Machine Learning to make predictions (learn the basics in under an hour)
-------------
Question=  56 Use or not to use word dictionary?
-------------
Question=  57 Compressing image data to two channels for grasp detection
selttext=  I'm utilizing transfer learning on an imagenet trained network to train a CNN for grasp location (via regression). I'm using image and depth data from a kinect. To use an imagenet trained model, the network needs to be 3 channel, and I've done this initially by arbitrarily throwing away the blue channel to go from RGBD to RGD. I have a working network from this, but I think it was a hacky solution. I've been thinking how for grasping locations, colour is irrelevant and in fact could lead to biases as my dataset is relatively small. In light of this, I was thinking I could convert RGB to HSV and throw away the H channel. This throws away colour information but preserves texture, which I think it more important for learning grasp features. Is this thinking sound?
comment no.= 0 -->  If you have to choose 3 channels from color+depth, then it indeed sounds like hue is probably the least important for you. However, I'm a little concerned about how well transfer learning is going to work if you change the features. So I could imagine RGD/RDB/DGB might work better than DSV, because those only "screw up" one channel. If you're training the ImageNet network yourself, then I suppose you could use HSV from the start, and avoid this a bit. 

It might also be possible to just augment the original network with a fourth channel. You can just add the connections for that if you wanted. If you think these could also benefit from transfer learning, you could initialize the weights based on those of the other channels. For instance, you could use their average. And then maybe multiply everything by 3/4 to keep the size of the inputs to the next layer roughly equal. 

Basically I don't know what will happen, so you may just have to try different options. That's usually the answer in these cases, but I hope it won't be too much effort / training time here.
-------------
Question=  58 How would you use this kind of data?
-------------
Question=  59 Single Feature Learning useful?
-------------
Question=  60 Best tutorial / nn for regression analysis?
selttext=  New to ML/NNs but not to programming or engineering.

I'm currently using an older version of [```fitnet```](http://www.mathworks.com/help/nnet/ref/fitnet.html) in MATLAB. I followed the [Neural Network Fitting Tool](http://www.mathworks.com/help/nnet/gs/fit-data-with-a-neural-network.html) to make a short script to predict my data, however I'm not happy with the results and would like to try using a deeper NN using Python.

However, there are so many options out there and a lot of them seem to be based on classifications of some sort, I just need a nudge in the right direction or just the right google search to get started.

My output data is a function of multiple input variables:

     y = f(x1,x2,x3,x4,...)

I'd prefer practical guides over the full background in math. I don't need to completely understand the math, just enough to be dangerous. (And perform better than Matlab's NN).


### [```fitnet```](http://www.mathworks.com/help/nnet/ref/fitnet.html) docs:

*Fitting networks are feedforward neural networks (feedforwardnet) used to fit an input-output relationship.*"

fitnet(hiddenSizes,trainFcn) takes these arguments,
	
| ```hiddenSizes``` | Row vector of one or more hidden layer sizes (default = 10) |
|-------------------|------------------------------------------------------------:|
| ```trainFcn```    |                     Training function (default = 'trainlm') |

and returns a fitting neural network.


Example:

    [x,t] = simplefit_dataset;
    net = fitnet(10);
    net = train(net,x,t);
    view(net)
    y = net(x);
    perf = perform(net,y,t)
comment no.= 0 -->  I don't know what you're doing exactly or why you're using neural network specifically, but have you not heard of scikit-learn, specifically their [regression page](http://scikit-learn.org/dev/supervised_learning.html#supervised-learning)? 

comment no.= 1 -->  I teach a bunch of courses on regression and classification using various linear and deep nets, and I find that the actual math you need to understand what's going on is usually too much math for those who want to focus on the practical bits. (At least from what I remember, you should have learned about maximum/log-likelihood, gradients, and the chain rule in undergrad).

That said, why don't you just try a more modern library like Theano or TensorFlow? You wouldn't need to calculate gradients yourself, and it contains APIs for more recently developed techniques. So using them proficiently is just a matter of reading and understanding the documentation.
-------------
Question=  61 Inference stage in Batch Normalised Network
-------------
Question=  62 Classification with numerical labels?
-------------
Question=  63 A confusion regarding kernels
selttext=  I fairly understand kernels in machine learning, how algorithms are kernalised and i also understand how kernels in image processing work, as they do in smoothing and in filters. But I can't help but wonder if they are related. 



For some time I began to relate the Guassian kernel to be some function to transform the image vector into a new feature space and all, but I'm unable to bring out any connection. Could someone help me. 
comment no.= 0 -->  I have been asking myself what is a kernel for some time now. I thought I understood it when it came to image processing, but then I started doing ML and I too couldn't see the connection.

Looking up the definition on Google, maybe that will help us:
kernel - "the central or most important part of something".
-------------
Question=  64 Machine Learning vs. Data Mining, and recommendations for someone with no background in this?
selttext=  Hi Machine Learning. I'm a doctoral student in an IT-related field and have an idea for my dissertation. It will involve utilizing big data and automated analysis to make recommendations.

My weak areas involve big data altogether. I don't have any background with machine learning, statistics, or data mining. I am skilled and knowledgeable in my area of study, and am willing and excited to learn about machine learning or data mining. I've been researching which subset of big data I should be using for my dissertation, but have ended up more confused than before. For example, [this reddit post](https://www.reddit.com/r/MachineLearning/comments/24sc5n/data_mining_vs_machine_learning/) attempts to explain it, but everyone's interpretation is different.

Basically, I need to begin studying one of the big data focuses to get my dissertation started, but I'm not sure which one I need. Essentially, I'm looking to incorporate a big data/machine learning/data mining approach that:

* Takes information about a large number of data I have collected
* Identifies patterns with the data
* Makes configuration recommendations to me based on the rules I set
* Optional: Automates the implementation of the "best" recommended config

Is this data mining, machine learning, or something else? Any other recommendations for someone with no background in mathematics, statistics, or big data (but can program, perform the other technical pieces, and learn/study about any topics I need to)?
comment no.= 0 -->  The distinction between data mining and machine learning is pretty fuzzy and there is a ton of overlap so I wouldn't worry too much about it.

Your question is pretty light on details (what kind of data do you have? what sorts of patterns are you looking for? what kind of rules will you create? are you planning on using the discovered patterns to make rules?) so I'm not sure how helpful the remainder of my answer will be. If you provide more detail about the specific problem you are trying to solve there's a chance someone here could provide some more helpful advice.

Since you aren't knowledgeable about DM/ML trying to find the correct way of formulating the problem so that it can be handled by DM/ML will be impossible. There are many kinds of problems that have been studied quite a bit that aren't discussed in introductory materials so relying on the standard textbooks isn't necessarily a good idea. You may very well start learning about ML/DM and spend weeks of study only to find out it isn't applicable or to realize that you've been studying the wrong portions of the field. 
Some of what you are doing (identifying patterns in data) sounds like it would benefit from ML/DM techniques, while other things (making recommendations based on user defined rules) does not (if you were trying to get a computer to learn potential rules from data you should look into association rule mining).

**Instead you should probably try to find previous work in the field solving similar problems and look at the sorts of techniques they employ and use that as a jumping off point for further background reading.** If you are trying to automate the configuration of databases then search something like "automated database configuration" in google scholar.

If you want to try to learn about DM/ML your best bet would be to [checkout some of the introductory links on /r/machinelearning](https://www.reddit.com/r/MachineLearning/wiki/index) and read the first couple chapters of one of the textbooks listed there or watch the introductory lessons from a MOOC so that you have a basic idea of what sorts of DM/ML tools are out there. But chances are good that your problem won't fit quite right into the basic paradigms discussed in introductory materials so it is important that you look at previous work on problems similar to the one you are trying to solve.
-------------
Question=  65 How to recognize fields in web pages ?
selttext=  Hi,
I have got into machine learning recently and I would like to ask advice on a couple of questions:
1. How do you recognize 'fields' in unstructured web page data? For example, I have 2,000 web pages about the same topic and I want to to recognize the top 5 fields contained on each page. Lets say that my 2k pages dataset is about cars (taken from the popular car reviewing websites). Then the output for the 5 fields would be:
>
>* Car manufacturer : Ford
>* Color: blue
>* Car type: pickup
>* Engine: 3.0L
>* Cylinders: 6
>

But , for example the field 'number of passengers' would not enter the 'top 5' fields list because, lets say, only 100 pages are talking about it, so , statistically , it is not included.
What is the sequence of steps/algorithms to achieve this and what open source package would you recommend me to use ?
I have found tools for topic creation and classification, but they seem to be focusing on some specific fields, like Name entities, or Places, but what I want is to statistically detect the 'top 5' fields.

2 Second question, if I may, of course:
How do you take advantage of already existent knowledge implicitly contained in HTML tags and logical webpage structure?
For example, the car model, can be already embedded int the 'title' tag of the HTML page describing the car. Or maybe you don't need to extract anything because there is already an html TABLE with a lot of fields. How do you extract this knowledge from HTML? But, you could not relay on it completely, because every web page will have different HTML template, so , I suppose, you must first scan the website fully, identify its template and then , extract the data from templates. I am correct? Do you know any tools that already parse HTML to prepare it as an input for machine learning algorithms?

Thank you very much in advance

comment no.= 0 -->  Found this:
https://www.youtube.com/watch?v=VINCQghQRuM
Maybe I could feed the HTML directly to the RNN , having such a complex internal logic it may understand the patterns and extract the data i need without any HTML preprocessing ?
-------------
Question=  66 Small Project on Evolutionary Algorithms / Machine Learning
selttext=  I have a small university project concerning evolutionary algorithms in which I will work on the [NCAA dataset](https://www.kaggle.com/c/march-machine-learning-mania-2016/data) that's part of the current kaggle competition.

The idea is:
1. Train some (simple?) learning algorithms on the dataset / subsets of the data
2. Create an ensemble predictor by weighing the different trained models, so we get a prediction based on all the base models.
3. Repeat a lot: Use evolutionary methods to find good weights for step 2. 

The focus of the project is to try different approaches in step 3 for selection, mutation, reproduction and evaluate which one works best. Of course this might be easier on another dataset, but taking part in kaggle competitions is super fun :) 

What I am still unsure about is **which base learners I should use** and this is my question that I hope you can help me with. I know that a random forest approach works well in many settings, so simply training a big amount of trees might work.

However, since the dataset contains numerical and ordinal data, as well as binary nominal data, I think it should be beneficial to use different learning methods that are able to handle those respective types of data well and combine them.

What do you think? Which models should I try to combine?

Thanks for your help!

EDIT: The task is predicting the probability of which basketball team wins a given matchup, so I'm looking for regression models.

EDIT2: After looking into this some more, I realised that what I want to do is called bagging. I am probably going to first try a big bag of regression trees (which I guess would resemble random forest). Secondly, I will try to combine different base models, so I will try to find good weights for a combination of a regression tree (or multiple ones?), an SVM, maybe MARS, maybe ANN. If you have any more suggestions on this, I'd be very happy to receive more suggestions :)
comment no.= 0 -->  If I understand correctly you have to predict some kind of probability, right? That means you need to do regression (as opposed to classification). Regression trees should work fairly well. They should be able to handle both qualitative and quantitative data. Maybe [MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines)... 

If you want to use something like a neural network, it deals fairly naturally with numerical data (although you may want to normalize). You can use one-hot encoding for nominal data (if you have one variable that can take on N classes, create N corresponding input nodes and turn all off except for the one that corresponds to the variable's current value). For (bounded) ordinal data you can use a progressive encoding, so if there are N possible values, create N-1 nodes, and if the current value is the third, turn on the first two nodes. (So if you have Temperature which can be Cold, Warm, or Hot, then make 2 nodes and turn them all off for Cold, turn the first on for Warm and turn both on for Hot.) These encodings might also work for other algorithms. 
-------------
Question=  67 Quick question about my Facebook dataset
-------------
Question=  68 Reframing K-means using neural networks.
selttext=  By all indications, K-means is a numerical method for unsupervised clustering. 
  
I'm looking at doing K-means using neural networks because I believe there would be a speedup in doing so.  
  
Unfortunately, I'm having trouble dealing with output representation. For this, self organizing maps were recommended.   
  
Can someone run-me through self organizing maps?  
(Explain it like Im 10)
  

comment no.= 0 -->  First of all, you should know that k-means and self organizing maps (SOMs) are different things and SOMs are not just a faster way to calculate a k-means clustering or something like that. The results for small SOMs will be similar to k-means though. 

In a SOM you have a number of nodes/neurons that are organized into a n-dimensional point lattice (almost always a 2D grid). If you have m-dimensional data (i.e. each data point has m features / values), then each node in the SOM has m weights or parameters. In that sense, these nodes could be compared to the cluster means in k-means. When the network is trained, each new data point is simply associated with the most similar node (just like in k-means). 

To train the network, you take a data point D and calculate the distance to each node N. Usually this just means the Euclidian distance. Remember those m values that the nodes and data points have? Just subtract the D's values from N's values, square each result, sum them together, and take the square root. We will call the closest / most similar node C. 

Now we want to move C, and the nodes that are close to C (on the grid), even closer to N. We want to pull C and nodes that are very close to it very hard in the direction of N, and pull nodes that are further away a bit softer. To do this, we define a similarity function called the "neighborhood function". A similarity function is basically the opposite (inverse) of a distance function. If two nodes are the same, it should be 1, and if they could not be more different, it should be 0. One way to do this is to calculate the Euclidean distance (on the grid, not of their feature vectors), plug it into a [Gaussian function](https://en.wikipedia.org/wiki/Gaussian_function) and take the absolute value (if it's negative, multiply by -1). You can start out with a really wide Gaussian (high value of `c` on that Wikipedia page) and make it narrower when you've been training the SOM for a long time. 

Now we're going to pull every\* node N towards the current data point D. The amount by which we change N's parameters is determined by the difference with D's features, multiplied by the neighborhood function, multiplied by the current learn rate. The learn rate is a number between 0 and 1 that determines how fast we should pull each node towards the current data point. You want this value to be high when you start training and then decrease over time. 

You do this for all of the data points in your data set, and then keep repeating that until you are satisfied with the result. What you end up with is not just a mapping from data points to a cluster as in k-means, but also a mapping to a lower-dimensional space (the m-dimensional point lattice / grid). This means you could also use it for dimensionality reduction (like e.g. principle component analysis). 
-------------
Question=  69 Trying to predict my manger's arrival times.
selttext=  I've collected data that records when my boss walks through the office e.g. 9am, 9:15am, 9:12am, 9:05am, 9:30am and I'd like to predict what the next values may be to make sure I'm at my desk as often as possible during his next walkthroughs. 

Is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure I'm at my desk during the average of those times or at least within 1 standard deviation?

Is there something more advanced in the machine learning area that can deduce something interesting about this data to help me?
comment no.= 0 -->  What are all factors you recorded?
comment no.= 1 -->  Check the auto correlation and see if there is a potential to use an ARMA model (using the minutes after 9 for each day as a time series).  To get more fancy try random forest regression and add some categorical variables for holidays, long weekends, sports events etc the night before.
comment no.= 2 -->  Trying to not get too far into creepville you can use this as an added component. http://googlegeodevelopers.blogspot.com/2015/11/predicting-future-with-google-maps-apis.html?m=1

I second adding calendar info (holidays, day of week, etc)

You can grab public transportation data if you think they might be using buses instead. You'll then get discrete chunks when the buses general arrive coupled with their walk into your department. 

However, I think you already narrowed their morning down to a 30min chunk, isn't that good enough? lol
-------------
Question=  70 Using ML to determine whether a webpage is an article or not?
selttext=  Does anyone know if this sort of work has been done or not and if there are any good labeled data sets to work with?

Any thoughts on where I can look to start approaching this problem would be great. 
comment no.= 0 -->  if there's nothing existing, what scraping a news site's archive, like Bloomberg, to get a shit ton of example articles? Then maybe a site like reddit for non-article examples. I bet this takes less than a few hours depending on how much web scraping you've done, since theres already existing reddit scraping apis
-------------
Question=  71 SVM kernels are like heuristics?
selttext=  In the Machine Learning coursera course, Andrew says that different Kernels are represented by different similarity functions. He then gives an example of the Gaussian Kernel which is just a similarity function.
So basically different kernels are different similarity functions/heuristics?
comment no.= 0 -->  Yes, in a sense. They are dot products that make computations easier and they do look for similarities for all intents and purposes.

Kernel functions are actually(or should be) based on the domain knowledge about how the data should appear. Is it linear? polynomial? radial? That's basically the best option for choosing kernel functions, although there are some automated functions out there now.

Sharing 3 links I enjoy on kernel functions:

[Quora](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)

[Great Visual Representation](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)

[Basically all kernel functions](http://crsouza.com/2010/03/kernel-functions-for-machine-learning-applications/)
-------------
Question=  72 How to learn user behavior?
-------------
Question=  73 Question Answering System where to start?
selttext=  Assuming I have a raw dataset with thousands of questions and answers, what would be the best way to tackle a system that is smart enough suggest possible answers for similar questions?

Are there any libraries or systems that can already do this and that I can build on top ?

Cheers!
comment no.= 0 -->  You might take a look at the papers we covered in an AI/ML course last term at PSU (Melanie Mitchell's class).  We started by covering Recurrent Neural Networks and LSTMs.  The papers starting Nov 12 specifically covered QA systems.

http://web.cecs.pdx.edu/~mm/ils/fall2015/fall2015.html
-------------
Question=  74 Using sub-datasets while experimenting with a learning algorithm
-------------
Question=  75 Help finding gradient of neural network for temporal difference learning.
-------------
Question=  76 Can someone explain to me or provide me with a practical application of the use of a Perceptron Learning Algorithm?
-------------
Question=  77 Multi Dimension inputs for NN
-------------
Question=  78 Should I get into ML?
selttext=  I'm not entirely sure how I should proceed and looking for some advice. 

I'm an artist who is looking to create a neural network and train it to my taste (Taste, from what I understand, is the way our neurons are wired which are wired based on experience) and have it search for art for me.  I'd basically train it to art pieces I like and ones I don't like. I realize this is not an accurate method for pinning down my taste, it'd be better if it could scan my memories, or even be a neural network modeled on my neural circuit, but these days it seems that's not possible.

Art discovery is something I try and do every single day, looking for fine art photography, music, movies, interior design, furniture, architecture, fashion, cooking, etc. 

I do this because, in a quote "A child never writes his own alphabet" - Jacque Fresco

And "You can't exceed your environment. If you give a cannibal a watch, he doesn't look at it and say 'the gears are not precise, they are ten thousandth of an inch off' He doesn't say that. It's impossible. That's what I mean by you can't exceed your environment. You can't exceed what you've been exposed to." - Jacque Fresco

So I'm basically trying to expand my environment by exposing myself to as many different systems as possible. I've been doing this for 3 years and my work has reached levels I never thought it could as I consistently experienced the dunning-kruger effect.

Now I'm wondering if it's possible I could create a subjective neural network, feed it my favorite films, music, books, etc. with a rating, and my least favorite with a rating. I'd be basically training it everyday as I discover more art. 

After that I want to program it to search for art for me, with the intent of finding *new information* on the internet (this is critical, because it is the whole point), that aligns with my taste. Art that maybe combines all areas of my taste. 

If I'm being honest, the subject of machine learning does not give me the same ecstasy I experience when making a film. Which is why I've come here to ask for advice and guidance. 

I'm not sure if it would be better to hire a developer (after I've saved up enough money) to write these for me - or would it be worth the time investment to get into ML on my own and build neural networks from scratch on Python? For the kind of program I want to build, how long do you think it would take?

I looked for programs that build neural networks and only found one so far called Simbrain, but it doesn't seem (although I'm probably wrong), it can do what I want. It seems it can only analyze data/numbers. 

Would love some help :/
comment no.= 0 -->  Hell yeah it's possible, that's what the 'recommended for you' sections of Amazon and Netflix are doing! You should totally teach yourself and build something on your own. Who knows, maybe the ecstasy will come later. Especially after you've made something that works! Look into recommender systems, that's what they're called. Also there's an online course called CS231n on convolutional neural networks that's used in image recognition. Sounds like a fun project.
-------------
Question=  79 R or Python, which is best for an ML beginner?
selttext=  I would like to know which language I should use for starting Machine Learning. From what I can tell most people in the field use R, but I've also heard that Python is being used more and more. I have some experience with Python, should I stick with what I know or should I learn R? 
comment no.= 0 -->  If you already know Python, continue using Python. For all intents and purposes, they're analogous. 

Packages: [here](https://github.com/rasbt/pattern_classification/blob/master/resources/python_data_libraries.md)
comment no.= 1 -->  > From what I can tell most people in the field use R

Not true at all. I fact it seems like R has become a minority language for ML at this point. Many popular ML & Deep learning packages are either written in Python, or have Python bindings (TensorFlow, Theano, Keras).
comment no.= 2 -->  Python, because it extends better to programming systems beyond where R covers. 

It's a more generally applicable tool, and you'll need to do "regular programming" for many tasks (data cleaning at a minimum). 

The Big Data infrastructures typically have Java and Python interfaces. 

If you small data and you can read everything into memory, and you have programmers you can order to write lots of stuff for you quickly, then R could be enough and you benefit from the wide package library. 

There are fewer jobs like that. 
comment no.= 3 -->  Focus on the statistical techniques. Use what's easier.
-------------
Question=  80 Tensorflow layer stacking question
-------------
Question=  81 Best Algorithms for learning on sparse data?
selttext=  I have a dataset that I have collected with ~ 5000 binary features and less than 1% of them are 1's.  I did a quick search to see if there are algorithms that are particularly good at working with sparse data and only found stochastic gradient descent.  Are there any others that are particularly good at working with sparse data?
comment no.= 0 -->  Not sure of your goals but recommendation engines/recommender systems are built on this premise.
comment no.= 1 -->  What are you trying to do?  Supervised classification?

If so, then linear models with L1 constraints/penalties (called 'lasso') have been developed with this problem in mind.  Comes up frequently in gene expression studies.
-------------
Question=  82 How do MS-TDNNs work?
-------------
Question=  83 Non-restricted Boltzman machines
-------------
Question=  84 Two output vectors?
-------------
Question=  85 Training set for job title classification.
-------------
Question=  86 Choosing hyperparameters of Gaussian Process
-------------
Question=  87 Help understanding Hiddeen Markov Models
-------------
Question=  88 I don't have the time to moderate this sub. Any volunteers?
-------------
Question=  89 PredictionIO vs other? Just starting out.
-------------
Question=  90 ML for regression using predefined set of parameters
-------------
Question=  91 Detect/count persons in picture of a vehicle, what method?
-------------
Question=  92 Automatically changing regularization level during training?
-------------
Question=  93 [Backpropagation]Is there a general update rule for both hidden and output layers?
-------------
Question=  94 Sparkit-learn random forests?
selttext=  Is it possible to use sparkit-learn to build random forests on a spark cluster? Does anybody have an example of this?
comment no.= 0 -->  Sounds like this group tweaked mllib (unfortunately no syntax).
https://spark-summit.org/2014/wp-content/uploads/2014/07/Sequoia-Forest-Random-Forest-of-Humongous-Trees-Sung-Chung.pdf
-------------
Question=  95 Algorithm to implement?
-------------
Question=  96 Q-learning and neural networks - the weight update step
-------------
Question=  97 sklearn tree pruning question
-------------
Question=  98 Can someone explain thresholding an image for me?
selttext=  Im doing an image recognizion task and I think thresholding would work. I need to make a program that detects bright spots on an image and circle them(active neurons in a calcium probe image). Im pretty new to these things. Thanks.
comment no.= 0 -->  At its most basic, if the luminance value of a pixel is above the defined threshold, then set it to white, otherwise set it to black.  

E.g.

    threshold = 200;
    pixels = [100, 150, 201, 150, 100];
    threshed_pixels = [0, 0, 255, 0, 0];
comment no.= 1 -->  Typically in neuro, people want to keep activated pixels/voxels above a value or zero out some values. 

You might also use thresholding to make a mask from an image. If you had a brain or other region that is clearly defined, you can turn it into all 1's with 0 everywhere else. Then any other image that is in register (aligned anatomically) can be masked by multiplication.

If this is MRI or PET, you can use fsl tool's fslmath to do the thresholding. fslmaths -h has all the options.

-------------
Question=  99 Where does Numenta's HTM fit into the current machine learning landscape?
-------------
Question=  100 What is the difference between a (dynamic) Bayes network and a HMM?
-------------
Question=  101 Format question using scikit
selttext=  attempting to put [this](https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data) data into scikit & work with it.  I am having a bit of an issue seeing how the data correlates to the features specified.  Can anyone help me ?

[here](https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope) is a link about the data 
comment no.= 0 -->  I'm not sure if I understand the question. Do you have problems to see what column corresponds to which feature?

In this case the [pandas](http://pandas.pydata.org/index.html) library might be helpful:

    import pandas as pd
    features = ["fLength", "fWidth", "fSize", "fConc", "fConc1", "fAsym", "fM3Long", "fM3Trans", "fAlpha", "fDist", "class"]
    df = pd.read_csv("magic04.data", header=None, names=features)
    df.head()

This will output the first few rows/datapoints of your data set with the header names as defined in the features list.

I also recommend using [jupyter notebook](http://jupyter.org/) for just trying around with datasets. The pandas library outputs datapoints in a neat little table, which makes it a bit easier to see what you are dealing with.

If I misunderstood the question, please clarify :)
-------------
Question=  102 Extra constant inputs to multilayer perceptron cause diverging
-------------
Question=  103 X-post from /r/machinelearning: Question about tensorflow/cifar10
selttext=  Hi guys,
I'm currently learning tensorflow. I'm working on the Cifar-10 tutorial. I'm a bit confused and wanted to check whether I'm understanding correctly how I would prepare data myself to train the model. Here's what I think I should do, I'd appreciate any feedback:

First, convert the images to a numpy array of shape height x length x channels (3 for RGB image). I'm unsure what I'd do with the labels, actually...


Let's say I call the final file foobar Then, I could use read_cifar10(foobar) to read in the image in my code. In the code, the individual images are then arranged in batches the way tensorflow wants all to receive them.


Is this correct? If I want to read in more than 1 image, what will I write instead of foobar, let's say the files are called foobar1 and 2?
comment no.= 0 -->  For each batch, you'll want to run tf.train.YourOptimizerOfChoice(your-learning-rate).minimize(your-cost)

The cost function Google uses in their examples is cross entropy, in the form of tf.reduce_sum(your_correct_labels * tf.log(your_algorithm's_output))
-------------
Question=  104 Machine learning uses the same few equations over and over
selttext=  The more papers and ML books that I read, the more I see the same equations repeated over and over, only with slightly different names. 

I'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation ~~to keep out those who are afraid to wade through the math.~~  

Edit: Maybe I was little steamed when I wrote that. :)
comment no.= 0 -->  Cite a few examples?
comment no.= 1 -->  I would also like to see some examples.

I'm imagining that you're describing something like the fact that the linear regression equations appear over and over again - they do! The basic concept of a linear model is the intuitive jumping off-point for just about every supervised learning model. The fact that this is a pattern we can exploit is wonderful and for some people (myself included) very counterintuitive. We use these over and over again because they are well-studied models. 

I find it difficult to believe any claims that it's simply a matter of spiteful obfuscation on the part of authors, though. There are many alternative hypotheses that fit the data here, such as:

1) The same equations appear over and over again, but the variations between each usage are hard enough to systematize that we have yet to find an overarching system which totally describes all of them. 
 
2) Most papers are written by experts in the field, who have seen these equations and patterns before. 
When they write their papers, they purposefully reuse ideas and equations, both because it gives them a theoretical base to build on and a formalism that their audience will already be familiar with. Note that in this case, it's the opposite of your claim - the various authors actually go out of their way to present the material in a familiar way, knowing that it would aid understanding to do so.

Moreover, why on earth would someone spend months or years of their life researching something, only to publish a paper or a book which is intentionally obscure?
-------------
Question=  105 Using an RBM to learn the distribution of an other RBM
-------------
Question=  106 If GLM performs better than GBM or RF, what does that mean ?
selttext=  Hi,

I was asked this question in interview and I admit I'm not sure about my answer. What would be yours ?
> If GLM performs better than GBM or RF, what does that mean ?

I think the answer expected was something about the data, and maybe a "solution".

Cheers

Note :  
- GLM : Logistic regression  
- GBM : Gradient boosted trees  
- RF : Random forest (of trees)
comment no.= 0 -->  Google is your friend my man... https://www.quora.com/What-are-the-advantages-of-logistic-regression-over-decision-trees

But the short version is it means there is a clear linear separation in the logistic regression via slope, whereas the data is not conducive to a clear split for decision trees which (for ease of explanation) are parallel to each axis.
-------------
Question=  107 [UFLDL][TensorFlow] Can I consider my Sparse Autoencoder implementation a success?
-------------
Question=  108 Tensorflow seq2seq model unsatisfying results
-------------
Question=  109 Which is your favorite tool for machine learning and predictive modelling when working with R/Python?
-------------
Question=  110 Interpreting the results of LSTM-based Recurrent Networks
selttext=  Does any of you know, if any research has been done interpreting the results of LSTM-based Recurrent Networks and linking them back to the features that were the input to the model?
comment no.= 0 -->  Visualizing and Understanding Recurrent Networks, http://arxiv.org/abs/1506.02078
-------------
Question=  111 Best starting place for beginners?
selttext=  I'm 15 and starting to learn about machine learning, I am currently read Artificial intelligence a Modern Approach but after that what should I do?
comment no.= 0 -->  I'm by no means an expert, but learning Python, or another similar language, so that you can actually implement the stuff they talk about in the books is probably a safe bet.
comment no.= 1 -->  You're 15 and already understand stochastic calculus, linear algebra, and optimization theory? Impressive.
-------------
Question=  112 Aren't features more important than any particular algorithm or ML method?
selttext=  Hi all,

I just finished my first neural network (Ng's Coursera assignment, so as basic as possible, but still quite cool to me), and I was curious about how these can be scaled when it comes to object recognition, etc. This assignment is digit recognition on a 20x20 image, a mere 400-element input array. But real-world images are exponentially bigger than this.

So my question is, if individual pixels aren't used as inputs, how do features get extracted from images for use in neural networks? 

It seems to me that learning how to develop quality features is a better investment of my time while trying to learn ML than diving deep into the particular algorithms. Is there any validity to that? 

For example, I learned of Josh Tenenbaum's name from the ML: A Probabilistic Perspective, and on his front page there's an intro of how children learn to distinguish a horse from just a handful of examples (which seems so common sense to me, but in the perspective of machine learning is quite remarkable). But is the problem that there's no algorithm that can adequately generalize after such few training points, or is it that the features being fed to these algorithms don't encapsulate enough information to allow such few training points?

I really appreciate your time and any insight you can give me.

Cheers
comment no.= 0 -->  For image work and neural networks, they often use convolutional neural networks (CNNs) to find features that are combinations of individual pixels.   As the CNNs are trained to find "useful" features, you get things like "vertical edge" or "diagonal edge".  These then become the features used by subsequent neural network layers.

I'm not an expert - I've just read about this in one of my classes.
-------------
Question=  113 random forests (to 7000 oaks)
-------------
Question=  114 Localizing an object with neural networks (Conv-Nets)?
-------------
Question=  115 What are some good resources for someone looking to delve deeper into Neural Networks after having taken an introductory undergraduate course in ML.
selttext=  I am familiar with feed forward networks and how they work and am even taking a graduate course on Neural Nets this coming spring semester but would like to do some reading in the mean time.

The thing is, a vast majority of the posts/papers I read over on /r/MachineLearning about the topic are way over my head.


Does anyone have any suggestions? Thanks!
comment no.= 0 -->  If you're explicitly interested in Neural Networks then I'd recommend Geoffrey Hinton's (One of the pioneers in Deep learning) course on Coursera which goes through a lot of the important topics relating to NNs.
You'll find the course with the name "Neural networks" offered by University of Toronto -Sorry can't get the link on Mobile-
comment no.= 1 -->  https://see.stanford.edu/Course/CS229
-------------
Question=  116 Looking for free offline resource to learn Machine Learning and prerequisite knowledge
selttext=  Can anyone point me to any? I'm new to ML, I have a basic understanding of Stats or Probability, and I wanted to be able to download it for offline reading. I'd prefer something in the EPub or Mobi format, but PDF works as well. 
comment no.= 0 -->  Scikit learn and ipython notebooks
comment no.= 1 -->  I already bookmarked [this](https://redd.it/1jeawf) reddit post to read at my leisure.  Great place to start (I'm new to ML as well), but I would also recommend Coursera.org ML courses to any beginner. 
comment no.= 2 -->  Elements of Statistical Learning is a good book.
-------------
Question=  117 Coding own CNL in TF
-------------
Question=  118 RNNs as generative models
-------------
Question=  119 How do you keep track of the progress of a machine learning project ?
selttext=  I am talking about what features and algorithms you use, their performance etc.

comment no.= 0 -->  Best way is often to use R Markdown language (Or whatever software you use). You have the syntax and output but you can also write a paragraph opener and closer in each section that will describe what you did and why and the final results.
-------------
Question=  120 How to use neural network for my android app?
selttext=  I made a neural network using [this tutorial](http://neuralnetworksanddeeplearning.com/chap1.html). The problem is I don't know how to implement it in my app. My app will take a picture of a number and send that number to this network. I don't know how to get the output out of this network. 
comment no.= 0 -->  The picture of the number will be translated to a long line of numbers (a number per pixel, each in the range [0, 255] according to NMIST specifications). These numbers will be inputted in your NN (it should have as many inputs as pixels in the pictures). As the possible displayed numbers on the pictures of the NMIST data set are in the range [0, 9] your NN should have 10 outputs. After training, only one of those outputs should be above 50% (larger then 0.5), which is the number the NN thinks it is.

     1    ->                       <- 1
     2    ->                       <- 2
     3    ->                       <- 3
     ...                 NN         ...
     N-1 ->                       <- 9
     N    ->                       <- 10
comment no.= 1 -->  If you haven't looked at tensorflow you should, that is the easiest way to get a neural net on android other than coding a complete package yourself.
-------------
Question=  121 Do I need a PhD?
selttext=  Is a PhD required to do work in machine learning?
comment no.= 0 -->  No.

Is that a long enough answer? There are data scientists with bachelors, masters, and PhDs. Hilary Maso, one of the most noted Data Scientists only has a Bachelors. It's all contingent on your ability to learn and apply information.
comment no.= 1 -->  Absolutely not,
If you don't have an upper level degree indicating you learned the foundation and have a project to show for it (thesis, dissertation), etc. Your next best option is to create projects with definitive results to show your competence and understanding. 

I am part of a machine learning meetup type group. About every month or two there are industry hirees looking for people to bring into their businesses to solve a machine learning problem. They essentially ask who in the group had done a similar problem then dig into that project. If they feel the individual or individuals can tackle it they get hired for the project or the job. 
A good chunk of the financial guys are either still in undergrad, have graduates in a completely different field, or have nothing at all in paper degrees, but have a huge amount of high results in online ml financial challenges. 

Unless your looking for grants or something that essentially has a clause indicating the pi needs to have a PhD your good to go on your merits of previous results. 

I've seen this same thing happen in python groups. Someone will ask what you're working on or present an open job/project available and as long as people know what you're working you'll be steered towards each other.

I personally feel this is true in all of computer science. Simply because a degree isn't always indicative of being a problem solver of novel questions. (Sure a degree helps, but this industry knows that it's not the only way, unlike say practicing medicine on a human.) Hirees for hard questions simply want results not a person in a cubicle. 
-------------
Question=  122 VC dimension of decision trees.
-------------
Question=  123 What is the most efficient way to do visual salience detection?
-------------
Question=  124 Deep learning neural network to perform SED fitting and having some problems with bad outputs.
-------------
Question=  125 Trying to predict user behavior. Achieved some results, stuck on choosing a better model / input.
selttext=  Total newbie here. Using SKLearn.

So I have an web app with a RESTful server. This means I have data about the user approximately in this form, I'm using Reddit urls just to give you an idea:

    user 123 requested /r/machinelearning at 15:43:32
    user 123 requested /r/machinelearning/top at 15:44:15
    user 123 requested /r/MLQuestions at 15:45:56
    user 123 requested /r/MLQuestions/submit?selftext=true at 15:47:01
    ...
    etc.

Basically, I have the user's request log of the web app. What I'm trying to achieve is classify whether the user adopts the app or not. There's a 30 day trial period where you can use the app for free and at the end of this period you'll have to start paying for the app in order to continue using it.

On my first try I disregarded the URLs completely and just fed in the number of requests per a day. So I might feed a vector like this to the ML algorithm:

	[24, 10, 0, 15]

So 24 actions taken on the first day, 10 actions on the second day, etc. With 14 days, I achieved very close to 90% accuracy. This was with a balanced set, so the ROC AUC score was also very close to 0.9. I just tried a bunch of classifiers from sklearn and chose the best one. Random Forest and a simple linear Logistic Regression performed the best.

I'm now trying to do the same thing again, but only looking at the first hour of usage. I'm also trying not to disregard the different categories fo actions. However, this has made the input matrix really huge and very sparse, etc for user 123 you'll have something like this:

    [
        [1, 2, 2, 1, 0, 0, ..., 0],    # actions performed within the 1st minute
        [0, 0, 0, 0, 0, 2, ..., 0],    # actions performed within the 2nd minute
         ...
        [0, 0, 0, 0, 0, 0, ..., 0]     # actions performes within the 60th minute
    ]

I thought I might feed each of the minutes to an unsupervised clustering algorithm for dimensionality rediction, to end up with a vector like this:

    [1, 2, 0, 0, 0, 5, ..., 0]

Where the numbers, hopefully, represent types of user behavior the clustering algorithm found. I just tried this using K-Means and MeanShift. MeanShift did find lots of clusters, probably around 1500 different clusters. It's a huge number, so maybe I'm doing something wrong. I don't thing new users can do 1500 different general things within the first hour of usage...

I could also try to reduce the number of different requests by just taking the first category, etc /r/me_irl and /r/me_irl/top would both be just /r/me_irl just so that I could reduce the dimensions of 1 minute.

Okay so my current plan is to feed these clusters to a classifier. The thing is, the whole thing is a time series, so would another algorithm be more suitable for this? I've heard LSTM is good for time series. Am I heading in the right direction or am I doing something stupid?
comment no.= 0 -->  Just saw this paper "Session-based Recommendations with Recurrent Neural Networks": http://arxiv.org/pdf/1511.06939v2.pdf.
I haven't read it yet. Maybe it helps to solve your problem.
-------------
Question=  126 Hardware?
selttext=  I'm new to the ML game.  Starting on Kaggle competitions. Thinking about getting a new computer.  What hardware will make the biggest difference?  Not looking to break the bank, just some guidelines i.e. minimum sys requirements, amp up the speed,  bang for my buck kinda stuff, etc.  
comment no.= 0 -->  The biggest speedup will come from a video card that supports CUDA (i.e. nVidia). I recently swapped my R9 280X for a 970 to utilize Theano's GPU support, and the speedup is considerable. 
comment no.= 1 -->  All major open source ML libraries support CUDA hardware acceleration. So something that supports an NVIDIA card.   
Correct implementation took some of my compute times down from 2 hours to 5 minutes.  
-------------
Question=  127 Are decision trees basically just a data structure to build an acyclic Bayes net?
-------------
Question=  128 Reshaping in TensorFlow MNIST tutorial?
selttext=  Greetings, I'm trying to follow [TensorFlow's expert_mnist tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html), and I cannot understand the role of reshaping. Can someone briefly explain why and how is it done? (I'm also a little confused by the negative value in the first place, what does that mean?). The code in question is...

    x_image = tf.reshape(x, [-1,28,28,1])
comment no.= 0 -->  A matrix is a vector of vector of values. Reshaping turns it into a single large vector of values. Then turns it into a Matrix of vectors again of whatever dimensions you want it in.    
  
simplest application for this is vec-ing a matrix to calculate a Jacobian or a Hessian  
     
-------------
Question=  129 Choosing a Learning Algorithm for Real-valued Instance Clustering/Classification
-------------
Question=  130 Will be possible to develop a Learning maching to try to predict tennis matches with RapidMiner?
selttext=  Hi, I've got tons of data with a lot of info about tennis matches of the last 5 years.
I would like to create a Machine Learning and train it with the data from 2011-2014 and then test it with 2015 data.
Is this possible? The info in the datasets is a little confusing. Is this possible with RapidMiner?
Thanks.
comment no.= 0 -->  Yes and Yes. Split the data into training and test sets (splitting by years may affect your tests predictability). Run neural nets, random forests, etc. Profit.

You have to make sure your data is clean and structured well.
comment no.= 1 -->  You might look at "Analyzing Baseball Data with R" by Max Marchi.  Different sport, but the principles are the same.
comment no.= 2 -->  At the most naive level, you might just look at each player and determine a mean and standard deviation for the number of points they score.  Then to predict the outcome of a game, for each player, randomly sample from a normal distribution using the mean and sd you found for each player.  The one with the most points wins.  Maybe you do that an odd number of times, and choose the winner from who won the most simulated games.  This is essentially a monte-carlo simulation.

You could get more sophisticated by using more of your variables in your data to predict a number of points.  You could even try to get more clever if your data can help you model when they'll score in the game and their probabilities of scoring points given things like... "late in the game, and behind", etc.  The possibilities are endless.  

You might also try to find ways to classify players based on your data... good on clay vs grass, aggressive, consistent, good defender, etc.  These would then be factors to help refine your prediction of their performance.

But I'd go with starting simple and adding complexity as you go.  I'd probably look at some simple regressions to see how well your variables predict points or outcomes.
-------------
Question=  131 Advice request: Using ML for neuroscience research, do I need to abandon MatLab...
selttext=  Hi all!

I'm a behavioral neuroscience researcher in the field of electrophysiology, meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function. 

One of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something, without any firm idea of HOW they're converting that information. The information can be encoded in so many different ways... By the timing of a spikes relative to an external triggering event, by the timecourse of the rate of spiking, by a precise temporal pattern of spikes, by the timing of spikes relative to the phase of a particular neural oscillation... 

It is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way, by using the neural activity as inputs to a classifier (and the different types of events / stimuli preceding or following that activity as the target categories). The underlying implicit assumption is that if a ML classifier can identify the stimulus using that information alone, then there's a decent chance that the brain areas that receive that information are doing so as well.

I started off learning MatLab for data analysis reasons. I had no real programming experience (just writing scripts for data analysis software, or to control stimulus delivery systems), had no idea what convolution was, and had no experience working with matrices and vectors and whatnot. 

I've become reasonably competent in MatLab (I think), and found the various ML tools in MatLab to be helpful for data analysis. But I have run into a number of difficulties. My issues are:

Difficulty with circular data (phase angles) using feedforward nets and/or SVMs.. I have been just providing inputs as the sine and cosine of the angles, but I wish I could just use complex inputs.

Difficulty training classifiers when the differentiability of the classes is not known (e.g. there are ten classes, but it is quite possible that the inputs only contain enough information to discriminate the samples into 1/2/3/4/5, 6/7/8, and 9/10); classifiers often get hung up trying to minimize errors instead of maximizing information. I have tried to get around this by using evolutionary algorithms to train ANNs, using mutual information (or normalized variation of information) between network outputs and targets as the fitness function. 

Difficulty figuring out how to get MatLab's training algorithms to use MY division of data into training and validation sets. 

Uncertainty about how to determine the best type of classifier for my data.. This is more of a 'me problem' than a MatLab problem. 

Uncertainty about how to expand my analyses to a broader range of input types: I have so far usually just narrowed my data down to some number of phase angles and then used those as inputs, but I might wish to do analyses in which the input is a continuous signal alongside a discrete event signal, with the goal being to identify periods in the signal during which certain events are occurring.

Things I DON'T need to do:

Image recognition

Gigantic super-complicated models

So, that's my situation. I could use advice on whether it's worth it for me to abandon MatLab in favor of something more flexible. I could also use general advice about how to solve any of the issues I've described. Any help is appreciated! And if you want to know more, just ask.
comment no.= 0 -->  What do your fellow researchers use? Do you need to share code?
-------------
Question=  132 Ranking (with SVMs?)
-------------
Question=  133 What is the difference between convolution and correlation. Why do CNN's use convolution?
selttext=  
comment no.= 0 -->  Correlation indicates a relationship between two variables.

Convolution is a product operation that combines two distributions (or series) to create a new distribution.

In the context of a time series, the convolution operation combines the past information of two time series to create a new series.  

In the context of a CNN, at each time step the network contains the history of all past inputs and activations.  In this way it is a generalized version of the convolution operation.

*edit: I just realized that I confused CNN and RNN.  :( must have been on crazy pills.
-------------
Question=  134 Guide to implementing RNN
selttext=  Hi! I want to build a simple RNN with matlab to learn reading text. I have tried to understand how to implement it in code, but I would love to see if there are good guides out there that gives exercises in implementing the code! Any tips would be great :)
comment no.= 0 -->  Sorry if I'm misinterpreting what you are saying, but, if you aren't implementing it to learn about NNs but rather to do a specific task, I'd really suggest not implementing your own NNs.  

Not matlab, but the deep learning for NLP class will help if you want to implement stuff.
comment no.= 1 -->  This is a very good post on RNN's helped me understand them alot better and has some good code samples, though not in matlab.
http://karpathy.github.io/2015/05/21/rnn-effectiveness/
-------------
Question=  135 Want to focus on the financial application of machine learning, looking for good study material
-------------
Question=  136 I want to make a program that will help me hunt for apartments. Is ML the right tool and where should I start?
selttext=  I have this project to write some kind of app (I'm more proficient in Ruby and JS) that would eventually be able to sift through apartments for rent ads and send me the relevant ones.

The way I see it is that I'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own.

Is this something that can be achieved with ML? Are there any frameworks that are better suited to this? I am a developer but I don't know much about statistics or maths and I've never worked on anything related to ML. Any pointers?

Thanks!
comment no.= 0 -->  it just sounds like a job for regular expressions. Anything more is overkill. 
comment no.= 1 -->  Machine learning can help you figure out how good of a deal a particular listing is, and if you want to get even more advanced then go ahead and try to train an algorithm that could even figure out if a given posting is too good to be true and is probably a scam. 
comment no.= 2 -->  I am planning to do the same thing for job hunts.. Let's do it together?

I have ML and NLP knowledge, wanna work together?
-------------
Question=  137 Deep Learning SIMPLIFIED: Episode 5 - An Old Problem
-------------
Question=  138 Trouble of using VGG for Super-Resolution
selttext=  I'm trying to implement the net in [Accurate Image Super-Resolution Using Very Deep Convolutional Networks](http://arxiv.org/abs/1511.04587). It's inspired by VGG: the only difference is that there are no pooling layers, so all intermediate weight layers have 64 channels.

I followed the details of the paper

1. Gradient clipping
2. Learning rate = 0.1, decreases by 10 every `K` epochs
3. Glorot-like weight initialization: `stddev=(2/(3*3*64))^0.5`, where filter width = 3, number of input channels = 64
4. Momentum = 0.9, L2 regularization = 1e-4

In Tensorboard, I'm keeping track of the weights/biases histograms, and it seems after a certain time, the weights stop changing. This may mean the gradient vanished, or we've settled on a bad local minimum. However, I am getting performance worse off than bicubic interpolation. In the paper, they get better performance even in the 1st epoch. I was wondering why is this happening?
comment no.= 0 -->  Initialize the weight matrix of the last convolution, the one that results in the residuals, with zeros. It also helps to use YCbCr instead of RGB.
-------------
Question=  139 Deep Learning SIMPLIFIED: Episode 4 - How to Choose
-------------
Question=  140 Pandas/Scikit-learn guy needs to do ML in C#. Can anyone recommend comparable libraries for matrix manipulation and model creation?
-------------
Question=  141 Deep Learning SIMPLIFIED: Episode 3 - Why Deep?
-------------
Question=  142 Deep Learning SIMPLIFIED: Episode 2 - What is a neural network?
-------------
Question=  143 Calculating the recall and precision values of multilayer neural network with TensorFlow
-------------
Question=  144 Looking for EM derivations exercises with solutions
-------------
Question=  145 Deep Learning SIMPLIFIED: YouTube Series
selttext=  Hi everyone! I am new to this sub-reddit and wanted to introduce myself. I have been working on a YouTube series for Deep Learning that you may like. If you are ever need to explain Deep Learning to a newbie (or are new to Deep learning yourselves), you may like this series. Content you'll typically find online on the topic is highly mathematical/technical, which is great! But if you're like me, you probably want to just understand the models and the intuition. Thats what this series is about! Here is the link to the series intro. Please take a look and let me know what you think!
https://www.youtube.com/watch?v=b99UVkWzYTQ
comment no.= 0 -->  This is the series intro - 6 total videos so far and many more to come. Enjoy :-)!
comment no.= 1 -->  Very nice! I watched the intro video. The production quality is great and I like the narrator's voice. I look forward to watching the rest. Thanks for doing this!
comment no.= 2 -->  I found that really useful! Thanks very much. 
-------------
Question=  146 Correct cost function to use with a softmax output layer with a continuous target distribution?
selttext=  Lets say I am training a neural network to play Rock, Paper, Scissors. The network outputs a probability distribution over the three actions. I am using a softmax as the final layer of the network to ensure that P_rock + P_paper + P_scissors = 1.0.

I understand (I think..) that if my networks target output was discrete, ie (1,0,0), (0,1,0) or (0,0,1),  I should use Cross-Entropy as my cost function. My question is what cost function should I use to train the network if my required target output is continuous, for example (1./3, 1./3, 1./3)?

I have tried mean squared error, but it does not converge. I don't know if I have a bug, or I am using the wrong cost function. Is mean square error or cross-entropy appropriate in this case? Would anyone be kind enough to point me in the right direction? Thanks in advance...

comment no.= 0 -->  Output of your network is still discrete, it is classification problem, but you will never get full one and zeros, because softmax is giving you probabilities that it should be that output. If you want result you need to sample from that distribution. So you should use cross-entropy.
-------------
Question=  147 Weird error message when tuning svm with polynomial kernel: "WARNING: reaching max number of iterations"
selttext=  It is my first time working with support vector machines. I am trying to solve this homework, but am receiving the above mentioned error... here is my code:

    library(e1071)
    test_data = #upload test data here.
    training_data= read.table('Digits_training.csv', sep =',', header = TRUE)
    y = training_data$y

    chosen_svm = function(y,training_data,kernel_name){
      obj <- tune.svm(y~., data = training_data, gamma = 10^(-3:1), cost = 10^(-3:1), kernel = kernel_name)
      gamma = obj$best.parameters$gamma
      cost = obj$best.parameters$cost
      model = svm(y~., data = training_data, gamma = gamma, cost = cost, kernel = kernel_name)
      return(model)
    }

    radial_svm = chosen_svm(y,training_data,'radial')
    lin_svm = chosen_svm(y,training_data,'linear')
    pol_svm = chosen_svm(y,training_data,'polynomial')

Any idea why this is happening?
comment no.= 0 -->  I am a bit busy to check but I would guess that SVM requires convergence and it was failing to converge.
-------------
Question=  148 What effect does loss function have on training?
-------------
Question=  149 CNN for 3D data sets
-------------
Question=  150 CNN-RNN architecture - Techniques to deal with large 5-D input data?
-------------
Question=  151 Given a list of centroids, how to find optimal set of length k?
-------------
Question=  152 Need help with understanding how to compute the weight gradient in a convolutional layer.
-------------
Question=  153 [Help] Supervised Classification Implementation
-------------
Question=  154 The typical 25 horse problem with a twist
selttext=  The typical interview question:

You have 25 horses and want to identify the 3 fastest horses. You have a track that can hold 5 horses at a time. What is the minimum number of races it would take for you to identify the 3 fastest horses.

Twist:
Same problem, except now you want to identify the 6 fastest horses.


Edit: I forgot to mention, the times are not recorded so you cannot just find the speed of each horse individually and compare.

EX: For the top 3 horses, the answer is 7 races

To display this, I will show label the horses with a letter and a number. The letter refers to their first race set and the number refers to their order for the first race.

Race 1|Race 2|Race 3|Race 4|Race 5
:--|:--|:--|:--|:--
a1|b1|c1|d1|e1
a2|b2|c2|d2|e2
a3|b3|c3|de|e3
a4|b4|c4|d4|e4
a5|b5|c5|d5|e5

We can then find the fastest horse by comparing the best in each of the first 5:

Race 6|NA
:--|:--
a1|na
b1|na
c1|na
d1|na
e1|na

Lets say that horse a1, b1, and c1 are the fastest, showing up in their respective orders (i.e. a1 is the fastest). Then to identify the 2nd and 3rd fastest, we race the set of candidates:

Race 7|NA
:--|:--
a2|na
a3|na
b1|na
b2|na
c1|na

This will give us the 3 fastest horses.
comment no.= 0 -->  5 for both assuming the horses are in random order and you do not have any information on them. Since 5 horses x 5 horses per track is 25 and you want to cover each horse exactly once because you do not have any prior information on them. Then you sort the results by speed. Any additional races can bias the results because horses used multiple times can be fatigued.
-------------
Question=  155 Machine learning on paths ?
-------------
Question=  156 Help with a Deep Convolution Network
-------------
Question=  157 Designing a Random Forest from scratch, with no ML libraries allowed.
selttext=  Hi all, not a programmer but I'm taking a machine learning module I've been asked to design a RF from scratch. I am lost.

I have basic skills in python - very basic. 

Can anyone help?

So far I've scraped a decision tree from the internet but I'm a bit lost on it as its telling me my features in the function argument aren't defined. 

My attempt at the code below.

    import numpy
    import csv
    with open('banks.csv', 'rt', encoding='ascii') as csvfile:        #      type name of datafile with extension in first set of apostrophes.
    data = csv.reader(csvfile, delimiter=' ', quotechar='|')
    # for row in data:
       # print (', '.join(row)) # prints out all data in file

    def newDT(data, features, targetClass, fitness_func): # define a new decision tree

    data=data[:]
    values=[record[targetClass] for record in data]
    empty=majorValue(data, targetClass)
    features=(data[:0])

    if not data or (len(features)-1)<=0:
        return empty
    elif values.count(values[0])==len(values):
        return values(0)
    else:
        best=choose_features(data, features, targetClass, fitness_func)
        tree={best:{}}

        for value in get_values(data, best):
            subtree=newDT(get_examples(data, best, value), [attr for attr in features if attr != best], targetClass, fitness_func)
            tree=[best][value]=subtree

    return tree

    newDT(data, features, targetClass, fitness_func)
comment no.= 0 -->  I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/machinelearning] [CrossPost from MLQuestions: Random Forest from scratch. Any help is appreciated.](https://np.reddit.com/r/MachineLearning/comments/3v2hts/crosspost_from_mlquestions_random_forest_from/)

[](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*

[](#bot)
comment no.= 1 -->  Hi! Once you implement a decision tree you are almost there, just build n of them, predict your validation set through all of them and for each input vector take the mean (regression) or the mode (classification). If you can share your banks.csv file I will take a look at it tomorrow. Plus a link to where you found the script.
-------------
Question=  158 How do I describe a Perceptron, comparing two inputs x/y triggering when x>y.
selttext=  I tried to setup a perceptron witch outputs an 1 when output x is bigger than y. How can I realise this? Like wtf?
comment no.= 0 -->  If I recall perceptron well, this Python code should describe such perceptron:

    def gtp(x, y): # inputs are x and y
        s = 0.0 + 1.0 * x + (-1.0) * y # bias is 0, weights are 1 and -1
        if s > 0.0: # threshold is 0
            return 1.0
        else:
            return 0.0

Training it is another matter, here the weights and bias are fixed.
comment no.= 1 -->  Y is an input? Are there only 2 inputs? Are you going to use back propagation for training?
-------------
Question=  159 Question regarding inversion of softmax function
-------------
Question=  160 Does changing contrast, brightness,etc in data set increase the accuracy of the trained net?
selttext=  I have a data set of around 15000 images, if I modify these images by changing their properties such as contrast, brightness, rotation ect to create a larger data set will the accuracy of my net be greater? 
Very new to this!
comment no.= 0 -->  Depending on what your images are, it can. I remember someone who won a Kaggle competition on identifying galaxies(?) did a write-up on his processes, and part of it was stretching the images in different ways to create a more generalized dataset

edit: http://benanne.github.io/2014/04/05/galaxy-zoo.html
-------------
Question=  161 Simple projects to begin with?
selttext=  Hello! I'm in a research program at my school and choose to learn about ML. Currently I'm learning Algebra II/Trig, so my math education is not nearly enough to easily understand ML. 

I've been working at a decent pace and I currently understand gradient descent thanks to Coursera and Andrew Ng, however suddenly my teacher requested 10 pages of original research done in a very short amount of time (Monday, 11/27/15). Are there any basic data sets and techniques I can use to at least get credit for my research thus far? ML seems to be a very deep topic you learn about over time, and I currently don't have time. 

I was thinking about trying to use some modified gradient descent algorithms on data sets, but my issues currently are: I don't know what kind of data I need/where to get it, I don't know how to make a gradient descent algorithm work for more than 2 parameters (linear regression).

Any advice would be appreciated. I'm sorry for asking this question, I know it's very "how do I learn ML quick" style, but really I just need a simple project to pass and continue learning at a normal rate.

comment no.= 0 -->  Andrew Ng's intro to ML is really good and I'm not sure how you would go about learning ML much faster than that. I think I would recommend binging a bit on his lectures so that you get a bit beyond linear regression with 2 parameters. If I recall correctly, there are assignments in this course, right? Maybe you can use that in your report (maybe extend the exercises a bit). 

I don't really know anything interesting to do with 2 parameter linear regression. You can fit a line through a bunch of data points. If you learn to use more parameters, you can make things slightly more interesting by comparing the lines you get with various numbers of parameters. 

If you learn about logistic regression, you can start doing classification/detection. You could make "networks" that simulate AND and OR logic gates. For XOR you need to use a nonlinear technique since it's not linearly separable. For instance, a neural network with a hidden layer (i.e. a multilayer perceptron). 

One fairly small but interesting problem is classifying the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits. However, this is quite a bit beyond linear regression with 2 parameters. 

You could also take a look at some of the challenges on [HackerRack](https://www.hackerrank.com/domains/ai/machine-learning).

Good luck!

BTW, I hope you meant Monday the 30th and not Friday the 27th (today/yesterday depending on your time zone). (Monday the 27th doesn't exist.)
-------------
Question=  162 Can two Machine-Learning computers be identital?
selttext=  Hi,

I have a question that's been eating me for some time and don't know where else to post it. So here it is:

Assuming that two computers (M1 & M2) are fed the exact-same training data, would they behave identically, to the point where we can accurately assess/predict the behaviour of M2 based on M1.

I guess I'm trying to find out whether it is impossible, due to inherent differences in the hardware (not the exact same chip, even if from same manufacturer) akin to a genetical difference. It's a bit like the rhetorical "If two humans grew up in the *exact* same environmnent, would they behave in the same way", though I would assume it's technically impossible to do such a test.

If anyone could shed some answer I would appreciate it, thank you :)

Peace
comment no.= 0 -->  I'm not sure if I understood the question correctly.

First of all this depends on the algorithm you use and if it has a random component. For example Random Forests are very unlikely to produce the same results, even if trained with the same data.

A nearest neighbor algorithm on the other hand would give you the same results if it is trained with the same training data, since it is deterministic, there simply is no random part. Edit: This means a simple k-nn algorithm, not some fancy approximation :)

In the latter case there **might** be a difference based on floating point operations and rounding errors as /u/Nixonite was asking, but I think this is rather unlikely if you run exactly the same setup/architecture and software. The amount of errors that may occur are probably not that significant to change the classification outcome. But this is just a rather unfounded guess of mine.
comment no.= 1 -->  You mean would their roundoff error be the same for the same algorithms?
comment no.= 2 -->  The hardware differences will not affect your algorithm in any way (other than how quickly it trains of course).
comment no.= 3 -->  I think it depends if the algorithm used is deterministic or not. If it uses randomness in some way, then there's no way to guarantee that the results will be exactly same.
-------------
Question=  163 How to create a custom objective/cost function?
-------------
Question=  164 [Optimization] Is multi-days Travelling Salesman Problem the same as mTSP?
-------------
Question=  165 TF as individual chatbot
-------------
Question=  166 Question about types of Neural nets
selttext=  Hey all! 

So I have one main question about the different types of Neural Nets that exist. It seems like a lot of different types of NNs fall under the umbrella term of Neural Net and I was wondering what kinds there are.

I'm familiar with a CNN, an RNN and so forth. Do those employ backpropogation? The only one I've had experience with has been the one as done in the Andrew Ng coursera lectures (feedforward with backprop- not sure if this is its own type of neural net or if this is just a general term)

I'm also aware that many different types of cost functions exist? Does this actually change the type of net or is this really just a different way to characterize "cost"?
comment no.= 0 -->  Those are the 3 main types of nets that you will usually see. Also, there are variations of these (LSTM, GRU, etc are types of RNNs). 

Back propagation is the optimization method pretty much always used. You could really use any method you wanted (genetic algorithms, particle swarm optimization, etc) but back prop is much faster pretty much always. 

The cost function you choose is more of a hyper-parameter, like learning rate and batch size. It will depend on what your data looks like, and mostly depends on whether you're doing regression or classification on which cost function you choose. 
comment no.= 1 -->  There are certainly many types of neural networks. Check out the Wikipedia page on [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network). The complement of RNNs are [feedforward NNs](https://en.wikipedia.org/wiki/Feedforward_neural_network). This is one of the more clear-cut distinctions since FFNNs are directed acyclic graphs and RNNs contain at least one cycle. However, I would say that a recurrent MLP is more similar to a feedforward multilayer perceptron than it is to a Hopfield network (which is also recurrent). Basically you can come up with all kinds of variations by adding, omitting, sharing and fixing connections or changing some activation functions. It's not always clear when a change constitutes a new "type" of network. 

What most of these networks have in common is the the node's activation is given by an activation function applied to the weighted sum of other nodes' activations, although some nodes in LSTMs compute the product. This is different in [spiking neural networks](https://en.wikipedia.org/wiki/Spiking_neural_network), but they are rarely used since they are hard to train. I would say [Boltzmann machines](https://en.wikipedia.org/wiki/Boltzmann_machine) are also quite different. 

Most of the time some form of backpropagation is used. I think backpropagation technically refers to a fairly specific algorithm, and people often use variants like RPROP or Quickprop instead, but it's still all about propagating an error back throughout the network. Things like momentum and dropout and other training/regularization tricks can be added. It's also possible to use other optimization techniques (like genetic algorithms). Hebbian learning (increase weight between nodes that are simultaneously active) is not used much anymore I think. (Restricted) Boltzmann machines have their own training procedure that uses Gibbs sampling and gradient descent. I would say that the used cost function is probably more part of the training procedure than it is of the network.

Finally, I should mention that there are also computational neuroscience projects that basically aim to actually simulate parts of the brain (like IBM's Blue Brain project). These are technically also neural networks, but they are also pretty different.

-------------
Question=  167 TF for DSP-Programmers
-------------
Question=  168 Sklearn PCA not making sense to me.
selttext=  I'm under the impression that if you have a 2D data set and you perform PCA on it, without any dimensionality reduction it will essentially rotate your data to a new coordinate system. Why then when I attempt this in sklearn does the data seem like its being transformed in some way?

Here is my code:

    import numpy as np
    from sklearn.decomposition import PCA
    import matplotlib.pyplot as plt

    x = np.linspace(0,10,101)
    y = x + 2*np.random.randn(1, 101)
    y = y[0]
    data = np.asarray(zip(x,y))
    pca = PCA(n_components=2)
    new_data = pca.fit_transform(data)
    plt.figure(0)
    plt.scatter(data[:,0], data[:, 1])
    plt.figure(1)
    plt.scatter(new_data[:,0], new_data[:, 1])
    plt.show()
comment no.= 0 -->  I think it may just be appearing that way because of the different scales. When I look at it the corresponding points seem to be in the correct spot.

Add colors = range(101), and pass in c=colors to scatter() and you'll see it more easily.
-------------
Question=  169 Whats the difference between manifold learning and decomposition in the context of dimensionality reduction?
-------------
Question=  170 Teaching a computerprogramm to "run" another
-------------
Question=  171 Reinforcement Learning help!
selttext=  Im learning reinforcement learning. Can someone provide me with some example problems which I can try out and improve my understanding. Thank You! 
comment no.= 0 -->  [RLPy](http://mloss.org/software/view/514/) is an open source framework for performing sequential decision making experiments in Python.  It has a bunch of [example domains](https://github.com/rlpy/rlpy/tree/master/examples). 
comment no.= 1 -->  In [the book by Sutton & Barto](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html) there are sometimes scenarios used in the chapters, as well as some case studies in the end, which you can use to create some own problems.

Did you mean something like this?
-------------
Question=  172 ML newbie: How to approach this problem, given a screenshot, find areas of interest or interaction. Suggestions?
-------------
Question=  173 Unsupervised Learning with Theano/CGT/TensorFlow
-------------
Question=  174 Regularized Linear Regression
-------------
Question=  175 Help me with my thesis: Need Facebook friends and Reddit IDs!
-------------
Question=  176 Are nonlinearities needed if you add bias at each layer?
selttext=  From what I understand the stacked weight matrices are no longer reducible to one matrix if you have bias at each layer... Yes or no, what am I missing here?


Btw, does the same apply if you have normalization at each layer during training and forward pass?
comment no.= 0 -->  Let's say you have a MLP with two inputs (x1 and x2) and two neurons:

    Neuron 1: w11*x1 + w12*x2 + b1
    Neuron 2: w21*x1 + w22*x2 + b2
    Output: W1*neuron1 + W2*neuron2 + B = W1*(w11*x1 + w12*x2 + b1) + W2*(w21*x1 + w22*x2 + b2) + B = A*x1 + B*x2 + C 

(The formulas for A, B and C are left for the reader, but they do not depend on the inputs x1 and x2).

Clearly the output is linear with regard to the inputs, the only thing we've accomplished is a convoluted way to determine the linear coefficients A, B and C, which would make the *training* nonlinear.

So the answer is no, biases do not add any nonlinearity to the network, activation functions are definitely required. 

What do you mean by normalization at each layer? Batch normalization?
-------------
Question=  177 [Career Question?] Help with preparing for an interview?
-------------
Question=  178 Text input BRNN example
-------------
Question=  179 Need help in audio signal processing
-------------
Question=  180 After 30 hours, I still can't figure out how to properly implement a Backpropagation Algorithm. Help please.
selttext=  I spent nearly 30 hours thinking, reading, and trying to implement this. Everything is correct. I checked them a hundred times, so there is no problem in the logic as far as I understand it. Below are the issues I'm facing.

1- So I want to approximate a function (x^2, sinx, ...) using a neural network, but how do I make this work? I have one input neuron, variable hidden neurons, and one output. How would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1?

2- My network can't even figure out the xor problem with 2 hidden neurons, two inputs, and one output. But the forward pass works if I manually set the weights and use a threshold activation function for both hidden and output layers.

3- It always produces the same output for any input.
comment no.= 0 -->  Code for weight correction in the hidden layer (modified for clarity), where I suspect it's wrong:

    float oGradient=(target[i] - y[i])*y[i] * (1 - y[i]);
    float h1Gradient= (phi(hidden1_sum)) * (1 - phi(hidden1_sum)) * hidden1_out_w1;
    float h2Gradient=(phi(hidden2_sum)) * (1 - phi(hidden2_sum)) * hidden2_out_w2;

    input1_hidden1_w+= eta*h1Gradient * oGradient * input_signal1;
    input1_hidden2_w+= eta*h2Gradient * oGradient * input_signal1;
    input2_hidden1_w+= eta*h1Gradient * oGradient * input_signal2;
    input2_hidden2_w+= eta*h2Gradient * oGradient *  input_signal2;

    hidden1_bias+= eta*h1Gradient * oGradient;
    hidden2_bias+= eta*h2Gradient * oGradient;

Where phi is the sigmoid function, and y[i] is the current output of the output neuron.
comment no.= 1 -->  PLEASE, can someone at least  give me the correct weights and biases for a 2-2-1 network that uses sigmoid(1/(1+e^(-x)) for all neurons? It's driving me insane. I just want to know where the error is.
comment no.= 2 -->  1 - you just give up sigmoid for the output layer. Keep sigmoid for hidden units only.

2 - Does it learn OR and AND functions without problems?
comment no.= 3 -->  On Mobile so please forgive smelling errors.  I'll try to give a more thorough answer when I get to my laptop.  In the meantime:

1:  You can either try linear output nodes instead of sigmoid, or just scale your answers by some constant that allows signal outputs to generate answers in the range you want.  Stick to approximately 0.3 to 0.8, not 0 ... 1. 

2: A single hidden layer ffnn with two hidden nodes can solve XOR, but backpropagation isn't guaranteed to find that answer.  TRY more nodes and different initialization.  Do not initialize weights to 0.


-------------
Question=  181 [Work question] What does the workday of someone working in Machine Learning look like?
-------------
Question=  182 In Machine Learning is it necessary to take software engineering?
selttext=  There's a Statistical and Machine Learning Major which I'm going into but I would like to pair it with either a Language Technology Minor or a Software Engineering minor. Right now I'm not sure which would come in more useful in the field.
comment no.= 0 -->  When you say "the field", what field do you mean? Software Engineering (or at least programming) is going to be relevant for all machine learning, no matter what direction you go. In fact, it is so relevant that I expect your major will already include quite a bit of it. I don't know too much about Language Technology minors, but it sounds like it might be useful if you want to do Natural Language Processing. If you want to do NLP, or want to learn something more "unique", then you should probably that Language Technology minor (you'll have to learn to program anyhow). If you just want to do ML in general, then software engineering is the most obvious choice.
-------------
Question=  183 Can I use machine learning to recognize professional photography (vs amateur photography)?
selttext=  I'm a complete beginner and have no experience with machine learning, I was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably.

An example of photos that should rate very high as professional:
https://www.flickr.com/photos/steamster/sets/72157656521743602

what kind of a dataset would I need to achieve this? and how much machine learning do I need to learn to implement it? (I currently know a decent amount of basic programming in python and java)

comment no.= 0 -->  So actually, you're going to need to use a neural network because you're dealing with complex non-linear hypothesis. The theory is similar, but the the implementation is different. I can't help there yet. 
comment no.= 1 -->  I'd say this is a pretty big problem. Should be possible but might need some month or even years of research.
comment no.= 2 -->  What you have to first understand is modelability vs model choice.

I work in finance using machine learning techniques. If the problem can be modelled, there is a few techniques that will work. I recommend the book computer vision with python to get you up to speed with lots of image problems in python.
http://www.amazon.com/Programming-Computer-Vision-Python-algorithms/dp/1449316549

Where I suspect the issue may come is modelability. Of course we can contrast some amazing shot with something I took on my 10 year old mobile and find some different features.

But, in lots and lots of cases the line between 'professional' and amateur will be very blurred. I have guys on my facebook who take pictures better than many family professional portrait photographers do, students who take pictures that could be on national geographic.

I don't want to dissuade you, but recognise that the important step is not really training a neural network - it's finding a reasonable hypothesis of what features select between amateur and professional.

This is why we don't take TBs of data, feed into some black box model and out pops predictions for stock prices, horse race winners etc.

I generally believe the best approach is 'I have data that separates based on XYZ features, how can I model this?' rather than 'How can I use this model in my problem'. I suggest you go through some resources and example problems on image classification like I referenced, this will enable you to get some experience investigating problems.

 
comment no.= 3 -->  Yes you could. There's something called a Classification Algorithm, or Logistic regression. Fancy words that just mean "is it probably this or is it probably that"? The more training examples you provide, the more accurate your results will get usually, and for this example you would benefit from having a bunch of amateur photos as well. Since there will be a large number of features, you won't really be aware of what the algorithm knows, but you would be able to tell if one matches your training set, or doesn't. Maybe someone else can provide more details, because I don't know exactly how to work with photos yet. I just know how to work with data, but with these photos the algorithms will simply look for patterns in the pixels. 

For increased comprehension, read slowly:

basically, what you do is provide a set of training examples with several features that the algorithm can focus on, then you provide it a definite answer. So you say, this is professional. This is not. The algorithm then identifies something called a hypothesis function that tries to match as many of the features observed in the training examples as possible, based on a set of numbers called parameters or weights, that when plugged into the hypothesis, can predict if the data you're feeding it falls in the matching side or the not matching side. 

It involves something called a cost function which figures out how different your hypothesis is from the actual data. 
This cost function is set to be minimized, or to find the partial derivative of the curve with respect to the parameters (basically, when the hypothesis differs from the actual data the LEAST), and this is done through another function called gradient descent. 

Gradient descent is a stepwise calculation that takes arbitrary initial values for your weights, calculates the tangent of the cost function, and repeats until it reaches a minimum, meaning the difference between the hypothesis and the actual training examples is minimized. 

Once you have the right parameters or weights, you can then feed the hypothesis new information, and using those parameters it figured out through gradient descent, it runs a comparison saying, yes this image likely matches the professional set, or no this image doesn't match the professional set. 

Like I said though someone else may be able to help a little bit with how to exactly do this with images. But all in all, it's just programming a few very simple functions. Nothing too algorithmically complex. 

EDIT: Grammar fixes and clarifications
-------------
Question=  184 Is there something driving the nodes of an RBM to learn different features?
-------------
Question=  185 Basketball (or any sport) predictions
-------------
Question=  186 ML n00b here. Starting to learn ML on my own. Classification technique guidance required.
-------------
Question=  187 Class of Concepts
-------------
Question=  188 General question about data sets with large number of boolean columns.
-------------
Question=  189 Checking to see if an annotation tool exists before building one -- labeling images?
-------------
Question=  190 Just started with Introduction to Statistical Learning
selttext=  and i have some basic questions.

equation 2.1 states:

    Y = f(X) + e

quoting:

"Here f is some fixed but unknown function of X1, ..., Xp, and e is a random error term, which is independent of X and has mean zero.  In this formulation, f represents the systematic information that X provides about Y."

questions:

- what do they mean here by systematic information?  
- if e has mean zero, then why even include it in the equation?
- are there any scenarios where e is non-zero?  if there are, what are some examples?
- is e a vector?  a set?  a scalar?

thank you.
comment no.= 0 -->  Let me see if I can help:

"systematic information" probably just means "information". The idea is that knowing X gives you some information of the value of "y". The  function "f" describes what different values of "X" tell us about "y" so it could be said to represent that information. "systematic" might be just be a reference to the fact "f" is deterministic.

e has the same form as Y and the output of f(X), so if we are trying to predict a scalar with f(X) it would be scalar, but if we are trying to predict a vector with f(X) it will be a vector of the same size. However note e is not a fixed, deterministic value. Instead it is a random variable (or vector of random variables).

It's actually quite important to include 'e' even if it is zero mean. We expect it to be non-zero is almost all cases where we are trying to model something occurring in the real word. 

To explain further, typically we would be using this sort of equation to model a real world process. For example, maybe we say Y = 0.18*X where "Y" is the amount a customer tipped and "X" is the amount they spent. However inevitably this equation will be wrong much of the time (many customer tip more, many tip less). Even if we tried to make the equation more complex (we could add terms to account for the time of day, where the customer was a repeat customer, ect.) the equation would still be wrong most of the time. In fact in general most real world processes are too complex to exactly model with a single deterministic equation "f", so we commonly model processes by finding an equation that is almost correct, and account for the fact there are other random or unaccounted for factors that affect what values of "y" we get for a given "X" in random, hard to predict ways by including the "e" term.

comment no.= 1 -->  if e has mean zero it means that the error can go on both sides of a regression line more or less with equal probability. i.e. positive and negative error (positive error as in your f + positive value e, negative error as in your f + negative value e) will occur normally.

if e is non-zero? Sure, if you tend to have a model which for example always over-estimates the regression output, then the error will be leaning towards negative values. e.g. if you predict on average a value of 110 for every true value of 100 then your model is estimating higher values on average and so the error would be 100-110 = -10. 

e can be called a vector sure if it's the list of errors for a list of predictions, but if you're talking about one prediction, then there is only 1 error value for it, this is of course assuming only a single variable. 
If you have a multivariate model then if you predict (10,100), the error will be (eX, eY) i.e. a vector of the same size with individual errors inside corresponding to the errors in the prediction for each axis. 
comment no.= 2 -->  also, is there a subreddit dedicated to the discussion of this book?
-------------
Question=  191 Good Data Preprocessing Python Module?
-------------
Question=  192 Resources to learn wavelet based dimension reduction.
-------------
Question=  193 How to whiten ReLU input units?
-------------
Question=  194 How to construct logistic regression model with scikit-learn's coefs and intercepts?
-------------
Question=  195 sklearn PCA with Pandas DataFrames
selttext=  When passing along a pandas DataFrame to a PCA() object,  how can I tell which of the input vectors are being chosen when using n_components='mle'?

    pca = sklearn.decomposition.PCA(n_components='mle')
    pca.fit(df)
comment no.= 0 -->  PCA doesn't choose input vectors. It (lineary) combines them all.
-------------
Question=  196 [Beginner]How to use K means clustiering on data with missing values?
-------------
Question=  197 Anyone care to take a stab at this non-descript classification task? Trying to validate my results.
selttext=  
comment no.= 0 -->  I get 80,4% on a 4 x 20 x 1 ReLU network
comment no.= 1 -->  I'm trying to validate the results I'm getting. I replicated (or so I think) an algorithm from a paper that achieved ~78% accuracy using a deep neural network ([4, 3, 2] hidden layer), but I cannot seem to get more than 63%. SVM gives me 55%, a generic MLP model from deeplearning.net gave me 48%.

The dataset has 4 inputs with domain [0,1]. The single output is a binary 0 or 1. There are 1000 samples in the dataset. I recommend you come up with your own train/test split to validate your model. I used 5x2 cross-validation, with 33% of the training data set aside for validation.

Feel free to use any model you wish, but please explain how you got it :o

Added difficulty: don't transform the inputs (i.e. use them as is).
-------------
Question=  198 [Beginner] What is a good ML textbook that contains pseudo code of algorithms?
selttext=  I have been looking at books. I like Tom Mitchell's book on machine learning as it has some pseudo code for C4.5. Does anyone else know of other similar books that have pseudo code? If not, does anybody know where decent documented source code is for a ML library? I took a look at WEKA and it wasn't the easiest code to follow. 
comment no.= 0 -->  [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/)

When I help companies interview data scientists, I advise that candidates should be able to open this book up to any page and explain the concepts.
-------------
Question=  199 [Beginner] Can I use Machine Learning to Predict the Likelihood of an Order Being Shipped?
-------------
Question=  200 Fraud detection with unsupervised learning
selttext=  I'll preface by saying I am brand new to ML.

I have a data set of ~10,000,000,000 (fake) transactions where each row is the username, time of transaction, credit card number, device used (iOS, android, laptop, etc) and amount. I need to determine which transactions are likely malicious/fraudulent and which aren't. I would prefer to approach this with unsupervised learning of some kind, as going through these and manually labeling ones I *think* are bad would be super tedious and not necessarily correct.

Is there a good approach to this problem with unsupervised learning? I've read about random forests and decision trees (high level overviews) and maybe they would do the trick?

Thanks
comment no.= 0 -->  Fraudulent transactions are likely different from 'normal' transactions, so you are basically looking for outliers. You could start looking at some histograms / scatterplot of your data to begin with. You could use clustering (unsupervised learning) to determine what are 'normal' transactions, and then you could find which transactions are not near clusters, and are thus likely fraud. You could start with some simple clustering algorithms such as kmeans or hierarchical clustering approaches. If you really want to do something fancy you could try using TSNE, but I would try easy approaches first. You will have to find some algorithms that will scale to your dataset though (look for large scale clustering toolbox or something like this)...
If you have no information about which transactions are fraud (it sounds like you don't have access to this information) you cannot train any supervised models such as random forests or decision trees (those are usually supervised models, unless you are talking about some special models). 
-------------
Question=  201 What type of machine learning or decision making should I start looking into for this task?
selttext=  Thank you in advance for reading this - I know enough to know that I don't really know anything.  But here is what I am trying to accomplish:

I have two populations of individuals, let's call them A and B.

A is a mostly static group of people with the normal traits a person would have, age, gender, locale, specific degrees in some form of education, a number of years involved in our project, etc.  They also have a history we could review of encounters and outcomes we could teach a system with.

B is an ever changing population of people with the same traits but with the addition that we want them to take an action.  Let's say we want them to read more books, or walk more.

The "game" is that a person from A is "up" to talk to someone and try to convince them to take an action.  I am wondering what sort of machine learning or approach to AI would watch and learn the successes and failures in population A and make the best decision on who from population B they should be matched up with.  What would most likely result in an action taken.

I simply have no idea on what direction to head with this?  A few word answer would get me going for some time on my own.

Maybe I am dreaming but I have always been fascinated by AI since I read Marvin Minskey's society of the mind decades ago.  I think that was the name.  I would love to pursue leveraging some sort of machine learning in this task.  Sadly aside from a fascination and some more light reading I trended more into mainstream programming :(

Thanks for any help, even if it is "Your nutz, this is not something for machine learning"

Thank you,

Bill


comment no.= 0 -->  If you have data on 'successful' interactions. That is when a member of particular A succeeds in getting a particular member of B to perform an action then I would model this as classification problem for edge prediction on a graph.

You have a bipartite graph of A's and B's and you have your known edges, when an A has successfully persuaded a B for a task. You then use this data to train a model to look for additional connections. You can either model each edge as 'successful' (binary classification ) or each edge as 'successful for action K' then a multiclass problem.

Either way this differs from normal classification in that you are using attributes of both A and B to make the prediction. 
This may make an advantage over just modelling A to predict if they can persuade any B.
Have a look at this paper: http://arxiv.org/pdf/0806.0215v2.pdf its a biological paper so you need to read through the biology but could be useful. 




-------------
Question=  202 Find the wonky data [x-post r/askstatistics]
-------------
Question=  203 [Beginner] error rate stays same from beginning to end
selttext=  I started learning NN. After some reading of theory I stumbled upon this [blog](http://iamtrask.github.io/) and decided to follow it and implement by myself. I am trying to use iris dataset (iris-setosa and iris-vericolor) but it seems that my NN stays on 0.5 error rate. I tried to change alpha, hidden layer size but it seems that in the end error rate is equal 0.5. Code is [here](http://pastebin.com/1AaMvQg9). Output:

    Error (iterations: 0): 0.436383143672
    Error (iterations: 10000): 0.500000789314
    Error (iterations: 20000): 0.500000195026
    Error (iterations: 30000): 0.50000011538
    Error (iterations: 40000): 0.500000692513
    Error (iterations: 50000): 0.500000173484
    Error (iterations: 60000): 0.500000148476
    Error (iterations: 70000): 0.500000059061
    Error (iterations: 80000): 0.500000134286
    Error (iterations: 90000): 0.500000466787

Any help or suggestions?
comment no.= 0 -->  If its a balanced binary classification problem then the expected error rate of a random guess should be 0.5.

I would suggest implementing grad check, though it may not help you here.
comment no.= 1 -->  Couldn't spot an obvious error in the code. I don't speak alot of python though.

Some questions that might help you:

- How do the learning curves look like? (alpha too big?)
- What are the values of the target vector? (y in [0,1]?)
- Is your data normalized? 
comment no.= 2 -->  Try

l2_error = l2 - y

?
-------------
Question=  204 Can a NN be taught to multiply two numbers?
selttext=  Hi,
I've been playing with regular neural networks with ReLU input, hidden and output units. 


I have successfully trained it to add two real numbers, just approximating binary functions for fun, of the form f(x, y) = z. 


The input is 4 numbers

[negpartofx pospartofx negpartofy pospartofy]

Ex. f(-5.1, 8) becomes f([5.1 0 0 8]). Output is similar [negz posz]


Addition is then simply to learn direct transfomration matrix

[1	-1	1	-1]

[-1	1	-1	1]


I've tried similar approach to learn general multiplication, but with no luck.
Ie. I expect it to work for any real numbers, it sort of works for numbers within a limited range, but the results become too low for large numbers it has not seen before. Something tells me it cannot be done to learn general multiplication... yes or no?
comment no.= 0 -->  I'm afraid your model does not fit the problem. You might want to think about what kind of functions you can model with a three layer ReLU feedforward net. 
As a first step: What is the smallest neural network that can be trained to execute general addition of real numbers? (You might need to use another activation function)
-------------
Question=  205 What is the best way to address the vanishing gradient problem in neural networks?
selttext=  I'm relatively new to neural nets and machine learning in general, and recently stumbled onto this problem when attempting to add more layers to a network used to classify digits. As I added more layers, my accuracy fell.

I have heard of a few ways this is addressed, but was wondering if someone could highlight what the best techniques were, and what the pros and cons of each of them were.
comment no.= 0 -->  Relu units are the most common solution.  Basically the idea of these is to have a non-linear function whose gradient is either on or off, that way it pushes the error signal all the way down without diluting it at each layer.  

I don't know enough myself to talk about pros and cons of different methods.
-------------
Question=  206 What's the hardest part about training ML algorithms?
selttext=  Is it getting the massive datasets and cleaning them? Or is it waiting for the algorithms to finish? Or is it guarding against overfitting?
comment no.= 0 -->  Cleaning > overfitting > waiting
-------------
Question=  207 CNN with engineered features
selttext=  If I am working with a data set of faces, 32x32 pixels each, and I want add an engineered feature like 'is the user wearing glasses'. Would it be reasonable to add a new row, 32x33 pixels, with a binary value to signify glasses or not. What's the pro / con of this approach ?    
    
Thanks 
comment no.= 0 -->  I suppose that an advantage of this approach is that it is easy to implement. It's usually a good idea to try the simplest approach first and see if the result is good enough for you before you invest more time and effort developing something more complicated.

I do think this approach isn't quite *right*... Convolution works because all the inputs are the same kind (i.e. pixels) and there is a meaningful structural relationship between them. Your high-level feature is not a pixel, and it's not really meaningful that pixel [32,32] is adjacent to a "glasses-pixel" and pixel [16,16] isn't. 

I think the more correct approach would be to just add a single input node to your network to represent this feature, that you then connect to *all* nodes in the first hidden layer. Or actually, you could play around a bit with this, because they say that each layer extracts higher level features, so perhaps it is better to connect your glasses-input to a hidden layer that has similarly high level features. Or you could connect your glasses-input to *all* hidden nodes regardless of layer. 

The "important" thing is that you treat this as the feature that it is and not a pixel (or row of pixels), but like I said: you might try that first and see if the result is good enough for you, because this probably involves fewer changes to your code.
comment no.= 1 -->  (Mostly just echoing what /u/CyberByte said) Adding just row is a bit odd because only some convolutional filter applications will 'see' the feature. So, if you add the indicator as the 33rd row, convolutional filters applied to the 1st row will take only pixels as input while convolutional filters applied to the last rows will take both pixels and the indicators as input, which is probably a bad thing considering those filters will have their weights tied. Instead I would add the an extra channel to the input, so instead of 1x32x32 input switch to a 2x32x32 input, where the second channel is binary value indicators. This paper did exactly that to encode information about player skill level when it comes to playing Go:

http://arxiv.org/pdf/1412.6564.pdf

A more efficient but maybe harder to implement method could be to add the indicator with a weight to the output of the first convolutional layer before the non-linear functions is applied. So you could perform convolution on the input, add w*b where "w" is a new parameter and "b" is an indicator to each output unit, then apply your activation function and proceed as normal.
-------------
Question=  208 Is over-training a risk when a person marks lots of messages in their preferred email client/site as spam/not spam?
selttext=  Let's say that I receive on the order of 50 messages a day.  If I diligently make sure that every one every day is correctly flagged as spam vs. not spam, is over-training a risk?

If so, how do I know at what point to stop training?

My knowledge level: I'm can describe a bit how Naive Bayes works, I can define the term over-training, and I know that I should be saying "ham" instead of "not spam" - but the latter seems more appropriate for a "questions" subreddit.  :)

(I use Thunderbird specifically, but I think this is a more general issue that would be applicable to any email client/app/site?)
comment no.= 0 -->  I haven't really used Naive Bayes much, but I very much doubt that you're going to "overtrain" your spam filter. The problem with overtraining a classifier is that it can lead to overfitting, which is when random error/details/noise (from the training set) is modelled instead of the real underlying relationship. This requires that the classifier is flexible enough to model all of those tiny details; this is usually called high variance/low bias. Variance typically increases when the classifier has more learnable parameters, so e.g. a big neural network has higher variance than a smaller one. Naive Bayes classifiers tend to have fairly low bias and are therefor less susceptible to overfitting.

Overtraining tends to only be a problem when a classifier's training procedure goes over the training set multiple times to incrementally refine its model. One example is (again) a neural network (multi-layer perceptron trained with backpropagation). If you have 500 training items and you can afford to run 1,000,000 training iterations, then you can train on each item 2,000 times and you might overfit. If your training set is larger (e.g. 5,000) then you can train less on each item (e.g. 200 times) and you are less likely to overfit. In fact, even if you do train each item 2,000 times as well, overfitting is still less likely because it's more difficult to capture all the details of 5,000 items than for 500. Expanding your training set virtually always leads to less overfitting. 

With Naive Bayes you really only consider each training item once. The only way to increase the number of training iterations (and potentially run the risk of overtraining) is to expand the data set.

It's probably not literally impossible to screw up a classifier by expanding the training set. For instance, if you add (almost) the same item a million times (and few other items), that will probably do the trick. But you should be fine if the e-mails that you flag form a sample that fairly represents the whole body of e-mail that you're getting. In ML terms you want your sample to be independent and identically distributed (i.i.d.).

tl;dr: Flagging more e-mails probably makes your spam filter more accurate, not less.
-------------
Question=  209 What do you use for gradient descent? Do you implement your own or use pre-built software?
selttext=  As I understand it, there is software that implements cross-validation and gradient descent for you, and all you have to do is supply the cost function and its derivative. I was wondering, do most people implement their own, or use off-the-shelf software/library for it?
comment no.= 0 -->  scikit-learn allday errday.
comment no.= 1 -->  I use fmincg inside of Matlab.
-------------
Question=  210 What is the best classifier from the ones tested here?
selttext=  For starters, I'm a complete beginner in this field and I'm just dipping my toes into this sea of knowledge. 

And now, what I'm trying to do is to test and understand which is the "best" classifier and the one who performed best from the below list of classifiers ([**click the link**](https://drive.google.com/open?id=0ByAaxTJ8CHF7flREZ05BMzRTdmhuVjZNYlRrRFRMeW1pVFZ2TnlyTDlFNHJXMGpuR3FXeWM)) - 
I've explained what the graphs mean, in detail, down below.

To put this into context, I'm trying to write a supervised-learning application with Python and Sklearn and what I am trying to do is find the right classifier for correctly classifying a resume from a list of resumes.

So far, my learning algorithm has these 2 phases:

* pre-processing
* model training
and then prediction based on the trained model.

The pre-processing phase is where I made all of the adjustments and tried different methods before generating both a CountVectorizer matrix and a TFIDFVectorizer matrix and compared the "performance" of the classifiers trained with them.

The distinct parts of my pre-processing phase are simply using a TFIdfVectorizer and CountVectorizer, or in combination with the following:

* stemming (with Lancaster / Snowball stemmer from NLTK)
* word correction using [Peter Norvig's approach](http://norvig.com/spell-correct.html)

So, for training my model I've tried combinations of all of these (which can be found in the link):

* a simple CountVectorizer over my training text
* a simple TFIdfVectorizer ... over my training text
* Lancaster stemming + CountVectorizer (... over my training text) etc.
* Snowball stemming + CountVectorizer
* Lancaster stemming + TFIdfVectorizer
* Snowball stemming + TFIdfVectorizer
* Lancaster stemming + Peter Norvig's word correction algorithm + CountVectorizer
* Snowball stemming + Peter Norvig's word correction algorithm + CountVectorizer
* Lancaster stemming + Peter Norvig's word correction algorithm + TFIdfVectorizer
* Snowball stemming + Peter Norvig's word correction algorithm + TFIdfVectorizer

and in order to have a better overview of how my classifiers would perform in a dynamic context (resumes can have more or less the same number of words), I've used a variable value for min_df (minimum document frequency) to range from 18 - the number of documents in the test scenario TO MAX(word document frequency) - WHICH IS WHAT IS DISPLAYED ON THE GRAPHS.

So, actually I'm testing the "performance" of my models and how they work with different numbers of training features.

As mentioned at the beginning, I'm a complete beginner and I've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ML.

One of my main questions, which I hope to find an answer to here is if some classifiers would be considered overfitting, since this is not currently clear to me. 

For example, I can understand that the Multinomial Naive Bayes is overfitting for most of the CountVectorizer approaches. 

* NuSVC is too "unstable", which I'm guessing makes it a poor choice of a classifier for this scenario.
* But how about Bernoulli Naive Bayes? Would it be considered a "good" classifier? Does it overfit at the beginning but then become a more "real" and better performing classifier towards the end, even if it's prediction rate is not 100%? 

* Same thing for Gaussian Naive Bayes..
Also, some people suggested that LDA ( Latent Dirichlet Allocation ) is a good classifer for this kind of scenario.

I'm hoping that someone could help me make some sense out of my results, since I'm a bit confused on how I should interpret them.

And if there's anyone interested in how I actually did this, with code, you can find it [**on GitHub**](https://github.com/radu-gheorghiu/RecommenderSystem/blob/master/Implementations/bag_of_words_sklearn/bag_of_words_sk.py). 

Disclaimer: I'm a terrible programmer with a very non-Pythonic way of writing code.

And in the end, thank you in advance for reading this "story" and your help is greatly and immensely appreciated!
comment no.= 0 -->  http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
comment no.= 1 -->  I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/machinelearning] [What is the best classifier from the ones tested here? : MLQuestions](https://np.reddit.com/r/MachineLearning/comments/3inerg/what_is_the_best_classifier_from_the_ones_tested/)

[](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*

[](#bot)
-------------
Question=  211 A question about softmax layer neurons.
selttext=  How are the partial derivatives ∂a^k' / ∂a^k = 0 for all k' != k, where a^k is the output from the kth neuron of the softmax layer. Shouldn't a change in the output of one neuron of the softmax layer cause a change in the output of other neurons as well?

I'm having a lot of trouble wrapping my head around this. Any help is appreciated, thanks!
comment no.= 0 -->  You should do the derivatives with respect to the node's *input*, which I'll call `z`. ∂a^k' / ∂z^k = a^k' \* ( 1 - a^k ) if k=k', and -a^k' \* a^k if k' != k. 
-------------
Question=  212 I have a square matrix of data (from simulations of 2D PDE's) What kind of fun things can I do?
selttext=  basically as title says, I have a lot of data from solving some basically stochastic/noisy diffusion fields in 2D and I am interested in just playing around with the data with some ML tools (probably with pythons sklearn toolkit)


I just don't want to head in blindly at the moment, so I wanted to ask for suggestions first on what could/can/should be done! (quite vague I guess, apologies for that but this entire field is very very new to me but fascinating)


Thank you :)

quick edit: forgot to add these solutions give some sorta nice pattern formations/fractals-like structures hence my interest in ML application
comment no.= 0 -->  plot that shit
-------------
Question=  213 Can you please help with a perceptron (neural networks) problem?
selttext=  I have the following neural networks problem and couldnt find any answer on the web. Any hints would help. I am not looking for a complete solution, just some pointing in the right direction.

PROBLEM:
Write the upper bound Ko of the number of iterations needed to a perceptron to learn a linear separable set with the Rosenblatt rule:
w(1) = w* / 10^6
What category of points makes the learning hard? Justify the answer.

Thank you,
Dan.
comment no.= 0 -->  It's basically asking you to prove the perceptron convergence theorem (easy to google for writeups).

Gvien that perceptrons are linear classifiers, points with non-linear boundaries are impossible to learn. The canonical example is the xor function. Minsky's perceptron book proved this and started the first AI winter.
-------------
Question=  214 Standard Deviation Between Different Runs of SVM Cross-Validation Folding
-------------
Question=  215 How should I use the textual data given below in order to carryout a classification task?(Please help by giving some example code)
-------------
Question=  216 Hoping to make a project involving the classification of scanned stamps (postal)
-------------
Question=  217 Dataset recommendation for binary classification?
-------------
Question=  218 Support Vector Regreesion Model/Equation with SMOReg in WEKA?
-------------
Question=  219 Training methods other than by example
-------------
Question=  220 What is "Deep Learning"
selttext=  As opposed to just "Machine Learning"?
comment no.= 0 -->  "Deep Learning" is a technique developed by the field of Machine Learning. Deep learning is a rebranding of neural networks, which historically didn't perform very well on many problems. More recently, neural networks have been performing well because machine learning researchers figured out a set of techniques that make it easier to learn deep (complex) neural networks. Machine learning does not necessarily involve neural networks.
comment no.= 1 -->  My understanding of this topic is a bit limited, so I'd be interested if this response is off target, but here's what I think it means:

When you have a neural network, you might train it using some form of the backpropagation algorithm. This algorithm basically looks at the rate of change of the value at a node as the final "answer" node varies, so you run your inputs through the network, look at the answer you get, and then adjust the weights based on how far off you were from the right answer. Follow so far?

Well, when a neural network is very "deep" (has a lot of layers of neurons), the gradient for adjusting those weights can get vanishingly small... or it can blow up, so the values swing wildly. That's because as you have more layers, instead of layer A depending on layer B, you have layer A depending on layer B, which depends on C, which depends on D, and so on. So when layer M changes a little bit, layer A either changes a huge amount or not at all. Point is: the numbers get a little fucky.

A solution to this was to have an algorithm "learn" what kinds of features are going to be most useful to distinguish various types of output in advance of trying to train the network. That way, when it comes to tweaking your weight vectors to get the output you desire, you don't have as far to go. If you can figure out how to do that, then you can have much "deeper" networks. Why are deeper networks good? Well, I think that it's because the network can model nonlinear interactions much more easily. In theory, a very simple neural network can approximate any function ("universal approximation theorem") but in practice that may require many thousands of nodes in a layer. A deep network gives you a lot more power with a lot fewer nodes, which means faster computation.

So the trick is to train in the "features" before you ever start the neural network learning. The way I learned to do this is with a restricted boltzmann machine, but I would imagine that many unsupervised learning tasks would be useful for this purpose. What you do is basically set up a network that becomes very good at taking the input and... reproducing inputs, honestly. Because to reproduce inputs "convincingly," (so it's hard to tell made-up inputs from real inputs) the network has to have a sense of what the inputs "look like" generally. So you run your RBM until it is pretty well converged. Then once it has learned the data, you can add additional layers on top of it. There are some tricks to doing this, but that's sort of the idea. That way, when you start your neural network learning, it already has some pretty well-trained layers in the first couple layers (in the sense that the first few layers are picking up important features from the data). That's important, too, because as you add layers, the lower layers become harder and harder to train (for the reasons explained earlier).

In summary, you basically use a bunch of tricks to train networks with a lot of layers because they are better at modeling more complicated functions with fewer nodes.
comment no.= 2 -->  "Deep" learning refers to having a hierarchical stack of models on top of each other, each of which uses the output of the previous "layer" to produce another intermediate result that is a little closer to what we want to get. This is in contrast to "shallow" models that only do a single (albeit very complicated) transformation of input to output. 

Hierarchical/layered approaches work very well in computer vision, because images are made up of objects, which are made up of parts, which are made up of edges... So having a succession of highly specialized models detect ever more complicated features is a much more practical approach than trying to have one model detect everything from raw pixel values. 

Currently this is almost always done with many-layered neural networks, but there is also growing interest in "deep" graphical models. 
-------------
Question=  221 Neural Network to learn to play Video Games
selttext=  Is it possible to use a Deep Belief Network to learn and play a video game (e.g Tetric, Pacman, Snake)? 

I've read about Deep Reinforcement Learning being used for it, but I'm unsure how far this is from a DBN.

Apologies for how nonsensical this may be, I'm still new to Neural Networks as a whole as I'm in the planning stage for an undergrad project involving it.
comment no.= 0 -->  Hey OP ! 

So I'm not very familiar with a reinforcement method but you should look up MarIO. Not sure if it's related but hopefully it can help give you some inspiration. I'd link it now but I'm on mobile. 
-------------
Question=  222 I'm an R user but know zip about ML. Help me out...
selttext=  Alright guys, I know at some point I'll need ML in R for future projects at work. I want to be ahead of the game so the transition is as smooth as possible. Dear experts, consider the following:
1) I have no idea about ML;
2) I know R;
3) I know Statistics/Econometrics well enough to understand how to model some real world issues;
Given that, would you answer me the following questions:
1) Where can I learn the absolute basic about ML?
2) Given that I've learned the foundations, is there any books focusing on R and ML? I've downloaded a few ones but wouldn't mind a few extra tips.

Thank you!
comment no.= 0 -->  ["Introduction to Statistical Learning"](http://www-bcf.usc.edu/~gareth/ISL/)
-------------
Question=  223 Tutorials for SVM?
-------------
Question=  224 ReLU derivative in 0
selttext=  Hi,

Recently I found a mistake in my code where I defined the derivative of ReLU as

1 if node unit is >= 0

This doesn't make much sense because it would be 1 for all nodes in a ReLu layer.


I corrected it to
1 if node unit is > 0


However, I've been running some tests switching back and forth, and it seems I have slightly faster convergence with the old derivative (>= 0).


What do you guys use?



It's undefined in 0

http://www.wolframalpha.com/input/?i=derivative+of+max%280%2C+x%29

http://www.wolframalpha.com/input/?i=derivative+of+%28x+%2B+abs%28x%29%29%2F2

comment no.= 0 -->  False alarm, I think I might be wrong on this one
-------------
Question=  225 Have a data set of more than 1GB, how to read it in R?
selttext=  the 'read.csv' command leads to a hang. And R doens't allow opening files more than 5MB. What can I do in this case? Any help will be highly appreciated. 
comment no.= 0 -->  [Found this bit of reading](https://theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun)

Or maybe you could push the data into an RDMS (i.e. MySQL) and then pull the data into R?
comment no.= 1 -->  What do you ULTIMATELY need to do with the data??
-------------
Question=  226 Any recommended papers or books on time-series/sequence classification?
-------------
Question=  227 FREE COURSE: Amazon Machine Learning
-------------
Question=  228 Can artificial neural networks be programmed to 'mutate'?
selttext=  I'm a regular scientist curious about machine learning. So, please be patient with me.

Can artificial neural networks be programmed to 'mutate' (quasi)randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance? 
comment no.= 0 -->  Yes, they can. The [NeuroEvolution of Augmenting Topologies (NEAT)](http://www.cs.ucf.edu/~kstanley/neat.html) algorithm is one method to do it. Such an approach works well when the optimal network topology isn't known (or surmised) in advanced, and when there is a good way to formulate incremental improvements. 

But generally speaking just trying different hyperparameters (what kind of net, how many layers, how many nodes in each layer...) and going with what works best tends to be preferable from a certain network size upwards, because it is quicker.
comment no.= 1 -->  I don't know enough to fully answer this question, but there are genetic algorithms in machine learning which perform similar to what you described. It is at least a starting point for you to research.
comment no.= 2 -->  Yes, most all machine learning algorithms do use a type of "artificial evolution". Even in something as simple as linear regression, or finding a line of best fit, a program will probably iterate through "generations", continuously adapting a hypothesis until cost between known values is minimal. 
-------------
Question=  229 Interpreting improvements to Logistic Regression model in R
-------------
Question=  230 Minimizer won't converge for logistic regression program
-------------
Question=  231 How many learning curves should I plot for a multi-class logistic regression classifier?
selttext=  If we have K classes, do I have to plot K learning curves?
Because it seems impossible to me to calculate the train/validation error against all K theta vectors at once.

To clarify, the learning curve is a plot of the training & cross validation/test set error/cost vs training set size. This plot should allow you to see if increasing the training set size improves performance. More generally, the learning curve allows you to identify whether your algorithm suffers from a bias (under fitting) or variance (over fitting) problem.
comment no.= 0 -->  ||Ypredict-Yactual||2
-------------
Question=  232 Cost function confusion
selttext=  In many of the guides I have read online, (in particular [this one](http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html)), there is a step at the beginning of back propogation where you calculate the discrepancy between your observed output and your target output.

The part that I am very confused about is that sometimes this discrepancy is calculated as 𝛿 = a - y (where "a" is the observed output and "y" is the target output). Later on, the guides will discuss a "cost function," which seems to me to be the same thing as the difference formula above? However, a totally different equation (http://imgur.com/guz1oxp) is then given.

Is the difference formula just an abstraction of the more complicated formula?


Thanks!
comment no.= 0 -->  In regression, the cost function is usually the mean or sum of squared errors -- each individual term in the sum is (a-y)^2. What you called the discrepancy is actually the derivative of this error wrt to a as used in backpropagation: http://www.wolframalpha.com/input/?i=derivative+.5*%28a-y%29%5E2+wrt+a

When you do binary classification the cost function is [the cross-entropy](http://deeplearning.stanford.edu/wiki/images/math/f/a/6/fa6565f1e7b91831e306ec404ccc1156.png) between the wanted distribution and what the network predicts. This is a special case of the [negative log-likelihood under a multinomial distribution](http://deeplearning.stanford.edu/wiki/images/math/7/6/3/7634eb3b08dc003aa4591a95824d4fbd.png)\* that is used in multiclass problems.

\* for neural networks this formula is slightly different
-------------
Question=  233 My classifier has 100% accuracy, is something wrong or is this good data?
selttext=  Hello everyone,

I ran several models on a binary classification problem, each performing at around 70-80% classification accuracy. 

I then used [stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) to build a new model on top of those and I got near 100% accuracy.

My process:

* run several models to see how well they performed for accuracy
* pick the top 4 models (lda, random forest - 15 trees, logistic regression, and random forest - 150 trees)
* create new features from those models - taking the predictions of each model (using cross validation so that I would train 4/5 of the data to predict the last 1/5, then swapping out so that there would be no data leakage).
* run a binary classifier on the new dataset which includes those extra features
* get 100% accuracy. 

I tried 5-fold cross-validation on the new model and it performed at around 96%+ accuracy.

Do I need to check something else to make sure that this model is legit? 
comment no.= 0 -->  Make sure you are evaluating the ensemble on totally untouched data. By untouched I mean that this data hasn't been used for fitting the base or "stacker" models.

edit: and also not used for selecting hyperparameters for your stacker model
-------------
Question=  234 Need ML Algorithm suggesstion ?
-------------
Question=  235 Probability distribution upto a normalizing constant
-------------
Question=  236 Gradients turn zero after a few epochs
-------------
Question=  237 Day of the week with generic backward prop NN's
-------------
Question=  238 What is the most common model for acoustic classification?
-------------
Question=  239 How to train lstm layer of deep network
-------------
Question=  240 How important is performing cross validation on your algorithm's parameters?
selttext=  
comment no.= 0 -->  a lot.

If you have a parameter you are changing it is just like another feature. 
If you run your algorithm with 100's or 1000's of parameter combinations.. you will have results which are 'significant' just due to chance.
-------------
Question=  241 Is there an accessible implementation of any of the image captioning neural nets that have recently gained attention?
-------------
Question=  242 Would you benefit from Deep Learning examples & tutorials?
selttext=  I'm wanting to build a sort of blog that dives into different aspects of Deep Learning,  a project based/case study style learning experience.

Assuming the content was good, would you subscribe to a blog like that? Would you be interested in working through the problems?
comment no.= 0 -->  Definitely ! It sounds like it could be fun 
comment no.= 1 -->  I would definitely subscribe. Go ahead please do it
comment no.= 2 -->  If it's python, if it provides some math - not too hard - not too dumbed down - something that a probability/linear-algebra background can figure out - pretty pictures - links to further reading - links to foundational reading, if the projects have varied lengths so some are short and some are medium (100 lines) and few are large (200+). 

Yes. 
comment no.= 3 -->  Pseudocode with clear description of every variable would be a goldmine

Trying to detangle other's code on github is very cumbersome
comment no.= 4 -->  Yes please, I would definitely benefit from tutorials/guided learning!
-------------
Question=  243 How to represent multi-dimensional data as colors in Self-Organizing Map?
-------------
Question=  244 Long boolean vectors as data set - looking for suitable model / algorithm
-------------
Question=  245 I've already implemented a Naive Bayes classifier to help me assess the sentiment of tweets. What other algorithms can I use?
-------------
Question=  246 Problem understanding Bias, Variance, and cost/learning curves for train set, cross validation set and test set.
-------------
Question=  247 Trying to decide on algorithm for my data
selttext=  Hi. I've began reading a lot about Machine Learning, but having no advanced background in statistics/maths, I find it hard to relate to most of the online information for selecting the proper algorithm on my data. I do programming for a living. This is a pet project as will be clear from my example, so I don't need an absolute best fit or optimal result. With this said, here is my problem:


I play Path of Exile, a free game. It's extremely similar to Diablo. It's got one of the most complex set of items attributes. The game allows players to have shops online (so list items with prices) as well as give the information about what the players are wearing. I spent months of time writing code to extract this data, so I have historical data about what items were worn, what they were replaced with, what items were listed for sale, which ones sold, how long it took to sell, etc.

This data is rather massive, were talking a few hundred gigabytes. My data is split in 2 big data sets:

* Inventories *(what players wear)*
* Items for sale *(pricing data is very noisy, as there are no consistency in pricing)*

Heres a breakdown of how items work, as far as stats are concerned:

* There are ~250 stats to chose from. Those stats never change. *(ex: +40% fire resistance)* 
* An item is a combination of 4-6 of those stats.
* Every stat has one category. When an item has one stat, it cannot get another stat within the same category. *(ex: if the item has +40% fire resistance, it cannot have the 2nd stat '+20% fire resistance'. There are ~30 categories)*
* Every stat has a known chance to occur. *(ex: 0.25% chance to get the stat +40% fire resistance)*
* If the item is listed for sale, it *may* have a listed price.

The system isn't much more complicated than that. I simplified a lot the details, but this is what matters. I have 2 questions I wish to answer with machine learning:

* **Predict an item pricing**
* **Try to find how strongly related are stats together (ie: out of the stats that players wear, estimate the demand)**

I tried the Andrew Ng online course, as well as reading a lot of documentation online, the weka tool, and none seem to make it clear what algorithm I should pick. This is what I am planning to use as input neurons:

* Every category (ie: set of stats) is an input neuron, with the currently selected stat within the category being the value. 
* The 'weight' of every stat in the category is simply its probability to occur. 
* The stats within the category are ordered from least probable to most probable (ie: best to worst).
* Proportionally adjust the categories ranges within 0-1.

ex: (one category)

Stat | Range (of fire resistance) | Probability
---|---|----
of the Magma | 42-45 | 0.10%
of the Volcano | 36-41 | 0.25%
of the Furnace | 30-35 | 0.75%
of the Kiln | 24-29 | 1%
of the Drake | 18-23 | 1%
of the Salamander | 12-17 | 1%
of the Whelpling | 6-11 | 1%

So if my item has "of the Magma", for that neuron, I would give it 0.10 value to that input in the NN. If it had "of the Volcano", I would give it 0.35.

Every category is given an input accordingly, with 0 denoting none was chosen.

So with this said, I am not sure which algorithm to pick to feed it the output neuron (pricing?). My data is very noisy for prices. Should I use a classifier with the output neurons being slices of prices (ie: neuron 1 = 0-1$, neuron 2 = 1-5$, neuron 3 = 5-10$, etc.) ? Or should I have only one output neuron? I thought the slices would effectively 'fix' the issue of very variable pricing and at least give a good idea of the price range to expect. And as far as detecting the strength of connections between stats for items that are worn, I do not see what output neuron I could map, so I was wondering if that was even possible. I though I should rank negatively stats that are being removed (ie: player stopped equipping item with stats x,y,z, so input those values but as negatives), but that still leaves me wondering how to extract the correlation between the stats. 

I would welcome any help as such. I don't expect any hand-holding, I just want a nudge in the proper direction! 

Thanks again
comment no.= 0 -->  Let me preface this by saying that I'm no ML expert, I've only part gone through Hinton's Coursera class.  I do play PoE though, and this is an interesting project.

I would say that classifying the items in price ranges would be easier, then your output could be something like "90% chance of it being in range X-Y, 7% chance of it being in range W-Z and 3% chance of it being in range U-V".  This seems like it would be easier to train than a single output of the expected price.  If you used one output of expected price, you would also need to convert all prices to one currency, whereas with the ranges it isn't the case (you could have outputs of 10-20 chaos and 30-40 exalts no problem - the NN only puts it in a class)

If it were me, my inputs to the NN would be as follows:

* Item Type (Helm/Chest/Amulet/Ring/etc - give this a number so ItemType=1 is for Helms for example)

* Defensive Stat Type (0 for None, 1 for Armour, 2 for ES, 3 for Evasion, 4 for Ar/ES, 5 for Ar/Ev, 6 for Ev/ES, 7 for Ar/Ev/ES - Sacrificial Garbs)

Then the rest of the input nodes would be all possible affixes.  Assuming it's a NN, your outputs would be the ranges of prices.  The price estimation is more straightforward to me, it's just a simple classifier.  The strengths of the connections come through your training cases, you're looking to do supervised learning (I think).

I don't see why you would start by specifying weights.  That comes through learning.  Just have the stat (e.g. Fire Resistance) as an input neuron, and that input value as the stat's value.  I think that's a better system because of hybrid affixes (e.g. Emperor's) don't show up directly - you need to infer their existence.

For flat damage, use the "average" value (e.g. "adds 6 to 15 physical damage" would be "Flat Phys neuron" with value of "10.5").  

For weapons, you would also need to look at the pDPS and DPS, on top of the APS and crit chance.

I think this is a very interesting project though!  You could use it to work out the best master crafting option to maximise the expected sale price.

I've no idea how you would work out the whole "items worn and their progression" for demand estimation.



comment no.= 1 -->  First, have you tried just taking something like scikit-learn and just fitting a simpler regression model, like linear regression or random forest regression, to the data first?  Often those will get you 90%+ of the accuracy of a more complex model in about 1% of the time, and help you figure out which sets of attributes and outputs will work well.  If you just use the 'raw' price as the output, you can use a regression model, and if you want to lump the prices into ranges, then use a classifier (where each price range is a separate class).

If you're determined to use a neural network, then for managing the output there's two simple ways you can do it.  The easiest is to have a single output neuron for price that uses a rectified linear activation function and squared error loss (in other words, treat it as a regression problem).  This will drive your model to try to estimate the average price across that set of inputs.  The second way to do it is to break the price into ranges and treat it as a classification problem: stick a softmax layer at the end with categorical cross-entropy as the loss function. This will make it estimate the probability that an item will sell in a given range.  You do have to be a bit careful to 1) have a 'bin' for every possible price, and 2) not have too many outputs, since this can make the model harder to fit.

Advantages of the linear output is that you only need one output neuron, it should fit quickly, and it will give you a flexible output.  Disadvantage is that it won't give you much in the way understanding the variance of the price (i.e. the average might be $1, but is it $1 +/- $0.01, or $1 +/- $0.99?)  Advantages of the 'binned' output is that it will give you some measure of uncertainty, so it might tell you that it's 95% sure that it will sell for $1-2, or it might tell you that it's 25% likely to sell for $1-2, 25% for $2-3, etc.  Disadvantages are that it will probably be slower to train, and if you slice the prices too small, then you could get odd results if you have very rare combinations of attributes that sell for unusual prices.

For analyzing the combinations of items, you might be able to use something like t-SNE on your trained network to get some idea of if or how particular combinations of traits cluster, but I'm less familiar with that sort of thing.  It might be easier to fit a separate model, either by filtering to only examine items that were actually purchased and then doing a (non neural network) clustering on those, or maybe through building a model that tries to predict the Nth attribute given the other N-1 on the item.
-------------
Question=  248 Trying to scale output of Mahout logistic regression to p(event), but it's not working. Help?
-------------
Question=  249 Question on Kernels
-------------
Question=  250 How do I handle large CSV datasets?
selttext=  I'm a beginner messing around with some basic application of ML, applying simple classifiers and stuff, and I stumbled along a CTR challenge Avazu posted a while back on Kaggle: https://www.kaggle.com/c/avazu-ctr-prediction. My problem is that the training set here is a single CSV ~2GB in size, and possibly a few hundred million entries. I tried opening this dataset in the WEKA viewer, and even after increasing the stack size to 4GB, it stopped responding after some time, forcing me to close it. This brings me back to the question, what tools/libraries do you guys use to handle such large datasets?
comment no.= 0 -->  Let's denote the problem straight: it doesn't matter to you if the dataset size is 4GB/20GB/3TB if you can hold only 1GB dataset. There is no other way to shrink down your dataset other than to take a subset of it. Every line-by-line approach to get this subset would work for you. 

Well, there are some more questions - like how to choose the entries of subset - randomly, proportionally or sets of same size for every factor. That depends on the task, there is plenty of information about subsampling.

If this subset is not enough for you to reach stability in model, there are still options of making a few subset and juggling with them in various ways.

Generally question "How do I just load my enormous dataset straight into my R/WEKA/Excel/sklearn/spintowintoolbox without cleaning it or taking subset of features and observations?" has no answer, you should handle this yourself.
comment no.= 1 -->  My uninformed opinion:

Read it chunk by chunk and consider using an on-line learner. 
comment no.= 2 -->  You could simply read it line by line with csv reader, put it in list then convert it to numpy array.
-------------
Question=  251 Help a beginner out?
-------------
Question=  252 Nearest Neighbor Help
-------------
Question=  253 Are there any datasets freely available on which I can test the autocomplete algorithm I have developed
selttext=  
comment no.= 0 -->  There is /r/datasets -- also ask there.
-------------
Question=  254 I need help with Reinforcement Learning
selttext=  I'm trying to do an AI with Machine Learning, so I started to learn how to do Machine Learning, but I don't think I understand enough of it to do one. I tried learning by watching some courses, but what they say doesn't seem to be related with what I'm trying to do.
If anyone could help me, I'm trying to do it with PyBrain and Python 3.4 at the moment. I'm currently trying to code the Environment class.
I am doing this just for fun and to learn how it works.
comment no.= 0 -->  So what *are* you trying to do? Without a concrete question, I doubt anyone can really help you. What are your problems with the Environment class (from [this tutorial](http://pybrain.org/docs/tutorial/reinforcement-learning.html) I assume)?

For a general introduction to RL, Sutton and Barto's famous RL book is freely available [here](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html). You might also like [this Udacity course](https://www.udacity.com/course/machine-learning-reinforcement-learning--ud820) by Charles Isbell and Michael Littman, or Olivier Georgeon's [IDEAL MOOC](http://liris.cnrs.fr/ideal/mooc/) although it's a bit more advanced.
-------------
Question=  255 Markov Decision Process in R for a song suggestion software?
-------------
Question=  256 Can I use SKlearn's Naive Bayes to classify tweets?
-------------
Question=  257 Optimization of SVM -- how to interpret model order giving best cross validated results
-------------
Question=  258 Subsequence Identification in Time-Series Data?
-------------
Question=  259 Trouble pre-processing large volumes of image data
-------------
Question=  260 Recommended sources for ML-related probabilities
-------------
Question=  261 RNN Library for generating handwriting samples
-------------
Question=  262 Standardization of Input Variables
-------------
Question=  263 Classification algorithms - a discussion
selttext=  Hi there,

I've been tasked with writing my own implementation of any classification algorithm for an assignment. I was going to pick the c4.5 algorithm but I thought I'd ask you guys what you think (as I thought it'd be interesting to see all the different opinions).

I chose the c4.5 because I am quite familiar with how it works and feel that it would be ok for a beginner to attempt it (I have never tried to write my own implementation of a classification algorithm before). My data deals with a bunch of measurements and I have to decide if the next entry is a certain type of animal. Just in case you were wondering! :)

So if you were a beginner, what algorithm would you choose? Better yet, if you had to decide on an algorithm with your current knowledge, would it be different than if you were just starting out? What pitfalls did you encounter when you were first starting? 

Also, if you could think of any resources that may be beneficial to my *learning* (not direct answers as I'd like to learn from my assignment!).....post away! :P

Sorry if this is the wrong place, I got a notion and I thought it'd be pretty cool to hear form an expert or two!

Thanks!
comment no.= 0 -->  The first classifier I implemented was logistic regression. Maybe that's too simple for your assignment though.
comment no.= 1 -->  I think C4.5 will be just fine. I guess you could also start with ID3 and then extend it to learn more about the differences. To prevent overfitting, you may want to look into some pruning algorithms. Then if you want to go further, you could look into Random Forests were really popular a few years ago I think (maybe they still are, but ML isn't my main field and I got the feeling their popularity has decreased now that deep learning works so well). 

In my earlier days I mostly implemented neural networks and genetic algorithms. You could do those too, although genetic algorithms may be slightly harder to apply to your problem. 
-------------
Question=  264 Pre-empting failures with log file based prediction
-------------
Question=  265 RF Model w/ ~15000S x ~150000F barely outperforming univariate model. Suggestions?
-------------
Question=  266 How do I scale non-normal (NB) features?
-------------
Question=  267 help needed for summarization of Amazon.com reviews
-------------
Question=  268 Row vs column vectors
-------------
Question=  269 AskML: Stacked Denoising Autoencoder - PEBKAC?
-------------
Question=  270 What does the ℝ symbol mean when constructing neural networks?
selttext=  I'm trying to read up on some papers about neural networks, and without a background in math or computer science, some terms and syntax escape me.

Several papers I have come across express that "we associate a column vector in ℝ^d" or  similar (where d is a dimensionality). But the only meaning of ℝ that I know of is the conventional one. Does it mean that every vector element is a real number, or does it mean something else?
comment no.= 0 -->  yes, your understanding is correct
comment no.= 1 -->  R usually denotes Real numbers so you are correct.
-------------
Question=  271 Number of parameters in multi class and two class logistic regression
selttext=  If you have f number of features, then in 2 class LR you have f parameters (ignoring the bias).  However in multiclass LR with k classes you have f*k parameters, correct?  

Which means that using multiclass (softmax) LR for a 2 class problem would have double the parameters of using 2 class (sigmoid) LR.

What is bothering me is why is it not f*(k-1) parameters for multiclass LR? 
comment no.= 0 -->  Actually it is f*(k-1)
[wiki](http://en.wikipedia.org/wiki/Multinomial_logistic_regression)
-------------
Question=  272 Propensity scores
-------------
Question=  273 Question about SVM solver algorithms, in matlab specifically (X post from machineLearning post faux-pas)
-------------
Question=  274 Predicting when a resource will run out?
-------------
Question=  275 Predicting customer purchasing behavior
-------------
Question=  276 Random Forests
selttext=  I'm a Machine learning newbie. To learn something in ML, I have implemented a random forest (scikit) based classifier for activity prediction based on accelerometer data. i.e The classifier predicts states like standing, running, etc based on tri-axial accelerometer output.

The RF classifier performs reasonably well in predicting state, however, I would like to add something akin to a feedback loop to it.

What I mean is that, if the classifier misclassified and I hand correct it. For the next run, on same dataset, there shouldn't be a misclassification. I'm using scikit.

Any pointers?
comment no.= 0 -->  With RF you should simply include that sample with correct (by hand) prediction in your training set and then retrain your classifier. 
Alternative is to use some [online learning](http://en.wikipedia.org/wiki/Online_machine_learning) alghoritam.
-------------
Question=  277 Why does UFLDL consider -1 to be an "inactive" tanh hidden neuron output?
selttext=  From the [Stanford UFLDL tutorial](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/):

> Informally, we will think of a neuron as being “active” (or as “firing”) if its output value is close to 1, or as being “inactive” if its output value is close to 0. We would like to constrain the neurons to be inactive most of the time. This discussion assumes a sigmoid activation function. If you are using a tanh activation function, then we think of a neuron as being inactive when it outputs values close to -1.


Tanh is symmetric about the X-axis.  If you flip the signs of the incident weights then an "inactive" neuron is effectively active again.  This piece of advice s even repeated in the older version of their tutorial.  Is it a mistake or is there a good reason that an output of -1 is somehow less "active" than 1?


comment no.= 0 -->  Sigmoids were originally devised as continuous versions of binary threshold neurons. Then people started using Tanh because it's faster than sigmoids and has the same shape. It's just a sigmoid with a bias of -0.5, multiplied by 2, basically. So essentially they are still thinking of tanh's as approximations of binary threshold neurons.

However it's not even true for binary neurons. NNs don't care whether a neuron is "active" or "inactive". They can just multiply by a negative weight and adjust the bias, and a zero becomes 1 and a one becomes 0.
-------------
Question=  278 Multiple Kernel Learning - Regression
-------------
Question=  279 Difference between Implementation and Research
-------------
Question=  280 [Baum-Welch algorithm for Hidden Markov Models]
selttext=  I was going over the Baum-Welch algorithm for updating and ran into a puzzling question about the new values for the symbol distribution -- basically that, unless every observation contains every symbol in the model, it looks like the algorithm will update the missing symbol probabilities to zero for all states. I'm probably just missing something incredibly simple here, but I'd really appreciate it if someone would point it out to me. For the sake of a common notation, [here's](http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) the wiki. 

Any updated probability of symbol O in state i [b i(O)] can be expressed as the sum of a disjoint subset of the gammas for that observation and state, so the sum overall the updated values of b i (O) will always be equal to one.

But unless every symbol in B is in every observation the model updates on, at least one of the symbols won't have a gamma that corresponds to it for any state, so it'll get updated to zero across all states (the numerator for b* i (k) is zero).

Assuming that the model updates on a sequence like that (say 'a,b', where each state has a distribution over 'a', 'b', and 'c'), then it can't update on a new sequence like 'b,c' with the discounted symbol in it. It'll also estimate the probability of such a sequence as zero. 

After updating on 'a,b', the probability of seeing c in any state is zero, which makes the the alpha/beta estimates corresponding to it also zero for all states. The denominator for gamma is the sum over states i, of [alpha i (t) * beta i (t)]. All of those are zero, so the divisor is zero.

Just estimating the probability of 'b,c' runs into a similar problem -- the probability of seeing c is zero; which takes the probability of an observation with c in it to zero. 

HMMs don't just work on permutations of their symbol sets, so I'm clearly missing something. Could someone please explain what?
comment no.= 0 -->  You should have non-zero initial emission probabilities for all symbols. This way, within each observation, the probability of the hidden states for that observation are used *with* the probability of observing that sequence in the first place.

When your pseudo probabilities are later normalized, the unobserved symbols and states will remain non-zero.
-------------
Question=  281 Noob question, how to deal with unequal amount of data for each class, using deep networks
selttext=  Hi I am trying to do classification using a cnn, but the amount of training data i have varies for each class. What is the correct method in order to deal with this?
comment no.= 0 -->  I don't think it matters. It could possibly learn incorrect prior probabilities, and possibly affect performance on cases where it is very uncertain. Try weighing the cases of the classes you want more of, more. I.e. putting multiple copies of them in the training set or multiplying the error so those cases matter more.
comment no.= 1 -->  There is no one easy way to do this and it's in no way specific to CNNs.

First, are you classes imbalanced *only* in your training data with respect to the underlying population?
Or is the underlying distribution imbalanced in the same way?

Having a handle on the class distribution is very helpful here.

If you Google for things like "class imbalance" or "covariate shift" you may get some ideas.

I've tested this briefly: http://blog.smola.org/post/4110255196/real-simple-covariate-shift-correction but it didn't appear to have much effect.
-------------
Question=  282 question regarding design of a cnn
-------------
Question=  283 Searching for a suitable distance measure.
-------------
Question=  284 Stochastic variational inference question
-------------
Question=  285 Dropping out for convolutional RBMs and Convolutional Autoencoders
selttext=  If I implement dropout in convolutional RBMs or Convolutional Autoencoders, do I dropout entire bases (i.e. consider say only 30 out of 60 kernels for a minibatch update), or do I dropout individual hidden units from the kernel activations. (i.e. I use all 60 kernels, but within each kernel's own hidden layer I dropout half of units).
comment no.= 0 -->  The latter is the more common approach. But, experiment and see what works better for your problem.
-------------
Question=  286 How to get data from a site?
selttext=  I don't know if this is the right place to post this, so please correct me if I am wrong.
Id like to make a program that can search for recipes by the name of food item. there is a big repository of recipes at http://www.recipesource.com/
I have never worked with getting data from the net, but from what I understand people generally use api provided by the sites?
So how can i search for recipes from this site automatically, Do i need them to provide some "usable API", or can is there some all-purpose program people use to automate such tasks?
comment no.= 0 -->  when there is no api i normally use C#/F#/Python with their standard http client libs for downloading and some regex to extract urls. you might have to play with some http headers / cookies to get sites to talk to you.

sometimes selenium works well if some website tries to prevent automatic web scraping.. but it's much slower.

comment no.= 1 -->  Most likely too late for you, but it could be useful to others:

Udacity offers a self-paced course on MongoDB. Lesson 2 is on screen scraping, with example code in python + BeautifulSoup library. There is no MongoDB in this lesson.
-------------
Question=  287 Dealing with imbalanced data set
selttext=  I'm working on a course project to classify Tweets as either "interesting" or "not interesting". I have hand labeled about 2000 tweets, and ended up with a ratio of about 1:10 (interesting : not interesting).

I downsampled the "not interesting" tweets and got 200/200 distribution. I use k-fold cross validation on this set and get very good performance with a naive bayes classifier. However, I feel it is a terrible waste to discard 1500 of the not-interesting tweets. 

So the question: could I still train my classifier using the imbalanced data set, as long as my validation set is balanced ? Or am I overlooking some issues with this?
comment no.= 0 -->  There's a lot of literature on imbalanced data sets.

First and foremost, what performance metric are you considering? You'll likely want to be considering Area Under the ROC or precision or recall rather than accuracy (you could have a 90% accurate classifier trivially).

The biggest issue with what you're proposing is that the class balance you're enforcing during training is not likely to persist when applied to a real stream of tweets. I'd be interested to know what happens if you split your data into two even sets with stratified classes (i.e. 100/900 in two sets), train on a 100/100 set and then apply your classifier to the other full 100/900 set.
comment no.= 1 -->  If you know the operating context (i.e. the class imbalance when you are deploying the model) then you can use ROC analysis to select an optimal classifier on the convex hull. The isometric for accuracy will be a line with a 45 degree angle for balanced classes and will change for unbalanced classes. (Steeper or shallower). The AUC gives a good measure if you do not know the operating context the model would be deployed in, but bare in mind some areas under the curve might not be realistic. In particular if you are doing an information retrieval type task (sounds like it) then you will be more interested in having a steep accent on the left hand side of the ROC curve than the AUC score.

This is a good paper:
http://bib.oxfordjournals.org/content/13/1/83.full.pdf+html
-------------
Question=  288 How can you predict the next frame of video?
selttext=  Lets say I have a very large series of sequential images. The images are all broadly similar, and sequential images are particularly similar. There exists some inscrutable rule that creates each image from the previous few images.

I want to create a program that can approximate that rule. I want to feed my program the last few sequential images, and have it guess the next.

How would I go about this? As a novice I have been reading around trying to puzzle things out for myself, but I feel I understand less the more I read.
comment no.= 0 -->  [This talk](http://research.microsoft.com/apps/video/default.aspx?id=180609) has a demonstration of Deep Gaussian Processes doing exactly this.

It's a very hard task though.
-------------
Question=  289 Recommended way to convert a set of probability distributions into a feature vector?
selttext=  I splitted a number of short texts into its sentences and ran Stanford's CoreNLP sentiment analysis on each one. This discards the words and works with the POS trees only. As a result, for each text now I have a number of probability distributions of being in one out of five possible sentiment classes (from very negative to very positive). Now I want to define the feature vector for the texts.

My first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier, then just populate it with the values of the probability distributions. Does this make sense? Wouldn't I be somehow losing the representativity of the sentiment magnitude?

Thanks!
comment no.= 0 -->  "I splitted a number of short texts into its sentences and ran Stanford's CoreNLP sentiment analysis on each one. This discards the words and works with the POS trees only. As a result, for each text now I have a number of probability distributions of being in one out of five possible sentiment classes (from very negative to very positive). Now I want to define the feature vector for the texts."

In addition to what you suggest, it might make sense to also include the mean as a feature.  

"My first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier, then just populate it with the values of the probability distributions. Does this make sense? Wouldn't I be somehow losing the representativity of the sentiment magnitude?"

Do you mean representing the probabilities as a vector (i.e. [0.1, 0.3, 0.2, 0.2, 0.2]) fails to take into account the fact that the scores are ordered - that 1 and 2 are more similar than 1 and 5?  

While this is a valid concern, I don't think that it will be a big deal if you have enough data.  It is common to put numerical features into buckets when using linear models.  This has the same issue but it works well in practice.  
-------------
Question=  290 Would anyone be willing to look over my simple NN Octave code?
-------------
Question=  291 Problem: simulate a mechanical device that outputs numbers from 1-100.
selttext=  So i ve recently gotten into statistical modeling and this problem popped into my head. So here s the rest of the setup:

the device outputs a list like: 5,34,65,88, 07, 32 without replacements and no repetition

So according to my readings this can be thought of as the classical ball and urn model and so the whole theory of hidden markov models can be used to develop a solution to this problem. So what I'd like is reassurance that yes I m on the right track with a solution to this and also what software should i use to actually try and solve this.
comment no.= 0 -->  When I first read your problem I thought of hidden markov models.  Another approach may be recurrent neural networks.
-------------
Question=  292 Common mistakes for beginners? My ANN is converging poorly.
selttext=  I'm getting into machine learning, and tried implementing a neural net in Java based off of [this](http://neuralnetworksanddeeplearning.com/chap1) online book. I've tested two different training scenarios - one was simply adding two numbers between 0 and 9 together, and the other was recognizing handwritten digits from the MNIST data set. The problem is convergence is slow and tops out at a fairly low level (~70-80% correct answers for addition and 25-35% for MNIST data). It's obviously working somewhat because I'm getting more correct answers than I would by chance alone, but I feel like I should be getting a lot more. The addition problem is easy and the book shows a python implementation getting over 90% correct after a single training epoch. Varying learning rate and size/number of hidden layers hasn't helped much (also I feel like I need to have a pretty high learning rate for things to get anywhere, like 1-10). I've been over and over my backpropagation algorithm many times and can't find any mistakes. Is there anything obvious I may have missed or should recheck? Any help would be much appreciated.
comment no.= 0 -->  You may want to try the UFLDL tutorial's technique for checking your feed-forward network code:

http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization


comment no.= 1 -->  You should be getting a lot more.

If you are doing the same number of iterations as that online book, you should be getting the same results. For a similar architecture, you can get up to 98.4% accuracy on MNIST (eg http://deeplearning.net/tutorial/mlp.html).

You clearly have a bug. I would write unit tests for the pieces until you find it. Good luck!
-------------
Question=  293 What class or resource do you recommend after Coursera Stanford ML course.
selttext=  The main question is in the title. For the interest of keeping the post generic it's really all you need read or address. It would help if you said why recommend and what background you recommend having before starting. below the line I give some personal background.

----------------------------------------------
If you read this and want to offer suggestions specific to me that would be great!

I'll be completing stanford's ML learning course through Coursera in the next couple weeks. I'm curious what would be the next logical thing for me to learn to deepen my knowledge of the subject.

Here is a break down of my background:

* Bs in CS by the end of the year
* Proficient in python
* Familiar with ruby, octave, R, python-pandas, python numpy, C, C++
* Math:  linear algebra and Calc 2.

Here is a list of online courses i'm familiar with:

* [Gerogia tech ML through Udacity](https://www.udacity.com/)
* [Caltech ML course](http://work.caltech.edu/telecourse.html)
* [Carnegie Mellon](https://www.cs.cmu.edu/~tom/10701_sp11/hws.shtml)

p.s, I'm sure this question has been addressed somewhere else, feel free to re-direct me. I'll also keep searching. Thanks for the help
comment no.= 0 -->  The CMU course was my favorite
comment no.= 1 -->  I think the [Unsupervised Feature Learning and Deep Learning Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial) is a good follow up.
-------------
Question=  294 Study note automation
selttext=  Hi everybody. Should start out by saying that I am new to ML. I have a strong background in stats and calculus, but none in programming (I'll be taking the coursera class in programming this June).

For a while now, I've had an idea for a program thats been bouncing around in my head. Specifically, I want to create a program that takes text (likely in .pdf or .doc formats) and convert it into a series of questions and answers.

For example, given the following:
>text- The capital of Canada is Ottawa. 
>converted into Q&A format- Q: What is the capital of Canada? A: What capital of Canada is Ottawa.

Any suggestions for starting this would be GREATLY appreciated. I know there are a lot of courses out there, but if someone could narrow down what areas I should focus on, then I could take a targeted approach to learning about what I need to know.

Thanks!

comment no.= 0 -->  Two quick comments.

First, you would probably want to deal with raw text rather than proprietary formats. There are tools to extract text from doc and pdf files, but they are error prone.

Second, this isn't so much a machine learning task as it is a syntax task. See any good NLP/linguistics/syntax textbook on the topic of Wh- movement, and you'll see that there's most likely a simple rule-based solution to this problem.
-------------
Question=  295 Question about clustering of word vectors (X-post from /r/MachineLearning)
-------------
Question=  296 Difficulty training simple XOR function.
selttext=  I've created a neural network, with the following structure:

Input1 - Input2 - **Input layer**.

N0 - N1 - **Hidden layer**. 3 Weights per node (one for bias).

N2 - **Output layer**. 3 Weights (one for bias).


I am trying to train it the XOR function with the following test data:

* **0 1** - desired result: **1**
* **1 0** - desired result: **1**
* **0 0** - desired result: **0**
* **1 1** - desired result: **0**

After training, the **mean square error** of test (when looking for a 1 result) {0, 1} = 0, which is good I presume. However the mean square error of test (when looking for a 0 result) {1, 1} = 0.5, which surely needs to be zero?

 During the train stage I notice the MSE of true results drops to zero very quickly, whereas MSE of false results lingers around 0.5.

I'm using back propagation to train the network, with a sigmoid function. The issue is that when I test any combination after the training, I get a ouput result of **1.000014 for true**, and **1.000104 for false**. 

I'm trying to get 1 for true, 0 for false. The network seems to learn very fast, even with an extremely small learning rate.

If it helps, here is the weights that are produced, with a learning rate of **0.1**:

N0-W0 = **-0.999**, N0-W1 = **0.655**, N0-W2 = **0.304** (**bias weight**) - Hidden Layer

N1-W0 = **0.674**, N1-W1 = **-0.893**, N1-W2 = **0.516** (**bias weight**) - Hidden Layer

N2-W0 = **2.135**, N2-W1 = **2.442**, N3-W2 = **1.543** (**bias weight**) - Output node

Back propagation steps:

1. **Feed forward a feature set, summing the weights x input set. Calculating sigmoid per node.**

2. **Apply bias.**

3. **Calculate output node error, desired output - actual output (sigmoid).**

4. **Back propagate error, layer above error x connecting weight. Per node.**

5. **Adjust weights, repeat till MSE low enough.**

 
Apologies for the long post, but I've been scratching my header over this for awhile now, and I can't determine what is wrong with my back propagation algorithm. Thanks in advance.
comment no.= 0 -->  Can you show me your code?  Its unclear what order you're doing some of the steps in the feed-forward process.  And also you need to backpropogate through the whole network.
comment no.= 1 -->  The bias is just another input to the neuron.  Don't treat it special (besides always getting an input value = 1).  Also, does your code work for the OR and AND cases?
comment no.= 2 -->  I just ran XOR on a 2-2-1 NN and got these weights

            h1        h2
      w0    1.4933    7.1091    //bias weight
      w1    2.2253   -4.3301    //weight to input 1
      w2   -0.6427   -5.3821    //weight to input 2

            o1
      w3   -10.3734    //bias weight
      w4    -5.2024    //weight to h1
      w5    10.0029    //weight to h2

Throw these numbers into your code to see if your forward propagation code works.  Results that I get using sigmoid units are:
    
        0.0097    // Result for 0 0
        0.9922    // Result for 0 1
        0.9871    // Result for 1 0
        0.0136    // Result for 1 1
-------------
Question=  297 Neural network learning fast, giving me false positives.
selttext=  I've recently started implementing a feed-forward neural network and I'm using back-propagation as the learning method. I've been using http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html as a guide.

However, after just the first epoch, my **error is 0**. Before using the network for my real purpose I've tried with the simple network structure:

* 4 binary inputs, 1, 1, 0, 0.
* 2 hidden layers, 4 neurons each.
* 1 output neuron, 1.0 should = valid input.

Each training epoch runs the test input (1, 1, 0, 0), calculates the output error (sigmoid derivative * (1.0 - sigmoid)), back propagates the error and finally adjusts the weights.

Each neuron's new weight = **weight + learning_rate * the neuron's error * the input to the weight.**

Each hidden neuron's error = **(sum of all output neuron's error * connected weight) * the neuron's sigmoid derivative.**

The issue is that my **learning rate has to be 0.0001** for me to see any sort of 'progress' between the epochs in terms of lowering the error. In this case, the error starts around ~30.0. Any greater learning rate and the error results in 0 after the first pass, and thus results in **false positives**.

Also when I try this network with my real data (a set of 32 audio features from sample - 32 neurons per hidden layer) - I get the same issue. To the point where any noise will trigger a false positive. Possibly this could be an input feature issue, but as I'm testing using a high pitch note I can clearly see the raw data differs from a low pitch one.

I'm a neural networks newbie, so I'm almost positive the issue is with my network. Any help would be greatly appreciated.
comment no.= 0 -->  I really don't know enough to say for sure but it sounds like you are just overfitting. NNs are very good at that. Try using a smaller network or some other generalization method.
comment no.= 1 -->  Randomize your initial weights.  I've had good results with random numbers between +/- 0.5

Give training sets of positive, and negative examples e.g. all instances that will produce 1, and all instances that will produce 0.

Also, each layer needs a bias weight with a unity input.


comment no.= 2 -->  sounds like it's overfitting the training data... have you tried adding a regularization term to your cost function?
-------------
Question=  298 How much data do you need to do ML?
selttext=  I'm personally most familiar with split testing on websites.

But to really do some ML, what kind of data sets do you need?
comment no.= 0 -->  The non-answer answer is that it depends. The more complicated your model is the more data you need in order to avoid over-fitting. I've [read](http://www.amazon.com/Learning-From-Data-Yaser-Abu-Mostafa/dp/1600490069/ref=sr_1_1?ie=UTF8&qid=1396934536&sr=8-1&keywords=learning+from+data) that a good rule of thumb for a supervised linear model is ten training examples for every parameter of the model. It is my understanding that linear models are generally considered relatively simple models so that number should probably increase for other sorts of models.
-------------
Question=  299 8-3-8 Neural Network Does Not Converge.
-------------
Question=  300 How could I beat 2048 with ML? (x-post: /r/MachineLearning)
selttext=  x-post from: http://www.reddit.com/r/MachineLearning/comments/20fmym/how_could_i_beat_2048_with_ml/

There is already an AI for playing the game 2048 here. It uses minimax, but I was wondering about the possibility of using machine learning to make an AI. How would I set up a neural network or something similar to play the game?

I am not very experienced with ML, so I'm looking for advice on how to set up neural network (nodes per layer) as well as what algorithms to use for adjusting weights. Or maybe neural networks aren't the best approach?
comment no.= 0 -->  I'm currently working on doing exactly this. Did anything come of your project?
comment no.= 1 -->  I tried an approach with reinforcement learning using a genetic algorithm to optimize a decision tree forest (https://github.com/Underflow/reinforcement-2048).

I'm really not conviced of my method, and it doesn't do better than 1024.
-------------
Question=  301 Is there any way to find the 'real/effective bandwidth' of an audio file?
selttext=  I have some previously recorded audio that I'm using for some ML projects.    The audio is currently encoded with a certain compression/codec.  But, before that, it may have gone through a number of other codecs/compression algorithms.  For instance, maybe GSM for a cell phone call, then ulaw, then g729. etc.   

Is there a way to calculate the how much useful information is still contained in the audio, as compared to a reference, (like say pcm16).  I don't have access to the original non-compressed files of course.

Seems like there should be some information theoretic approach that would work....

comment no.= 0 -->  I've never worked with this, but my understanding is that useful information is synonymous with Shannon Entropy.

http://en.wikipedia.org/wiki/Shannon_entropy#Definition

If I were doing this, I would start with the negative sum over i of P(xi) * log_b[ P(xi) ] expression.

Here are a couple of approaches which do this:

http://www.kennethghartman.com/calculate-file-entropy/

http://www.kennethghartman.com/shannon-entropy-of-file-formats/

and:

http://stackoverflow.com/questions/990477/how-to-calculate-the-entropy-of-a-file
-------------
Question=  302 Pattern Recognition: Have method, need name.
selttext=  This seems like it might be the best place to post this. I don't know any of the pattern recognition lingo in any real depth, but I need to know if there is a name for the algorithm in a program I've already finished. If there isn't, then I need to know the closest thing to compare it to. 

I have a simple signal of one variable vs. another. I fit a number of polynomials at various places in the signal to extract some points, and simplify the signal by some fundamental simple shapes which pass through these points. This seems to be something like a Hough Transform.

I then calculate a few hundred attributes, based on these shapes for the set of all signals. For instance length of a side or angle between two sides and so on. I have categorized by eye a subset of these signals into two categories, good or bad. I use this training set to find upper and lower limits acceptable for these attributes, and apply a boolean test for all the attributes of all the signals using the test set limits.

In other words finding the box of dimension (# of attributes) that the training set lives in and reporting which events of the total set are within it. This defines the output set of good signals for which there are no false negatives, and very very few false positives (with these signals anyhow). This seems to be something like Linear Discriminant Analysis with hard limits instead of dealing with probabilities. 

I then do various operations to find the smallest set of these cuts which return the exact same output. This seems to be like Feature Selection.

What do you think? Is there possibly a name for this exact thing overall? Have I gotten close with the sub-method names?
comment no.= 0 -->  Your description is a bit too vague. You say that 

> I have a simple signal of one variable vs. another.

Which makes it sound like you have 2 variables in total (i.e, you could plot all of your dataset in a 2D plane) but the rest of your text absolutely doesn't sound like that. I haven't got the foggiest what you mean by "fitting polynomials at various places in the signal", so I'm just going to ignore that part and assume  that you are dealing with a set of geometric shapes, because that's what it sounds like.

Then, you obviously totally confuse training and testset (maybe just a typo), because apparently you find some thresholds for some attributes on a trianingset, and then apply a boolean test "using the *test set* limits"???


> This defines the output set of good signals for which there are no false negatives, and very very few false positives (with these signals anyhow).

I don't know why you think there are no false negatives in that set


> This seems to be something like Linear Discriminant Analysis with hard limits instead of dealing with probabilities. 

absolutely not. First of, LDA isn't dealing with probabilities either, but more importantly: what you're doing sounds nothing like LDA. 


From what I've understood of your method, it is probably most similar to a decision tree.



-------------
Question=  303 Pros and cons of user-based vs item-based collaborative filtering
selttext=  So let's say you have a bunch of nondescript users and a bunch of nondescript items. You want to perform collaborative filtering in order to recommend user A a new item. No metadata is used in any case.

I'm trying to better understand when you'd do user-based vs item-based collaborative filtering. User-based collaborative filtering makes sense to me - given a user, you match him/her up with all other users and come up with a similarity-weighted average rating of all songs, at which point you can find the most highly-rated thing that the user hasn't seen yet.

What are the pros and cons of using one vs the other? If you construct the matrix of ratings, with let's say users as columns and items as rows, both end up being extremely similar - in user-based CF, you look at correlations between user columns (using cosine similarity or something similar), and in item-based CF, you look at correlations between item rows (using the same thing). What applications make sense for using one vs the other?
comment no.= 0 -->  It seems to me that user-based or item-based are just inverses of each other.

In terms of the approach that makes the most sense to me -- clustering -- you would have two possibilities.  Given *m* users and *n* items:

* Users are a point in *n*-dimensional item space, where each of the *n* dimensions is that user's rating for the *n*th item

* Items are a point in *m*-dimensional user space, where each of the *m* dimensions is that item's rating given by the *m*th user

The former lets you find users who are similar to each other based upon their item ratings, allowing you to make predictions for the unrated items by comparing this user to their most similar users and how they rated that item, and making the assumption that similar users will continue to make similar ratings.  i.e. "Here are the users most like you", or going one-level deeper, "Users most similar to you prefer these items", or for the NSA: "person A is a terrorist; he is most similar to these people in his buying or rating patterns, so check them out as persons of interest".

The latter lets you easily find items who are similar to each other based upon their ratings by users, allowing you to make predictions for the users who haven't rated that item by comparing this item to the most similar items and how the user rated that item (which is just the inverse of the previous paragraph), and making the assumption that similar items would be ranked similarly by similar users.  i.e. "Here are the items most similar to this item", or going one-level deeper, "items similar to this were bought by these users", or for narcotics police: "item A is discovered to be a mind-altering substance; these items are most similar based on user buying patterns, so investigate them as potential mind-altering substances as well", or perhaps "item A is a mind-altering substance; items similar to this were bought by these users, so check them out as potential drug users".

**It's just two sides of the same coin.**

EDIT: removed Amazon examples -- they weren't working out for me.  :-P
-------------
Question=  304 nonparametric sequential hypothesis testing
-------------
Question=  305 Are Markov-chain models of language used for anything productive?
selttext=  I've encountered Markov models of language before, and they are very entertaining because they can be used to generate sequences of realistic sounding nonsense, but do they have any other uses?
Are these models ever employed in machine translation or speech recognition, or have they been superseded by something better?

Also, I recall there being some kind of markov chain tool in Ubuntu but I can't remember the name. If you know it, please share :)
comment no.= 0 -->  Yes, those language generation tools essentially sample from an [n-gram markov model](http://en.wikipedia.org/wiki/N-gram).
n-grams are used to model the prior probability of a target sentence in speech recognition, handwriting recognition, machine translation, etc.
Often some kind of discounting is used to assign some probability to n-grams that have not been observed during training.

There are of course alternative approaches to language modelling, mainly neural networks. Especially recurrent neural networks should be suited for this.
comment no.= 1 -->  They are used in word prediction. For example, autocorrect on mobile keyboards. There was a really cool keyboard app I saw awhile ago that could fit in a very small space and relied on predicting what you meant to type. Google search uses them to make suggestions and detect misspellings. I *think* they might use something like them in Google translate but I'm not certain.

I don't know if they are used in speech recognition but it's certainly a good application. E.g. is it more likely to user said "I recognize *beach*" or "I recognize *speech*." It's also theoretically possible to use them to get good text compression, but I don't know if anyone actually does that.
-------------
Question=  306 SVR advice wanted.
selttext=  I am trying my hand at a [Hackerrank](https://www.hackerrank.com/challenges/predicting-house-prices "Challenge link") challenge. After determining after a couple of days that writing my own linear regression in C was going to be rife with trouble I decided I would try to use an SVR from libSVM.  

After playing with SVM toy for a couple of minutes I felt like I had a few configurations worth giving a shot. But when I try them on the test data the resulting plane seems to be fairly horizontal and near the average of all the training values. 

For more specifics I am using the libSVM nu-SVR and experimenting with both the various kernels and tweaking gamma. Pointers and links to explanations to help me better understand what I'm missing would be most appreciated. Thanks.
comment no.= 0 -->  What parameters for the nu-SVR are you using?  Its possible you're regularizing too much.  Have you gotten r^2 or mean absolute errors of your cross-validation outputs?  

EDIT: it would also be worthwhile just to find an implementation of OLS and put your data through that to see what the results look like.
comment no.= 1 -->  Just replying to let any passers by know that It was a piece of cake to solve this problem with Scikit-learn. The hardest part was using Python with nearly no previous experience. If Scikit-learn continues to be this straightforward I look forward to being very grateful for the recommendation. For now consider this problem solved.

-------------
Question=  307 Why is machine learning important and how is it applied in business?
selttext=  Background: I am an business analyst. My analysis to this point has been mainly restricted to excel and access, with some use of business objects and SQL. In my personal life, I am learning more about computer science and practicing python. My degree was in Anthropology and Poli Sci.

Complete ML noob.
comment no.= 0 -->  It becomes more difficult to see trends, and to make intelligent decisions from data, when the amount of data increases beyond what a person or team of people can handle. In order to make the most of a large amount of data, it is necessary to automate the processes we use to analyze, and make decisions from it.


comment no.= 1 -->  I'm not sure what sector of business you're in, but I know finance uses a lot of machine learning.

Consider the idea of stock prediction.  Say you want to predict the stock price at the end of the day.  What types of information may be useful?  The price at the previous day would be useful, the price the day before would be as well.  Stocks in similar industries may also be useful.  You can then use these pieces of information together to build a predictive model of what the stock price at the end of the day will be. 

In general more information is used as well and I can get you some links for that if you would be interested.
-------------
