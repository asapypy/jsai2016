 best algorithms for learning on sparse data ? : mlquestions i have a dataset that i have collected with ~ NUM binary features and less than NUM % of them are NUM 's . i did a quick search to see if there are algorithms that are particularly good at working with sparse data and only found stochastic gradient descent . are there any others that are particularly good at working with sparse data ? EOQ not sure of your goals but recommendation engines/recommender systems are built on this premise . EOA 
 non-restricted boltzman machines : mlquestions how are non-restricted boltzman machines trained ? what are they used for ? are they used at all ? EOQ the maximum likelihood gradient of any boltzmann machine learning breaks down into two terms , the positive phase and negative phase . part of the reason that rbms are comparatively tractable is that an unbiased estimate of the positive phase statistics can be had in closed form . the negative phase statistics need to be estimated via sampling (it is an expectation under the distribution p(v)) , and because you can't sample p(v) directly , you need to run a markov chain , the most popular method being to do block gibbs sampling of p(h|v) and p(v|h) alternating . deep boltzmann machines are another form of restricted topology where things break down into layers . they can be trained , by the method outlined in that paper ( involving rbm pretraining ) and also jointly all at once in a few different ways . neither the positive phase statistics nor the negative phase statistics are tractable but people have successfully used mean field approximations for the positive phase and monte carlo ( you can do block gibbs sampling by sampling the odd layers given the even layers and vice versa ) . training general , fully-connected boltzmann machines is hard because there's no block structure you can exploit for efficient sampling . this tech report outlines a procedure that can apparently train at least some reasonably-sized general boltzmann machines . boltzmann machines ( including rbms ) aren't really used for much anymore ; rbms were used for pretraining layerwise deep neural networks but it's become crystal clear that they are unnecessary most of the time ( there are a small number of tasks where rbm-pretrained networks still reign supreme for the time being , apparently, notably some large-vocabulary speech recognition tasks i think ) . the exception might be collaborative filtering , i know this variant is supposedly one of the models in production at netflix . dbms were an active area of research but interest in undirected graphical models for deep learning seems to have dried up . EOA 
 help understanding hiddeen markov models : mlquestions i'm a bit confused about inputs and outputs when it comes to hidden markov models . say there is a general problem of some sequence x with some labels y . for example , x can be a sequence of words , and y can be their parts of speech tagging . let's assume that the alphabet ( state space ) for x is size d and the alphabet of y is size l . say i want to create a hidden markov model ( hmm ) to infer y from x ( i.e., guess parts of speech from words ) by training a transition matrix a and an observation matrix o . is the observation matrix the probability of seeing an x given a y or the other way around ? is the transition matrix a the normalized transition between the x states or is it between the y states ? do we normalize it for all previous states , or for all current states ? in other words , is the transition matrix a lxl matrix or a dxd matrix ? EOQ you are talking about learning an hmm with fully observed data . can i refer you to a few pages of kevin murphy's book ? it's a simple read through . and the transition matrix will be lxl , the other matrix would be an emission matrix which would be either dxl or lxd . EOA 
 detect/count persons in picture of a vehicle , what method ? : mlquestions hello ! i am looking for a method to detect/maybe count people in a grayscale-photo of a vehicle . example picture , but with backseat included . we could change the viewangle as we wish : URL there are a lot of available methods but we are unsure of which have the potential to work decently well and are quite “easy” to implement since we are quite new to the area . we are mainly working in c-or python . we have been looking on some open source libraries like opencv etc . we got a tip that machine learning , random forest , neural networks might work in some extent . there are also a bunch of object detection algorithms like cascade classifiers “traincascade”(haar , hog or lbp) . and at last there are a few face detection algorithms ( fisher,eigen or lbh ) . we don’t have the time to test all of the methods and would like to know what you would choose or not choose for the task . EOQ nice try copper . deep learning is best for image recognition . look at software ( they have turorials ) such as torch , theano, and cafe . EOA 
 automatically changing regularization level during training ? : mlquestions so i've been playing with neural nets in lasagne , which outputs the train error/validation error ratio during training . it seems like a very useful way to tell if your nn is suffering from high bias or suffering from overfitting . anyway , i noticed i would often see that the error ratio was too low , then regularize the net with a higher dropout p or weight decay term and start training over . this seems like it could be very easy/beneficial to automate . why not just average the train/val error ratio of the past NUM epochs and change the dropout probability/weight decay term size accordingly ? ( eg if the val error is too much higher than the training error , increase dropout p and l2 regularization size. if the val error is too much lower then training error , decrease dropout p. ) it seems like doing this would help by optimizing the regularization level so the net will never suffer too high bias or too high variance . the main thing i'm worried about is it will somehow lead to the net sort of over fitting the validation set . i also don't have nearly enough programming skills to actually implement and test this myself , so can you folks offer any insight ? -is this a good idea , or will it lead to poor results on the test set ? -if it will end up generalizing badly , how/why? -has something like this already been done ? EOQ i don't it's a bad idea , it might work out . this is somewhat similar to a learning rate schedule , e.g. halve the learning rate when validation loss stops improving . EOA 
 sparkit-learn random forests ? : mlquestions is it possible to use sparkit-learn to build random forests on a spark cluster ? does anybody have an example of this ? EOQ sounds like this group tweaked mllib ( unfortunately no syntax ) . URL EOA 
 sparkit-learn random forests ? : mlquestions is it possible to use sparkit-learn to build random forests on a spark cluster ? does anybody have an example of this ? EOQ i use mllib with python right now . it works and i have been getting good results using their random forests , but some features that scikit-learn has would be nice. like out of bag error , feature importance , ... also some of the stuff that you are able to set using mllib with java hasn't been implemented to work with python yet such as setting stopping criteria . EOA 
 algorithm to implement ? : mlquestions hello , i am working on a project where i take data to train an algorithm to predict a behaviour . for example , my training data is a,b,c,d,e. then i apply a type of linear regression to obtain a predictive function from a,b,c,d,e. this new function f(a,b,c,d) should give me an output e . based on my training dataset . the flaw i found is that by using linear regression i am predicting an increasing value and not a constant behaviour . any guidance ? EOQ it could be that your a , b, c , and d are increasing overall as well , so the slope of the linear function that you get from the regression is increasing . if the order is important , there is not much you can do with a simple linear regression , perhaps get more and more diverse training data . however, the predictions will always be on that resulting linear function . if the order is not important , you could try to shuffle the data . another approach would be to use non-linear regression . EOA 
 can someone explain thresholding an image for me ? : mlquestions im doing an image recognizion task and i think thresholding would work . i need to make a program that detects bright spots on an image and circle them(active neurons in a calcium probe image) . im pretty new to these things . thanks. EOQ at its most basic , if the luminance value of a pixel is above the defined threshold , then set it to white , otherwise set it to black . e.g. threshold-NUM ; pixels-[ NUM , NUM , NUM , NUM , NUM ] ; threshed.pixels-[ NUM , NUM , NUM , NUM , NUM ] ; EOA 
 can someone explain thresholding an image for me ? : mlquestions im doing an image recognizion task and i think thresholding would work . i need to make a program that detects bright spots on an image and circle them(active neurons in a calcium probe image) . im pretty new to these things . thanks. EOQ typically in neuro , people want to keep activated pixels/voxels above a value or zero out some values . you might also use thresholding to make a mask from an image . if you had a brain or other region that is clearly defined , you can turn it into all NUM 's with NUM everywhere else . then any other image that is in register ( aligned anatomically ) can be masked by multiplication . if this is mri or pet , you can use fsl tool's fslmath to do the thresholding . fslmaths-h has all the options . EOA 
 format question using scikit : mlquestions attempting to put this data into scikit & work with it . i am having a bit of an issue seeing how the data correlates to the features specified . can anyone help me ? here is a link about the data EOQ i'm not sure if i understand the question . do you have problems to see what column corresponds to which feature ? in this case the pandas library might be helpful : import pandas as pd features-[ flength , fwidth, fsize , fconc, fconc1 , fasym, fm3long , fm3trans, falpha , fdist, class ] df-pd.read.csv(magic04.data, header-none , names-features) df.head() this will output the first few rows/datapoints of your data set with the header names as defined in the features list . i also recommend using jupyter notebook for just trying around with datasets . the pandas library outputs datapoints in a neat little table , which makes it a bit easier to see what you are dealing with . if i misunderstood the question , please clarify :) EOA 
 format question using scikit : mlquestions attempting to put this data into scikit & work with it . i am having a bit of an issue seeing how the data correlates to the features specified . can anyone help me ? here is a link about the data EOQ actually you did a great job of answering my question , though this presents a new one . i started a new thread for this question here thanks again for your assistance EOA 
 x-post from /r/machinelearning : question about tensorflow/cifar10 : mlquestions hi guys , i'm currently learning tensorflow . i'm working on the cifar-10 tutorial . i'm a bit confused and wanted to check whether i'm understanding correctly how i would prepare data myself to train the model . here's what i think i should do , i'd appreciate any feedback : first , convert the images to a numpy array of shape height x length x channels ( NUM for rgb image ) . i'm unsure what i'd do with the labels , actually... let's say i call the final file foobar then , i could use read.cifar10(foobar) to read in the image in my code . in the code , the individual images are then arranged in batches the way tensorflow wants all to receive them . is this correct ? if i want to read in more than NUM image , what will i write instead of foobar , let's say the files are called foobar1 and NUM ? EOQ for each batch , you'll want to run tf.train.youroptimizerofchoice(your-learning-rate).minimize(your-cost) the cost function google uses in their examples is cross entropy , in the form of tf.reduce.sum(your.correct.labels-tf.log(your.algorithm's.output)) EOA 
 x-post from /r/machinelearning : question about tensorflow/cifar10 : mlquestions hi guys , i'm currently learning tensorflow . i'm working on the cifar-10 tutorial . i'm a bit confused and wanted to check whether i'm understanding correctly how i would prepare data myself to train the model . here's what i think i should do , i'd appreciate any feedback : first , convert the images to a numpy array of shape height x length x channels ( NUM for rgb image ) . i'm unsure what i'd do with the labels , actually... let's say i call the final file foobar then , i could use read.cifar10(foobar) to read in the image in my code . in the code , the individual images are then arranged in batches the way tensorflow wants all to receive them . is this correct ? if i want to read in more than NUM image , what will i write instead of foobar , let's say the files are called foobar1 and NUM ? EOQ i get how to run it . what i don't get is if i wanted to make my own samples , how i'd do that . i am asking since i'd like to take the cifar code , modify it , and use other images , let's say flowers or whatever , to train it on that . and for that i need to know how to prepare my data . EOA 
 x-post from /r/machinelearning : question about tensorflow/cifar10 : mlquestions hi guys , i'm currently learning tensorflow . i'm working on the cifar-10 tutorial . i'm a bit confused and wanted to check whether i'm understanding correctly how i would prepare data myself to train the model . here's what i think i should do , i'd appreciate any feedback : first , convert the images to a numpy array of shape height x length x channels ( NUM for rgb image ) . i'm unsure what i'd do with the labels , actually... let's say i call the final file foobar then , i could use read.cifar10(foobar) to read in the image in my code . in the code , the individual images are then arranged in batches the way tensorflow wants all to receive them . is this correct ? if i want to read in more than NUM image , what will i write instead of foobar , let's say the files are called foobar1 and NUM ? EOQ use the decoders built into tensorflow ( i'm sure there's one for pngs , and i believe one for jpegs ) ; if i'm not mistaken it returns a heightxwidthxchannels array . i'm not sure of the method names offhand , but they should be available on the api docs . dealing with different image sizes is trickier . you could scale the images so their major dimension matches the max dimension of your network , then zero pad it to meet the minor dimension . or you could run a recursive network on the linearly-arranged NUM d convolutional windows . or depending on your application , you may or may not need to consider the full image in one run anyway and could use another size-agnostic structure . as far as preparing the data , standardizing your inputs to NUM , NUM rather than NUM , NUM wouldn't hurt i suppose . EOA 
 machine learning uses the same few equations over and over : mlquestions the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation to keep out those who are afraid to wade through the math . edit : maybe i was little steamed when i wrote that . :) EOQ cite a few examples ? EOA 
 machine learning uses the same few equations over and over : mlquestions the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation to keep out those who are afraid to wade through the math . edit : maybe i was little steamed when i wrote that . :) EOQ for example , the logistic function , sigmoid function , and softmax function , logistic regression , boltzmann distribution , multinomial logistic regression , softmax regression , maximum entropy classification , multinomial logit , neural networks , entropy, cross-entropy , kl divergence , bayes' theorem . then throw in things like maximum likelihood estimation , lagrange multipliers , some common probability distributions (normal , binomial, poisson , etc.). don't forget rms error , bias, variance . then stir , and you have like NUM % of machine learning . i realize i'm being a bit flip ( and not rigorous in my list above ) , but still , i think ml could be streamlined from the typical NUM page textbook to about maybe NUM-40 pages of condensed math and a little text . this assumes one already knows college-level calculus , a little linear algebra , and basic probability . EOA 
 machine learning uses the same few equations over and over : mlquestions the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation to keep out those who are afraid to wade through the math . edit : maybe i was little steamed when i wrote that . :) EOQ you are right in some ways : the actual math underlying machine learning is fairly narrow . the catch is that since machine learning is a bit of a multi-disciplinary field ( and in particular all manner of fields from biology , to psychology to economics etc . dabble in machine learning ) you tend to see a lot of re-invention of the wheel coming at what boils down to the same problem in a different way with different intuitions . the problem looks different because you're working in different field and use entirely different nomenclature and are unaware of the results in field x , so you just cobble together a solution with tools use have using the terminology that makes sense to you . rinse and repeat this many times and ... this tends to result in the same underlying math explained again and again in myriad ways with different , overlapping and sometimes incompatible terminology and notation . EOA 
 machine learning uses the same few equations over and over : mlquestions the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation to keep out those who are afraid to wade through the math . edit : maybe i was little steamed when i wrote that . :) EOQ i would also like to see some examples . i'm imagining that you're describing something like the fact that the linear regression equations appear over and over again-they do ! the basic concept of a linear model is the intuitive jumping off-point for just about every supervised learning model . the fact that this is a pattern we can exploit is wonderful and for some people ( myself included ) very counterintuitive. we use these over and over again because they are well-studied models . i find it difficult to believe any claims that it's simply a matter of spiteful obfuscation on the part of authors , though. there are many alternative hypotheses that fit the data here , such as : NUM ) the same equations appear over and over again , but the variations between each usage are hard enough to systematize that we have yet to find an overarching system which totally describes all of them . NUM ) most papers are written by experts in the field , who have seen these equations and patterns before . when they write their papers , they purposefully reuse ideas and equations , both because it gives them a theoretical base to build on and a formalism that their audience will already be familiar with . note that in this case , it's the opposite of your claim-the various authors actually go out of their way to present the material in a familiar way , knowing that it would aid understanding to do so . moreover , why on earth would someone spend months or years of their life researching something , only to publish a paper or a book which is intentionally obscure ? EOA 
 machine learning uses the same few equations over and over : mlquestions the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation to keep out those who are afraid to wade through the math . edit : maybe i was little steamed when i wrote that . :) EOQ i posted a list in another reply ( above ) . mostly i was thinking about a few basic ideas around , for example , logistic regression , entropy, bayes , and max likelihood are used to produce many different ml modeling approaches , but the core math could probably be written down in under NUM-40 pages , maybe much less . as far as experts' motivations go , i think the field splintered along the way , with experts in their own subfields using their own terminology , and not necessarily realizing that they're studying the same things . consider that claude shannon's big insights came about because he spent time interacting with researchers from different fields like genetics , quantum and statistical mechanics , signal processing , cryptography, and was able to show how they're related through entropy . EOA 
 machine learning uses the same few equations over and over : mlquestions the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation to keep out those who are afraid to wade through the math . edit : maybe i was little steamed when i wrote that . :) EOQ the single most useful thing you could do with this point of view would be to write those NUM-40 pages . i've been in the field for a couple of years and would kill to read the essential machine learning mathematics and its relationships ( hell , even if you start a blog , i'd appreciate a link ) EOA 
 machine learning uses the same few equations over and over : mlquestions the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation to keep out those who are afraid to wade through the math . edit : maybe i was little steamed when i wrote that . :) EOQ it seems that quite a few people are on this quest : URL EOA 
 if glm performs better than gbm or rf , what does that mean ? : mlquestions hi , i was asked this question in interview and i admit i'm not sure about my answer . what would be yours ? if glm performs better than gbm or rf , what does that mean ? i think the answer expected was something about the data , and maybe a solution . cheers note :-glm : logistic regression-gbm : gradient boosted trees-rf : random forest ( of trees ) EOQ google is your friend my man ... URL but the short version is it means there is a clear linear separation in the logistic regression via slope , whereas the data is not conducive to a clear split for decision trees which ( for ease of explanation ) are parallel to each axis . EOA 
 if glm performs better than gbm or rf , what does that mean ? : mlquestions hi , i was asked this question in interview and i admit i'm not sure about my answer . what would be yours ? if glm performs better than gbm or rf , what does that mean ? i think the answer expected was something about the data , and maybe a solution . cheers note :-glm : logistic regression-gbm : gradient boosted trees-rf : random forest ( of trees ) EOQ thanks . i had done the homework and found similar answers , but this one is definitely the best . EOA 
 interpreting the results of lstm-based recurrent networks : mlquestions does any of you know , if any research has been done interpreting the results of lstm-based recurrent networks and linking them back to the features that were the input to the model ? EOQ visualizing and understanding recurrent networks , URL EOA 
 interpreting the results of lstm-based recurrent networks : mlquestions does any of you know , if any research has been done interpreting the results of lstm-based recurrent networks and linking them back to the features that were the input to the model ? EOQ thank you ! EOA 
 which is your favorite tool for machine learning and predictive modelling when working with r/python ? : mlquestions some examples would be : r-caret , h2o, etc . python-scikit-learn , pybrain, etc . EOQ numpy EOA 
 best starting place for beginners ? : mlquestions i'm NUM and starting to learn about machine learning , i am currently read artificial intelligence a modern approach but after that what should i do ? EOQ i'm by no means an expert , but learning python , or another similar language , so that you can actually implement the stuff they talk about in the books is probably a safe bet . EOA 
 best starting place for beginners ? : mlquestions i'm NUM and starting to learn about machine learning , i am currently read artificial intelligence a modern approach but after that what should i do ? EOQ i'm quite advanced in python already as i am doing it for my gcse computing as well as learning it at home however , i haven't really delved in to any modules which i guess could be the next step . thank you for the suggestion . EOA 
 best starting place for beginners ? : mlquestions i'm NUM and starting to learn about machine learning , i am currently read artificial intelligence a modern approach but after that what should i do ? EOQ copy . well this is a good start i think URL EOA 
 best starting place for beginners ? : mlquestions i'm NUM and starting to learn about machine learning , i am currently read artificial intelligence a modern approach but after that what should i do ? EOQ you're NUM and already understand stochastic calculus , linear algebra , and optimization theory ? impressive. EOA 
 aren't features more important than any particular algorithm or ml method ? : mlquestions hi all , i just finished my first neural network ( ng's coursera assignment , so as basic as possible , but still quite cool to me ) , and i was curious about how these can be scaled when it comes to object recognition , etc. this assignment is digit recognition on a NUM x20 image , a mere NUM-element input array . but real-world images are exponentially bigger than this . so my question is , if individual pixels aren't used as inputs , how do features get extracted from images for use in neural networks ? it seems to me that learning how to develop quality features is a better investment of my time while trying to learn ml than diving deep into the particular algorithms . is there any validity to that ? for example , i learned of josh tenenbaum's name from the ml : a probabilistic perspective , and on his front page there's an intro of how children learn to distinguish a horse from just a handful of examples ( which seems so common sense to me , but in the perspective of machine learning is quite remarkable ) . but is the problem that there's no algorithm that can adequately generalize after such few training points , or is it that the features being fed to these algorithms don't encapsulate enough information to allow such few training points ? i really appreciate your time and any insight you can give me . cheers EOQ for image work and neural networks , they often use convolutional neural networks ( cnns ) to find features that are combinations of individual pixels . as the cnns are trained to find useful features , you get things like vertical edge or diagonal edge . these then become the features used by subsequent neural network layers . i'm not an expert-i've just read about this in one of my classes . EOA 
 aren't features more important than any particular algorithm or ml method ? : mlquestions hi all , i just finished my first neural network ( ng's coursera assignment , so as basic as possible , but still quite cool to me ) , and i was curious about how these can be scaled when it comes to object recognition , etc. this assignment is digit recognition on a NUM x20 image , a mere NUM-element input array . but real-world images are exponentially bigger than this . so my question is , if individual pixels aren't used as inputs , how do features get extracted from images for use in neural networks ? it seems to me that learning how to develop quality features is a better investment of my time while trying to learn ml than diving deep into the particular algorithms . is there any validity to that ? for example , i learned of josh tenenbaum's name from the ml : a probabilistic perspective , and on his front page there's an intro of how children learn to distinguish a horse from just a handful of examples ( which seems so common sense to me , but in the perspective of machine learning is quite remarkable ) . but is the problem that there's no algorithm that can adequately generalize after such few training points , or is it that the features being fed to these algorithms don't encapsulate enough information to allow such few training points ? i really appreciate your time and any insight you can give me . cheers EOQ ah ok , neural networks to do a kind of signal conditioning . thanks for the input that helps a lot ! EOA 
 what are some good resources for someone looking to delve deeper into neural networks after having taken an introductory undergraduate course in ml . : mlquestions i am familiar with feed forward networks and how they work and am even taking a graduate course on neural nets this coming spring semester but would like to do some reading in the mean time. the thing is , a vast majority of the posts/papers i read over on /r/machinelearning about the topic are way over my head . does anyone have any suggestions ? thanks! EOQ if you're explicitly interested in neural networks then i'd recommend geoffrey hinton's ( one of the pioneers in deep learning ) course on coursera which goes through a lot of the important topics relating to nns . you'll find the course with the name neural networks offered by university of toronto-sorry can't get the link on mobile-EOA 
 what are some good resources for someone looking to delve deeper into neural networks after having taken an introductory undergraduate course in ml . : mlquestions i am familiar with feed forward networks and how they work and am even taking a graduate course on neural nets this coming spring semester but would like to do some reading in the mean time. the thing is , a vast majority of the posts/papers i read over on /r/machinelearning about the topic are way over my head . does anyone have any suggestions ? thanks! EOQ URL EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ scikit learn and ipython notebooks EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ i already bookmarked this reddit post to read at my leisure . great place to start ( i'm new to ml as well ) , but i would also recommend coursera.org ml courses to any beginner . EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ elements of statistical learning is a good book . EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ an introduction to statistical learning is an even better book for beginners . EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ isl doesn't have a chapter on neural networks though .. EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ that's true . however, i think a thorough understanding of at least the first six chapters in isl are a basic requirement before one can efficiently work with nns . imho these are much better accessible for beginners than the equivalent content in esl . at the end , both books are freely available . hence, it doesn't hurt to have a look at both or even switch back and forth . EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ nice !! u've marketed this book really well :) i'll try reading it when i can :) EOA 
 looking for free offline resource to learn machine learning and prerequisite knowledge : mlquestions can anyone point me to any ? i'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i'd prefer something in the epub or mobi format , but pdf works as well . EOQ additionally for intro to deep learning , u could look at andrew ng's notes on deep learning . yoshua bengio is also writing a book on deep learning , he explains well , it would be a good resource to look into . EOA 
 rnns as generative models : mlquestions i was going through and in section NUM .2 he states an rnn defines a generative model over sequences if the loss function satisfies l(zt ; yt )-log(p(yt ; zt )) for some parameterized family of distributions p(・ ; z) and yt-vt-1 ( emphasis mine ) does this mean that the rnn will not act as a generative model if we use , let's say , the squared loss function ? EOQ-log(p(yt ; zt)) is the negative log likelihood . it does not specify a specific loss function ( maybe you missread and thought that it is cross-entropy? ) it is also a generative model if the loss function satisfies l(zt ; yt)-p(yt ; zt) for ... ( adding the log doesn't change anything , as log is monotonous and increasing function ) . maybe related to your question : we write the mean squared error without the log because it doesn't help . cross entropy has exponential terms so there are computational issues because of limited precisions of floats , right? but squared loss is a sum so there isn't such a problem . please , correct me if i'm wrong . EOA 
 how to use neural network for my android app ? : mlquestions i made a neural network using this tutorial . the problem is i don't know how to implement it in my app . my app will take a picture of a number and send that number to this network . i don't know how to get the output out of this network . EOQ the picture of the number will be translated to a long line of numbers ( a number per pixel , each in the range [0 , NUM ] according to nmist specifications) . these numbers will be inputted in your nn ( it should have as many inputs as pixels in the pictures ) . as the possible displayed numbers on the pictures of the nmist data set are in the range [ NUM , NUM ] your nn should have NUM outputs . after training , only one of those outputs should be above NUM % (larger then NUM ), which is the number the nn thinks it is . NUM -> ; < ;-NUM NUM -> ; < ;-NUM NUM -> ; < ;-NUM ... nn ... n-1-> ; < ;-NUM n -> ; < ;-NUM EOA 
 how to use neural network for my android app ? : mlquestions i made a neural network using this tutorial . the problem is i don't know how to implement it in my app . my app will take a picture of a number and send that number to this network . i don't know how to get the output out of this network . EOQ if you haven't looked at tensorflow you should , that is the easiest way to get a neural net on android other than coding a complete package yourself . EOA 
 how do you keep track of the progress of a machine learning project ? : mlquestions i am talking about what features and algorithms you use , their performance etc . EOQ best way is often to use r markdown language ( or whatever software you use ) . you have the syntax and output but you can also write a paragraph opener and closer in each section that will describe what you did and why and the final results . EOA 
 how do you keep track of the progress of a machine learning project ? : mlquestions i am talking about what features and algorithms you use , their performance etc . EOQ i use python and jupyter notebooks . apart from what to use to document your progress and your process , i am talking more about the following : do you keep everything in one notebook/script/file when do you start another notebook notebook/script/file how do you name your notebooks/scripts/files to be descriptive of what you are doing EOA 
 do i need a phd ? : mlquestions is a phd required to do work in machine learning ? EOQ no . is that a long enough answer ? there are data scientists with bachelors , masters, and phds . hilary maso , one of the most noted data scientists only has a bachelors . it's all contingent on your ability to learn and apply information . EOA 
 do i need a phd ? : mlquestions is a phd required to do work in machine learning ? EOQ i think that's more the exception rather than the rule . it's true , you can be a high school dropout and work in machine learning , but if you want to be paid for that work and don't have an existing body of work you can point to as evidence of your experience , most places are going to require a masters or higher . i see very few job postings asking for anything less . EOA 
 do i need a phd ? : mlquestions is a phd required to do work in machine learning ? EOQ oh i concur absolutely that a graduate degree helps . it's a huge foot in the door . due to the shortage of data scientists and analytics maangers ( according to mckinsey NUM k , NUM m respectively ) , i don't think it will be impossible for someone to get a job with a data science title if they can articulate the body of evidence that makes them qualified e.g. github , blog, etc . if they don't have an advance statistical/cs degree . i'll also add that having an advanced degree in industrial/organizational psychology , one of the articles that stood out in my tenure was eye tracking software for hr recruiters . they found that recruiters primarily looked at education , job NUM and NUM , title NUM and NUM . i have a hypothesis that data science title is like managerial title-hard to initially receive , but after having it , it's impossible not to get another job like it . EOA 
 trying to predict user behavior . achieved some results , stuck on choosing a better model / input . : mlquestions total newbie here . using sklearn . so i have an web app with a restful server . this means i have data about the user approximately in this form , i'm using reddit urls just to give you an idea : user NUM requested /r/machinelearning at NUM :43:32 user NUM requested /r/machinelearning/top at NUM :44:15 user NUM requested /r/mlquestions at NUM :45:56 user NUM requested /r/mlquestions/submit?selftext-true at NUM :47:01 ... etc . basically , i have the user's request log of the web app . what i'm trying to achieve is classify whether the user adopts the app or not . there's a NUM day trial period where you can use the app for free and at the end of this period you'll have to start paying for the app in order to continue using it . on my first try i disregarded the urls completely and just fed in the number of requests per a day . so i might feed a vector like this to the ml algorithm : [ NUM , NUM , NUM , NUM ] so NUM actions taken on the first day , NUM actions on the second day , etc. with NUM days , i achieved very close to NUM % accuracy . this was with a balanced set , so the roc auc score was also very close to NUM . i just tried a bunch of classifiers from sklearn and chose the best one . random forest and a simple linear logistic regression performed the best . i'm now trying to do the same thing again , but only looking at the first hour of usage . i'm also trying not to disregard the different categories fo actions . however, this has made the input matrix really huge and very sparse , etc for user NUM you'll have something like this : [ [1 , NUM , NUM , NUM , NUM , NUM , ..., NUM ] , # actions performed within the NUM st minute [ NUM , NUM , NUM , NUM , NUM , NUM , ..., NUM ] , # actions performed within the NUM nd minute ... [ NUM , NUM , NUM , NUM , NUM , NUM , ..., NUM ] # actions performes within the NUM th minute ] i thought i might feed each of the minutes to an unsupervised clustering algorithm for dimensionality rediction , to end up with a vector like this : [ NUM , NUM , NUM , NUM , NUM , NUM , ..., NUM ] where the numbers , hopefully, represent types of user behavior the clustering algorithm found . i just tried this using k-means and meanshift . meanshift did find lots of clusters , probably around NUM different clusters . it's a huge number , so maybe i'm doing something wrong . i don't thing new users can do NUM different general things within the first hour of usage ... i could also try to reduce the number of different requests by just taking the first category , etc /r/me.irl and /r/me.irl/top would both be just /r/me.irl just so that i could reduce the dimensions of NUM minute . okay so my current plan is to feed these clusters to a classifier . the thing is , the whole thing is a time series , so would another algorithm be more suitable for this ? i've heard lstm is good for time series . am i heading in the right direction or am i doing something stupid ? EOQ just saw this paper session-based recommendations with recurrent neural networks : URL. i haven't read it yet . maybe it helps to solve your problem . EOA 
 trying to predict user behavior . achieved some results , stuck on choosing a better model / input . : mlquestions total newbie here . using sklearn . so i have an web app with a restful server . this means i have data about the user approximately in this form , i'm using reddit urls just to give you an idea : user NUM requested /r/machinelearning at NUM :43:32 user NUM requested /r/machinelearning/top at NUM :44:15 user NUM requested /r/mlquestions at NUM :45:56 user NUM requested /r/mlquestions/submit?selftext-true at NUM :47:01 ... etc . basically , i have the user's request log of the web app . what i'm trying to achieve is classify whether the user adopts the app or not . there's a NUM day trial period where you can use the app for free and at the end of this period you'll have to start paying for the app in order to continue using it . on my first try i disregarded the urls completely and just fed in the number of requests per a day . so i might feed a vector like this to the ml algorithm : [ NUM , NUM , NUM , NUM ] so NUM actions taken on the first day , NUM actions on the second day , etc. with NUM days , i achieved very close to NUM % accuracy . this was with a balanced set , so the roc auc score was also very close to NUM . i just tried a bunch of classifiers from sklearn and chose the best one . random forest and a simple linear logistic regression performed the best . i'm now trying to do the same thing again , but only looking at the first hour of usage . i'm also trying not to disregard the different categories fo actions . however, this has made the input matrix really huge and very sparse , etc for user NUM you'll have something like this : [ [1 , NUM , NUM , NUM , NUM , NUM , ..., NUM ] , # actions performed within the NUM st minute [ NUM , NUM , NUM , NUM , NUM , NUM , ..., NUM ] , # actions performed within the NUM nd minute ... [ NUM , NUM , NUM , NUM , NUM , NUM , ..., NUM ] # actions performes within the NUM th minute ] i thought i might feed each of the minutes to an unsupervised clustering algorithm for dimensionality rediction , to end up with a vector like this : [ NUM , NUM , NUM , NUM , NUM , NUM , ..., NUM ] where the numbers , hopefully, represent types of user behavior the clustering algorithm found . i just tried this using k-means and meanshift . meanshift did find lots of clusters , probably around NUM different clusters . it's a huge number , so maybe i'm doing something wrong . i don't thing new users can do NUM different general things within the first hour of usage ... i could also try to reduce the number of different requests by just taking the first category , etc /r/me.irl and /r/me.irl/top would both be just /r/me.irl just so that i could reduce the dimensions of NUM minute . okay so my current plan is to feed these clusters to a classifier . the thing is , the whole thing is a time series , so would another algorithm be more suitable for this ? i've heard lstm is good for time series . am i heading in the right direction or am i doing something stupid ? EOQ thanks ! i'll definitely take a look at that paper . EOA 
 hardware? : mlquestions i'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e. minimum sys requirements , amp up the speed , bang for my buck kinda stuff , etc. EOQ the biggest speedup will come from a video card that supports cuda ( i.e. nvidia ) . i recently swapped my r9 NUM x for a NUM to utilize theano's gpu support , and the speedup is considerable . EOA 
 hardware? : mlquestions i'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e. minimum sys requirements , amp up the speed , bang for my buck kinda stuff , etc. EOQ to further back that up , here's a good post on hardware requirements for deep learning ( which you'll probably want to get into if you're trying succeed at kaggle ) . URL EOA 
 hardware? : mlquestions i'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e. minimum sys requirements , amp up the speed , bang for my buck kinda stuff , etc. EOQ all major open source ml libraries support cuda hardware acceleration . so something that supports an nvidia card . correct implementation took some of my compute times down from NUM hours to NUM minutes . EOA 
 hardware? : mlquestions i'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e. minimum sys requirements , amp up the speed , bang for my buck kinda stuff , etc. EOQ wow . well worth it EOA 
 reshaping in tensorflow mnist tutorial ? : mlquestions greetings , i'm trying to follow tensorflow's expert.mnist tutorial , and i cannot understand the role of reshaping . can someone briefly explain why and how is it done ? (i'm also a little confused by the negative value in the first place , what does that mean?). the code in question is ... x.image-tf.reshape(x, [-1,28,28,1]) EOQ a matrix is a vector of vector of values . reshaping turns it into a single large vector of values . then turns it into a matrix of vectors again of whatever dimensions you want it in . simplest application for this is vec-ing a matrix to calculate a jacobian or a hessian EOA 
 reshaping in tensorflow mnist tutorial ? : mlquestions greetings , i'm trying to follow tensorflow's expert.mnist tutorial , and i cannot understand the role of reshaping . can someone briefly explain why and how is it done ? (i'm also a little confused by the negative value in the first place , what does that mean?). the code in question is ... x.image-tf.reshape(x, [-1,28,28,1]) EOQ oook , it's a lot more clearer now , thank you very much . EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ yes and yes . split the data into training and test sets ( splitting by years may affect your tests predictability ) . run neural nets , random forests , etc. profit . you have to make sure your data is clean and structured well . EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ is there any tutorial where i can get inspiration ? EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ not without knowing what you do and don't know . the field is huge and skillsets are exceedingly diverse . EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ i would like to implement this paper using rapidminer : URL what do you think ? EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ you might look at analyzing baseball data with r by max marchi . different sport , but the principles are the same . EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ at the most naive level , you might just look at each player and determine a mean and standard deviation for the number of points they score . then to predict the outcome of a game , for each player , randomly sample from a normal distribution using the mean and sd you found for each player . the one with the most points wins . maybe you do that an odd number of times , and choose the winner from who won the most simulated games . this is essentially a monte-carlo simulation . you could get more sophisticated by using more of your variables in your data to predict a number of points . you could even try to get more clever if your data can help you model when they'll score in the game and their probabilities of scoring points given things like ... late in the game , and behind , etc. the possibilities are endless . you might also try to find ways to classify players based on your data ... good on clay vs grass , aggressive, consistent , good defender , etc. these would then be factors to help refine your prediction of their performance . but i'd go with starting simple and adding complexity as you go . i'd probably look at some simple regressions to see how well your variables predict points or outcomes . EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ i would like to implement this paper using rapidminer : URL what do you think ? EOA 
 will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? : mlquestions hi , i've got tons of data with a lot of info about tennis matches of the last NUM years . i would like to create a machine learning and train it with the data from NUM-2014 and then test it with NUM data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks. EOQ i don't have any experience with rapidminer , but i understand it's a pretty solid package . you should be able to do at least some of this paper's work with it , if not all of it . again , i'd recommend starting with the simpler things and see how it goes . EOA 
 advice request : using ml for neuroscience research , do i need to abandon matlab ... : mlquestions hi all ! i'm a behavioral neuroscience researcher in the field of electrophysiology , meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something , without any firm idea of how they're converting that information . the information can be encoded in so many different ways ... by the timing of a spikes relative to an external triggering event , by the timecourse of the rate of spiking , by a precise temporal pattern of spikes , by the timing of spikes relative to the phase of a particular neural oscillation ... it is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / stimuli preceding or following that activity as the target categories ) . the underlying implicit assumption is that if a ml classifier can identify the stimulus using that information alone , then there's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis reasons . i had no real programming experience ( just writing scripts for data analysis software , or to control stimulus delivery systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and whatnot . i've become reasonably competent in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of difficulties . my issues are : difficulty with circular data ( phase angles ) using feedforward nets and/or svms .. i have been just providing inputs as the sine and cosine of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the differentiability of the classes is not known ( e.g. there are ten classes , but it is quite possible that the inputs only contain enough information to discriminate the samples into NUM /2/3/4/5 , NUM /7/8, and NUM /10 ) ; classifiers often get hung up trying to minimize errors instead of maximizing information . i have tried to get around this by using evolutionary algorithms to train anns , using mutual information ( or normalized variation of information ) between network outputs and targets as the fitness function . difficulty figuring out how to get matlab's training algorithms to use my division of data into training and validation sets . uncertainty about how to determine the best type of classifier for my data .. this is more of a 'me problem' than a matlab problem . uncertainty about how to expand my analyses to a broader range of input types : i have so far usually just narrowed my data down to some number of phase angles and then used those as inputs , but i might wish to do analyses in which the input is a continuous signal alongside a discrete event signal , with the goal being to identify periods in the signal during which certain events are occurring . things i don't need to do : image recognition gigantic super-complicated models so , that's my situation . i could use advice on whether it's worth it for me to abandon matlab in favor of something more flexible . i could also use general advice about how to solve any of the issues i've described . any help is appreciated ! and if you want to know more , just ask . EOQ what do your fellow researchers use ? do you need to share code ? EOA 
 advice request : using ml for neuroscience research , do i need to abandon matlab ... : mlquestions hi all ! i'm a behavioral neuroscience researcher in the field of electrophysiology , meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something , without any firm idea of how they're converting that information . the information can be encoded in so many different ways ... by the timing of a spikes relative to an external triggering event , by the timecourse of the rate of spiking , by a precise temporal pattern of spikes , by the timing of spikes relative to the phase of a particular neural oscillation ... it is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / stimuli preceding or following that activity as the target categories ) . the underlying implicit assumption is that if a ml classifier can identify the stimulus using that information alone , then there's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis reasons . i had no real programming experience ( just writing scripts for data analysis software , or to control stimulus delivery systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and whatnot . i've become reasonably competent in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of difficulties . my issues are : difficulty with circular data ( phase angles ) using feedforward nets and/or svms .. i have been just providing inputs as the sine and cosine of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the differentiability of the classes is not known ( e.g. there are ten classes , but it is quite possible that the inputs only contain enough information to discriminate the samples into NUM /2/3/4/5 , NUM /7/8, and NUM /10 ) ; classifiers often get hung up trying to minimize errors instead of maximizing information . i have tried to get around this by using evolutionary algorithms to train anns , using mutual information ( or normalized variation of information ) between network outputs and targets as the fitness function . difficulty figuring out how to get matlab's training algorithms to use my division of data into training and validation sets . uncertainty about how to determine the best type of classifier for my data .. this is more of a 'me problem' than a matlab problem . uncertainty about how to expand my analyses to a broader range of input types : i have so far usually just narrowed my data down to some number of phase angles and then used those as inputs , but i might wish to do analyses in which the input is a continuous signal alongside a discrete event signal , with the goal being to identify periods in the signal during which certain events are occurring . things i don't need to do : image recognition gigantic super-complicated models so , that's my situation . i could use advice on whether it's worth it for me to abandon matlab in favor of something more flexible . i could also use general advice about how to solve any of the issues i've described . any help is appreciated ! and if you want to know more , just ask . EOQ in my field , code sharing isn't generally considered necessary , and certainly far from mandatory ( unless a reviewer gets a bug up their ass ) . honestly , nobody i know in the field is doing any remotely heavy duty machine learning . it's mostly stuff from a decade or two ago . last paper in my field that used a classifier was using a naive bayes . my primary concern is for being able to write analyses that are flexible enough that i can extend them easily to taking over more and more of the decisions in the analysis process , to avoid being limited by my/our own assumptions . EOA 
 advice request : using ml for neuroscience research , do i need to abandon matlab ... : mlquestions hi all ! i'm a behavioral neuroscience researcher in the field of electrophysiology , meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something , without any firm idea of how they're converting that information . the information can be encoded in so many different ways ... by the timing of a spikes relative to an external triggering event , by the timecourse of the rate of spiking , by a precise temporal pattern of spikes , by the timing of spikes relative to the phase of a particular neural oscillation ... it is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / stimuli preceding or following that activity as the target categories ) . the underlying implicit assumption is that if a ml classifier can identify the stimulus using that information alone , then there's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis reasons . i had no real programming experience ( just writing scripts for data analysis software , or to control stimulus delivery systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and whatnot . i've become reasonably competent in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of difficulties . my issues are : difficulty with circular data ( phase angles ) using feedforward nets and/or svms .. i have been just providing inputs as the sine and cosine of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the differentiability of the classes is not known ( e.g. there are ten classes , but it is quite possible that the inputs only contain enough information to discriminate the samples into NUM /2/3/4/5 , NUM /7/8, and NUM /10 ) ; classifiers often get hung up trying to minimize errors instead of maximizing information . i have tried to get around this by using evolutionary algorithms to train anns , using mutual information ( or normalized variation of information ) between network outputs and targets as the fitness function . difficulty figuring out how to get matlab's training algorithms to use my division of data into training and validation sets . uncertainty about how to determine the best type of classifier for my data .. this is more of a 'me problem' than a matlab problem . uncertainty about how to expand my analyses to a broader range of input types : i have so far usually just narrowed my data down to some number of phase angles and then used those as inputs , but i might wish to do analyses in which the input is a continuous signal alongside a discrete event signal , with the goal being to identify periods in the signal during which certain events are occurring . things i don't need to do : image recognition gigantic super-complicated models so , that's my situation . i could use advice on whether it's worth it for me to abandon matlab in favor of something more flexible . i could also use general advice about how to solve any of the issues i've described . any help is appreciated ! and if you want to know more , just ask . EOQ i don't think there's any reason you need to abandon matlab , but if you want to switch you should consider python . using numpy is a lot like using matlab . EOA 
 advice request : using ml for neuroscience research , do i need to abandon matlab ... : mlquestions hi all ! i'm a behavioral neuroscience researcher in the field of electrophysiology , meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something , without any firm idea of how they're converting that information . the information can be encoded in so many different ways ... by the timing of a spikes relative to an external triggering event , by the timecourse of the rate of spiking , by a precise temporal pattern of spikes , by the timing of spikes relative to the phase of a particular neural oscillation ... it is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / stimuli preceding or following that activity as the target categories ) . the underlying implicit assumption is that if a ml classifier can identify the stimulus using that information alone , then there's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis reasons . i had no real programming experience ( just writing scripts for data analysis software , or to control stimulus delivery systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and whatnot . i've become reasonably competent in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of difficulties . my issues are : difficulty with circular data ( phase angles ) using feedforward nets and/or svms .. i have been just providing inputs as the sine and cosine of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the differentiability of the classes is not known ( e.g. there are ten classes , but it is quite possible that the inputs only contain enough information to discriminate the samples into NUM /2/3/4/5 , NUM /7/8, and NUM /10 ) ; classifiers often get hung up trying to minimize errors instead of maximizing information . i have tried to get around this by using evolutionary algorithms to train anns , using mutual information ( or normalized variation of information ) between network outputs and targets as the fitness function . difficulty figuring out how to get matlab's training algorithms to use my division of data into training and validation sets . uncertainty about how to determine the best type of classifier for my data .. this is more of a 'me problem' than a matlab problem . uncertainty about how to expand my analyses to a broader range of input types : i have so far usually just narrowed my data down to some number of phase angles and then used those as inputs , but i might wish to do analyses in which the input is a continuous signal alongside a discrete event signal , with the goal being to identify periods in the signal during which certain events are occurring . things i don't need to do : image recognition gigantic super-complicated models so , that's my situation . i could use advice on whether it's worth it for me to abandon matlab in favor of something more flexible . i could also use general advice about how to solve any of the issues i've described . any help is appreciated ! and if you want to know more , just ask . EOQ i guess my issue with matlab is that the ann etc . tools are fairly constrained . if i want to add a major functionality that's not in the toolbox , it needs to be coded from scratch . there are relatively few such tools available for matlab , since most of the people making such things are working in python or something else . for example , using a complex valued network seems ( from what i can tell ) like i'd need to more or less code a neural network from scratch . and i have zero actual training in programming or computer science . i can do it , but it's gonna be messy and slow . so it sounds to me like the advantage of switching to python ( or whatever ) would be the availability of a wider variety of existing implementations to work with , and greater ability to find and incorporate novel functionalities ( such as an activation function appropriate for complex values ) . but , i could be wrong . it could be that doing that kind of stuff would be just as difficult if i were using a pre-existing python ann implementation as my starting point . what do you think ? EOA 
 advice request : using ml for neuroscience research , do i need to abandon matlab ... : mlquestions hi all ! i'm a behavioral neuroscience researcher in the field of electrophysiology , meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something , without any firm idea of how they're converting that information . the information can be encoded in so many different ways ... by the timing of a spikes relative to an external triggering event , by the timecourse of the rate of spiking , by a precise temporal pattern of spikes , by the timing of spikes relative to the phase of a particular neural oscillation ... it is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / stimuli preceding or following that activity as the target categories ) . the underlying implicit assumption is that if a ml classifier can identify the stimulus using that information alone , then there's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis reasons . i had no real programming experience ( just writing scripts for data analysis software , or to control stimulus delivery systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and whatnot . i've become reasonably competent in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of difficulties . my issues are : difficulty with circular data ( phase angles ) using feedforward nets and/or svms .. i have been just providing inputs as the sine and cosine of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the differentiability of the classes is not known ( e.g. there are ten classes , but it is quite possible that the inputs only contain enough information to discriminate the samples into NUM /2/3/4/5 , NUM /7/8, and NUM /10 ) ; classifiers often get hung up trying to minimize errors instead of maximizing information . i have tried to get around this by using evolutionary algorithms to train anns , using mutual information ( or normalized variation of information ) between network outputs and targets as the fitness function . difficulty figuring out how to get matlab's training algorithms to use my division of data into training and validation sets . uncertainty about how to determine the best type of classifier for my data .. this is more of a 'me problem' than a matlab problem . uncertainty about how to expand my analyses to a broader range of input types : i have so far usually just narrowed my data down to some number of phase angles and then used those as inputs , but i might wish to do analyses in which the input is a continuous signal alongside a discrete event signal , with the goal being to identify periods in the signal during which certain events are occurring . things i don't need to do : image recognition gigantic super-complicated models so , that's my situation . i could use advice on whether it's worth it for me to abandon matlab in favor of something more flexible . i could also use general advice about how to solve any of the issues i've described . any help is appreciated ! and if you want to know more , just ask . EOQ do you have access to the neural network toolbox ? maybe that would help . python does have a lot of nn stuff ( e.g. a bunch of deep learning libraries ) available but i think to do what you want to do ( complex valued network ) would still require a lot of manual fiddling . ( i've never heard of anyone doing nn's with complex numbers , how does that work? ) EOA 
 advice request : using ml for neuroscience research , do i need to abandon matlab ... : mlquestions hi all ! i'm a behavioral neuroscience researcher in the field of electrophysiology , meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something , without any firm idea of how they're converting that information . the information can be encoded in so many different ways ... by the timing of a spikes relative to an external triggering event , by the timecourse of the rate of spiking , by a precise temporal pattern of spikes , by the timing of spikes relative to the phase of a particular neural oscillation ... it is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / stimuli preceding or following that activity as the target categories ) . the underlying implicit assumption is that if a ml classifier can identify the stimulus using that information alone , then there's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis reasons . i had no real programming experience ( just writing scripts for data analysis software , or to control stimulus delivery systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and whatnot . i've become reasonably competent in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of difficulties . my issues are : difficulty with circular data ( phase angles ) using feedforward nets and/or svms .. i have been just providing inputs as the sine and cosine of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the differentiability of the classes is not known ( e.g. there are ten classes , but it is quite possible that the inputs only contain enough information to discriminate the samples into NUM /2/3/4/5 , NUM /7/8, and NUM /10 ) ; classifiers often get hung up trying to minimize errors instead of maximizing information . i have tried to get around this by using evolutionary algorithms to train anns , using mutual information ( or normalized variation of information ) between network outputs and targets as the fitness function . difficulty figuring out how to get matlab's training algorithms to use my division of data into training and validation sets . uncertainty about how to determine the best type of classifier for my data .. this is more of a 'me problem' than a matlab problem . uncertainty about how to expand my analyses to a broader range of input types : i have so far usually just narrowed my data down to some number of phase angles and then used those as inputs , but i might wish to do analyses in which the input is a continuous signal alongside a discrete event signal , with the goal being to identify periods in the signal during which certain events are occurring . things i don't need to do : image recognition gigantic super-complicated models so , that's my situation . i could use advice on whether it's worth it for me to abandon matlab in favor of something more flexible . i could also use general advice about how to solve any of the issues i've described . any help is appreciated ! and if you want to know more , just ask . EOQ well , it requires different activation functions . using angular data is a big part of why i'd like to at least try that , since an angle expressed in degrees or radians will represent NUM ° and NUM ° as non-adjacent . the typical solution is to present an angle as sine and cosine , but that makes the problem harder to solve because the model then assumes that they are two independent variables , and anything it learns about one will have to be learned independently about the others . it would make much more sense to represent the angle in polar coordinates , similar to how an analytic signal generated by hilbert transform is represented . supposedly, cvnns perform better than real-valued anns on some of these types of problems . edit : whoops , yes, i am working with the neutral network toolbox . EOA 
 what is the difference between convolution and correlation . why do cnn's use convolution ? : mlquestions correlation indicates a relationship between two variables . convolution is a product operation that combines two distributions ( or series ) to create a new distribution . in the context of a time series , the convolution operation combines the past information of two time series to create a new series . in the context of a cnn , at each time step the network contains the history of all past inputs and activations . in this way it is a generalized version of the convolution operation . -edit : i just realized that i confused cnn and rnn . :( must have been on crazy pills . EOQ it's just that mathematically , one is the just the other flipped ( and so they produce the same results in symmetrical situations )-so how can i unify these two explanations ? EOA 
 what is the difference between convolution and correlation . why do cnn's use convolution ? : mlquestions correlation indicates a relationship between two variables . convolution is a product operation that combines two distributions ( or series ) to create a new distribution . in the context of a time series , the convolution operation combines the past information of two time series to create a new series . in the context of a cnn , at each time step the network contains the history of all past inputs and activations . in this way it is a generalized version of the convolution operation . -edit : i just realized that i confused cnn and rnn . :( must have been on crazy pills . EOQ please check out how and why convolution is defined in signal processing . flipping is required when we want to know the result of a signal x being convolved in h and that's why it's flipped in convolution . however correlation is-as you may already know-measure of how similar two sequences are , and therefore you don't flip . in cnn , you can say each kernel is learned as a convolution filter coefficients . or you may say it's computing how local values are correlated with certain values of each kernel . there's no difference so in some framework they don't flip it because it doesn't need to be while in some other framework it's flipped . probably at the first stage people found it better to explain the network with convolution rather than correlation . EOA 
 guide to implementing rnn : mlquestions hi ! i want to build a simple rnn with matlab to learn reading text . i have tried to understand how to implement it in code , but i would love to see if there are good guides out there that gives exercises in implementing the code ! any tips would be great :) EOQ sorry if i'm misinterpreting what you are saying , but, if you aren't implementing it to learn about nns but rather to do a specific task , i'd really suggest not implementing your own nns . not matlab , but the deep learning for nlp class will help if you want to implement stuff . EOA 
 guide to implementing rnn : mlquestions hi ! i want to build a simple rnn with matlab to learn reading text . i have tried to understand how to implement it in code , but i would love to see if there are good guides out there that gives exercises in implementing the code ! any tips would be great :) EOQ this is a very good post on rnn's helped me understand them alot better and has some good code samples , though not in matlab . URL EOA 
 want to focus on the financial application of machine learning , looking for good study material : mlquestions i've already completed the coursera machine learning course offered by stanford so i believe i have a good understanding of the basics of machine learning . i want to now focus on the financial applications of machine learning , but i want to make sure i pick the right material to study . does anybody have any recommendations ? i am primarily interested in the prediction of stock prices . thank you . EOQ it's worth doing some finance related course to get more of an understanding of things from that angle . this coursera one is quite good URL EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ it just sounds like a job for regular expressions . anything more is overkill . EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ machine learning can help you figure out how good of a deal a particular listing is , and if you want to get even more advanced then go ahead and try to train an algorithm that could even figure out if a given posting is too good to be true and is probably a scam . EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ i am planning to do the same thing for job hunts .. let's do it together ? i have ml and nlp knowledge , wanna work together ? EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ i've implemented something similar and heres something i came up with . how good of a match is a given job post to your criteria how focused is a job posting . i.e do you get a posting saying looking for c-programmer , must know java , php, linux , c#. this advert is written by someone who clearly doesn't know what is up . lets track companies , how often are they posting . NUM a . what kind of jobs are they looking for , is it the same backend developer role ? it looks like they just can't seem to retain people . what trends are developing . is this an advert written by a recruiter ? if so can you build a bot that can find the actual company who hired the recruiter . EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ can i access your code ? github? EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ its proprietary and is used by another company who is still in business . i would have to break my nda with them . EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ cool then :) we'll make our own :)) EOA 
 i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? : mlquestions i have this project to write some kind of app ( i'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i don't know much about statistics or maths and i've never worked on anything related to ml . any pointers ? thanks ! EOQ go for it , neural networks help a lot with those kind of analysis . most of the adverts that are written by recruiters won't have company names among other factors , you could actually bunch up a lot of posts by recruiters and then get posts written by actual companies and create a training set out of those . EOA 
 deep learning simplified : episode NUM -an old problem : mlquestions while deep neural nets are the state of the art in machine learning , the flipside is that they are really hard to train . up until NUM , there was no way to train them satisfactorily . here is a clip that explains further . URL EOQ this one is on the vanishing gradient . enjoy :-) EOA 
 trouble of using vgg for super-resolution : mlquestions i'm trying to implement the net in accurate image super-resolution using very deep convolutional networks . it's inspired by vgg : the only difference is that there are no pooling layers , so all intermediate weight layers have NUM channels . i followed the details of the paper gradient clipping learning rate-NUM , decreases by NUM every k epochs glorot-like weight initialization : stddev-(2/(3-3-64))^0.5, where filter width-NUM , number of input channels-NUM momentum-NUM , l2 regularization-NUM e-4 in tensorboard , i'm keeping track of the weights/biases histograms , and it seems after a certain time , the weights stop changing . this may mean the gradient vanished , or we've settled on a bad local minimum . however, i am getting performance worse off than bicubic interpolation . in the paper , they get better performance even in the NUM st epoch . i was wondering why is this happening ? EOQ initialize the weight matrix of the last convolution , the one that results in the residuals , with zeros . it also helps to use ycbcr instead of rgb . EOA 
 trouble of using vgg for super-resolution : mlquestions i'm trying to implement the net in accurate image super-resolution using very deep convolutional networks . it's inspired by vgg : the only difference is that there are no pooling layers , so all intermediate weight layers have NUM channels . i followed the details of the paper gradient clipping learning rate-NUM , decreases by NUM every k epochs glorot-like weight initialization : stddev-(2/(3-3-64))^0.5, where filter width-NUM , number of input channels-NUM momentum-NUM , l2 regularization-NUM e-4 in tensorboard , i'm keeping track of the weights/biases histograms , and it seems after a certain time , the weights stop changing . this may mean the gradient vanished , or we've settled on a bad local minimum . however, i am getting performance worse off than bicubic interpolation . in the paper , they get better performance even in the NUM st epoch . i was wondering why is this happening ? EOQ may i ask why the last layer should be initialized to zeros ? EOA 
 trouble of using vgg for super-resolution : mlquestions i'm trying to implement the net in accurate image super-resolution using very deep convolutional networks . it's inspired by vgg : the only difference is that there are no pooling layers , so all intermediate weight layers have NUM channels . i followed the details of the paper gradient clipping learning rate-NUM , decreases by NUM every k epochs glorot-like weight initialization : stddev-(2/(3-3-64))^0.5, where filter width-NUM , number of input channels-NUM momentum-NUM , l2 regularization-NUM e-4 in tensorboard , i'm keeping track of the weights/biases histograms , and it seems after a certain time , the weights stop changing . this may mean the gradient vanished , or we've settled on a bad local minimum . however, i am getting performance worse off than bicubic interpolation . in the paper , they get better performance even in the NUM st epoch . i was wondering why is this happening ? EOQ no real reasons , i just got better results this way . what psnr do you get ? EOA 
 trouble of using vgg for super-resolution : mlquestions i'm trying to implement the net in accurate image super-resolution using very deep convolutional networks . it's inspired by vgg : the only difference is that there are no pooling layers , so all intermediate weight layers have NUM channels . i followed the details of the paper gradient clipping learning rate-NUM , decreases by NUM every k epochs glorot-like weight initialization : stddev-(2/(3-3-64))^0.5, where filter width-NUM , number of input channels-NUM momentum-NUM , l2 regularization-NUM e-4 in tensorboard , i'm keeping track of the weights/biases histograms , and it seems after a certain time , the weights stop changing . this may mean the gradient vanished , or we've settled on a bad local minimum . however, i am getting performance worse off than bicubic interpolation . in the paper , they get better performance even in the NUM st epoch . i was wondering why is this happening ? EOQ psnr never above that of bicubic . initialization didn't change the error it eventually converged to . neither did setting learning rate to NUM , NUM 1, NUM 01, etc . EOA 
 trouble of using vgg for super-resolution : mlquestions i'm trying to implement the net in accurate image super-resolution using very deep convolutional networks . it's inspired by vgg : the only difference is that there are no pooling layers , so all intermediate weight layers have NUM channels . i followed the details of the paper gradient clipping learning rate-NUM , decreases by NUM every k epochs glorot-like weight initialization : stddev-(2/(3-3-64))^0.5, where filter width-NUM , number of input channels-NUM momentum-NUM , l2 regularization-NUM e-4 in tensorboard , i'm keeping track of the weights/biases histograms , and it seems after a certain time , the weights stop changing . this may mean the gradient vanished , or we've settled on a bad local minimum . however, i am getting performance worse off than bicubic interpolation . in the paper , they get better performance even in the NUM st epoch . i was wondering why is this happening ? EOQ how do you prepare the input images ? bicubic? with the mentioned zero initialization , the network starts as good as the input method ... so are you saying that yours actually gets worse ? it might be worth a try to remove the regularization . i didn't use any of that ; no regularization , no gradient clipping , no learning rate schedule , no sgd . just adam ( the optimizer ) with a learning rate of NUM-4 . EOA 
 deep learning simplified : episode NUM -how to choose : mlquestions deep learning as a field has developed quite a bit in the last decade , and we now have a variety of models to pick from with new models and improvements arriving frequently . the flip side to this is , the burden of choice now falls on you to figure out which model to pick for what application . here is a clip that gives you some guidelines to help you decide. URL EOQ some simple rules of thumb on how to pick a deep net . enjoy :-) EOA 
 deep learning simplified : youtube series : mlquestions hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like. if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you'll typically find online on the topic is highly mathematical/technical , which is great ! but if you're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! URL EOQ this is the series intro-NUM total videos so far and many more to come . enjoy :-)! EOA 
 deep learning simplified : youtube series : mlquestions hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like. if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you'll typically find online on the topic is highly mathematical/technical , which is great ! but if you're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! URL EOQ going to check them out .. thanks for the work EOA 
 deep learning simplified : youtube series : mlquestions hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like. if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you'll typically find online on the topic is highly mathematical/technical , which is great ! but if you're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! URL EOQ very nice ! i watched the intro video . the production quality is great and i like the narrator's voice. i look forward to watching the rest . thanks for doing this ! EOA 
 deep learning simplified : youtube series : mlquestions hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like. if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you'll typically find online on the topic is highly mathematical/technical , which is great ! but if you're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! URL EOQ glad you like it :-)! EOA 
 deep learning simplified : youtube series : mlquestions hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like. if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you'll typically find online on the topic is highly mathematical/technical , which is great ! but if you're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! URL EOQ i found that really useful ! thanks very much . EOA 
 correct cost function to use with a softmax output layer with a continuous target distribution ? : mlquestions lets say i am training a neural network to play rock , paper, scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p.rock-p.paper-p.scissors-NUM . i understand ( i think.. ) that if my networks target output was discrete , ie (1,0,0), ( NUM ,1,0 ) or (0,0,1), i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example (1./3, NUM /3, NUM /3)? i have tried mean squared error , but it does not converge . i don't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... EOQ output of your network is still discrete , it is classification problem , but you will never get full one and zeros , because softmax is giving you probabilities that it should be that output . if you want result you need to sample from that distribution . so you should use cross-entropy . EOA 
 correct cost function to use with a softmax output layer with a continuous target distribution ? : mlquestions lets say i am training a neural network to play rock , paper, scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p.rock-p.paper-p.scissors-NUM . i understand ( i think.. ) that if my networks target output was discrete , ie (1,0,0), ( NUM ,1,0 ) or (0,0,1), i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example (1./3, NUM /3, NUM /3)? i have tried mean squared error , but it does not converge . i don't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... EOQ so if i have a target t , and a network output y , i can use : e-t-log[y]-( NUM -t )-log[1-y] with partiale / partialy-t/y-( NUM -t ) / ( NUM -y ) where t-NUM or t-NUM . i just need to make sure my targets are sampled from (1/3,1/3,1/3), if i want my network to output ( NUM /3 , NUM /3, NUM /3 ) ? EOA 
 correct cost function to use with a softmax output layer with a continuous target distribution ? : mlquestions lets say i am training a neural network to play rock , paper, scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p.rock-p.paper-p.scissors-NUM . i understand ( i think.. ) that if my networks target output was discrete , ie (1,0,0), ( NUM ,1,0 ) or (0,0,1), i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example (1./3, NUM /3, NUM /3)? i have tried mean squared error , but it does not converge . i don't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... EOQ sorry i'm lost and not really sure what are you asking . during training , use actual outputs ( NUM /3,1/3,1/3 ) when computing error . then when you actually use trained network you don't need probability distribution so you will sample with multinomial . am i helping ? EOA 
 correct cost function to use with a softmax output layer with a continuous target distribution ? : mlquestions lets say i am training a neural network to play rock , paper, scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p.rock-p.paper-p.scissors-NUM . i understand ( i think.. ) that if my networks target output was discrete , ie (1,0,0), ( NUM ,1,0 ) or (0,0,1), i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example (1./3, NUM /3, NUM /3)? i have tried mean squared error , but it does not converge . i don't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... EOQ hey thanks for taking the time to help ! lets say i want my network to output ( NUM /5 , NUM /5, NUM /5 ) , but currently my network outputs ( NUM /3 , NUM /3, NUM /3 ) . in the partiale / partialy equation above i set y-( NUM /3 , NUM /3, NUM /3 ) . but my confusion is t , it expects the t ( target ) to be either one or zero , but my target is ( NUM /5 , NUM /5, NUM /5 ) . so your saying if during training i use a target ( NUM ,0,0 ) NUM /5th of the time , ( NUM ,1,0 ) NUM /5th of the time and ( NUM ,0,1 ) NUM /5th of the time it should work ? EOA 
 correct cost function to use with a softmax output layer with a continuous target distribution ? : mlquestions lets say i am training a neural network to play rock , paper, scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p.rock-p.paper-p.scissors-NUM . i understand ( i think.. ) that if my networks target output was discrete , ie (1,0,0), ( NUM ,1,0 ) or (0,0,1), i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example (1./3, NUM /3, NUM /3)? i have tried mean squared error , but it does not converge . i don't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... EOQ i don't understand why you want your target to be ( NUM /5 , NUM /5, NUM /5 ) ? i guess you want to get in the end one of the choices , so you should learn with target ( NUM ,0,0 ) all the time for specific input . i haven't seen anybody doing this ( having target with softmax probability distribution , not one and zeros ) , so you're either doing something completely different or completely wrong , i guess . EOA 
 correct cost function to use with a softmax output layer with a continuous target distribution ? : mlquestions lets say i am training a neural network to play rock , paper, scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p.rock-p.paper-p.scissors-NUM . i understand ( i think.. ) that if my networks target output was discrete , ie (1,0,0), ( NUM ,1,0 ) or (0,0,1), i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example (1./3, NUM /3, NUM /3)? i have tried mean squared error , but it does not converge . i don't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... EOQ i'm trying to see if a neural network can learn a nash equilibrium strategy , in self play , in normal form games . rock, paper , scissors has an obvious ( NUM /3,1/3,1/3 ) nash equilibrium , because the payoffs are symmetric , but you can change the payoffs to change the equilibrium . so maybe a different game has a equilibrium strategy ( NUM /5 , NUM /5, NUM /5 ) . just as a first experiment , i want to see the network learn a pattern-> ; strategy mapping , where the pattern is some representation of the game we are playing , and the strategy is a given nash equilibrium calculated by another method , maybe fictitious play or linear programming . i have had some success in games with two actions , matching pennies for example , using a network with a single output neuron . in that case the strategy over the two actions , given by the networks output is (output , (1-output)) . but moving to three action games i need to ensure the output strategies are valid , that is the three action probabilities sum to one , and so hence the softmax output layer . thinking about this more last night , a mean squared error cost function should work , even if it's not the ideal cost to use with a softmax output . so i think i must have a bug in my code . thanks for helping me think this through ! EOA 
 correct cost function to use with a softmax output layer with a continuous target distribution ? : mlquestions lets say i am training a neural network to play rock , paper, scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p.rock-p.paper-p.scissors-NUM . i understand ( i think.. ) that if my networks target output was discrete , ie (1,0,0), ( NUM ,1,0 ) or (0,0,1), i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example (1./3, NUM /3, NUM /3)? i have tried mean squared error , but it does not converge . i don't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... EOQ you're welcome , even though i confused both of us . good luck with that . EOA 
 weird error message when tuning svm with polynomial kernel : warning: reaching max number of iterations : mlquestions it is my first time working with support vector machines . i am trying to solve this homework , but am receiving the above mentioned error ... here is my code : library(e1071) test.data-#upload test data here . training.data-read.table('digits.training.csv', sep-',', header-true) y-training.data$y chosen.svm-function(y,training.data,kernel.name){ obj any idea why this is happening ? EOQ i am a bit busy to check but i would guess that svm requires convergence and it was failing to converge . EOA 
 weird error message when tuning svm with polynomial kernel : warning: reaching max number of iterations : mlquestions it is my first time working with support vector machines . i am trying to solve this homework , but am receiving the above mentioned error ... here is my code : library(e1071) test.data-#upload test data here . training.data-read.table('digits.training.csv', sep-',', header-true) y-training.data$y chosen.svm-function(y,training.data,kernel.name){ obj any idea why this is happening ? EOQ any idea how this can be fixed ? i changed the range for gamma and cost , as well as changed the polynomial degree . the dataset is also balanced and normalized . any advice would be really helpful .. EOA 
 weird error message when tuning svm with polynomial kernel : warning: reaching max number of iterations : mlquestions it is my first time working with support vector machines . i am trying to solve this homework , but am receiving the above mentioned error ... here is my code : library(e1071) test.data-#upload test data here . training.data-read.table('digits.training.csv', sep-',', header-true) y-training.data$y chosen.svm-function(y,training.data,kernel.name){ obj any idea why this is happening ? EOQ URL tldr : -normalize your data . -make sure your classes are more or less balanced ( have similar size ) . if they don't , use parameter-w to assign them different weights . EOA 
 what effect does loss function have on training ? : mlquestions i understand it is only used to observe progress , ie. it does not really have any fingers in how weights are updated ? if so , why is the choice of loss function important ? the network will converge to what it will converge anyway ... a slightly related question about training criterion , ie. error for the output layer , from what i've seen sometimes its just target-output sometimes its (target-output)-deriv(output) is it chosen independently of loss function ? EOQ false alarm , figured it out EOA 
 given a list of centroids , how to find optimal set of length k ? : mlquestions think this is an easy one i'm having a brain fart about . given some dataset where each observation x has a pre-calculated distance to n centroids . if i know i want to separate the entire set into k centroids , how can i pick which k ? or more concretely , lets say i have NUM rocks and each rock has a set of NUM distances to some centroid representing a feature ( maybe color , likelihood to be found in a yard , etc ) and i know i want to cut the whole set of rocks into NUM centroids ( either a rock is closest to the yard centroid or to the color centroid ) . how do i find the NUM centroids which will minimize my distance to the whole set of rocks ? i feel like there's some kind of reverse-knn i'm just not thinking of . EOQ k-means clustering . EOA 
 need help with understanding how to compute the weight gradient in a convolutional layer . : mlquestions hi , so i have a have a hard time understanding how to compute the gradient for the convolutional layer . to test my understanding i have constructed a simple convolutional network featuring one convolutional layer , one maxpool layer and a fully connected output layer . the whole network looks like this : URL so when applying backpropegation computing the output delta is simple , but i have some uncertainties when computing the other deltas . for the maxpool layer most instructions will say to simply repeat the errors from the previous layer since this layer does not do any learning . so for delta3 i compute a matrix with the error values from delta4 for the pooled indices and NUM everywhere else . so for the delta for the convolutional layer ( delta2 ) i guess one only computes the hadamard product between d/dz and delta3 ?. when computing the gradient for the weights i furthermore compute the convolution between the input ( i ) and delta2 . using this gradient does not yield usefull results ( i.e reduce the error ) when updating the weights , so i am obviously doing something wrong . how i compute the gradient is shown in this picture : URL basically my question is what is the correct way of obtaining the weight gradient in this example . EOQ figured it out , to compute delta2 you ofcourse compute it thusly-upsamle(wt delta3) hadamard ( sigma' (z)) . where delta3-delta4 . EOA 
 [help] supervised classification implementation : mlquestions this is my frist ml project , so i am trying to build a really simple web analytics tool . i want to use apache web logs to determine whether or not to issue a coupon to a customer . the web logs contain ip address and browsing history , so i can determine what pages a customer went to and whether or not they bought the product . using this data as a training set , i hope to build a model that can report to me who i should issue new coupons to , in order to improve sales . my question is : how can i implement this ? any details i can get will be really helpful . i already have a java program that can take the web logs and split them into individual pieces of data . how do i get this data into a form that is acceptable , and how do i make the program ? its hard for me to find tutorials that are low-level and implementation focused . thanks ! EOQ let's get the data extraction sorted out . what you want is the pattern of a customer's browsing before he/she makes a buy/doesn't ever make a buy ( no buy for next NUM weeks ) . so your data is only limited to ppl making a buy . can you see how to extract this data ? now you have a time series data ( don't be scared , fancy way of saying simple stuff ) of buying patterns . split into training and test . i guess a hidden markov network would do good here . the observed data is the page being browsed , and hidden state is probability to buy . learning this model is straightforward ( tractable ) . read kevin murphy's section on learning a partially observed hmm . you can limit the length of the hmm to a value based on the amount of data and performance on test set . smaller lenght may give less test performance , longer hmm might not have enough data to train on . feel free to ask follow-ups :) EOA 
 the typical NUM horse problem with a twist : mlquestions the typical interview question : you have NUM horses and want to identify the NUM fastest horses . you have a track that can hold NUM horses at a time. what is the minimum number of races it would take for you to identify the NUM fastest horses . twist : same problem , except now you want to identify the NUM fastest horses . edit : i forgot to mention , the times are not recorded so you cannot just find the speed of each horse individually and compare . ex : for the top NUM horses , the answer is NUM races to display this , i will show label the horses with a letter and a number . the letter refers to their first race set and the number refers to their order for the first race . race NUM race NUM race NUM race NUM race NUM a1 b1 c1 d1 e1 a2 b2 c2 d2 e2 a3 b3 c3 de e3 a4 b4 c4 d4 e4 a5 b5 c5 d5 e5 we can then find the fastest horse by comparing the best in each of the first NUM : race NUM na a1 na b1 na c1 na d1 na e1 na lets say that horse a1 , b1, and c1 are the fastest , showing up in their respective orders ( i.e. a1 is the fastest ) . then to identify the NUM nd and NUM rd fastest , we race the set of candidates : race NUM na a2 na a3 na b1 na b2 na c1 na this will give us the NUM fastest horses . EOQ NUM for both assuming the horses are in random order and you do not have any information on them . since NUM horses x NUM horses per track is NUM and you want to cover each horse exactly once because you do not have any prior information on them . then you sort the results by speed . any additional races can bias the results because horses used multiple times can be fatigued . EOA 
 the typical NUM horse problem with a twist : mlquestions the typical interview question : you have NUM horses and want to identify the NUM fastest horses . you have a track that can hold NUM horses at a time. what is the minimum number of races it would take for you to identify the NUM fastest horses . twist : same problem , except now you want to identify the NUM fastest horses . edit : i forgot to mention , the times are not recorded so you cannot just find the speed of each horse individually and compare . ex : for the top NUM horses , the answer is NUM races to display this , i will show label the horses with a letter and a number . the letter refers to their first race set and the number refers to their order for the first race . race NUM race NUM race NUM race NUM race NUM a1 b1 c1 d1 e1 a2 b2 c2 d2 e2 a3 b3 c3 de e3 a4 b4 c4 d4 e4 a5 b5 c5 d5 e5 we can then find the fastest horse by comparing the best in each of the first NUM : race NUM na a1 na b1 na c1 na d1 na e1 na lets say that horse a1 , b1, and c1 are the fastest , showing up in their respective orders ( i.e. a1 is the fastest ) . then to identify the NUM nd and NUM rd fastest , we race the set of candidates : race NUM na a2 na a3 na b1 na b2 na c1 na this will give us the NUM fastest horses . EOQ i forgot to mention . the horses are not timed . you only have their relative speeds compared to each other . i edited the question to clarify . EOA 
 machine learning on paths ? : mlquestions hello everyone , i'm currently a student working on a projet with a database from wikispeedia ( from their website ) you are given two wikipedia articles starting from the first article , your goal is to reach the second one , exclusively by following links in the articles you encounter . i'd like to train an algorithm to guess the shortest path to go from one article to another , knowing only the categories ( e.g historical figure , geography, japanese videogames... ) the idea is that computing every possible path from one point to another is simply impossible on such a large graph in a reasonable amount of time , so i want to use machine learning to have a satisfying solution in a relatively short amount of time. i'd like to say that i'm a novice un machine learning , although i already completed a few projects , but i never worked on graphs before : do you have an idea where i could find some ideas on what kind of algorithms to use for my project ? ( if it can help , i code in python and used scikit-learn for my previous projects ) thank for your help EOQ this seems similar to ( not exactly ) how google maps would give you a route from one end of us to another .. ( edit ) the difference being that in maps you have a measure of closeness to the destination ( euclidean distance ) , in this problem it's not that obvious . my take at it ( multiple ideas ) extract a graph from wikipedia , the articles are the nodes and the links are directed edges . then, this article is a good start : URL NUM ) a-search from source to destination . you'll have to figure out the heuristics to use . we can discuss further on this . NUM )by breaking down the problem into subparts ( sub-graphs ) based on major topic in wikipedia(or other clustering metric) . you can run single source/all pairs shortest path on all subparts and then join the outputs ( details need to be figured out ) . this would be useful if you want shortest paths from all articles to all other . the graph you construct would be sparse , that property could also be used . at the least storage should be done by an array of linkedlists . computational gains might also be needed to look into . EOA 
 help with a deep convolution network : mlquestions i'm building a deep convolution neural network on my own with c-and am getting stuck . how does the convolution layers update its weights ? i understand how the hidden layer and the output layer's weights are updated , but i can't think of how to update the convolution filters . can anyone explain how to update the convolution layer , provide an algorithm , or resources where i can learn more about this ? oh , for some context , the goal of my network is to learn to play breakout . with some hand coded filters i was able to get it to bounce the ball NUM-3 times , but not that reliably . EOQ it's same as in the dnn . cs231n would be a good start to check it out , or see how people have implemented it in frameworks e.g. keras . EOA 
 designing a random forest from scratch , with no ml libraries allowed . : mlquestions hi all , not a programmer but i'm taking a machine learning module i've been asked to design a rf from scratch . i am lost . i have basic skills in python-very basic . can anyone help ? so far i've scraped a decision tree from the internet but i'm a bit lost on it as its telling me my features in the function argument aren't defined . my attempt at the code below . import numpy import csv with open('banks.csv', 'rt' , encoding-'ascii') as csvfile : # type name of datafile with extension in first set of apostrophes . data-csv.reader(csvfile, delimiter-' ' , quotechar-'|') # for row in data : # print (' , '.join(row)) # prints out all data in file def newdt(data , features, targetclass , fitness.func): # define a new decision tree data-data[:] values-[record[targetclass] for record in data] empty-majorvalue(data , targetclass) features-(data[:0]) if not data or (len(features)-1) EOQ i'm a bot , bleep, bloop . someone has linked to this thread from another place on reddit : [ /r/machinelearning ] crosspost from mlquestions : random forest from scratch . any help is appreciated . if you follow any of the above links , please respect the rules of reddit and don't vote in the other threads . ( info / contact ) EOA 
 designing a random forest from scratch , with no ml libraries allowed . : mlquestions hi all , not a programmer but i'm taking a machine learning module i've been asked to design a rf from scratch . i am lost . i have basic skills in python-very basic . can anyone help ? so far i've scraped a decision tree from the internet but i'm a bit lost on it as its telling me my features in the function argument aren't defined . my attempt at the code below . import numpy import csv with open('banks.csv', 'rt' , encoding-'ascii') as csvfile : # type name of datafile with extension in first set of apostrophes . data-csv.reader(csvfile, delimiter-' ' , quotechar-'|') # for row in data : # print (' , '.join(row)) # prints out all data in file def newdt(data , features, targetclass , fitness.func): # define a new decision tree data-data[:] values-[record[targetclass] for record in data] empty-majorvalue(data , targetclass) features-(data[:0]) if not data or (len(features)-1) EOQ hi ! once you implement a decision tree you are almost there , just build n of them , predict your validation set through all of them and for each input vector take the mean ( regression ) or the mode ( classification ) . if you can share your banks.csv file i will take a look at it tomorrow . plus a link to where you found the script . EOA 
 designing a random forest from scratch , with no ml libraries allowed . : mlquestions hi all , not a programmer but i'm taking a machine learning module i've been asked to design a rf from scratch . i am lost . i have basic skills in python-very basic . can anyone help ? so far i've scraped a decision tree from the internet but i'm a bit lost on it as its telling me my features in the function argument aren't defined . my attempt at the code below . import numpy import csv with open('banks.csv', 'rt' , encoding-'ascii') as csvfile : # type name of datafile with extension in first set of apostrophes . data-csv.reader(csvfile, delimiter-' ' , quotechar-'|') # for row in data : # print (' , '.join(row)) # prints out all data in file def newdt(data , features, targetclass , fitness.func): # define a new decision tree data-data[:] values-[record[targetclass] for record in data] empty-majorvalue(data , targetclass) features-(data[:0]) if not data or (len(features)-1) EOQ hi thanks for getting back to me . the csv file is on my desktop , this was an assignment that has passed its due that . however i still want to know how to do this . i realise i've a lot to learn with regards to manipulation of csv files . i think once i figure that out i can move onto building a random forest . as i said to my prof i assumed a much lower level of coding when i undertook the course . EOA 
 how do i describe a perceptron , comparing two inputs x/y triggering when x > ;y. : mlquestions i tried to setup a perceptron witch outputs an NUM when output x is bigger than y . how can i realise this ? like wtf ? EOQ if i recall perceptron well , this python code should describe such perceptron : def gtp(x , y): # inputs are x and y s-NUM -NUM -x-(-1.0 )-y # bias is NUM , weights are NUM and-1 if s > ; NUM : # threshold is NUM return NUM else : return NUM training it is another matter , here the weights and bias are fixed . EOA 
 how do i describe a perceptron , comparing two inputs x/y triggering when x > ;y. : mlquestions i tried to setup a perceptron witch outputs an NUM when output x is bigger than y . how can i realise this ? like wtf ? EOQ y is an input ? are there only NUM inputs ? are you going to use back propagation for training ? EOA 
 does changing contrast , brightness,etc in data set increase the accuracy of the trained net ? : mlquestions i have a data set of around NUM images , if i modify these images by changing their properties such as contrast , brightness, rotation ect to create a larger data set will the accuracy of my net be greater ? very new to this ! EOQ depending on what your images are , it can . i remember someone who won a kaggle competition on identifying galaxies(?) did a write-up on his processes , and part of it was stretching the images in different ways to create a more generalized dataset edit : URL EOA 
 simple projects to begin with ? : mlquestions hello ! i'm in a research program at my school and choose to learn about ml . currently i'm learning algebra ii/trig , so my math education is not nearly enough to easily understand ml . i've been working at a decent pace and i currently understand gradient descent thanks to coursera and andrew ng , however suddenly my teacher requested NUM pages of original research done in a very short amount of time (monday , NUM /27/15). are there any basic data sets and techniques i can use to at least get credit for my research thus far ? ml seems to be a very deep topic you learn about over time , and i currently don't have time. i was thinking about trying to use some modified gradient descent algorithms on data sets , but my issues currently are : i don't know what kind of data i need/where to get it , i don't know how to make a gradient descent algorithm work for more than NUM parameters ( linear regression ) . any advice would be appreciated . i'm sorry for asking this question , i know it's very how do i learn ml quick style , but really i just need a simple project to pass and continue learning at a normal rate . EOQ andrew ng's intro to ml is really good and i'm not sure how you would go about learning ml much faster than that . i think i would recommend binging a bit on his lectures so that you get a bit beyond linear regression with NUM parameters . if i recall correctly , there are assignments in this course , right? maybe you can use that in your report ( maybe extend the exercises a bit ) . i don't really know anything interesting to do with NUM parameter linear regression . you can fit a line through a bunch of data points . if you learn to use more parameters , you can make things slightly more interesting by comparing the lines you get with various numbers of parameters . if you learn about logistic regression , you can start doing classification/detection . you could make networks that simulate and and or logic gates . for xor you need to use a nonlinear technique since it's not linearly separable . for instance , a neural network with a hidden layer ( i.e. a multilayer perceptron ) . one fairly small but interesting problem is classifying the mnist dataset of handwritten digits . however, this is quite a bit beyond linear regression with NUM parameters . you could also take a look at some of the challenges on hackerrack . good luck ! btw , i hope you meant monday the NUM th and not friday the NUM th ( today/yesterday depending on your time zone ) . ( monday the NUM th doesn't exist. ) EOA 
 simple projects to begin with ? : mlquestions hello ! i'm in a research program at my school and choose to learn about ml . currently i'm learning algebra ii/trig , so my math education is not nearly enough to easily understand ml . i've been working at a decent pace and i currently understand gradient descent thanks to coursera and andrew ng , however suddenly my teacher requested NUM pages of original research done in a very short amount of time (monday , NUM /27/15). are there any basic data sets and techniques i can use to at least get credit for my research thus far ? ml seems to be a very deep topic you learn about over time , and i currently don't have time. i was thinking about trying to use some modified gradient descent algorithms on data sets , but my issues currently are : i don't know what kind of data i need/where to get it , i don't know how to make a gradient descent algorithm work for more than NUM parameters ( linear regression ) . any advice would be appreciated . i'm sorry for asking this question , i know it's very how do i learn ml quick style , but really i just need a simple project to pass and continue learning at a normal rate . EOQ thanks for the reply . yeah i'll try to get a few more done to get into multi param . how hard would it be to jump into neural networks do you think ? i understand the basics about cost and gradient descent , along with genetic algorithms (i did the tutorial from ai-junkie.com). i was thinking about making a program that learned how to play flappy bird , maybe i could use opencv in java to read the line of pixels directly in front of the bird and feed those into the input layer . the output layer would be NUM neuron for whether or not to press space bar . would this be very difficult , do you think ? also , yeah i meant monday the NUM th haha . i'm under a bit of stress from this paper , so i can't think very clearly . thank you very much for your reply ! EOA 
 can two machine-learning computers be identital ? : mlquestions hi , i have a question that's been eating me for some time and don't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it's a bit like the rhetorical if two humans grew up in the exact same environmnent , would they behave in the same way , though i would assume it's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you :) peace EOQ i'm not sure if i understood the question correctly . first of all this depends on the algorithm you use and if it has a random component . for example random forests are very unlikely to produce the same results , even if trained with the same data . a nearest neighbor algorithm on the other hand would give you the same results if it is trained with the same training data , since it is deterministic , there simply is no random part . edit: this means a simple k-nn algorithm , not some fancy approximation :) in the latter case there might be a difference based on floating point operations and rounding errors as /u/nixonite was asking , but i think this is rather unlikely if you run exactly the same setup/architecture and software . the amount of errors that may occur are probably not that significant to change the classification outcome . but this is just a rather unfounded guess of mine . EOA 
 can two machine-learning computers be identital ? : mlquestions hi , i have a question that's been eating me for some time and don't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it's a bit like the rhetorical if two humans grew up in the exact same environmnent , would they behave in the same way , though i would assume it's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you :) peace EOQ you mean would their roundoff error be the same for the same algorithms ? EOA 
 can two machine-learning computers be identital ? : mlquestions hi , i have a question that's been eating me for some time and don't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it's a bit like the rhetorical if two humans grew up in the exact same environmnent , would they behave in the same way , though i would assume it's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you :) peace EOQ the hardware differences will not affect your algorithm in any way ( other than how quickly it trains of course ) . EOA 
 can two machine-learning computers be identital ? : mlquestions hi , i have a question that's been eating me for some time and don't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it's a bit like the rhetorical if two humans grew up in the exact same environmnent , would they behave in the same way , though i would assume it's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you :) peace EOQ i think it depends if the algorithm used is deterministic or not . if it uses randomness in some way , then there's no way to guarantee that the results will be exactly same . EOA 
 tf as individual chatbot : mlquestions what happens if you feed in your whole chat-history of facebook to the tensorflow-framework as sequence to sequence model whereas the target-output it the text i wrote and the input the text my facebookfriends wrote . will the result of the training be a chatbot which is using language like me ? EOQ i doubt it . the biggest challenge in nlp is actually understanding the context , not just following the grammatical rules of humans . also, i imagine the training data is not going to be large enough since i assume most of the conversations are unique and largely depends on the situation . of course it will output in the language of yours , but it won't effectively answer the question . EOA 
 question about types of neural nets : mlquestions hey all ! so i have one main question about the different types of neural nets that exist . it seems like a lot of different types of nns fall under the umbrella term of neural net and i was wondering what kinds there are . i'm familiar with a cnn , an rnn and so forth . do those employ backpropogation ? the only one i've had experience with has been the one as done in the andrew ng coursera lectures ( feedforward with backprop-not sure if this is its own type of neural net or if this is just a general term ) i'm also aware that many different types of cost functions exist ? does this actually change the type of net or is this really just a different way to characterize cost ? EOQ those are the NUM main types of nets that you will usually see . also, there are variations of these ( lstm , gru, etc are types of rnns ) . back propagation is the optimization method pretty much always used . you could really use any method you wanted ( genetic algorithms , particle swarm optimization , etc ) but back prop is much faster pretty much always . the cost function you choose is more of a hyper-parameter , like learning rate and batch size. it will depend on what your data looks like , and mostly depends on whether you're doing regression or classification on which cost function you choose . EOA 
 question about types of neural nets : mlquestions hey all ! so i have one main question about the different types of neural nets that exist . it seems like a lot of different types of nns fall under the umbrella term of neural net and i was wondering what kinds there are . i'm familiar with a cnn , an rnn and so forth . do those employ backpropogation ? the only one i've had experience with has been the one as done in the andrew ng coursera lectures ( feedforward with backprop-not sure if this is its own type of neural net or if this is just a general term ) i'm also aware that many different types of cost functions exist ? does this actually change the type of net or is this really just a different way to characterize cost ? EOQ there are certainly many types of neural networks . check out the wikipedia page on recurrent neural networks . the complement of rnns are feedforward nns . this is one of the more clear-cut distinctions since ffnns are directed acyclic graphs and rnns contain at least one cycle . however, i would say that a recurrent mlp is more similar to a feedforward multilayer perceptron than it is to a hopfield network ( which is also recurrent ) . basically you can come up with all kinds of variations by adding , omitting, sharing and fixing connections or changing some activation functions . it's not always clear when a change constitutes a new type of network . what most of these networks have in common is the the node's activation is given by an activation function applied to the weighted sum of other nodes' activations , although some nodes in lstms compute the product . this is different in spiking neural networks , but they are rarely used since they are hard to train . i would say boltzmann machines are also quite different . most of the time some form of backpropagation is used . i think backpropagation technically refers to a fairly specific algorithm , and people often use variants like rprop or quickprop instead , but it's still all about propagating an error back throughout the network . things like momentum and dropout and other training/regularization tricks can be added . it's also possible to use other optimization techniques ( like genetic algorithms ) . hebbian learning ( increase weight between nodes that are simultaneously active ) is not used much anymore i think . ( restricted ) boltzmann machines have their own training procedure that uses gibbs sampling and gradient descent . i would say that the used cost function is probably more part of the training procedure than it is of the network . finally , i should mention that there are also computational neuroscience projects that basically aim to actually simulate parts of the brain ( like ibm's blue brain project ) . these are technically also neural networks , but they are also pretty different . EOA 
 sklearn pca not making sense to me . : mlquestions i'm under the impression that if you have a NUM d data set and you perform pca on it , without any dimensionality reduction it will essentially rotate your data to a new coordinate system . why then when i attempt this in sklearn does the data seem like its being transformed in some way ? here is my code : import numpy as np from sklearn.decomposition import pca import matplotlib.pyplot as plt x-np.linspace(0,10,101) y-x-NUM-np.random.randn(1, NUM ) y-y[0] data-np.asarray(zip(x,y)) pca-pca(n.components-2) new.data-pca.fit.transform(data) plt.figure(0) plt.scatter(data[:,0], data[ :, NUM ]) plt.figure(1) plt.scatter(new.data[:,0], new.data[ :, NUM ]) plt.show() EOQ i think it may just be appearing that way because of the different scales . when i look at it the corresponding points seem to be in the correct spot . add colors-range(101) , and pass in c-colors to scatter() and you'll see it more easily . EOA 
 sklearn pca not making sense to me . : mlquestions i'm under the impression that if you have a NUM d data set and you perform pca on it , without any dimensionality reduction it will essentially rotate your data to a new coordinate system . why then when i attempt this in sklearn does the data seem like its being transformed in some way ? here is my code : import numpy as np from sklearn.decomposition import pca import matplotlib.pyplot as plt x-np.linspace(0,10,101) y-x-NUM-np.random.randn(1, NUM ) y-y[0] data-np.asarray(zip(x,y)) pca-pca(n.components-2) new.data-pca.fit.transform(data) plt.figure(0) plt.scatter(data[:,0], data[ :, NUM ]) plt.figure(1) plt.scatter(new.data[:,0], new.data[ :, NUM ]) plt.show() EOQ i think this is true as well . if you carefully notice the x and y scales , it will make more sense to op . EOA 
 sklearn pca not making sense to me . : mlquestions i'm under the impression that if you have a NUM d data set and you perform pca on it , without any dimensionality reduction it will essentially rotate your data to a new coordinate system . why then when i attempt this in sklearn does the data seem like its being transformed in some way ? here is my code : import numpy as np from sklearn.decomposition import pca import matplotlib.pyplot as plt x-np.linspace(0,10,101) y-x-NUM-np.random.randn(1, NUM ) y-y[0] data-np.asarray(zip(x,y)) pca-pca(n.components-2) new.data-pca.fit.transform(data) plt.figure(0) plt.scatter(data[:,0], data[ :, NUM ]) plt.figure(1) plt.scatter(new.data[:,0], new.data[ :, NUM ]) plt.show() EOQ yea you're right , definitely just the scaling . just glad i'm understanding what's going on . EOA 
 reinforcement learning help ! : mlquestions im learning reinforcement learning . can someone provide me with some example problems which i can try out and improve my understanding . thank you ! EOQ . EOA 
 reinforcement learning help ! : mlquestions im learning reinforcement learning . can someone provide me with some example problems which i can try out and improve my understanding . thank you ! EOQ in the book by sutton & barto there are sometimes scenarios used in the chapters , as well as some case studies in the end , which you can use to create some own problems . did you mean something like this ? EOA 
 reinforcement learning help ! : mlquestions im learning reinforcement learning . can someone provide me with some example problems which i can try out and improve my understanding . thank you ! EOQ i did use that book for learning . i would like a more applied(code:preferably python) example . i do feel i understand better when there is a coded example . please do help ! thanks! EOA 
 unsupervised learning with theano/cgt/tensorflow : mlquestions i have a huge amount of respect for computational graph frameworks , and would love to start working with them more , but most of what i want to do is unsupervised or generative. i've not been able to figure out what kind of loss function to use or how to utilize them to accomplish what i want , so i've always been relegated to writing my own libraries . a google search yields a few pages from deeplearning.net wherein they define an rbm loss function , but that's really about it . the rest are just links to frameworks . i realize the universal trend is to do unsupervised pre-training on the bottom layers of the network and slap a fc network on top , but i really want to do unsupervised all the way to the top . are there good resources or tutorials on using any of these frameworks ? any advice for writing loss functions there ? edit : or am i relegated to only using rbms/auto-encoders with graph frameworks ? EOQ [ deleted ] EOA 
 unsupervised learning with theano/cgt/tensorflow : mlquestions i have a huge amount of respect for computational graph frameworks , and would love to start working with them more , but most of what i want to do is unsupervised or generative. i've not been able to figure out what kind of loss function to use or how to utilize them to accomplish what i want , so i've always been relegated to writing my own libraries . a google search yields a few pages from deeplearning.net wherein they define an rbm loss function , but that's really about it . the rest are just links to frameworks . i realize the universal trend is to do unsupervised pre-training on the bottom layers of the network and slap a fc network on top , but i really want to do unsupervised all the way to the top . are there good resources or tutorials on using any of these frameworks ? any advice for writing loss functions there ? edit : or am i relegated to only using rbms/auto-encoders with graph frameworks ? EOQ it's not particularly difficult to implement a loss function . it's even easier to just directly write a weight update function . coming up with one , however, is rather tricky . EOA 
 ml newbie : how to approach this problem , given a screenshot , find areas of interest or interaction . suggestions? : mlquestions the problem ( in detail ) : given a screenshot , i want to be able to detect areas where a user would typically interact-whether that is through clicking or typing . i would want my output to be somewhat similar to an ocr pipeline. approach NUM -ocr like document analysis : use some kind of modified version of more suited to this problem . the problem : i really have no idea how to even begin here . approach NUM -attempt to learn areas of interest : my idea would be to download a bunch of web pages , screenshot them , and use that as training data because areas of interaction can be identified fairly easily in a webpage ( as either an input , anchor, or button tag ... it doesn't get everything , but may be good enough ) . the problem : my hypothesis is that something like a neural net would not be able to distinguish interactive and non-interactive elements with a high enough accuracy . another problem , even if i think it could be accurate , how could i leverage this as a classification problem to be useful to me ? i.e. my output would just tell me whether or not that screenshot contains an interactive element ? i'm looking at this the wrong way but i can't see another approach . i'm definitely lost , so any pointers would be greatly appreciated ! EOQ you are like NUM % of the way to a complete solution . it looks like what you really need is a classifier that , given an image , produces bounding boxes around the interactive elements . happily algorithms to both detect ( decide if an entity exists in the image ) and locate ( draw bounding boxes around the entity ) are quite common in computer vision . its called object detection or object recognition , and its a common enough thing that you could find tutorials about online. see the pascal voc challenge for more cutting edge algorithms . i am not an expert in this field but if you dig around you should be able find implementations of such algorithms , including ones that use deep learning and ones that don't . as you say , you could then imagine downloading webpages , identifying the interactive elements by parsing the html ( in practice this might be the hard part ) , and then using that as training data for your classifier . since you have potentially unlimited training data , and i would guess that this is a difficult but not extremely difficult classification problem , my guess is that that approach could achieve very good accuracy . note, of course , it is liable to only work for screen shots of webpages , not screen shots in general , since that is where what your training data coming from . EOA 
 are nonlinearities needed if you add bias at each layer ? : mlquestions from what i understand the stacked weight matrices are no longer reducible to one matrix if you have bias at each layer ... yes or no , what am i missing here ? btw , does the same apply if you have normalization at each layer during training and forward pass ? EOQ let's say you have a mlp with two inputs ( x1 and x2 ) and two neurons : neuron NUM : w11-x1-w12-x2-b1 neuron NUM : w21-x1-w22-x2-b2 output : w1-neuron1-w2-neuron2-b-w1-(w11-x1-w12-x2-b1)-w2-(w21-x1-w22-x2-b2)-b-a-x1-b-x2-c ( the formulas for a , b and c are left for the reader , but they do not depend on the inputs x1 and x2 ) . clearly the output is linear with regard to the inputs , the only thing we've accomplished is a convoluted way to determine the linear coefficients a , b and c , which would make the training nonlinear . so the answer is no , biases do not add any nonlinearity to the network , activation functions are definitely required . what do you mean by normalization at each layer ? batch normalization ? EOA 
 are nonlinearities needed if you add bias at each layer ? : mlquestions from what i understand the stacked weight matrices are no longer reducible to one matrix if you have bias at each layer ... yes or no , what am i missing here ? btw , does the same apply if you have normalization at each layer during training and forward pass ? EOQ ok , i see now yes , batch normalization , which i guess is also a linear transformation but dropout is a nonlinearity of some sort ( although only during training ) EOA 
 need help in audio signal processing : mlquestions so i am implementing a project on determining a birds species from its song ( wav file ) i figured this method can be tried out...i need help on implementing it on python ... i'd give anything to anyone who gets it xd EOQ you might want to check out librosa . it's a python library that takes care of the audio processing ( by way of another library audioread ) and also implements many of the common audio features . check out the tutorial . librosa makes it trivial to get a spectrogram and transform it into the cepstral domain . the filter bank mentioned sounds similar to a mel filter bank which is included in librosa . i didn't know we had equal-loudness curves for birds , that's awesome . if you have that it shouldn't be too hard to apply it to the spectrogram , once you're familiar with the matrix representation . hopeufully this helps . good luck ! EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ code for weight correction in the hidden layer ( modified for clarity ) , where i suspect it's wrong : float ogradient-(target[i]-y[i])-y[i]-(1-y[i]) ; float h1gradient-(phi(hidden1.sum))-(1-phi(hidden1.sum))-hidden1.out.w1 ; float h2gradient-(phi(hidden2.sum))-(1-phi(hidden2.sum))-hidden2.out.w2 ; input1.hidden1.w-eta-h1gradient-ogradient-input.signal1 ; input1.hidden2.w-eta-h2gradient-ogradient-input.signal1 ; input2.hidden1.w-eta-h1gradient-ogradient-input.signal2 ; input2.hidden2.w-eta-h2gradient-ogradient- input.signal2 ; hidden1.bias-eta-h1gradient-ogradient ; hidden2.bias-eta-h2gradient-ogradient ; where phi is the sigmoid function , and y[i] is the current output of the output neuron . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ well , what i'm seeing here seems correct to me , except you also need to update the weights to the outputs : hidden1.out.w-eta-ogradient-phi(hidden1.sum) ; hidden2.out.w-eta-ogradient-phi(hidden2.sum) ; out.bias-eta-ogradient ; if it's not working , you might have a bug elsewhere in your code , so i suggest posting it all . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ do you mean i should update the weights to the outputs , and then use them to update the input-to-hidden weights ? the unmodified code is way too messy for anyone to understand it and i checked many times . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ you should calculate h1gradient and h2gradient before updating hidden1.out.w1 and hidden2.out.w2 . other than that , the order shouldn't really matter . this code is also super messy , but it should work . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ it's just no use . my code follows that logic . i checked it a hundred times , but it just keeps outputting nearly the same results for all inputs . it's like the weights are modified to produce a certain result for all inputs and nothing else . it's been NUM days now and i really have no idea what else to do . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ how many epochs are you training ? if i use random initializations ( uncomment line NUM and remove the NUM ) and a relatively high learnrate of NUM , i often need to train more than NUM epochs . you won't see a lot of changes in the first few . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ just tried NUM , and i still get the same outputs ranging from NUM 3 to NUM 1. EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ it can approximate the sigmoid function to a reasonable accuracy though . but for NUM (sinx-1) it just gives me the same output . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ thank you so much cyberbyte for the help . it was actually working , but it wasn't as precise as i expected . i plotted the points and i do get something as the original function but i think it'll need some tweaking to get it fit more closely . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ but still , shouldn't the algorithm find the correct weights to solve the xor problem if all nodes use sigmoid ? could there be something else that's wrong in my code ? the functions it tries to approximate don't always look good and they are squashed into a small region of space . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ okay , if i use NUM neurons and the linear activation function for both hidden and output , it can learn and , or, and xor . but that's just useless . i want to be able to approximate functions . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ please , can someone at least give me the correct weights and biases for a NUM-2-1 network that uses sigmoid(1/(1-e-x) for all neurons ? it's driving me insane . i just want to know where the error is . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ that depends on what you want your network to learn . i thought you already had the weights for xor , but here they are again : input1 -> ; hidden1 : NUM input2 -> ; hidden1 : NUM bias -> ; hidden1 :-5000 input1 -> ; hidden2 :-10000 input2 -> ; hidden2 :-10000 bias -> ; hidden2 : NUM hidden1-> ; output : NUM hidden2-> ; output : NUM bias -> ; output : -15000 basically , this turns the first hidden node into an or gate , the second into a nand and the output into an and . i used very large values to get close to the extreme values of the sigmoid . that's not really necessary , but the larger they are , the closer you'll get . as others have pointed out , you should just remove ( or replace ) the sigmoid function in the output layer if you want unbounded outputs . also, it's important to initialize your network so that the weights start out being different from each other . they should probably start close but not exactly equal to zero . if a weight is zero , then the input doesn't matter , because NUM-0 is the same as NUM-0 . i suspect that is your problem . if you post your code , we could maybe help you look for any mistakes . use these guidelines for posting code . edit : corrected values ... EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ [ deleted ] EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ thanks , i tried them and i just get NUM for all inputs . but that's what i get when i calculate by hand as well . so is there something wrong with my logic ? say i get input (0,0), which means each hidden has its own bias as the total sum so : h1-15000 h2-15000 calculating sigmoids : phi(h1)-NUM phi(h2)-NUM which means the output neuron gets a total sum of : NUM-5000-5000 which outputs a phi(5000)-NUM it can never output a zero . what exactly is wrong here ? i just don't get it . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ i'm very sorry , but i switched the biases for the first hidden node and the output . i was worried i would get something wrong and actually tested it for myself in matlab , and somehow i still messed up transcribing the values here ... you are entirely correct that my idiot values always give NUM as output , because the corresponding network encoded ( a and b ) or not ( a and b ) , which is always true . it should now be ( a or b ) and not ( a and b ) , which is what you want . i edited the values into my first post , but as i said , you only need to change the biases of hidden1 and output . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ i plugged in those values and it works fine. there is absolutely nothing wrong with the forward pass , but the backpropagation phase doesn't seem to have any errors either , so i just don't understand why it can't find the correct weights . edit : i set those values as initial weights and biases , and they don't change at the end . i still get the correct results . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ well , they are already correct , so they don't need to change . of course , in theory they should change a little bit , but in practice it might be unnoticeable even if your implementation is correct . there are two reasons for this : NUM ) the error is almost zero and NUM ) the gradient of the sigmoid function at these extreme values is almost zero . if you divide the weights i gave you by NUM , you get more realistic values that still have some room for improvement . you might also want to slightly change them so that no two weights are exactly equal . in this case it shouldn't really matter , but normally you need this to break the symmetry . to really test your backprop algorithm you should just do a bunch of random initializations ( initialize weights uniformly at random between-1 and NUM ) and see if it learns . it can also help to pick some values and then do one manual pass of the backprop algorithm . calculate all of the values for yourself and then check to see where your code gets a different answer . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ i calculated the values for one pass by hand for input (1,1). there is no difference at all , but it still doesn't work . maybe i need to add momentum ? but i thought that is not essential . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ it's not . momentum is useful , but you should probably ensure that your code works without it first before adding extra complications . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ NUM -you just give up sigmoid for the output layer . keep sigmoid for hidden units only . NUM -does it learn or and and functions without problems ? EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ but if i give up sigmoid , does that mean the gradient for the output neuron will be just the error ( desired-result ) ? it can learn or and and , but only if the output neuron is no longer a sigmoid . if i set the weights manually with this configuration ( hidden : sigmoid, output : linear threshold ) , i get the right results , but it can't learn . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ yes , error ( desired-result ) is exactly what you want to minimize with gradient descent . also, you need to initialize weights randomly ( like-1.0 to NUM ) if you have hidden layer(s) . if it can learn or and and but not xor , it is possible you backpropagate error wrongly to hidden units . calculate things by hand and check . bp is a bit tricky to implement correctly . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ well , i posted the code but it sinked at the bottom . i'll check the backpropagation phase and output stuff to see where it goes wrong . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ on mobile so please forgive smelling errors . i'll try to give a more thorough answer when i get to my laptop . in the meantime : NUM : you can either try linear output nodes instead of sigmoid , or just scale your answers by some constant that allows signal outputs to generate answers in the range you want . stick to approximately NUM to NUM , not NUM ... NUM . NUM : a single hidden layer ffnn with two hidden nodes can solve xor , but backpropagation isn't guaranteed to find that answer . try more nodes and different initialization . do not initialize weights to NUM . EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ but wouldn't using linear outputs be significantly less accurate since we don't have a derivative we can use to update the weights and biases ? EOA 
 after NUM hours , i still can't figure out how to properly implement a backpropagation algorithm . help please . : mlquestions i spent nearly NUM hours thinking , reading, and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i'm facing . NUM-so i want to approximate a function ( x2 , sinx, ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from NUM to NUM ? NUM-my network can't even figure out the xor problem with NUM hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . NUM-it always produces the same output for any input . EOQ linear output nodes sum incident weights and then instead of transforming that sum by y-sigmoid(x) , they transform the sum by y-x . the derivative of y-x is NUM . ...so you have a derivative you can work with . backpropagation is a notoriously difficult algorithm to code , not because it is complex , but because many common bugs will leave an implementation in a state where it partially works . it's painful , but sometimes it helps to calculate one or two iterations of backprop over a small network on paper , and then compare to your program . EOA 
 in machine learning is it necessary to take software engineering ? : mlquestions there's a statistical and machine learning major which i'm going into but i would like to pair it with either a language technology minor or a software engineering minor . right now i'm not sure which would come in more useful in the field . EOQ when you say the field , what field do you mean ? software engineering ( or at least programming ) is going to be relevant for all machine learning , no matter what direction you go . in fact , it is so relevant that i expect your major will already include quite a bit of it . i don't know too much about language technology minors , but it sounds like it might be useful if you want to do natural language processing . if you want to do nlp , or want to learn something more unique , then you should probably that language technology minor ( you'll have to learn to program anyhow ) . if you just want to do ml in general , then software engineering is the most obvious choice . EOA 
 in machine learning is it necessary to take software engineering ? : mlquestions there's a statistical and machine learning major which i'm going into but i would like to pair it with either a language technology minor or a software engineering minor . right now i'm not sure which would come in more useful in the field . EOQ thank you . the major does include quite a bit of it [ programming ] but i want more . software engineering would be the way to go to learn more programming . i thought that because there is a lot of interesting work in nlp without specific language technology knowledge i might not do as well , though i don't want to specialize in it . EOA 
 [work question] what does the workday of someone working in machine learning look like ? : mlquestions what do you love the most about your job ? what is your least favorite thing ? what steps have you taken to get to your position ? what are personal qualities that are necessary to succeed in machine learning ? if you could give a piece of advice to a young person seeking a career in machine learning , what advice would you give ? EOQ i wouldn't say i every really worked in ml , but i have had work where i made heavy use of ml . currently i'm a phd student in ai , which means that i'm mostly just reading a lot of articles and then occasionally implementing some algorithms to try them out , but ml isn't my focus . i used to work for a small computer vision company where i worked on custom software for clients , which included stuff like facial expression recognition , video surveillance , body pose estimation , object recognition/detection/tracking and behavior analysis . when a project started we would have some meetings about what the client wanted , what we expected to be able to deliver , and how we might be able to tackle the challenges . we would usually have some initial ideas of what techniques could be used , after which i would dive into the scientific literature about those techniques ( if it was my project ) . then at several points in time i would present my ideas and progress to the rest of the team , who would then give feedback . also at different points in time the project would get redefined in several ways because the client wanted something different , or some algorithm turned out to really not work , or we adjusted our expectations of what was possible , etc. i probably spent most of my time programming . after deciding what techniques to use , they needed to be implemented . you can read about these things in scientific papers , but it's ( almost ) never quite what you need , so you have to adapt it to your specific situation and problem , and usually this would involve cobbling together multiple ml algorithms . that also means that you have multiple points of failure , and that you need to train a bunch of separate algorithms . a lot of time was also spent gathering or producing data , training the algorithms and then testing them . this often involved creating separate annotation and training and training tools , and then actually doing the annotations . sometimes we could tell the client what kind of setup they should use ( i.e. number and kind of cameras and computers ) , which would mean researching what was best/cheap and building the setup . otherwise, i occasionally visited some clients in order to get data from their actual setups , or i'd try to recreate locally it to get data that was somewhat similar . once i had all of that , i could bring it all together in some piece of software for the client . even if you have all of the ideas , this still takes quite a bit of time. if you have a whole pipeline of algorithms , it can be difficult to get them to work together , and there is always something that is inaccurate or too slow , which means you might have to replace it or gather more data/retrain it or change the setup ( if that's an option ) . finally, you need some nice way of visualizing the results for the client , and the program needs to actually be usable by other people who are not necessarily experts . this is usually pretty easy compared to the other things , but it can still take a lot of time . aside from that it's just regular company and miscellaneous stuff . meetings, some administration and reporting , creating tools for internal use , trying to upgrade the dev stack , updating/upgrading the website , learning new stuff , organizing and attending events , and generally just having a nice time with coworkers ... EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ so actually , you're going to need to use a neural network because you're dealing with complex non-linear hypothesis . the theory is similar , but the the implementation is different . i can't help there yet . EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ i'd say this is a pretty big problem . should be possible but might need some month or even years of research . EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ what you have to first understand is modelability vs model choice . i work in finance using machine learning techniques . if the problem can be modelled , there is a few techniques that will work . i recommend the book computer vision with python to get you up to speed with lots of image problems in python . URL where i suspect the issue may come is modelability . of course we can contrast some amazing shot with something i took on my NUM year old mobile and find some different features . but , in lots and lots of cases the line between 'professional' and amateur will be very blurred . i have guys on my facebook who take pictures better than many family professional portrait photographers do , students who take pictures that could be on national geographic . i don't want to dissuade you , but recognise that the important step is not really training a neural network-it's finding a reasonable hypothesis of what features select between amateur and professional . this is why we don't take tbs of data , feed into some black box model and out pops predictions for stock prices , horse race winners etc . i generally believe the best approach is 'i have data that separates based on xyz features , how can i model this?' rather than 'how can i use this model in my problem' . i suggest you go through some resources and example problems on image classification like i referenced , this will enable you to get some experience investigating problems . EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ i don't actually care if a photo is in face professional or amateur , but whether it looks professional or amateur , that is the quality of the photograph . so if for example it is a great photo i don't care that it was taken with an iphone EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ yes you could . there's something called a classification algorithm , or logistic regression . fancy words that just mean is it probably this or is it probably that ? the more training examples you provide , the more accurate your results will get usually , and for this example you would benefit from having a bunch of amateur photos as well . since there will be a large number of features , you won't really be aware of what the algorithm knows , but you would be able to tell if one matches your training set , or doesn't . maybe someone else can provide more details , because i don't know exactly how to work with photos yet . i just know how to work with data , but with these photos the algorithms will simply look for patterns in the pixels . for increased comprehension , read slowly : basically , what you do is provide a set of training examples with several features that the algorithm can focus on , then you provide it a definite answer . so you say , this is professional . this is not . the algorithm then identifies something called a hypothesis function that tries to match as many of the features observed in the training examples as possible , based on a set of numbers called parameters or weights , that when plugged into the hypothesis , can predict if the data you're feeding it falls in the matching side or the not matching side. it involves something called a cost function which figures out how different your hypothesis is from the actual data . this cost function is set to be minimized , or to find the partial derivative of the curve with respect to the parameters ( basically , when the hypothesis differs from the actual data the least ) , and this is done through another function called gradient descent . gradient descent is a stepwise calculation that takes arbitrary initial values for your weights , calculates the tangent of the cost function , and repeats until it reaches a minimum , meaning the difference between the hypothesis and the actual training examples is minimized . once you have the right parameters or weights , you can then feed the hypothesis new information , and using those parameters it figured out through gradient descent , it runs a comparison saying , yes this image likely matches the professional set , or no this image doesn't match the professional set . like i said though someone else may be able to help a little bit with how to exactly do this with images . but all in all , it's just programming a few very simple functions . nothing too algorithmically complex . edit : grammar fixes and clarifications EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ wow i think most of the in-depth part went over my head-would i need to understand all of that entirely to implement something like this ? EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ think of it this way-you're showing a toddler NUM pictures of scary monsters , and NUM pictures of nice people . this is your training set . the toddler picks a set of features and maps them out in its memory , saying this collection of features are in this way when it is a scary monster . next, the toddler generates a hypothesis that says-anything that has features that are like this is probably a monster , and anything that doesn't is probably a nice person . but it can't just know what the hypothesis is , and you can't program it because it's likely too complex . you need to let it discover it itself . and this is where a gradient descent function is used . it is like saying ... if i put a hat on someone does it make them a monster ? and the answer from the cost function is 'barely' . then it goes if i paint their skin green does it make them a monster ... and the answer is 'a little closer' ... and it keeps repeating over and over until the hypothesis is like if the skin is green and they have red eyes , and pointy teeth and a mean looking face is it a monster ? and then the hypothesis in the toddler's mind is like ... that pretty much fits what i think a monster is . so now you have your weights or parameters that the toddler is using to judge the images . so when you show the toddler another image in the future-they're like ... yup, probably a monster . or nope , probably not a monster . so you need to know how to do a few of those things . but they're not that complicated . EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ so how do i go about implementing this and what size data-set would i need ? would NUM ,000 images be enough ? EOA 
 can i use machine learning to recognize professional photography ( vs amateur photography ) ? : mlquestions i'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : URL what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) EOQ not sure what's best for images like i said . maybe someone else does ? dunno why my original comment was downvoted ... here's a start : URL EOA 
 basketball ( or any sport ) predictions : mlquestions are there generally accepted algorithms used for predicting the outcomes of sports games ? i'm getting ready to scrape historic data for ncaa bb and want to actually use it for something . i've come across the pythagorean formula , but there has to be something better than a predictor for baseball . EOQ check out this blog : URL what a wonderful breakdown of many different prediction algorithms ! definitely one of the best blogs out there on basketball prediction . EOA 
 ml n00b here . starting to learn ml on my own . classification technique guidance required . : mlquestions im starting to learn ml by myself . i have a course starting ml from next year , but im too eager to learn it before the course begins . i always learn by starting to solve a problem and then read up the theory behind it . so in a way , when i solve the problem id have read the theory behind it and applied it at the same time. so now , im trying to classify images into NUM categories . to make my life easier i have even got the feature vectors . i just need to classify them into multiple categories . how do i go about doing this ? what are the various techniques ? how accurate are they ? how do i tweak the performance ? which programming language should i use ? please help me out with this . thanks in advance ! EOQ for a complete beginner i'd recommend playing around with more than anything , it has the shallowest learning curve of any ml toolkit i've worked with , and it has great pointers should you want to learn more . also , read up on evaluation . it's really key to getting good results and comparing models : accuracy is not everything , and sometimes it's not informative. for example , if you were trying to predict whether a car has a boot on it from an image , you could probably get NUM % accuracy by always predicting no boot , since most cars are not booted . EOA 
 class of concepts : mlquestions i have a class of concepts c of the form ( a ? x ? b ) ^ ( c ? y ? d ) where a , b, c , d ∈ { NUM , NUM , NUM } . i have two questions . NUM how many distinct concepts c can be formed ? NUM what happens if we allow a , b to take also negative integer values , that is , a, b ∈ { ?2,?1, NUM , NUM , NUM } while c and d remain with values in { NUM , NUM , NUM } ? how many distinct concepts as described above are there in this case ? EOQ your notation is a little bit unclear to me . what does ^ mean , and why can't you just say that x and y are elements of { NUM ,1,2 } or { ?2,?1, NUM , NUM , NUM } ? what are you trying to do here ? EOA 
 general question about data sets with large number of boolean columns . : mlquestions so let's say i have a data set where each row is a very long list of booleans and i want to predict the value of certain columns given other columns , but not always necessarily the entire row ( minus the column i'm predicting ) . so basically i will have a subset of a complete row and would like to predict the target column's value . what methods should i be focusing on to accurately tackle this problem ? EOQ a bayesian network would probably be ideal , since it lets you input the values you know and then gives you probability distributions over the rest . this can be a fairly good option if you know the structure of your domain so that you can construct the network manually so that only probability distributions need to be learned . otherwise you need to do structure learning as well , which is more difficult . neural networks may also be an option . i've never seen them used in the wild , but it sounds like maybe you could use interactive activation networks ( see chapter NUM here ) , which actually end up functioning a lot like bayes nets . otherwise you could try using some kind of autoencoder . these are basically networks that are trained to output their input . if you then specify incomplete input , it should autocomplete . make your true/false inputs NUM and-1 so that you can use NUM for missing data . EOA 
 just started with introduction to statistical learning : mlquestions and i have some basic questions . equation NUM states : y-f(x)-e quoting : here f is some fixed but unknown function of x1 , ..., xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . questions : what do they mean here by systematic information ? if e has mean zero , then why even include it in the equation ? are there any scenarios where e is non-zero ? if there are , what are some examples ? is e a vector ? a set ? a scalar ? thank you . EOQ let me see if i can help : systematic information probably just means information . the idea is that knowing x gives you some information of the value of y . the function f describes what different values of x tell us about y so it could be said to represent that information . systematic might be just be a reference to the fact f is deterministic . e has the same form as y and the output of f(x) , so if we are trying to predict a scalar with f(x) it would be scalar , but if we are trying to predict a vector with f(x) it will be a vector of the same size. however note e is not a fixed , deterministic value . instead it is a random variable ( or vector of random variables ) . it's actually quite important to include 'e' even if it is zero mean . we expect it to be non-zero is almost all cases where we are trying to model something occurring in the real word . to explain further , typically we would be using this sort of equation to model a real world process . for example , maybe we say y-NUM 8-x where y is the amount a customer tipped and x is the amount they spent . however inevitably this equation will be wrong much of the time ( many customer tip more , many tip less ) . even if we tried to make the equation more complex ( we could add terms to account for the time of day , where the customer was a repeat customer , ect. ) the equation would still be wrong most of the time. in fact in general most real world processes are too complex to exactly model with a single deterministic equation f , so we commonly model processes by finding an equation that is almost correct , and account for the fact there are other random or unaccounted for factors that affect what values of y we get for a given x in random , hard to predict ways by including the e term . EOA 
 just started with introduction to statistical learning : mlquestions and i have some basic questions . equation NUM states : y-f(x)-e quoting : here f is some fixed but unknown function of x1 , ..., xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . questions : what do they mean here by systematic information ? if e has mean zero , then why even include it in the equation ? are there any scenarios where e is non-zero ? if there are , what are some examples ? is e a vector ? a set ? a scalar ? thank you . EOQ just rephrasing what you said-if e has a zero mean , this means that f(x) would be an unbiased estimator of the y . EOA 
 just started with introduction to statistical learning : mlquestions and i have some basic questions . equation NUM states : y-f(x)-e quoting : here f is some fixed but unknown function of x1 , ..., xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . questions : what do they mean here by systematic information ? if e has mean zero , then why even include it in the equation ? are there any scenarios where e is non-zero ? if there are , what are some examples ? is e a vector ? a set ? a scalar ? thank you . EOQ if e has mean zero it means that the error can go on both sides of a regression line more or less with equal probability . i.e. positive and negative error ( positive error as in your f-positive value e , negative error as in your f-negative value e ) will occur normally . if e is non-zero ? sure, if you tend to have a model which for example always over-estimates the regression output , then the error will be leaning towards negative values . e.g. if you predict on average a value of NUM for every true value of NUM then your model is estimating higher values on average and so the error would be NUM-110-10 . e can be called a vector sure if it's the list of errors for a list of predictions , but if you're talking about one prediction , then there is only NUM error value for it , this is of course assuming only a single variable . if you have a multivariate model then if you predict (10,100), the error will be ( ex , ey ) i.e. a vector of the same size with individual errors inside corresponding to the errors in the prediction for each axis . EOA 
 just started with introduction to statistical learning : mlquestions and i have some basic questions . equation NUM states : y-f(x)-e quoting : here f is some fixed but unknown function of x1 , ..., xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . questions : what do they mean here by systematic information ? if e has mean zero , then why even include it in the equation ? are there any scenarios where e is non-zero ? if there are , what are some examples ? is e a vector ? a set ? a scalar ? thank you . EOQ also , is there a subreddit dedicated to the discussion of this book ? EOA 
 just started with introduction to statistical learning : mlquestions and i have some basic questions . equation NUM states : y-f(x)-e quoting : here f is some fixed but unknown function of x1 , ..., xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . questions : what do they mean here by systematic information ? if e has mean zero , then why even include it in the equation ? are there any scenarios where e is non-zero ? if there are , what are some examples ? is e a vector ? a set ? a scalar ? thank you . EOQ not that i know of EOA 
 good data preprocessing python module ? : mlquestions i have to do a lot of data preprocessing for my machine learning task . specifically scaling , inserting missing values , splitting large multidimensional numpy arrays into training , validation and testing sets , handling timestamps and timeseries data . i have been building small methods to handle the data with mostly sklearn and various numpy functions . is there a good library i can use that will help me with preprocessing data ? is sklearn the best option ? EOQ for scaling , inserting missing values and handling timestamps and timeseries data the answer is pandas . it has labelled dataframes as basic objects , a wide variety of tools and methods for handling null or missing data (filling , dropping, back filling , forward filling , interpolating, etc.), and unparalleled timestamp and timeseries facilities . it also has great tools for sampling , munging, merging , filtering, summarising and plotting data , but those are just gravy at this point . if you don't know pandas you should learn it . there's a great tutorial by brandon rhodes that will get you started , and after that the docs are excellent . EOA 
 sklearn pca with pandas dataframes : mlquestions when passing along a pandas dataframe to a pca() object , how can i tell which of the input vectors are being chosen when using n.components-'mle' ? pca-sklearn.decomposition.pca(n.components-'mle') pca.fit(df) EOQ pca doesn't choose input vectors . it ( lineary ) combines them all . EOA 
 sklearn pca with pandas dataframes : mlquestions when passing along a pandas dataframe to a pca() object , how can i tell which of the input vectors are being chosen when using n.components-'mle' ? pca-sklearn.decomposition.pca(n.components-'mle') pca.fit(df) EOQ you can look at explained.variance.ratio. to see which input vectors are contributing to the variance ( and where you output vectors are those of maximal variance ) . it would give you some idea of the varying contributions . EOA 
 [beginner] what is a good ml textbook that contains pseudo code of algorithms ? : mlquestions i have been looking at books . i like tom mitchell's book on machine learning as it has some pseudo code for c4.5. does anyone else know of other similar books that have pseudo code ? if not , does anybody know where decent documented source code is for a ml library ? i took a look at weka and it wasn't the easiest code to follow . EOQ introduction to statistical learning with applications in r when i help companies interview data scientists , i advise that candidates should be able to open this book up to any page and explain the concepts . EOA 
 [beginner] what is a good ml textbook that contains pseudo code of algorithms ? : mlquestions i have been looking at books . i like tom mitchell's book on machine learning as it has some pseudo code for c4.5. does anyone else know of other similar books that have pseudo code ? if not , does anybody know where decent documented source code is for a ml library ? i took a look at weka and it wasn't the easiest code to follow . EOQ have you ever met any self-taught data scientists ? EOA 
 [beginner] what is a good ml textbook that contains pseudo code of algorithms ? : mlquestions i have been looking at books . i like tom mitchell's book on machine learning as it has some pseudo code for c4.5. does anyone else know of other similar books that have pseudo code ? if not , does anybody know where decent documented source code is for a ml library ? i took a look at weka and it wasn't the easiest code to follow . EOQ well , considering data science as an official major is just starting , i'd say most people have been self-taught . that being said , most have backgrounds in computer science , statistics, and research/scientific thinking . EOA 
 [beginner] what is a good ml textbook that contains pseudo code of algorithms ? : mlquestions i have been looking at books . i like tom mitchell's book on machine learning as it has some pseudo code for c4.5. does anyone else know of other similar books that have pseudo code ? if not , does anybody know where decent documented source code is for a ml library ? i took a look at weka and it wasn't the easiest code to follow . EOQ nice , thanks for the response and book recommendation . i ordered it yesterday . hopefully i can stay focused enough to finish it . EOA 
 [beginner] what is a good ml textbook that contains pseudo code of algorithms ? : mlquestions i have been looking at books . i like tom mitchell's book on machine learning as it has some pseudo code for c4.5. does anyone else know of other similar books that have pseudo code ? if not , does anybody know where decent documented source code is for a ml library ? i took a look at weka and it wasn't the easiest code to follow . EOQ excellent . does it discuss pseudo code ( even if very centric to r ) . i am trying to go deeper than just being able to use libraries and packages , to understanding the algorithms themselves . EOA 
 fraud detection with unsupervised learning : mlquestions i'll preface by saying i am brand new to ml . i have a data set of ~10,000,000,000 ( fake ) transactions where each row is the username , time of transaction , credit card number , device used ( ios , android, laptop , etc ) and amount . i need to determine which transactions are likely malicious/fraudulent and which aren't . i would prefer to approach this with unsupervised learning of some kind , as going through these and manually labeling ones i think are bad would be super tedious and not necessarily correct . is there a good approach to this problem with unsupervised learning ? i've read about random forests and decision trees ( high level overviews ) and maybe they would do the trick ? thanks EOQ fraudulent transactions are likely different from 'normal' transactions , so you are basically looking for outliers . you could start looking at some histograms / scatterplot of your data to begin with . you could use clustering ( unsupervised learning ) to determine what are 'normal' transactions , and then you could find which transactions are not near clusters , and are thus likely fraud . you could start with some simple clustering algorithms such as kmeans or hierarchical clustering approaches . if you really want to do something fancy you could try using tsne , but i would try easy approaches first . you will have to find some algorithms that will scale to your dataset though ( look for large scale clustering toolbox or something like this ) ... if you have no information about which transactions are fraud ( it sounds like you don't have access to this information ) you cannot train any supervised models such as random forests or decision trees ( those are usually supervised models , unless you are talking about some special models ) . EOA 
 fraud detection with unsupervised learning : mlquestions i'll preface by saying i am brand new to ml . i have a data set of ~10,000,000,000 ( fake ) transactions where each row is the username , time of transaction , credit card number , device used ( ios , android, laptop , etc ) and amount . i need to determine which transactions are likely malicious/fraudulent and which aren't . i would prefer to approach this with unsupervised learning of some kind , as going through these and manually labeling ones i think are bad would be super tedious and not necessarily correct . is there a good approach to this problem with unsupervised learning ? i've read about random forests and decision trees ( high level overviews ) and maybe they would do the trick ? thanks EOQ yeah i don't have access to info on any of the fraudulent transactions . could the clustering be done based on all of the different columns in the data set ? EOA 
 fraud detection with unsupervised learning : mlquestions i'll preface by saying i am brand new to ml . i have a data set of ~10,000,000,000 ( fake ) transactions where each row is the username , time of transaction , credit card number , device used ( ios , android, laptop , etc ) and amount . i need to determine which transactions are likely malicious/fraudulent and which aren't . i would prefer to approach this with unsupervised learning of some kind , as going through these and manually labeling ones i think are bad would be super tedious and not necessarily correct . is there a good approach to this problem with unsupervised learning ? i've read about random forests and decision trees ( high level overviews ) and maybe they would do the trick ? thanks EOQ yes , these are typically the 'features' you are clustering . however, note that if you have for example text in these columns , you will need to convert them to numbers , otherwise most clustering methods will not deal with it . EOA 
 what type of machine learning or decision making should i start looking into for this task ? : mlquestions thank you in advance for reading this-i know enough to know that i don't really know anything . but here is what i am trying to accomplish : i have two populations of individuals , let's call them a and b . a is a mostly static group of people with the normal traits a person would have , age, gender , locale, specific degrees in some form of education , a number of years involved in our project , etc. they also have a history we could review of encounters and outcomes we could teach a system with . b is an ever changing population of people with the same traits but with the addition that we want them to take an action . let's say we want them to read more books , or walk more . the game is that a person from a is up to talk to someone and try to convince them to take an action . i am wondering what sort of machine learning or approach to ai would watch and learn the successes and failures in population a and make the best decision on who from population b they should be matched up with . what would most likely result in an action taken . i simply have no idea on what direction to head with this ? a few word answer would get me going for some time on my own . maybe i am dreaming but i have always been fascinated by ai since i read marvin minskey's society of the mind decades ago . i think that was the name . i would love to pursue leveraging some sort of machine learning in this task . sadly aside from a fascination and some more light reading i trended more into mainstream programming :( thanks for any help , even if it is your nutz , this is not something for machine learning thank you , bill EOQ if you have data on 'successful' interactions . that is when a member of particular a succeeds in getting a particular member of b to perform an action then i would model this as classification problem for edge prediction on a graph . you have a bipartite graph of a's and b's and you have your known edges , when an a has successfully persuaded a b for a task . you then use this data to train a model to look for additional connections . you can either model each edge as 'successful' ( binary classification ) or each edge as 'successful for action k' then a multiclass problem . either way this differs from normal classification in that you are using attributes of both a and b to make the prediction . this may make an advantage over just modelling a to predict if they can persuade any b . have a look at this paper : URL its a biological paper so you need to read through the biology but could be useful . EOA 
 what type of machine learning or decision making should i start looking into for this task ? : mlquestions thank you in advance for reading this-i know enough to know that i don't really know anything . but here is what i am trying to accomplish : i have two populations of individuals , let's call them a and b . a is a mostly static group of people with the normal traits a person would have , age, gender , locale, specific degrees in some form of education , a number of years involved in our project , etc. they also have a history we could review of encounters and outcomes we could teach a system with . b is an ever changing population of people with the same traits but with the addition that we want them to take an action . let's say we want them to read more books , or walk more . the game is that a person from a is up to talk to someone and try to convince them to take an action . i am wondering what sort of machine learning or approach to ai would watch and learn the successes and failures in population a and make the best decision on who from population b they should be matched up with . what would most likely result in an action taken . i simply have no idea on what direction to head with this ? a few word answer would get me going for some time on my own . maybe i am dreaming but i have always been fascinated by ai since i read marvin minskey's society of the mind decades ago . i think that was the name . i would love to pursue leveraging some sort of machine learning in this task . sadly aside from a fascination and some more light reading i trended more into mainstream programming :( thanks for any help , even if it is your nutz , this is not something for machine learning thank you , bill EOQ thanks ! i started looking up and digesting the terminology this morning and grabbed the paper to read today . probably take me a day for it all to sink in then i may have a clarifying question or two . i suspect i am getting the concept . thanks again EOA 
 [beginner] error rate stays same from beginning to end : mlquestions i started learning nn . after some reading of theory i stumbled upon this blog and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on NUM error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal NUM . code is here . output: error (iterations : NUM ): NUM 36383143672 error (iterations : NUM ): NUM 00000789314 error (iterations : NUM ): NUM 00000195026 error (iterations : NUM ): NUM 0000011538 error (iterations : NUM ): NUM 00000692513 error (iterations : NUM ): NUM 00000173484 error (iterations : NUM ): NUM 00000148476 error (iterations : NUM ): NUM 00000059061 error (iterations : NUM ): NUM 00000134286 error (iterations : NUM ): NUM 00000466787 any help or suggestions ? EOQ if its a balanced binary classification problem then the expected error rate of a random guess should be NUM . i would suggest implementing grad check , though it may not help you here . EOA 
 [beginner] error rate stays same from beginning to end : mlquestions i started learning nn . after some reading of theory i stumbled upon this blog and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on NUM error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal NUM . code is here . output: error (iterations : NUM ): NUM 36383143672 error (iterations : NUM ): NUM 00000789314 error (iterations : NUM ): NUM 00000195026 error (iterations : NUM ): NUM 0000011538 error (iterations : NUM ): NUM 00000692513 error (iterations : NUM ): NUM 00000173484 error (iterations : NUM ): NUM 00000148476 error (iterations : NUM ): NUM 00000059061 error (iterations : NUM ): NUM 00000134286 error (iterations : NUM ): NUM 00000466787 any help or suggestions ? EOQ couldn't spot an obvious error in the code . i don't speak alot of python though . some questions that might help you : how do the learning curves look like ? ( alpha too big? ) what are the values of the target vector ? (y in [0,1]?) is your data normalized ? EOA 
 [beginner] error rate stays same from beginning to end : mlquestions i started learning nn . after some reading of theory i stumbled upon this blog and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on NUM error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal NUM . code is here . output: error (iterations : NUM ): NUM 36383143672 error (iterations : NUM ): NUM 00000789314 error (iterations : NUM ): NUM 00000195026 error (iterations : NUM ): NUM 0000011538 error (iterations : NUM ): NUM 00000692513 error (iterations : NUM ): NUM 00000173484 error (iterations : NUM ): NUM 00000148476 error (iterations : NUM ): NUM 00000059061 error (iterations : NUM ): NUM 00000134286 error (iterations : NUM ): NUM 00000466787 any help or suggestions ? EOQ try l2.error-l2-y ? EOA 
 [beginner] error rate stays same from beginning to end : mlquestions i started learning nn . after some reading of theory i stumbled upon this blog and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on NUM error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal NUM . code is here . output: error (iterations : NUM ): NUM 36383143672 error (iterations : NUM ): NUM 00000789314 error (iterations : NUM ): NUM 00000195026 error (iterations : NUM ): NUM 0000011538 error (iterations : NUM ): NUM 00000692513 error (iterations : NUM ): NUM 00000173484 error (iterations : NUM ): NUM 00000148476 error (iterations : NUM ): NUM 00000059061 error (iterations : NUM ): NUM 00000134286 error (iterations : NUM ): NUM 00000466787 any help or suggestions ? EOQ it worked . thanks :) EOA 
 can a nn be taught to multiply two numbers ? : mlquestions hi , i've been playing with regular neural networks with relu input , hidden and output units . i have successfully trained it to add two real numbers , just approximating binary functions for fun , of the form f(x , y)-z . the input is NUM numbers [ negpartofx pospartofx negpartofy pospartofy ] ex . f(-5.1, NUM ) becomes f([5.1 NUM NUM NUM ]) . output is similar [ negz posz ] addition is then simply to learn direct transfomration matrix [ NUM -1 NUM -1 ] [-1 NUM -1 NUM ] i've tried similar approach to learn general multiplication , but with no luck . ie. i expect it to work for any real numbers , it sort of works for numbers within a limited range , but the results become too low for large numbers it has not seen before . something tells me it cannot be done to learn general multiplication ... yes or no ? EOQ i'm afraid your model does not fit the problem . you might want to think about what kind of functions you can model with a three layer relu feedforward net . as a first step : what is the smallest neural network that can be trained to execute general addition of real numbers ? ( you might need to use another activation function ) EOA 
 can a nn be taught to multiply two numbers ? : mlquestions hi , i've been playing with regular neural networks with relu input , hidden and output units . i have successfully trained it to add two real numbers , just approximating binary functions for fun , of the form f(x , y)-z . the input is NUM numbers [ negpartofx pospartofx negpartofy pospartofy ] ex . f(-5.1, NUM ) becomes f([5.1 NUM NUM NUM ]) . output is similar [ negz posz ] addition is then simply to learn direct transfomration matrix [ NUM -1 NUM -1 ] [-1 NUM -1 NUM ] i've tried similar approach to learn general multiplication , but with no luck . ie. i expect it to work for any real numbers , it sort of works for numbers within a limited range , but the results become too low for large numbers it has not seen before . something tells me it cannot be done to learn general multiplication ... yes or no ? EOQ i was reading this proof that neural nets can compute any function , so i'm playing around with what i can actually make them do to learn addition does not require hidden layers , it can be mapped directly for input to output , and in fact my network learns it quite easily multiplication , however, can only be approximated within reasonable a range , as it seems to me so far ... i think rnns could do it in general fashion . EOA 
 can a nn be taught to multiply two numbers ? : mlquestions hi , i've been playing with regular neural networks with relu input , hidden and output units . i have successfully trained it to add two real numbers , just approximating binary functions for fun , of the form f(x , y)-z . the input is NUM numbers [ negpartofx pospartofx negpartofy pospartofy ] ex . f(-5.1, NUM ) becomes f([5.1 NUM NUM NUM ]) . output is similar [ negz posz ] addition is then simply to learn direct transfomration matrix [ NUM -1 NUM -1 ] [-1 NUM -1 NUM ] i've tried similar approach to learn general multiplication , but with no luck . ie. i expect it to work for any real numbers , it sort of works for numbers within a limited range , but the results become too low for large numbers it has not seen before . something tells me it cannot be done to learn general multiplication ... yes or no ? EOQ i was reading this proof that neural nets can compute any function depends on the defintion of compute . as to g . cybenko-approximation by superpositions of a sigmoidal function the set of functions that you can compute with two layer feedforward nets is dense in the continous functions . thus you can get abitrary close to multiplication . but unlike addition it is no element of the set thus can't be expressed as a two layer feedforward network . i don't know any literature on approximation by mlp yet though and also think that rnn might help . could you provide a link to the proof you are refering ? EOA 
 can a nn be taught to multiply two numbers ? : mlquestions hi , i've been playing with regular neural networks with relu input , hidden and output units . i have successfully trained it to add two real numbers , just approximating binary functions for fun , of the form f(x , y)-z . the input is NUM numbers [ negpartofx pospartofx negpartofy pospartofy ] ex . f(-5.1, NUM ) becomes f([5.1 NUM NUM NUM ]) . output is similar [ negz posz ] addition is then simply to learn direct transfomration matrix [ NUM -1 NUM -1 ] [-1 NUM -1 NUM ] i've tried similar approach to learn general multiplication , but with no luck . ie. i expect it to work for any real numbers , it sort of works for numbers within a limited range , but the results become too low for large numbers it has not seen before . something tells me it cannot be done to learn general multiplication ... yes or no ? EOQ sorry , you're right , the key word is approximate ... it's basically explanation of the same study you've cited above URL EOA 
 can a nn be taught to multiply two numbers ? : mlquestions hi , i've been playing with regular neural networks with relu input , hidden and output units . i have successfully trained it to add two real numbers , just approximating binary functions for fun , of the form f(x , y)-z . the input is NUM numbers [ negpartofx pospartofx negpartofy pospartofy ] ex . f(-5.1, NUM ) becomes f([5.1 NUM NUM NUM ]) . output is similar [ negz posz ] addition is then simply to learn direct transfomration matrix [ NUM -1 NUM -1 ] [-1 NUM -1 NUM ] i've tried similar approach to learn general multiplication , but with no luck . ie. i expect it to work for any real numbers , it sort of works for numbers within a limited range , but the results become too low for large numbers it has not seen before . something tells me it cannot be done to learn general multiplication ... yes or no ? EOQ thanks ! this article is actually where i found the paper/study . i have to point out that my fromer statement about approximation via two layer feedforward nets is inprecise/incorrect as one needs a linear combination of the seconds layer . EOA 
 what is the best way to address the vanishing gradient problem in neural networks ? : mlquestions i'm relatively new to neural nets and machine learning in general , and recently stumbled onto this problem when attempting to add more layers to a network used to classify digits . as i added more layers , my accuracy fell . i have heard of a few ways this is addressed , but was wondering if someone could highlight what the best techniques were , and what the pros and cons of each of them were . EOQ relu units are the most common solution . basically the idea of these is to have a non-linear function whose gradient is either on or off , that way it pushes the error signal all the way down without diluting it at each layer . i don't know enough myself to talk about pros and cons of different methods . EOA 
 what is the best way to address the vanishing gradient problem in neural networks ? : mlquestions i'm relatively new to neural nets and machine learning in general , and recently stumbled onto this problem when attempting to add more layers to a network used to classify digits . as i added more layers , my accuracy fell . i have heard of a few ways this is addressed , but was wondering if someone could highlight what the best techniques were , and what the pros and cons of each of them were . EOQ i've looked into relus , but haven't used them myself yet . my difficulty with it is that it seems to ruin the notion of activations corresponding to probabilities . EOA 
 what's the hardest part about training ml algorithms ? : mlquestions is it getting the massive datasets and cleaning them ? or is it waiting for the algorithms to finish ? or is it guarding against overfitting ? EOQ cleaning > ; overfitting > ; waiting EOA 
 what's the hardest part about training ml algorithms ? : mlquestions is it getting the massive datasets and cleaning them ? or is it waiting for the algorithms to finish ? or is it guarding against overfitting ? EOQ what do you usually use to clean data ? what's wrong with something like openrefine ? EOA 
 what's the hardest part about training ml algorithms ? : mlquestions is it getting the massive datasets and cleaning them ? or is it waiting for the algorithms to finish ? or is it guarding against overfitting ? EOQ i use a shitton of regular expressions in python for my data cleaning jobs , and pandas too . never heard of openrefine , just looked at the web page . not sure if anything is wrong with it . EOA 
 cnn with engineered features : mlquestions if i am working with a data set of faces , NUM x32 pixels each , and i want add an engineered feature like 'is the user wearing glasses' . would it be reasonable to add a new row , NUM x33 pixels , with a binary value to signify glasses or not . what's the pro / con of this approach ? thanks EOQ i suppose that an advantage of this approach is that it is easy to implement . it's usually a good idea to try the simplest approach first and see if the result is good enough for you before you invest more time and effort developing something more complicated . i do think this approach isn't quite right ... convolution works because all the inputs are the same kind ( i.e. pixels ) and there is a meaningful structural relationship between them . your high-level feature is not a pixel , and it's not really meaningful that pixel [ NUM ,32 ] is adjacent to a glasses-pixel and pixel [ NUM ,16 ] isn't . i think the more correct approach would be to just add a single input node to your network to represent this feature , that you then connect to all nodes in the first hidden layer . or actually , you could play around a bit with this , because they say that each layer extracts higher level features , so perhaps it is better to connect your glasses-input to a hidden layer that has similarly high level features . or you could connect your glasses-input to all hidden nodes regardless of layer . the important thing is that you treat this as the feature that it is and not a pixel ( or row of pixels ) , but like i said : you might try that first and see if the result is good enough for you , because this probably involves fewer changes to your code . EOA 
 cnn with engineered features : mlquestions if i am working with a data set of faces , NUM x32 pixels each , and i want add an engineered feature like 'is the user wearing glasses' . would it be reasonable to add a new row , NUM x33 pixels , with a binary value to signify glasses or not . what's the pro / con of this approach ? thanks EOQ thanks ! that makes a lot of sense . EOA 
 cnn with engineered features : mlquestions if i am working with a data set of faces , NUM x32 pixels each , and i want add an engineered feature like 'is the user wearing glasses' . would it be reasonable to add a new row , NUM x33 pixels , with a binary value to signify glasses or not . what's the pro / con of this approach ? thanks EOQ ( mostly just echoing what /u/cyberbyte said ) adding just row is a bit odd because only some convolutional filter applications will 'see' the feature . so, if you add the indicator as the NUM rd row , convolutional filters applied to the NUM st row will take only pixels as input while convolutional filters applied to the last rows will take both pixels and the indicators as input , which is probably a bad thing considering those filters will have their weights tied . instead i would add the an extra channel to the input , so instead of NUM x32x32 input switch to a NUM x32x32 input , where the second channel is binary value indicators . this paper did exactly that to encode information about player skill level when it comes to playing go : URL a more efficient but maybe harder to implement method could be to add the indicator with a weight to the output of the first convolutional layer before the non-linear functions is applied . so you could perform convolution on the input , add w-b where w is a new parameter and b is an indicator to each output unit , then apply your activation function and proceed as normal . EOA 
 is over-training a risk when a person marks lots of messages in their preferred email client/site as spam/not spam ? : mlquestions let's say that i receive on the order of NUM messages a day . if i diligently make sure that every one every day is correctly flagged as spam vs . not spam , is over-training a risk ? if so , how do i know at what point to stop training ? my knowledge level : i'm can describe a bit how naive bayes works , i can define the term over-training , and i know that i should be saying ham instead of not spam-but the latter seems more appropriate for a questions subreddit . :) ( i use thunderbird specifically , but i think this is a more general issue that would be applicable to any email client/app/site? ) EOQ i haven't really used naive bayes much , but i very much doubt that you're going to overtrain your spam filter . the problem with overtraining a classifier is that it can lead to overfitting , which is when random error/details/noise ( from the training set ) is modelled instead of the real underlying relationship . this requires that the classifier is flexible enough to model all of those tiny details ; this is usually called high variance/low bias . variance typically increases when the classifier has more learnable parameters , so e.g. a big neural network has higher variance than a smaller one . naive bayes classifiers tend to have fairly low bias and are therefor less susceptible to overfitting . overtraining tends to only be a problem when a classifier's training procedure goes over the training set multiple times to incrementally refine its model . one example is ( again ) a neural network ( multi-layer perceptron trained with backpropagation ) . if you have NUM training items and you can afford to run NUM ,000,000 training iterations , then you can train on each item NUM ,000 times and you might overfit . if your training set is larger ( e.g. NUM ,000 ) then you can train less on each item ( e.g. NUM times ) and you are less likely to overfit . in fact , even if you do train each item NUM ,000 times as well , overfitting is still less likely because it's more difficult to capture all the details of NUM ,000 items than for NUM . expanding your training set virtually always leads to less overfitting . with naive bayes you really only consider each training item once . the only way to increase the number of training iterations ( and potentially run the risk of overtraining ) is to expand the data set . it's probably not literally impossible to screw up a classifier by expanding the training set . for instance , if you add ( almost ) the same item a million times ( and few other items ) , that will probably do the trick . but you should be fine if the e-mails that you flag form a sample that fairly represents the whole body of e-mail that you're getting . in ml terms you want your sample to be independent and identically distributed (i.i.d.). tl;dr: flagging more e-mails probably makes your spam filter more accurate , not less . EOA 
 is over-training a risk when a person marks lots of messages in their preferred email client/site as spam/not spam ? : mlquestions let's say that i receive on the order of NUM messages a day . if i diligently make sure that every one every day is correctly flagged as spam vs . not spam , is over-training a risk ? if so , how do i know at what point to stop training ? my knowledge level : i'm can describe a bit how naive bayes works , i can define the term over-training , and i know that i should be saying ham instead of not spam-but the latter seems more appropriate for a questions subreddit . :) ( i use thunderbird specifically , but i think this is a more general issue that would be applicable to any email client/app/site? ) EOQ great explanation-thanks ! EOA 
 what do you use for gradient descent ? do you implement your own or use pre-built software ? : mlquestions as i understand it , there is software that implements cross-validation and gradient descent for you , and all you have to do is supply the cost function and its derivative. i was wondering , do most people implement their own , or use off-the-shelf software/library for it ? EOQ scikit-learn allday errday . EOA 
 what do you use for gradient descent ? do you implement your own or use pre-built software ? : mlquestions as i understand it , there is software that implements cross-validation and gradient descent for you , and all you have to do is supply the cost function and its derivative. i was wondering , do most people implement their own , or use off-the-shelf software/library for it ? EOQ i use fmincg inside of matlab . EOA 
 what is the best classifier from the ones tested here ? : mlquestions for starters , i'm a complete beginner in this field and i'm just dipping my toes into this sea of knowledge . and now , what i'm trying to do is to test and understand which is the best classifier and the one who performed best from the below list of classifiers ( click the link )- i've explained what the graphs mean , in detail , down below . to put this into context , i'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these NUM phases : pre-processing model training and then prediction based on the trained model . the pre-processing phase is where i made all of the adjustments and tried different methods before generating both a countvectorizer matrix and a tfidfvectorizer matrix and compared the performance of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : stemming ( with lancaster / snowball stemmer from nltk ) word correction using peter norvig's approach so , for training my model i've tried combinations of all of these ( which can be found in the link ) : a simple countvectorizer over my training text a simple tfidfvectorizer ... over my training text lancaster stemming-countvectorizer ( ... over my training text ) etc . snowball stemming-countvectorizer lancaster stemming-tfidfvectorizer snowball stemming-tfidfvectorizer lancaster stemming-peter norvig's word correction algorithm-countvectorizer snowball stemming-peter norvig's word correction algorithm-countvectorizer lancaster stemming-peter norvig's word correction algorithm-tfidfvectorizer snowball stemming-peter norvig's word correction algorithm-tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i've used a variable value for min.df ( minimum document frequency ) to range from NUM -the number of documents in the test scenario to max(word document frequency)-which is what is displayed on the graphs . so , actually i'm testing the performance of my models and how they work with different numbers of training features . as mentioned at the beginning , i'm a complete beginner and i've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . nusvc is too unstable , which i'm guessing makes it a poor choice of a classifier for this scenario . but how about bernoulli naive bayes ? would it be considered a good classifier ? does it overfit at the beginning but then become a more real and better performing classifier towards the end , even if it's prediction rate is not NUM % ? same thing for gaussian naive bayes .. also, some people suggested that lda ( latent dirichlet allocation ) is a good classifer for this kind of scenario . i'm hoping that someone could help me make some sense out of my results , since i'm a bit confused on how i should interpret them . and if there's anyone interested in how i actually did this , with code , you can find it on github . disclaimer : i'm a terrible programmer with a very non-pythonic way of writing code . and in the end , thank you in advance for reading this story and your help is greatly and immensely appreciated ! EOQ URL EOA 
 what is the best classifier from the ones tested here ? : mlquestions for starters , i'm a complete beginner in this field and i'm just dipping my toes into this sea of knowledge . and now , what i'm trying to do is to test and understand which is the best classifier and the one who performed best from the below list of classifiers ( click the link )- i've explained what the graphs mean , in detail , down below . to put this into context , i'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these NUM phases : pre-processing model training and then prediction based on the trained model . the pre-processing phase is where i made all of the adjustments and tried different methods before generating both a countvectorizer matrix and a tfidfvectorizer matrix and compared the performance of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : stemming ( with lancaster / snowball stemmer from nltk ) word correction using peter norvig's approach so , for training my model i've tried combinations of all of these ( which can be found in the link ) : a simple countvectorizer over my training text a simple tfidfvectorizer ... over my training text lancaster stemming-countvectorizer ( ... over my training text ) etc . snowball stemming-countvectorizer lancaster stemming-tfidfvectorizer snowball stemming-tfidfvectorizer lancaster stemming-peter norvig's word correction algorithm-countvectorizer snowball stemming-peter norvig's word correction algorithm-countvectorizer lancaster stemming-peter norvig's word correction algorithm-tfidfvectorizer snowball stemming-peter norvig's word correction algorithm-tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i've used a variable value for min.df ( minimum document frequency ) to range from NUM -the number of documents in the test scenario to max(word document frequency)-which is what is displayed on the graphs . so , actually i'm testing the performance of my models and how they work with different numbers of training features . as mentioned at the beginning , i'm a complete beginner and i've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . nusvc is too unstable , which i'm guessing makes it a poor choice of a classifier for this scenario . but how about bernoulli naive bayes ? would it be considered a good classifier ? does it overfit at the beginning but then become a more real and better performing classifier towards the end , even if it's prediction rate is not NUM % ? same thing for gaussian naive bayes .. also, some people suggested that lda ( latent dirichlet allocation ) is a good classifer for this kind of scenario . i'm hoping that someone could help me make some sense out of my results , since i'm a bit confused on how i should interpret them . and if there's anyone interested in how i actually did this , with code , you can find it on github . disclaimer : i'm a terrible programmer with a very non-pythonic way of writing code . and in the end , thank you in advance for reading this story and your help is greatly and immensely appreciated ! EOQ wow , that looks simple .. so you're suggesting i should keep with the naive bayes approach and try to improve on that ? EOA 
 what is the best classifier from the ones tested here ? : mlquestions for starters , i'm a complete beginner in this field and i'm just dipping my toes into this sea of knowledge . and now , what i'm trying to do is to test and understand which is the best classifier and the one who performed best from the below list of classifiers ( click the link )- i've explained what the graphs mean , in detail , down below . to put this into context , i'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these NUM phases : pre-processing model training and then prediction based on the trained model . the pre-processing phase is where i made all of the adjustments and tried different methods before generating both a countvectorizer matrix and a tfidfvectorizer matrix and compared the performance of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : stemming ( with lancaster / snowball stemmer from nltk ) word correction using peter norvig's approach so , for training my model i've tried combinations of all of these ( which can be found in the link ) : a simple countvectorizer over my training text a simple tfidfvectorizer ... over my training text lancaster stemming-countvectorizer ( ... over my training text ) etc . snowball stemming-countvectorizer lancaster stemming-tfidfvectorizer snowball stemming-tfidfvectorizer lancaster stemming-peter norvig's word correction algorithm-countvectorizer snowball stemming-peter norvig's word correction algorithm-countvectorizer lancaster stemming-peter norvig's word correction algorithm-tfidfvectorizer snowball stemming-peter norvig's word correction algorithm-tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i've used a variable value for min.df ( minimum document frequency ) to range from NUM -the number of documents in the test scenario to max(word document frequency)-which is what is displayed on the graphs . so , actually i'm testing the performance of my models and how they work with different numbers of training features . as mentioned at the beginning , i'm a complete beginner and i've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . nusvc is too unstable , which i'm guessing makes it a poor choice of a classifier for this scenario . but how about bernoulli naive bayes ? would it be considered a good classifier ? does it overfit at the beginning but then become a more real and better performing classifier towards the end , even if it's prediction rate is not NUM % ? same thing for gaussian naive bayes .. also, some people suggested that lda ( latent dirichlet allocation ) is a good classifer for this kind of scenario . i'm hoping that someone could help me make some sense out of my results , since i'm a bit confused on how i should interpret them . and if there's anyone interested in how i actually did this , with code , you can find it on github . disclaimer : i'm a terrible programmer with a very non-pythonic way of writing code . and in the end , thank you in advance for reading this story and your help is greatly and immensely appreciated ! EOQ yes . also try feature selection . EOA 
 what is the best classifier from the ones tested here ? : mlquestions for starters , i'm a complete beginner in this field and i'm just dipping my toes into this sea of knowledge . and now , what i'm trying to do is to test and understand which is the best classifier and the one who performed best from the below list of classifiers ( click the link )- i've explained what the graphs mean , in detail , down below . to put this into context , i'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these NUM phases : pre-processing model training and then prediction based on the trained model . the pre-processing phase is where i made all of the adjustments and tried different methods before generating both a countvectorizer matrix and a tfidfvectorizer matrix and compared the performance of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : stemming ( with lancaster / snowball stemmer from nltk ) word correction using peter norvig's approach so , for training my model i've tried combinations of all of these ( which can be found in the link ) : a simple countvectorizer over my training text a simple tfidfvectorizer ... over my training text lancaster stemming-countvectorizer ( ... over my training text ) etc . snowball stemming-countvectorizer lancaster stemming-tfidfvectorizer snowball stemming-tfidfvectorizer lancaster stemming-peter norvig's word correction algorithm-countvectorizer snowball stemming-peter norvig's word correction algorithm-countvectorizer lancaster stemming-peter norvig's word correction algorithm-tfidfvectorizer snowball stemming-peter norvig's word correction algorithm-tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i've used a variable value for min.df ( minimum document frequency ) to range from NUM -the number of documents in the test scenario to max(word document frequency)-which is what is displayed on the graphs . so , actually i'm testing the performance of my models and how they work with different numbers of training features . as mentioned at the beginning , i'm a complete beginner and i've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . nusvc is too unstable , which i'm guessing makes it a poor choice of a classifier for this scenario . but how about bernoulli naive bayes ? would it be considered a good classifier ? does it overfit at the beginning but then become a more real and better performing classifier towards the end , even if it's prediction rate is not NUM % ? same thing for gaussian naive bayes .. also, some people suggested that lda ( latent dirichlet allocation ) is a good classifer for this kind of scenario . i'm hoping that someone could help me make some sense out of my results , since i'm a bit confused on how i should interpret them . and if there's anyone interested in how i actually did this , with code , you can find it on github . disclaimer : i'm a terrible programmer with a very non-pythonic way of writing code . and in the end , thank you in advance for reading this story and your help is greatly and immensely appreciated ! EOQ i'm a bot , bleep, bloop . someone has linked to this thread from another place on reddit : [ /r/machinelearning ] what is the best classifier from the ones tested here ? : mlquestions if you follow any of the above links , please respect the rules of reddit and don't vote in the other threads . ( info / contact ) EOA 
 i have a square matrix of data ( from simulations of NUM d pde's ) what kind of fun things can i do ? : mlquestions basically as title says , i have a lot of data from solving some basically stochastic/noisy diffusion fields in NUM d and i am interested in just playing around with the data with some ml tools ( probably with pythons sklearn toolkit ) i just don't want to head in blindly at the moment , so i wanted to ask for suggestions first on what could/can/should be done ! ( quite vague i guess , apologies for that but this entire field is very very new to me but fascinating ) thank you :) quick edit : forgot to add these solutions give some sorta nice pattern formations/fractals-like structures hence my interest in ml application EOQ plot that shit EOA 
 i have a square matrix of data ( from simulations of NUM d pde's ) what kind of fun things can i do ? : mlquestions basically as title says , i have a lot of data from solving some basically stochastic/noisy diffusion fields in NUM d and i am interested in just playing around with the data with some ml tools ( probably with pythons sklearn toolkit ) i just don't want to head in blindly at the moment , so i wanted to ask for suggestions first on what could/can/should be done ! ( quite vague i guess , apologies for that but this entire field is very very new to me but fascinating ) thank you :) quick edit : forgot to add these solutions give some sorta nice pattern formations/fractals-like structures hence my interest in ml application EOQ well of course i've done that :p but that isn't quite ml EOA 
 a question about softmax layer neurons . : mlquestions how are the partial derivatives ∂ak' / ∂ak-NUM for all k' !-k , where ak is the output from the kth neuron of the softmax layer . shouldn't a change in the output of one neuron of the softmax layer cause a change in the output of other neurons as well ? i'm having a lot of trouble wrapping my head around this . any help is appreciated , thanks! EOQ you should do the derivatives with respect to the node's input , which i'll call z . ∂ak' / ∂zk-ak'-( NUM -ak ) if k-k' , and-ak'-ak if k' !-k . EOA 
 a question about softmax layer neurons . : mlquestions how are the partial derivatives ∂ak' / ∂ak-NUM for all k' !-k , where ak is the output from the kth neuron of the softmax layer . shouldn't a change in the output of one neuron of the softmax layer cause a change in the output of other neurons as well ? i'm having a lot of trouble wrapping my head around this . any help is appreciated , thanks! EOQ hi i have a similar question URL in the case of k' !-k , the normalization summation shouldn't be over k but a temp variable correct ? also in your answer shouldn't the case where k-k' be ak'-(1-ak') ? EOA 
 a question about softmax layer neurons . : mlquestions how are the partial derivatives ∂ak' / ∂ak-NUM for all k' !-k , where ak is the output from the kth neuron of the softmax layer . shouldn't a change in the output of one neuron of the softmax layer cause a change in the output of other neurons as well ? i'm having a lot of trouble wrapping my head around this . any help is appreciated , thanks! EOQ ya you're right on both accounts . thanks ! EOA 
 can you please help with a perceptron ( neural networks ) problem ? : mlquestions i have the following neural networks problem and couldnt find any answer on the web . any hints would help . i am not looking for a complete solution , just some pointing in the right direction . problem : write the upper bound ko of the number of iterations needed to a perceptron to learn a linear separable set with the rosenblatt rule : w(1)-w-/ NUM what category of points makes the learning hard ? justify the answer . thank you , dan. EOQ it's basically asking you to prove the perceptron convergence theorem ( easy to google for writeups ) . gvien that perceptrons are linear classifiers , points with non-linear boundaries are impossible to learn . the canonical example is the xor function . minsky's perceptron book proved this and started the first ai winter . EOA 
 can you please help with a perceptron ( neural networks ) problem ? : mlquestions i have the following neural networks problem and couldnt find any answer on the web . any hints would help . i am not looking for a complete solution , just some pointing in the right direction . problem : write the upper bound ko of the number of iterations needed to a perceptron to learn a linear separable set with the rosenblatt rule : w(1)-w-/ NUM what category of points makes the learning hard ? justify the answer . thank you , dan. EOQ as long as they say the set is linear separable i thought that would exclude the xor type of sets . my problem is that i couldnt figure out how to calculate the exact number of iterations ... EOA 
 can you please help with a perceptron ( neural networks ) problem ? : mlquestions i have the following neural networks problem and couldnt find any answer on the web . any hints would help . i am not looking for a complete solution , just some pointing in the right direction . problem : write the upper bound ko of the number of iterations needed to a perceptron to learn a linear separable set with the rosenblatt rule : w(1)-w-/ NUM what category of points makes the learning hard ? justify the answer . thank you , dan. EOQ ah , right. i skipped over linearly separable . by convergence theorem , the iterations grows inversely to the margin . if there is a really small margin , that upper bound grows a lot . EOA 
 dataset recommendation for binary classification ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012dataset recommendation for binary classification?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;kevinw88sorry, this has been archived and can no longer be voted onEOQ hi , i'm new to machine learning . i found the faq on /r/machinelearning listing many datasets , but i'm a bit overwhelmed . i'm reading machine learning : the art and science of algorithms that make sense of data book . i'm currently going over binary classification and then onto multi-class classification . i learn really well by working through examples , so i'm trying to find some datasets i can use both binary and multi-class classifications . any recommendations ? EOA 
 support vector regreesion model/equation with smoreg in weka ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012support vector regreesion model/equation with smoreg in weka?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;kazakerkegisorry, this has been archived and can no longer be voted onEOQ i used weka smoreg with nonlineer kernels ( rbf and polykernel ) , it only gives me back the support vectors and predicted values as an output , but not the actual regression equation that is used to predict the values . how can i use this information to predict the value associated with new value with new inputs ? also, how would i know the regression model/equation to predict the target value ? support vectors are like these in result window -run information-instances : NUM attributes : NUM smoreg support vectors :-0.15864153997456623-k[0]-0.5556899179226912-k[1]-0.3813964173495271-k[2]-0.7863452506278186-k[3]-0.22848095367203652-k[4]-0.3789268674809888-k[5]-1.0-k[6]-0.012246858338156526-k[7]-0.31639508776784564-k[8]-1.0-k[9]-1.0-k[10]-1.0-k[11]-1.0-k[12]-1.0-k[13]-1.0-k[14]-0.016511307060502304-k[15]-0.31879316260936497-k[16]-0.9368514873432039-k[17]-0.33499478810833816-k[18]-0.7735889553066824-k[19]-1.0-k[20]-0.15813378569986267-k[21]-1.0-k[22]-0.17899023856485627-k[23]-0.24047912685109943-k[24]-1.0-k[25]-0.2521579759493566-k[26]-0.5170618399262431-k[27]-1.0-k[28]-1.0-k[29]-0.2081860536197917-k[30]-1.0-k[31]-1.0-k[32]-1.0-k[33]-1.0-k[34]-1.0-k[35]-1.0-k[36]-0.6138397231932878-k[37]-1.0-k[38]-1.0-k[39]-1.0-k[40]-1.0-k[41]-1.0-k[42]-1.0-k[43]-1.0-k[44]-1.0-k[45]-0.17819164089848483-k[46]-1.0-k[47]-0.45835644097285244-k[48]-0.7331426020396378-k[49] - NUM 251 EOA 
 training methods other than by example : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123training methods other than by example&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;veswill3sorry, this has been archived and can no longer be voted onEOQ hey team , pretty general question for you . every time we talk about training a machine learning algorithm , we are referring to training by example . are there any other methods to training ? take image classification via neural network for example . you can show a convoluted deep net NUM ,000 images of airplanes , and eventually it learns to find patterns in the edges and so forth . is it not possible to help the algorithm along by letting it know that most bi-planes have NUM sets of stacked wings , or something similar ? EOA 
 training methods other than by example : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123training methods other than by example&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;veswill3sorry, this has been archived and can no longer be voted onEOQ that is an idea older then ml . however, explaining to algorithm facts about bi-plane is far from trivial . so ml is exactly trying to avoid most of explaining and lets algorithm learn facts from examples . EOA 
 what is deep learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123what is deep learning&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;tightanalorifice789sorry, this has been archived and can no longer be voted onEOQ as opposed to just machine learning ? EOA 
 what is deep learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123what is deep learning&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;tightanalorifice789sorry, this has been archived and can no longer be voted on EOQ deep learning is a technique developed by the field of machine learning . deep learning is a rebranding of neural networks , which historically didn't perform very well on many problems . more recently , neural networks have been performing well because machine learning researchers figured out a set of techniques that make it easier to learn deep ( complex ) neural networks . machine learning does not necessarily involve neural networks . EOA 
 what is deep learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123what is deep learning&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;tightanalorifice789sorry, this has been archived and can no longer be voted on EOQ my understanding of this topic is a bit limited , so i'd be interested if this response is off target , but here's what i think it means : when you have a neural network , you might train it using some form of the backpropagation algorithm . this algorithm basically looks at the rate of change of the value at a node as the final answer node varies , so you run your inputs through the network , look at the answer you get , and then adjust the weights based on how far off you were from the right answer . follow so far ? well , when a neural network is very deep ( has a lot of layers of neurons ) , the gradient for adjusting those weights can get vanishingly small ... or it can blow up , so the values swing wildly . that's because as you have more layers , instead of layer a depending on layer b , you have layer a depending on layer b , which depends on c , which depends on d , and so on . so when layer m changes a little bit , layer a either changes a huge amount or not at all . point is : the numbers get a little fucky . a solution to this was to have an algorithm learn what kinds of features are going to be most useful to distinguish various types of output in advance of trying to train the network . that way , when it comes to tweaking your weight vectors to get the output you desire , you don't have as far to go . if you can figure out how to do that , then you can have much deeper networks . why are deeper networks good ? well, i think that it's because the network can model nonlinear interactions much more easily . in theory , a very simple neural network can approximate any function ( universal approximation theorem ) but in practice that may require many thousands of nodes in a layer . a deep network gives you a lot more power with a lot fewer nodes , which means faster computation . so the trick is to train in the features before you ever start the neural network learning . the way i learned to do this is with a restricted boltzmann machine , but i would imagine that many unsupervised learning tasks would be useful for this purpose . what you do is basically set up a network that becomes very good at taking the input and ... reproducing inputs , honestly. because to reproduce inputs convincingly , ( so it's hard to tell made-up inputs from real inputs ) the network has to have a sense of what the inputs look like generally . so you run your rbm until it is pretty well converged . then once it has learned the data , you can add additional layers on top of it . there are some tricks to doing this , but that's sort of the idea . that way , when you start your neural network learning , it already has some pretty well-trained layers in the first couple layers ( in the sense that the first few layers are picking up important features from the data ) . that's important , too, because as you add layers , the lower layers become harder and harder to train ( for the reasons explained earlier ) . in summary , you basically use a bunch of tricks to train networks with a lot of layers because they are better at modeling more complicated functions with fewer nodes . EOA 
 what is deep learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123what is deep learning&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;tightanalorifice789sorry, this has been archived and can no longer be voted on EOQ deep learning refers to having a hierarchical stack of models on top of each other , each of which uses the output of the previous layer to produce another intermediate result that is a little closer to what we want to get . this is in contrast to shallow models that only do a single ( albeit very complicated ) transformation of input to output . hierarchical/layered approaches work very well in computer vision , because images are made up of objects , which are made up of parts , which are made up of edges ... so having a succession of highly specialized models detect ever more complicated features is a much more practical approach than trying to have one model detect everything from raw pixel values . currently this is almost always done with many-layered neural networks , but there is also growing interest in deep graphical models . EOA 
 neural network to learn to play video games : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012neural network to learn to play video games&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;jesmaailsorry, this has been archived and can no longer be voted onEOQ is it possible to use a deep belief network to learn and play a video game ( e.g tetric , pacman, snake ) ? i've read about deep reinforcement learning being used for it , but i'm unsure how far this is from a dbn . apologies for how nonsensical this may be , i'm still new to neural networks as a whole as i'm in the planning stage for an undergrad project involving it . EOA 
 neural network to learn to play video games : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012neural network to learn to play video games&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;jesmaailsorry, this has been archived and can no longer be voted on EOQ hey op ! so i'm not very familiar with a reinforcement method but you should look up mario . not sure if it's related but hopefully it can help give you some inspiration . i'd link it now but i'm on mobile. EOA 
 neural network to learn to play video games : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012neural network to learn to play video games&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;jesmaailsorry, this has been archived and can no longer be voted on EOQ that was my initial inspiration to face my neural network project to video games , however it uses an approach called neat , and i'm leaning more towards a dbn/drl implementation because it was in the initial project specification and my supervisor has more knowledge regarding dbns(but less so drls sadly) . thanks for the reply :) EOA 
 i'm an r user but know zip about ml . help me out ... : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012i'm an r user but know zip about ml . help me out...&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;z.mconsorry, this has been archived and can no longer be voted onEOQ alright guys , i know at some point i'll need ml in r for future projects at work . i want to be ahead of the game so the transition is as smooth as possible . dear experts , consider the following : NUM ) i have no idea about ml ; NUM ) i know r ; NUM ) i know statistics/econometrics well enough to understand how to model some real world issues ; given that , would you answer me the following questions : NUM ) where can i learn the absolute basic about ml ? NUM ) given that i've learned the foundations , is there any books focusing on r and ml ? i've downloaded a few ones but wouldn't mind a few extra tips . thank you ! EOA 
 i'm an r user but know zip about ml . help me out ... : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012i'm an r user but know zip about ml . help me out...&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;z.mconsorry, this has been archived and can no longer be voted on EOQ introduction to statistical learning EOA 
 i'm an r user but know zip about ml . help me out ... : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012i'm an r user but know zip about ml . help me out...&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;z.mconsorry, this has been archived and can no longer be voted on EOQ thanks . EOA 
 tutorials for svm ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123tutorials for svm?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;hlyatessorry, this has been archived and can no longer be voted onEOQ where is a tutorial to use/implement svm using a programming language such as r , python, java , or c# ? i would like to both gain a deeper theoretical understanding and how to implement it in a programming language please . i tried google , but have come up empty . please help . EOA 
 tutorials for svm ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123tutorials for svm?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;hlyatessorry, this has been archived and can no longer be voted on EOQ machine learning with r by brett lantz and python data science essentials by alberto boschetti ; luca massaron might be the answer to my own question EOA 
 relu derivative in NUM : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012relu derivative in NUM &#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;personalitysonsorry, this has been archived and can no longer be voted onEOQ hi , recently i found a mistake in my code where i defined the derivative of relu as NUM if node unit is > ;-NUM this doesn't make much sense because it would be NUM for all nodes in a relu layer . i corrected it to NUM if node unit is > ; NUM however , i've been running some tests switching back and forth , and it seems i have slightly faster convergence with the old derivative ( > ;-NUM ) . what do you guys use ? it's undefined in NUM URL URL EOA 
 relu derivative in NUM : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012relu derivative in NUM &#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;personalitysonsorry, this has been archived and can no longer be voted on EOQ false alarm , i think i might be wrong on this one EOA 
 relu derivative in NUM : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012relu derivative in NUM &#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;personalitysonsorry, this has been archived and can no longer be voted on EOQ what did you end up concluding ? i'm planning on trying relus-but as you said the differential at NUM is undefined & i'm unsure what to do in that case . EOA 
 relu derivative in NUM : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012relu derivative in NUM &#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;personalitysonsorry, this has been archived and can no longer be voted on EOQ derivative should be NUM is node unit is > ;0 NUM if < ;-0 EOA 
 relu derivative in NUM : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012relu derivative in NUM &#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;personalitysonsorry, this has been archived and can no longer be voted on EOQ thank you EOA 
 have a data set of more than NUM gb , how to read it in r ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012have a data set of more than NUM gb , how to read it in r?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;thenamesalreadytakensorry, this has been archived and can no longer be voted onEOQ the 'read.csv' command leads to a hang . and r doens't allow opening files more than NUM mb . what can i do in this case ? any help will be highly appreciated . EOA 
 have a data set of more than NUM gb , how to read it in r ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012have a data set of more than NUM gb , how to read it in r?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;thenamesalreadytakensorry, this has been archived and can no longer be voted onEOQ found this bit of reading or maybe you could push the data into an rdms ( i.e. mysql ) and then pull the data into r ? EOA 
 have a data set of more than NUM gb , how to read it in r ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012have a data set of more than NUM gb , how to read it in r?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;thenamesalreadytakensorry, this has been archived and can no longer be voted onEOQ what do you ultimately need to do with the data ?? EOA 
 have a data set of more than NUM gb , how to read it in r ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012have a data set of more than NUM gb , how to read it in r?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;thenamesalreadytakensorry, this has been archived and can no longer be voted onEOQ apply different machine learning algorithms on it . after i get a good grasp of it . EOA 
 any recommended papers or books on time-series/sequence classification ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123any recommended papers or books on time-series/sequence classification?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ hello everyone , so i tried solving a previous research problem ( first one i had the pleasure of working on alone ) like a noob , i.e. i didn't really understand things like hmm and dynamic time warping so i didn't use it . in retrospect , it would have seriously helped out with my problems , and it would have definitely given me a magnitude of difference in results . anyway , that was my first attempt at research as an undergraduate . am i alone in this 'ooooh that would have solved all of my problems but i didn't use it' feeling ? i hope not . anyway , i'm curious now , can you all recommend papers that would help with sequence classification for time-series data ? my goal is to classify sensor data into one of several classes . EOA 
 any recommended papers or books on time-series/sequence classification ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123any recommended papers or books on time-series/sequence classification?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ i am working on anomaly detection from dynamic strain or accelerometer measurements . in my case i try to extract some damage sensitive features ( modal parameter ) before the anomaly detection step . the following book may be helpful for you . the authors describe multiple methods for damage sensitive feature extraction from time-series . farrar , charles r ., and keith worden . structural health monitoring : a machine learning perspective. john wiley & sons , NUM EOA 
 can artificial neural networks be programmed to 'mutate' ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123can artificial neural networks be programmed to 'mutate'?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;magnusozsorry, this has been archived and can no longer be voted onEOQ i'm a regular scientist curious about machine learning . so, please be patient with me . can artificial neural networks be programmed to 'mutate' (quasi)randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance ? EOA 
 can artificial neural networks be programmed to 'mutate' ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123can artificial neural networks be programmed to 'mutate'?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;magnusozsorry, this has been archived and can no longer be voted onEOQ yes , they can . the algorithm is one method to do it . such an approach works well when the optimal network topology isn't known ( or surmised ) in advanced , and when there is a good way to formulate incremental improvements . but generally speaking just trying different hyperparameters ( what kind of net , how many layers , how many nodes in each layer... ) and going with what works best tends to be preferable from a certain network size upwards , because it is quicker . EOA 
 can artificial neural networks be programmed to 'mutate' ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123can artificial neural networks be programmed to 'mutate'?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;magnusozsorry, this has been archived and can no longer be voted onEOQ i don't know enough to fully answer this question , but there are genetic algorithms in machine learning which perform similar to what you described . it is at least a starting point for you to research . EOA 
 can artificial neural networks be programmed to 'mutate' ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123can artificial neural networks be programmed to 'mutate'?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;magnusozsorry, this has been archived and can no longer be voted onEOQ yes , most all machine learning algorithms do use a type of artificial evolution . even in something as simple as linear regression , or finding a line of best fit , a program will probably iterate through generations , continuously adapting a hypothesis until cost between known values is minimal . EOA 
 interpreting improvements to logistic regression model in r : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012interpreting improvements to logistic regression model in r&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;theclawwhisperersorry, this has been archived and can no longer be voted onEOQ i have an existing logistic regression model that works fairly well ( NUM % precision and NUM % recall in initial training ) , but i'm looking to make improvements to it after about NUM months time. i've seen some false positives in that time and i've added some features ( from about NUM to NUM ) to improve the accuracy . the initial model was fit on about NUM observations ( NUM train , NUM test ) , and was fit using stepwise regression to determine the optimal feature set based on aic . i used r to fit this model using glm . i'm looking into a couple options for improving , primarily pca ( prcomp ) and regularized regression ( glmnet ) . in addition to my goal to improve accuracy , i'm also interested in keeping the model interpret-able to someone without r access ( i.e. if someone were to query new data , can they easily apply my model to predict the result without needing r? ) my problem is that i don't know whether to use pca and then apply logistic regression on those pcs ( i.e. principal component regression ) or work toward regularized regression instead . i'm leaning toward regularized logistic regression using glmnet , but having a hard time interpreting the plots from glmnet . i've been following the glmnet vignette , but i'm pretty lost , to be honest . i understand some of the use of the alpha and lambda parameters , but how do i use the output from glmnet to determine : a) the appropriate feature set , and b) the appropriate values for lambda and alpha ? when would i use lambda.1se over lambda.min? it's been a while since i was in university learning about regression , so i only remember the basics...i get lost quickly in the mathematical notation . i'm also not able to share anything related to my data . can anyone link to a resource showing a detailed walkthrough of using glmnet , or explain to me more simply than the vignette ? finally , i fear i'll end up with a model result that i'm not able to fully grasp . let's say i were to use this model result and later find prediction errors in production...how do i go back and determine where my model is failing ( and therefore determine how to improve it ) ? i know the answers to these questions are heavily reliant on my specific data set , but i'm looking more for general answers or links to good resources that could help me figure these answers out for myself . EOA 
 interpreting improvements to logistic regression model in r : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012interpreting improvements to logistic regression model in r&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;theclawwhisperersorry, this has been archived and can no longer be voted onEOQ [ deleted ] EOA 
 interpreting improvements to logistic regression model in r : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012interpreting improvements to logistic regression model in r&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;theclawwhisperersorry, this has been archived and can no longer be voted onEOQ this looks promising . thank you ! EOA 
 minimizer won't converge for logistic regression program : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012minimizer won't converge for logistic regression program&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;iamiamwhoamisorry, this has been archived and can no longer be voted onEOQ i'm trying to do the logistic regression assignment from andrew ng's course in python . i got it to work in octave , but for some reason when i try migrating it over to python the mininimzation procedure fails . the cost function just diverges to nan . i can't figure out what's causing it . to me everything looks exactly the same . here's my python program . here's the dataset . here's a description of the assignment . can anyone see an obvious bug ? EOA 
 minimizer won't converge for logistic regression program : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012minimizer won't converge for logistic regression program&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;iamiamwhoamisorry, this has been archived and can no longer be voted onEOQ you should try using a debugger or print statements to inspect values EOA 
 how many learning curves should i plot for a multi-class logistic regression classifier ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012how many learning curves should i plot for a multi-class logistic regression classifier?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;ragedchimaerasorry, this has been archived and can no longer be voted onEOQ if we have k classes , do i have to plot k learning curves ? because it seems impossible to me to calculate the train/validation error against all k theta vectors at once . to clarify , the learning curve is a plot of the training & cross validation/test set error/cost vs training set size. this plot should allow you to see if increasing the training set size improves performance . more generally , the learning curve allows you to identify whether your algorithm suffers from a bias ( under fitting ) or variance ( over fitting ) problem . EOA 
 how many learning curves should i plot for a multi-class logistic regression classifier ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012how many learning curves should i plot for a multi-class logistic regression classifier?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;ragedchimaerasorry, this has been archived and can no longer be voted onEOQ ||ypredict-yactual||2 EOA 
 cost function confusion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012cost function confusion&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;weirdbuzzingsorry, this has been archived and can no longer be voted onEOQ in many of the guides i have read online , ( in particular this one ) , there is a step at the beginning of back propogation where you calculate the discrepancy between your observed output and your target output . the part that i am very confused about is that sometimes this discrepancy is calculated as -a-y ( where a is the observed output and y is the target output ) . later on , the guides will discuss a cost function , which seems to me to be the same thing as the difference formula above ? however, a totally different equation ( URL ) is then given . is the difference formula just an abstraction of the more complicated formula ? thanks ! EOA 
 cost function confusion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012cost function confusion&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;weirdbuzzingsorry, this has been archived and can no longer be voted onEOQ in regression , the cost function is usually the mean or sum of squared errors-each individual term in the sum is (a-y)2 . what you called the discrepancy is actually the derivative of this error wrt to a as used in backpropagation : URL when you do binary classification the cost function is the cross-entropy between the wanted distribution and what the network predicts . this is a special case of the negative log-likelihood under a multinomial distribution-that is used in multiclass problems . -for neural networks this formula is slightly different EOA 
 cost function confusion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012cost function confusion&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;weirdbuzzingsorry, this has been archived and can no longer be voted onEOQ thanks for the explanation ! i'm still a bit confused about how all these formulas fit together in an implementation of the neural net . it sounds like the actual value that we end up back propagating into the network is not the actual ( squared? ) error but the derivative of the error ? why is this useful/preferred ? is this derivative of the squared error a cost function ? or am i mixing up terminology ? when are we supposed to use the derivative of the squared error cost function as opposed to the cross-entropy cost function ? EOA 
 cost function confusion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012cost function confusion&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;weirdbuzzingsorry, this has been archived and can no longer be voted onEOQ first , please read URL in the case of neural network fitting , f(x) is the cost function that is evaluated from the network's output . backpropagation is a smart way of calculating the gradient of f(x) . the gradient of f(x) is a vector of partial derivatives with regards to all parameters of the nn that are being adjusted during training . it can be understood as an application of the chain rule of calculus and dynamic programming . one example of cost function you can use for regression problems , is the mse . when applying backpropagation , you will need to calculate its derivative point #2 should make it clear , but in any case you use different cost functions depending on the kind of problem . mse-> ; regression and cross-entropy-> ;binary classification EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ hello everyone , i ran several models on a binary classification problem , each performing at around NUM-80% classification accuracy . i then used my process : run several models to see how well they performed for accuracy pick the top NUM models ( lda , random forest-NUM trees , logistic regression , and random forest-NUM trees ) create new features from those models-taking the predictions of each model ( using cross validation so that i would train NUM /5 of the data to predict the last NUM /5 , then swapping out so that there would be no data leakage ) . run a binary classifier on the new dataset which includes those extra features get NUM % accuracy . i tried NUM-fold cross-validation on the new model and it performed at around NUM %-accuracy . do i need to check something else to make sure that this model is legit ? EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ make sure you are evaluating the ensemble on totally untouched data . by untouched i mean that this data hasn't been used for fitting the base or stacker models . edit : and also not used for selecting hyperparameters for your stacker model EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ wait what do you mean by 'i mean that this data hasn't been used for fitting the base or 'stacker' models' ? can you give me an example of that ? all predictions are generated with train/test k-folds so none of the test data is seen when making predictions for constructing the base model prediction features . furthermore the top layer classifier is using all of the new feature data-old data in a train/test split to evaluate the performance . EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ consider a full dataset d which you separate in d.train and d.test for the sake of simplicity-in the end you only get a pontual estimate of performance . you do all your model fitting and hyperparameter selection in d.train , which may be via k-fold , simple hold-out cv , bootstrap or w/e . your actual model accuracy must be evaluated on d.test , since the estimate you get while choosing hyperparameters in d.train is biased . EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ yes , isn't that what i described ? or is that different from what i described ? i mean isn't that what cross-validation does ? it evaluates the performance for each training set by predicting the hold-outs . EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ you did NUM-fold cv , but if you are using the predictions that you do on those NUM /5 to select hyperparameters , you can't say the performance measurements you did are representative of the generalization error ( it is biased ) . you have a set h of hyperparameters combinations from which you want to pick the best and then estimate the generalization error with the best choice . consider for a moment cross-validation with held-out split , that means : you separate d in d.train , d.validation and d.test : the way you should do it is for each h in h , fit the model in d.train and evaluate it on d.validation . then you select the one with best performance on d.validation . next, you fit the final model on d.train-d.validation using the chosen hyperparams and evaluate on d.test . the performance on d.test is what you should present as the performance . now , say, you want a better procedure to choose the best hyperparameter . you want to do a NUM-fold cv to select it . first , separate d in d.train and d.test : for each fold realization in d.train , you fit models with the NUM /5 train partition and evaluate on the NUM /5 validation partition . take the average or accumulate the loss across all NUM folds' validation partitions pick the best one ( least loss ) next , you fit the final model on d.train using the chosen hyperparams and evaluate on d.test . the performance on d.test is what you should present as the performance . now consider the situation in which you want the performance of a type of model in a given dataset . in order to do it , you need multiple generalization error measurements . one way to do it is doing nested k-fold cv , meaning you do the procedure outlined above , but instead of making a single d.train and d.test split , you do a k1-fold cv and another k2-fold cv inside each of the k1 folds . then you take the performance in the k1-folds as your estimates . if you aren't doing something in this spirit , then your estimate is probably wrong ... EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ i had to look online for a while to understand what you're saying . i think i understand my errors now , but just to make sure-is this the proper way to do it ? URL this is someone else's implementation of stacking . EOA 
 my classifier has NUM % accuracy , is something wrong or is this good data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }567my classifier has NUM % accuracy , is something wrong or is this good data?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;nixonitesorry, this has been archived and can no longer be voted onEOQ that's one way , but here are a couple of points regarding this script : it does no hyperparameter tuning just because its base classifiers are some boosting methods that are pretty robust wrt the choice hyperparameters . for some classifiers , certain choices of hyperparameters would be bad ! in the main section there's a loop to get a better result . don't do that or you will be overestimating generalization error . EOA 
 need ml algorithm suggesstion ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123need ml algorithm suggesstion ?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;konsoleiiitsorry, this has been archived and can no longer be voted onEOQ hi , i have read ml long time ago . i've crunch time and in need to choose the algorithm to complete my following task : traveller , is visiting my website. i make them fill the form and have alll the necessary signal ( attributes ) with me like whether they have booked flight or not , whether email is guenine is not , phone no is given or not , trip date is fixed , destination location is fixed or not . but along with that i have many visitor who don't fill the form completely or just uses fake phone number . i again re-iterate , i have lot of signal available with me , and i need to filter out the traveller who is certain to go for travelling so that i can personally contact them . i also need some score as well on the scale of NUM . which ml algorithm is best suited for this job and why ? EOA 
 need ml algorithm suggesstion ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123need ml algorithm suggesstion ?&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;konsoleiiitsorry, this has been archived and can no longer be voted onEOQ regularized logistic regression . because you need a starting point , you can't expect to find the perfect method on the first attempt . EOA 
 probability distribution upto a normalizing constant : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012probability distribution upto a normalizing constant&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;okayshokaysorry, this has been archived and can no longer be voted onEOQ i have always had trouble understanding what it means when we say : we know a probability distribution upto a normalizing constant . what do we exactly mean here ? do we mean that we know the probability distribution p(x)-p~(x)/z as a whole with the normalizing constant z ? or do we mean that we know the probability distribution only in terms of p~(x) and do not know the normalizing constant z ? please help . EOA 
 gradients turn zero after a few epochs : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123gradients turn zero after a few epochs&#32;(self.mlquestions)submitted&#32;6 months ago&#32;by&#32;sutzpahsorry, this has been archived and can no longer be voted onEOQ i'm training a cnn with NUM layers ( uses relu ) and using a nll cost function . for some reason , the gradients seem to vanish after a few epochs and learning stalls . i've tried playing with learning rates and optimisation schemes-no luck . any experts who could point me to what could be causing this ? thanks! EOA 
 day of the week with generic backward prop nn's : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123day of the week with generic backward prop nn's&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;progcatsorry, this has been archived and can no longer be voted onEOQ i have been with temporal data and have been representing monday as NUM and friday as NUM . in general is this a good idea or is it better to represent the data as the day of week in binary ? ( NUM ,001,010,011,100,101 ) i realize this is a pretty generic question . i am just looking for a rule of thumb . EOA 
 day of the week with generic backward prop nn's : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123day of the week with generic backward prop nn's&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;progcatsorry, this has been archived and can no longer be voted onEOQ from what i know , first method is common for categorical data ( days of the week ) , named one-hot encoding . i don't see any advantages in second method besides slightly smaller number of inputs . EOA 
 what is the most common model for acoustic classification ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012what is the most common model for acoustic classification?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;bldyknucklessorry, this has been archived and can no longer be voted onEOQ i'm writing a few labs on the subject and i'd like to get some opinions on what i should teach . EOA 
 what is the most common model for acoustic classification ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012what is the most common model for acoustic classification?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;bldyknucklessorry, this has been archived and can no longer be voted onEOQ this paper here might be useful :) URL EOA 
 how to train lstm layer of deep network : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012how to train lstm layer of deep network&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;theirfredditsorry, this has been archived and can no longer be voted onEOQ i'm using a lstm and feed-forward network to classify text . i convert the text into one-hot vectors and feed each into the lstm so i can summarise it as a single representation . then i feed it to the other network . but how do i train the lstm ? i just want to sequence classify the text ? should i feed it without training ? i just want to represent the passage as a single item i can feed into the input layer of the classifier and then the classifier classifies it . i would greatly appreciate any advice with this ! EOA 
 how important is performing cross validation on your algorithm's parameters ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how important is performing cross validation on your algorithm's parameters?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ > a lot . if you have a parameter you are changing it is just like another feature . if you run your algorithm with NUM 's or NUM 's of parameter combinations .. you will have results which are 'significant' just due to chance . EOA 
 how important is performing cross validation on your algorithm's parameters ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how important is performing cross validation on your algorithm's parameters?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ > EOQ so should i use something like scikit-learns grid search cv ? EOA 
 how important is performing cross validation on your algorithm's parameters ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how important is performing cross validation on your algorithm's parameters?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ > EOQ i am afraid i dont know what that is . i have used ga's and other optimisation techniques in the past . so i run the ga inside the cross validation loop . quite an expensive way of doing it mind you . EOA 
 how important is performing cross validation on your algorithm's parameters ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how important is performing cross validation on your algorithm's parameters?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ > EOQ yes , even that it is not hard to make your own ( more informative ) grid search for few parameters . EOA 
 is there an accessible implementation of any of the image captioning neural nets that have recently gained attention ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123is there an accessible implementation of any of the image captioning neural nets that have recently gained attention?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;kingsoftheonsorry, this has been archived and can no longer be voted onEOQ i'm not necessarily looking for a trainable network . a general purpose pre-trained model will also suffice. basically , something i can plug in images to and see the quality of image titles generated . EOA 
 would you benefit from deep learning examples & tutorials ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123would you benefit from deep learning examples & tutorials?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;jasonsilvermannsorry, this has been archived and can no longer be voted onEOQ i'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? EOA 
 would you benefit from deep learning examples & tutorials ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123would you benefit from deep learning examples & tutorials?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;jasonsilvermannsorry, this has been archived and can no longer be voted onEOQ definitely ! it sounds like it could be fun EOA 
 would you benefit from deep learning examples & tutorials ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123would you benefit from deep learning examples & tutorials?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;jasonsilvermannsorry, this has been archived and can no longer be voted onEOQ i would definitely subscribe. go ahead please do it EOA 
 would you benefit from deep learning examples & tutorials ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123would you benefit from deep learning examples & tutorials?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;jasonsilvermannsorry, this has been archived and can no longer be voted onEOQ if it's python , if it provides some math-not too hard-not too dumbed down-something that a probability/linear-algebra background can figure out-pretty pictures-links to further reading-links to foundational reading , if the projects have varied lengths so some are short and some are medium ( NUM lines ) and few are large ( NUM-) . yes . EOA 
 would you benefit from deep learning examples & tutorials ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123would you benefit from deep learning examples & tutorials?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;jasonsilvermannsorry, this has been archived and can no longer be voted onEOQ pseudocode with clear description of every variable would be a goldmine trying to detangle other's code on github is very cumbersome EOA 
 would you benefit from deep learning examples & tutorials ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123would you benefit from deep learning examples & tutorials?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;jasonsilvermannsorry, this has been archived and can no longer be voted onEOQ yes please , i would definitely benefit from tutorials/guided learning ! EOA 
 how to represent multi-dimensional data as colors in self-organizing map ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012how to represent multi-dimensional data as colors in self-organizing map?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;soulrezsorry, this has been archived and can no longer be voted onEOQ currently , i am able to represent NUM-dimensional rgb data in my self-organizing map , however, i'd like to represent more advanced data that can have up to NUM features . is there a method or function i can apply to represent the advanced data as colors in my som ? thank you in advance ! EOA 
 long boolean vectors as data set-looking for suitable model / algorithm : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123long boolean vectors as data set-looking for suitable model / algorithm&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;mlstartersorry, this has been archived and can no longer be voted onEOQ hey ! i am new to this topic , therefore any help is greatly appreciated ! my data set consists of two pieces : one factor ranging roughly from-1 to-1 a rather long ( say NUM-dim. ) boolean array in the training process , i want to find correlations between the factor and what values in the boolean array are set . runtime, the algorithm should be able to predict a value for the factor when analyzing an array . the previous solution used some kind of svm . unfortunately i don't have access to the source code and can't really think of a way to re-implement this . thank you for any kind of help ! EOA 
 long boolean vectors as data set-looking for suitable model / algorithm : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123long boolean vectors as data set-looking for suitable model / algorithm&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;mlstartersorry, this has been archived and can no longer be voted onEOQ my uninformed opinion : one-hot encoding for the array boolean values , assuming you have enough data for that dimension size. then run a logistic regression on it for some interpretable results ( by reading the coefficients ) , or run a random forest for high accuracy . maybe you can determine the importance of features with tree-feature selection ... ah i can't link to it because sourceforge is down at the moment ( they host scikit-learn ) . then if you cut down on the features , your logistic regression model may become more interpretable . EOA 
 i've already implemented a naive bayes classifier to help me assess the sentiment of tweets . what other algorithms can i use ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234i've already implemented a naive bayes classifier to help me assess the sentiment of tweets . what other algorithms can i use?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ i have a data set of tweets , and i need to classify them as either pro , anti, or neutral with respect to some topic . i've already used the mulitnomial naive bayes classifier on sklearn , and would like a second algorithm to compare the classification against . my first thought would be k-nearest neighbours ( i can use countvectorizer to turn the tweet into a vector of characteristics , and then run it through the alg ) . any other suggestions ? EOA 
 i've already implemented a naive bayes classifier to help me assess the sentiment of tweets . what other algorithms can i use ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234i've already implemented a naive bayes classifier to help me assess the sentiment of tweets . what other algorithms can i use?&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ i've tried using a nb classifier like that for sentiment analysis on larger passages , but it didn't work too well . you could build a long short-term neural network and then classify it's output , which is how sentiment analysis is often done with neural-networks . take a look at : URL EOA 
 problem understanding bias , variance, and cost/learning curves for train set , cross validation set and test set . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012problem understanding bias , variance, and cost/learning curves for train set , cross validation set and test set.&#32;(self.mlquestions)submitted&#32;7 months ago&#32;by&#32;throwawaytartansorry, this has been archived and can no longer be voted onEOQ could anyone give me an eli5 ? i'm in the middle of doing the coursera on ml and even though i've finished the assignment ( i'm just good at following instructions ) i don't actually understand what the significance is , and how we draw conclusions from the above . also , am i right for thinking that the cv set is for determining the right value of lambda to use ? while the train is used for the thetas ? EOA 
 trying to decide on algorithm for my data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001trying to decide on algorithm for my data&#32;(self.mlquestions)submitted&#32;7 months ago&#32;-&#32;by&#32;manly.sorry, this has been archived and can no longer be voted onEOQ hi . i've began reading a lot about machine learning , but having no advanced background in statistics/maths , i find it hard to relate to most of the online information for selecting the proper algorithm on my data . i do programming for a living . this is a pet project as will be clear from my example , so i don't need an absolute best fit or optimal result . with this said , here is my problem : i play path of exile , a free game . it's extremely similar to diablo . it's got one of the most complex set of items attributes . the game allows players to have shops online ( so list items with prices ) as well as give the information about what the players are wearing . i spent months of time writing code to extract this data , so i have historical data about what items were worn , what they were replaced with , what items were listed for sale , which ones sold , how long it took to sell , etc. this data is rather massive , were talking a few hundred gigabytes . my data is split in NUM big data sets : inventories ( what players wear ) items for sale ( pricing data is very noisy , as there are no consistency in pricing ) heres a breakdown of how items work , as far as stats are concerned : there are ~250 stats to chose from . those stats never change . ( ex:-40% fire resistance ) an item is a combination of NUM-6 of those stats . every stat has one category . when an item has one stat , it cannot get another stat within the same category . ( ex: if the item has-40% fire resistance , it cannot have the NUM nd stat '-20% fire resistance' . there are ~30 categories ) every stat has a known chance to occur . ( ex: NUM 5% chance to get the stat-40% fire resistance ) if the item is listed for sale , it may have a listed price . the system isn't much more complicated than that . i simplified a lot the details , but this is what matters . i have NUM questions i wish to answer with machine learning : predict an item pricing try to find how strongly related are stats together ( ie : out of the stats that players wear , estimate the demand ) i tried the andrew ng online course , as well as reading a lot of documentation online , the weka tool , and none seem to make it clear what algorithm i should pick . this is what i am planning to use as input neurons : every category ( ie : set of stats ) is an input neuron , with the currently selected stat within the category being the value . the 'weight' of every stat in the category is simply its probability to occur . the stats within the category are ordered from least probable to most probable ( ie : best to worst ) . proportionally adjust the categories ranges within NUM-1 . ex : ( one category ) stat range ( of fire resistance ) probability of the magma NUM-45 NUM 0% of the volcano NUM-41 NUM 5% of the furnace NUM-35 NUM 5% of the kiln NUM-29 NUM % of the drake NUM-23 NUM % of the salamander NUM-17 NUM % of the whelpling NUM-11 NUM % so if my item has of the magma , for that neuron , i would give it NUM 0 value to that input in the nn . if it had of the volcano , i would give it NUM 5. every category is given an input accordingly , with NUM denoting none was chosen . so with this said , i am not sure which algorithm to pick to feed it the output neuron (pricing?). my data is very noisy for prices . should i use a classifier with the output neurons being slices of prices ( ie : neuron NUM -NUM-1$ , neuron NUM -NUM-5$ , neuron NUM -NUM-10$ , etc. ) ? or should i have only one output neuron ? i thought the slices would effectively 'fix' the issue of very variable pricing and at least give a good idea of the price range to expect . and as far as detecting the strength of connections between stats for items that are worn , i do not see what output neuron i could map , so i was wondering if that was even possible . i though i should rank negatively stats that are being removed ( ie : player stopped equipping item with stats x,y,z, so input those values but as negatives ) , but that still leaves me wondering how to extract the correlation between the stats . i would welcome any help as such . i don't expect any hand-holding , i just want a nudge in the proper direction ! thanks again EOA 
 trying to decide on algorithm for my data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001trying to decide on algorithm for my data&#32;(self.mlquestions)submitted&#32;7 months ago&#32;-&#32;by&#32;manly.sorry, this has been archived and can no longer be voted onEOQ let me preface this by saying that i'm no ml expert , i've only part gone through hinton's coursera class . i do play poe though , and this is an interesting project . i would say that classifying the items in price ranges would be easier , then your output could be something like NUM % chance of it being in range x-y , NUM % chance of it being in range w-z and NUM % chance of it being in range u-v . this seems like it would be easier to train than a single output of the expected price. if you used one output of expected price , you would also need to convert all prices to one currency , whereas with the ranges it isn't the case ( you could have outputs of NUM-20 chaos and NUM-40 exalts no problem-the nn only puts it in a class ) if it were me , my inputs to the nn would be as follows : item type ( helm/chest/amulet/ring/etc-give this a number so itemtype-1 is for helms for example ) defensive stat type ( NUM for none , NUM for armour , NUM for es , NUM for evasion , NUM for ar/es , NUM for ar/ev , NUM for ev/es , NUM for ar/ev/es-sacrificial garbs ) then the rest of the input nodes would be all possible affixes . assuming it's a nn , your outputs would be the ranges of prices . the price estimation is more straightforward to me , it's just a simple classifier . the strengths of the connections come through your training cases , you're looking to do supervised learning ( i think ) . i don't see why you would start by specifying weights . that comes through learning . just have the stat ( e.g. fire resistance ) as an input neuron , and that input value as the stat's value . i think that's a better system because of hybrid affixes ( e.g. emperor's ) don't show up directly-you need to infer their existence . for flat damage , use the average value (e.g. adds NUM to NUM physical damage would be flat phys neuron with value of NUM ). for weapons , you would also need to look at the pdps and dps , on top of the aps and crit chance . i think this is a very interesting project though ! you could use it to work out the best master crafting option to maximise the expected sale price . i've no idea how you would work out the whole items worn and their progression for demand estimation . EOA 
 trying to decide on algorithm for my data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001trying to decide on algorithm for my data&#32;(self.mlquestions)submitted&#32;7 months ago&#32;-&#32;by&#32;manly.sorry, this has been archived and can no longer be voted onEOQ oh , well i built an entire framework , i already detect all possible affix combinations ie : if theres NUM % ipd , then it will know its emperor-tyrannical . i already do the job of splitting the affixes in code before passing it onto the nn . the reason i wanted to weight the values by the affixes drop rates is that people can only wear what they have seen drop/what has dropped . in other words , we don't have accept to perfect gear , even if we had the currency for it (lets ignore mirrors for now...), so people have to pick through existing items upon which the perfect combination of stats probably never dropped , hence why i think that scaling the neurons based on affix spawn rates should help . a big detail i failed to mention , i planned to make one nn per item base (ie : amulets, rings , etc.). and i'll try to re-split it per specific base depending on how well it goes ( ie : onyx amulets ) . i do have all dps data , as well as all possible craft options . the code already starts from one item and calculates all possible bench options that can be done ( since i do parse affixes , not just read the text/parse values ) . the code already does all that . the big things i don't know , well, i've never used a nn before . for pricings i think i want a naive bayes , so that my output neurons would be price slices (1-2$ , NUM-5$, etc.). but for the correlation between stats worn , that seems like an unsupervised problem from my limited understanding . i don't get how to extract the strength between neurons from the nn once it's built ( for the inventories/items worn db ) . is there an algorithm you would suggest for classifying the data ? it's frustrating that i read those courses and it's not quite clear how to do this . it feels like it should be a simple nn problem , with nothing fancy ( for pricings ) . which category of nn should i look into ? classifiers? EOA 
 trying to decide on algorithm for my data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001trying to decide on algorithm for my data&#32;(self.mlquestions)submitted&#32;7 months ago&#32;-&#32;by&#32;manly.sorry, this has been archived and can no longer be voted onEOQ first , have you tried just taking something like scikit-learn and just fitting a simpler regression model , like linear regression or random forest regression , to the data first ? often those will get you NUM %-of the accuracy of a more complex model in about NUM % of the time , and help you figure out which sets of attributes and outputs will work well . if you just use the 'raw' price as the output , you can use a regression model , and if you want to lump the prices into ranges , then use a classifier ( where each price range is a separate class ) . if you're determined to use a neural network , then for managing the output there's two simple ways you can do it . the easiest is to have a single output neuron for price that uses a rectified linear activation function and squared error loss ( in other words , treat it as a regression problem ) . this will drive your model to try to estimate the average price across that set of inputs . the second way to do it is to break the price into ranges and treat it as a classification problem : stick a softmax layer at the end with categorical cross-entropy as the loss function . this will make it estimate the probability that an item will sell in a given range . you do have to be a bit careful to NUM ) have a 'bin' for every possible price , and NUM ) not have too many outputs , since this can make the model harder to fit . advantages of the linear output is that you only need one output neuron , it should fit quickly , and it will give you a flexible output . disadvantage is that it won't give you much in the way understanding the variance of the price ( i.e. the average might be $1 , but is it $1-/-$0.01, or $1-/-$0.99? ) advantages of the 'binned' output is that it will give you some measure of uncertainty , so it might tell you that it's NUM % sure that it will sell for $1-2 , or it might tell you that it's NUM % likely to sell for $1-2 , NUM % for $2-3 , etc. disadvantages are that it will probably be slower to train , and if you slice the prices too small , then you could get odd results if you have very rare combinations of attributes that sell for unusual prices . for analyzing the combinations of items , you might be able to use something like t-sne on your trained network to get some idea of if or how particular combinations of traits cluster , but i'm less familiar with that sort of thing . it might be easier to fit a separate model , either by filtering to only examine items that were actually purchased and then doing a ( non neural network ) clustering on those , or maybe through building a model that tries to predict the nth attribute given the other n-1 on the item . EOA 
 trying to decide on algorithm for my data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001trying to decide on algorithm for my data&#32;(self.mlquestions)submitted&#32;7 months ago&#32;-&#32;by&#32;manly.sorry, this has been archived and can no longer be voted onEOQ no , i havent had the chance to try scikit-learn yet . it is currently installing . i am eagerly waiting to try your suggestions as well as read up on them . the prices are fairly non-linear , so i didn't think linear regression would work ( unless the 'linear' part refers to something else entirely ) . thank you for pointing me towards a more proper direction-i'll give it an honest shot and read up on the suggestions ! EOA 
 trying to decide on algorithm for my data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001trying to decide on algorithm for my data&#32;(self.mlquestions)submitted&#32;7 months ago&#32;-&#32;by&#32;manly.sorry, this has been archived and can no longer be voted onEOQ generally-speaking as someone who does this for a living-i almost always try a random forest first ; they're fast , give pretty reliably good results , and can usually give you a good idea of what features are important and if you have a problem with how you've decided to encode your output . you only reach for more complex models if the simpler ones don't give you adequate results . neural networks in particular can take a long time to fit , and deciding how to set hyperparameters ( like the number of layers , the number of nodes in each layer , the right activation functions , so on ) can be a challenge , especially when just starting out . not that there's anything wrong with fooling with some more complex models just for fun , i always enjoy trying whatever new shiny thing i just read about on whatever i'm working on , but for real work , i'd start simpler and build up . EOA 
 trying to decide on algorithm for my data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001trying to decide on algorithm for my data&#32;(self.mlquestions)submitted&#32;7 months ago&#32;-&#32;by&#32;manly.sorry, this has been archived and can no longer be voted onEOQ alright , sounds reasonable . i installed scikit-learn and , well, fortunately i do programming for a living . i thought i could import my data in the tool and try a variety of algorithms without having to write code . weka does that , so that's what i've been trying to fit my data with . the problem i seem to face from my limited testing with weka and various clustering algorithm is that my input neurons don't seem to be well choosen . my data is very sparse ; NUM to NUM neurons containing data with the rest being basically inactive ( on average NUM-40 ) . i changed my inputs to help the algorithms : -1 : denotes inactive NUM-1 : from lowest possible value to highest possible but looking at the internal values within weka algorithms it seems like the-1s are heavily biasing the data ( although it might be just my understanding thats limited too ) . would i tend to be better off trying models where an inactive neuron is NUM , the same as the lowest possible roll ? i looked at an online presentation from google about t-sne , the results looked truly amazing ! EOA 
 trying to scale output of mahout logistic regression to p(event) , but it's not working . help? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012trying to scale output of mahout logistic regression to p(event) , but it's not working . help?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;wil.doggsorry, this has been archived and can no longer be voted onEOQ hi we are embedding a simple logistic regression step into our machine learning system so that the output of our java-based genetic algorithm ( ga ) can be automatically calibrated to the scale of p(event-NUM ) where the p.event score will never be outside of the range of ( NUM ,1 ) and will be well calibrated within that range . the current output of the ga is an interval level vector of scores that rank orders very well and has a nice normal-ish distribution , but the scale is arbitrary and meaningless . transforming that to p(event) scale is needed in order to use the score in downstream bi and decision sciences . currently we calibrate using sas proc logistic and the usual transformation of the resulting log odds to p.event , but i want to eliminate the i/o step that involves scoring the raw data file , running proc logistic , saving the output of that regression , and then writing the equation to get the log odds , and then transforming log odds to probability of event , and then finally saving the data set at the end of that last data step . the issue we are running into is that we have implemented mahout's sgd classifier , which yields final scaling that ranges from NUM to NUM , but the calibration is off . sometimes it is off by just a little , sometimes it is very far off . here's an example of where it is far off : URL depvar.arthritis is the dependent variable , predicted probability is what we get when we take the score generated by the ga and run it through logistic regression , derive the log odds , and transform to p.arthritis , and mlrp is what we get from implementing mahout's sgd logistic regression as described on this web page : URL as you can see the mahout's value is scaled correctly between NUM and NUM across the entire range of observed scores , but it is not calibrated as well as what we get when we use simple logistic regression in sas or spss . in fact , the calibration sucks . the rate of arthritis diagnosis is .62, but the average p.arthritis score coming out of the system is .66. ugh . however , the results vary from case to case . in this second example , the average value of the output of mahout's procedure is equal to the actual event rate to the NUM rd decimal place , but the calibration is off at both tails of the distribution , relative to the calibration achieved by running the score the ga creates through proc logistic and then scaling the log odds to p.outcome . URL we are not beholden to mahout's sgd method of logistic regression , we just need something that works in java , that can take the output of what the ga has created that is on an arbitrary numeric scale , and translate that to p.event in a manner that is well calibrated and meaningful / easy to interpret / works every time. the faster it operates the better . any assistance / guidance would be appreciated , am posting here first to give redditors first dibs on a solution but also will follow up with mahout experts at the various websites if we come up dry here . EOA 
 question on kernels : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001question on kernels&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;polalaviksorry, this has been archived and can no longer be voted onEOQ working through andrew ng's coursera class and trying to wrap my head around kernels . from my understanding svms work by projecting the data into a higher dimension and slicing it with a hyper plane and projecting it back into the original dimension to create a decision boundary , or at least maybe this is how polynomial kernels work ? trying to figure out intuitively how the guassian kernel maps the data into a higher dimension to be able to find a decision boundary . EOA 
 how do i handle large csv datasets ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how do i handle large csv datasets?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;crazymonezyysorry, this has been archived and can no longer be voted onEOQ i'm a beginner messing around with some basic application of ml , applying simple classifiers and stuff , and i stumbled along a ctr challenge avazu posted a while back on kaggle : URL. my problem is that the training set here is a single csv ~2gb in size , and possibly a few hundred million entries . i tried opening this dataset in the weka viewer , and even after increasing the stack size to NUM gb , it stopped responding after some time , forcing me to close it . this brings me back to the question , what tools/libraries do you guys use to handle such large datasets ? EOA 
 how do i handle large csv datasets ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how do i handle large csv datasets?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;crazymonezyysorry, this has been archived and can no longer be voted onEOQ let's denote the problem straight : it doesn't matter to you if the dataset size is NUM gb/20gb/3tb if you can hold only NUM gb dataset . there is no other way to shrink down your dataset other than to take a subset of it . every line-by-line approach to get this subset would work for you . well , there are some more questions-like how to choose the entries of subset-randomly , proportionally or sets of same size for every factor . that depends on the task , there is plenty of information about subsampling . if this subset is not enough for you to reach stability in model , there are still options of making a few subset and juggling with them in various ways . generally question how do i just load my enormous dataset straight into my r/weka/excel/sklearn/spintowintoolbox without cleaning it or taking subset of features and observations ? has no answer , you should handle this yourself . EOA 
 how do i handle large csv datasets ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how do i handle large csv datasets?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;crazymonezyysorry, this has been archived and can no longer be voted onEOQ my uninformed opinion : read it chunk by chunk and consider using an on-line learner . EOA 
 how do i handle large csv datasets ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how do i handle large csv datasets?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;crazymonezyysorry, this has been archived and can no longer be voted onEOQ you could simply read it line by line with csv reader , put it in list then convert it to numpy array . EOA 
 how do i handle large csv datasets ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how do i handle large csv datasets?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;crazymonezyysorry, this has been archived and can no longer be voted onEOQ for example : import pandas as pd df-pd.from.csv(filename) or if you just want a list : with open(filename) as f : content-f.readlines() EOA 
 help a beginner out ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012help a beginner out?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;truthseeker1990sorry, this has been archived and can no longer be voted onEOQ hi ! i am a college student trying to diversify my resume with some projects that are a little different than a standard crud application and have always been interested in ml . i am going through 'mahout in action' right now . i am just curious if anyone could suggest a decent data clustering project ? i am completely unable to think of anything interest right now :/ any advice or suggestions will be appreciated . thanks EOA 
 help a beginner out ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012help a beginner out?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;truthseeker1990sorry, this has been archived and can no longer be voted onEOQ anyone here ?? EOA 
 nearest neighbor help : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012nearest neighbor help&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;desegelsorry, this has been archived and can no longer be voted onEOQ hi , i am trying to find similarities between binary files based on strings extracted from the binary data . let's say i have a database of a thousand files . first i'm extracting all the strings from the binary , then somehow i want to index them . then, i want to query a new binary's strings and find which are the most similar binaries . do you have an idea on how can i achieve that ? is lsh suitable here ? ( as i don't have a corpus of strings... ) my second challenge , is to combine the strings feature to other more conventional features , such as file size. but how can i combine lsh and numeric features into one vector with one query ? thank you very much EOA 
 are there any datasets freely available on which i can test the autocomplete algorithm i have developed : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123are there any datasets freely available on which i can test the autocomplete algorithm i have developed&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;rohanpotasorry, this has been archived and can no longer be voted onEOQ > there is /r/datasets-also ask there . EOA 
 are there any datasets freely available on which i can test the autocomplete algorithm i have developed : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123are there any datasets freely available on which i can test the autocomplete algorithm i have developed&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;rohanpotasorry, this has been archived and can no longer be voted onEOQ > EOQ ok EOA 
 i need help with reinforcement learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123i need help with reinforcement learning&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;xdki113rsorry, this has been archived and can no longer be voted onEOQ i'm trying to do an ai with machine learning , so i started to learn how to do machine learning , but i don't think i understand enough of it to do one . i tried learning by watching some courses , but what they say doesn't seem to be related with what i'm trying to do . if anyone could help me , i'm trying to do it with pybrain and python NUM at the moment . i'm currently trying to code the environment class . i am doing this just for fun and to learn how it works . EOA 
 i need help with reinforcement learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123i need help with reinforcement learning&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;xdki113rsorry, this has been archived and can no longer be voted onEOQ so what are you trying to do ? without a concrete question , i doubt anyone can really help you . what are your problems with the environment class ( from this tutorial i assume ) ? for a general introduction to rl , sutton and barto's famous rl book is freely available here . you might also like this udacity course by charles isbell and michael littman , or olivier georgeon's ideal mooc although it's a bit more advanced . EOA 
 i need help with reinforcement learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123i need help with reinforcement learning&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;xdki113rsorry, this has been archived and can no longer be voted onEOQ yes , that tutorial indeed . in fact , i'm trying to understand what is happening in the examples from pybrain . what i am trying to do is an ai for a clone of agar.io i am coding with some friends . the problem is i don't know where to start from because i'm not familiar with rl and ml in general . i'm not quite sure how to adapt the rl for my needs . edit: i'm currently reading the book and found another problem i don't know how to fix : how can i set the goal to be get as big as possible ? do i simply need to give it a reward for growing or do i need to give it a maximum value to try to reach ? i will check sutton and barto's book this weekend . EOA 
 i need help with reinforcement learning : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123i need help with reinforcement learning&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;xdki113rsorry, this has been archived and can no longer be voted onEOQ very usefull ! i'll do like you said and try to start simple with it . i don't know if you looked at pybrain in debt or not , but i was wondering if the reward is what's returned by the getsensors() method . thank you for your answers ! p.s.: i'll pm you if you want some updates about this projet . EOA 
 markov decision process in r for a song suggestion software ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123markov decision process in r for a song suggestion software?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;edevcimotsorry, this has been archived and can no longer be voted onEOQ okay , so i'm not exactly sure if this belongs here , but this is my problem : we have a music player that has different playlists and automatically suggests songs from the current playlist i'm in . what i want the program to learn is , that if i skip the song , it should decrease the probability to be played in this playlist again . i think this is what's called reinforcement learning and i've read a bit about the algorithms , decidin that mdp seems to be exactly what we have here . i know that in mdp there are more than one state , so i figured for this case it would mean the different playlists . like depending on the state ( playlist ) i'm in , it chooses the songs that it thinks fits the best and get punished ( by skipping ) if it has chosen wrongly . so what i'm asking is , if you guys think this is the right approach ? or would you suggest a different algorithm ? does all of this even make any sense , should i provide more information ? if it does sound right , i'd like to ask for some tutorials or starting points getting about mdp in r . i've searched online but have only found the mdp toolbox in r and it kind of doesn't really make sense to me . do you have any suggestions ? i'm really helpful for any kind of advice. :) EOA 
 markov decision process in r for a song suggestion software ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123markov decision process in r for a song suggestion software?&#32;(self.mlquestions)submitted&#32;8 months ago&#32;by&#32;edevcimotsorry, this has been archived and can no longer be voted onEOQ mdp researcher here . i would avoid the mdp formulation for this problem since what you would have here is a partially observable ( in the state of the person selecting songs ) markov decision process , which are really difficult . depending on your requirements , i think this could be a good way to proceed that doesn't take a lot of advanced ml education : confine your worldview to a single playlist . switching playlists is a separate and likely more complicated problem that you don't need to deal with . if you have the resources to build song features that describe the attributes of each song ( think of these like pandora's genres ) then you can build a model where an action ( punishing ) for one song can inform whether you want another song to be played . if you don't have the ability to create song features , then your modeling choices become very simple . assuming you have song features , you should start with a logistic regression . from there , many tweaks and improvements are possible . EOA 
 can i use sklearn's naive bayes to classify tweets ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234can i use sklearn's naive bayes to classify tweets?&#32;(self.mlquestions)submitted&#32;9 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ i have a data set of tweets with keywords relating to vaccine perception . i want to be able to classify a new tweet as either pro-vaccine , anti-vaccine, or neither . i hear naive bayes is a way to do this , but i can not find any documentation on how can implement the classifier with words as feautres rather than numbers . can anyone provide some insight ? EOA 
 can i use sklearn's naive bayes to classify tweets ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234can i use sklearn's naive bayes to classify tweets?&#32;(self.mlquestions)submitted&#32;9 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ yes you can , but you have to figure out a way of representing tweets so that the classifier can do its work . go over the bag-of-words representation and how you use it to fit the multinomial naive bayes model . other groups have done this kind of stuff with so-called word vectors , where individual words are given a representation in an arbitrary vector space learnt from a huge text corpus via neural network models (see if you're using python , the sklearn library has automatic functions for this kind of feature extraction , see EOA 
 optimization of svm-how to interpret model order giving best cross validated results : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123optimization of svm-how to interpret model order giving best cross validated results&#32;(self.mlquestions)submitted&#32;9 months ago&#32;by&#32;neurone214sorry, this has been archived and can no longer be voted onEOQ hey everyone . i'm using svm to classify neural activity data into two states both on and off a particular drug . i find that i can get very good cross-validated classification for either condition provided i optimize the polynomial order ( btw , i'm using a polynomial kernel ) , kernel scale , and a parameter that matlab refers to as the box constraint , which is related to the 'softness' of the margin . interestingly , there's a very obvious ( and statistically significant ) difference between the two sets of models in terms of the optimized parameters ; the svms optimized for the drug data require a NUM nd order kernel expansion whereas the svms optimized for the control condition require only a NUM st order expansion . now , the nature of the data aside ( i.e. the fact that it's neural activity ) , how might one interpret what it means to require a higher order expansion to classify data ? i understand that the expansion is the equivalent of including as features all the pairwise products of the features ; so, does this mean that a better separating hyperplane exists in a nonlinear space relative to linear ? i'm sure i'm being very fast and lose with my terminology here , but i do appreciate any thought guidance that you might offer here . EOA 
 subsequence identification in time-series data ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012subsequence identification in time-series data?&#32;(self.mlquestions)submitted&#32;10 months ago&#32;by&#32;hammerheadquarksorry, this has been archived and can no longer be voted onEOQ hi /r/mlquestions , i'm researching a problem that's a bit out of my comfort zone and i'm not sure where to start . the problem i'm dealing with pressure readings during package transport . i'm trying to identify when the package takes off and lands during shipment ( it's sitting on the tarmac most of the time ) . this may happen multiple times per shipment . the way i see it , i'm trying to identify a occurrences of a similarly-shaped subsequence for each shipment ( something akin to finding instances of a particular word in dialogue ) . here are a few details about the data each instance of the data is a time series representing a single shipment . the data is unlabeled (i don't know which points belong to flight vs . tarmac). the pressure readings are recorded at regular intervals . my first guess at how to approach this paper : URL describes a method for generating a template for time series data using dynamic time warping ( dtw ) . i could hand-pick examples of the subsequence i'm looking for , create a template , then for each shipment , test every subsequence for closeness in the dtw-sense to my template and pick the best ones that don't overlap or something . my questions my approach seems super slow . i figure this problem has to be similar enough to something else , but i'm not sure what . i'm not even sure of the name for my problem . i'm hoping to find some relevant reading material or just some general advice ( e.g. another problem similar enough that i could build from it ) . thanks in advance ! EOA 
 trouble pre-processing large volumes of image data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123trouble pre-processing large volumes of image data&#32;(self.mlquestions)submitted&#32;10 months ago&#32;-&#32;by&#32;aweeeezysorry, this has been archived and can no longer be voted onEOQ this is my first time applying my limited understanding of ml . my knowledge is mainly from andrew ng's coursera class . any how , i've drafted up a skimage.io.imread()). now , my computer is pretty old and only has NUM gb of ram...i'll be getting a new computer soon with NUM gb and nearly double the processing speed , but in the mean time , i'd like to find a way to more efficiently generate , store, and represent this data so my ml scripts can work with it reasonably at a later stage of the project . the method you see in bogs up my computer and doesn't finish execution . i thought of throwing a condition statement in the suite of process.images() that checks if i'm at , let's just say , a NUM % interval of the total data set-> ; if so , then write the binary of the dictionary , image.data, to file and then set it to none so i can free up memory-> ; continue where i left off , repeatedly appending to the same binary file until i've gone through all NUM ,000 training examples . the problem with this is that after testing this out on just NUM images , my binary file is about NUM m (32.2m each , which accounting for all NUM ,000 images scales up to a NUM tb!). how the hell can i possibly work with this much data efficiently ?! what do ml experts usually do to pre-process high res image data ? how do i normalize/standardize the rgb values of each pixel in an image for every image in this huge of a data set ? i know that's more than a couple questions , but i'm not really sure where to go from here . advice, please and thank you ! EOA 
 trouble pre-processing large volumes of image data : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123trouble pre-processing large volumes of image data&#32;(self.mlquestions)submitted&#32;10 months ago&#32;-&#32;by&#32;aweeeezysorry, this has been archived and can no longer be voted onEOQ i'm sorry to say it more or less boils down to resources . a dataset that big really requires that faster pc you're talking about . NUM gb of ram just isn't enough to do what you need . i just took a look at your code and the competition . for a start you want to use cpickle over pickle . it is many , many, many times more performant than standard pickle . that alone may solve a lot of your issues . EOA 
 recommended sources for ml-related probabilities : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012recommended sources for ml-related probabilities&#32;(self.mlquestions)submitted&#32;10 months ago&#32;by&#32;alexandrezanisorry, this has been archived and can no longer be voted onEOQ i am wondering if there are good resources for people with a limited exposure to probability theory to get up to speed on the theories underlying important ml concepts . specifically, i'm thinking of learning theory , but i'm also interested in the theory behind maximum entropy models . EOA 
 rnn library for generating handwriting samples : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012rnn library for generating handwriting samples&#32;(self.mlquestions)submitted&#32;10 months ago&#32;by&#32;1243424sorry, this has been archived and can no longer be voted onEOQ i was interested in generating handwritten text using neural networks ( something like URL ) . are there any libraries that can be used to predict data in a similar way . i've looked into rnnlib by alex graves but the documentation does not provide much information about prediction . EOA 
 standardization of input variables : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012standardization of input variables&#32;(self.mlquestions)submitted&#32;10 months ago&#32;-&#32;by&#32;nick898sorry, this has been archived and can no longer be voted onEOQ i'm new to machine learning and have to implement a k-nearest neighbor algorithm on some data set . i've acquired the data set and it consists of NUM attributes . there are NUM data points i'm using for the training set . so , to summarize each data point has NUM attributes associated to it . each attribute is a measure of some different characteristic of the data and as you can guess , the range of each attribute varies . i need to normalize/standardize the data before i can use it as an input into the algorithm . i understand the difference between normalization/standardization and how to compute these . my question is do i standardize each of the NUM attributes separately ? for example , for just one attribute , i find the mean and standard deviation over all the data points for that one attribute and then compute the new standardized measure for each data point ? then rinse and repeat for all the other NUM attributes ? can any type of problem arise doing it like this ? for example , most of the attributes have a measure that is positive. but some of the attributes are negative for some data points . is this going to create some sort of distortion when i standardize ? basically, are there any common problems you might run in to when standardizing data that i may not be aware of given that this is my first time working with data like this ? EOA 
 classification algorithms-a discussion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012classification algorithms-a discussion&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ hi there , i've been tasked with writing my own implementation of any classification algorithm for an assignment . i was going to pick the c4.5 algorithm but i thought i'd ask you guys what you think ( as i thought it'd be interesting to see all the different opinions ) . i chose the c4.5 because i am quite familiar with how it works and feel that it would be ok for a beginner to attempt it ( i have never tried to write my own implementation of a classification algorithm before ) . my data deals with a bunch of measurements and i have to decide if the next entry is a certain type of animal . just in case you were wondering ! :) so if you were a beginner , what algorithm would you choose ? better yet , if you had to decide on an algorithm with your current knowledge , would it be different than if you were just starting out ? what pitfalls did you encounter when you were first starting ? also , if you could think of any resources that may be beneficial to my learning (not direct answers as i'd like to learn from my assignment!).....post away ! :p sorry if this is the wrong place , i got a notion and i thought it'd be pretty cool to hear form an expert or two ! thanks ! EOA 
 classification algorithms-a discussion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012classification algorithms-a discussion&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ the first classifier i implemented was logistic regression . maybe that's too simple for your assignment though . EOA 
 classification algorithms-a discussion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012classification algorithms-a discussion&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ out of curiosity , what language did you use ? did you stick with that language for your future projects or did you decide to use something else ? :) EOA 
 classification algorithms-a discussion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012classification algorithms-a discussion&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ once in python and once in c-. i'm a professional software engineer , so i jump between languages too much to say that i stuck with any particular one . i think python is a really nice language to implement new ideas though . especially if you tag in numpy . EOA 
 classification algorithms-a discussion : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012classification algorithms-a discussion&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ i think c4.5 will be just fine. i guess you could also start with id3 and then extend it to learn more about the differences . to prevent overfitting , you may want to look into some pruning algorithms . then if you want to go further , you could look into random forests were really popular a few years ago i think ( maybe they still are , but ml isn't my main field and i got the feeling their popularity has decreased now that deep learning works so well ) . in my earlier days i mostly implemented neural networks and genetic algorithms . you could do those too , although genetic algorithms may be slightly harder to apply to your problem . EOA 
 pre-empting failures with log file based prediction : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pre-empting failures with log file based prediction&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;ukcompscimssorry, this has been archived and can no longer be voted onEOQ hi , so i'm reasonably new when it comes to machine learning , and i'm aware i've barely scratched the surface . currently in my job we receive failure alerts when something fails/errors and we usually have a specific set of steps for remediation which we're automating . we already have a spark-as-a-service cluster in place , what i'd like to do is build a streaming project that learns over time when a log event correlates to an impending failure and as such initiates pre-emptive remediation ( whether that be restarting etc ) . obviously remediation is usually specific to the error so it needs to be able to link the log event to an error . my initial thoughts over the last hour or so have been two streaming projects . the first project correlates log events to errors : when an error arrives in the message queue : grab the last fifteen minutes log events from our infrastructure . pre-process the log events to remove specifics ( time , machine ip . etc ) pearson's chi-squared test to indicate whether a log event is significant to causing the error . write the results to a k/v store/database such that log events map to errors a second spark-streaming project : pulls in and builds a map from the datastore . listens for log events as they come in pre-process to remove machine-specific data (time , ip). if they match up with something in the map , initiate the remediation . does this seem reasonable ? something about this seems to me like it could be simplified but i'm unsure , so thought i'd throw it out there for comment . thanks EOA 
 rf model w/ ~15000s x ~150000f barely outperforming univariate model . suggestions? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012rf model w/ ~15000s x ~150000f barely outperforming univariate model . suggestions?&#32;(self.mlquestions)submitted&#32;11 months ago&#32;-&#32;by&#32;pasdedeuxsorry, this has been archived and can no longer be voted onEOQ hi everyone ! i'm developing an rf model to try and predict patient health care usage ( $/time ) after an intervention . my model currently includes their pre-intervention usage , text from their medical record ( tokens , bigrams, trigrams ) , and structured text from the medical record . i'm approaching this as a series of binary classification problems i.e. will the patient's post-intervention usage be > ; x %ile of usage ( a specific cutoff value ) ? i thought i was getting close to having a really impressive model ( auc NUM 7-0.90 ) until i compared my model to a base case of simply assuming that the patient would be in the same usage class post-intervention as they were pre-intervention . turns out that the model does modestly outperform that base-case for predicting ultra-high usage ( NUM th percentile and NUM th percentile ) , which is a useful contribution but surprisingly , it in some cases underperforms the base case for lower levels of usage . under the assumption that i haven't made any significant technical errors ( a possibility , but something that would be hard to identify in this forum. ) any thoughts on why an rf model would underperform a very crude prior-probability single-variable based decision ? i'm using the sklearn rfclassifier n.estimators-NUM . normalization of the data doesn't appear to have any significant effect , same with gini vs . entropy. max.features-none may outperform but the runtime is so long that it's hard for me to iterate on the model . i've tried other approaches , changing up feature selection strategies , using a small subsample to train an rf for feature selection , not seeing significant progress . thanks ! EOA 
 how do i scale non-normal ( nb ) features ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how do i scale non-normal ( nb ) features?&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;whyoysorry, this has been archived and can no longer be voted onEOQ hello /r/mlquestions ! thanks for your time , i'll be brief . i have a labeled dataset NUM million x NUM . about half are discrete , approximately negative binomial or log-concave poisson according to bioinformatics literature . these counts vary over several orders of magnitude and are not log-normal . i am hoping to use svm or nn for cross validation . do i need to scale these data ? it is frequently suggested ( ng , mostafa, etc. ) that features should be scaled to the range-1:1 or NUM :1 for svms . does this apply for nn as well ? if scaling is required , what would you suggest for these count variables ? is it appropriate to use the empirical cumulative distribution function ( ecdf ) to scale the values to NUM :1? EOA 
 help needed for summarization of amazon.com reviews : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012help needed for summarization of amazon.com reviews&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;kshitizsethiasorry, this has been archived and can no longer be voted onEOQ x-post from URL ( please post answers there if you can ) hi , a few friends and i have an idea : summarize the reviews of amazon.com products to show concise pros and cons of the product . this stemmed out of the time i waste reading reviews before buying a product . we're starting with nlp , and need to know where to start looking to search for answers to ( components of ) this problem . we've read jointly learning to extract and compress by taylor berg-kirkpatrick , dan gillick and dan klein , and have a dataset : URL thanks :) p.s.: this is my first post on reddit . EOA 
 help needed for summarization of amazon.com reviews : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012help needed for summarization of amazon.com reviews&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;kshitizsethiasorry, this has been archived and can no longer be voted onEOQ found some useful suggestions on : URL could still use your opinion ! :) EOA 
 row vs column vectors : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123row vs column vectors&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;alexandrezanisorry, this has been archived and can no longer be voted onEOQ is there a convention as to whether features are represented as row or column vectors ? or is it a free for all on that front ? EOA 
 row vs column vectors : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123row vs column vectors&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;alexandrezanisorry, this has been archived and can no longer be voted onEOQ from what i've seen ( using sklearn , matlab, caffe... ) features are always columns ( each sample is row in matrix ) . however, it is only convention as far as i know . EOA 
 askml: stacked denoising autoencoder-pebkac ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001askml : stacked denoising autoencoder-pebkac?&#32;(self.mlquestions)submitted&#32;11 months ago&#32;by&#32;omgitsjosorry, this has been archived and can no longer be voted onEOQ i'd wanted to train a denoising autoencoder for a personal project . towards this end , i built a neural network class which can ( does ) learn xor , or, and and without much difficulty . when training a denoising autoencoder , my understanding of the literature was that i should build the network with layer sizes a , b, a , for input , hidden, and output , respectively. then train the autoencoder as i would a standard neural network ( adding noise on input and using the original on output ) , then discard the final b-> ;a matrix . the resulting a-> ;b weight matrix is one layer of my denoising autoencoder . is this understanding correct ? can i use these deep autoencoders for generative purposes , or do i need to use rbms ? i saw a paper recently on arxiv that i haven't had a chance to read which seems to indicate it's possible to generate data with denoising autoencoders , but i'd like some extra input . EOA 
 number of parameters in multi class and two class logistic regression : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012number of parameters in multi class and two class logistic regression&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;theinfelicitousdandysorry, this has been archived and can no longer be voted onEOQ if you have f number of features , then in NUM class lr you have f parameters ( ignoring the bias ) . however in multiclass lr with k classes you have f-k parameters , correct? which means that using multiclass ( softmax ) lr for a NUM class problem would have double the parameters of using NUM class ( sigmoid ) lr . what is bothering me is why is it not f-(k-1) parameters for multiclass lr ? EOA 
 number of parameters in multi class and two class logistic regression : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012number of parameters in multi class and two class logistic regression&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;theinfelicitousdandysorry, this has been archived and can no longer be voted onEOQ actually it is f-(k-1) wiki EOA 
 number of parameters in multi class and two class logistic regression : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012number of parameters in multi class and two class logistic regression&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;theinfelicitousdandysorry, this has been archived and can no longer be voted onEOQ thanks . i feel like i have never seen this implemented though , is it common just to use f-k parameters in implementation ? also is it k-1 for any time using softmax , such as in a multilayer neural net ? edit : think i found the answer here URL EOA 
 propensity scores : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123propensity scores&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;whiteraven4sorry, this has been archived and can no longer be voted onEOQ no idea if this is the right place to ask this . if it isn't , please let me know where i should ask . i'm trying to turn the results of classifiers ( specially j48 in weka ) into propensity scores . i've read about it in a few papers , but i can't find any explanation for the correct way to do it . i'm thinking of just using the probability of being in the yes class in each node as the propensity score , but i have no idea if that's the right way . thanks. EOA 
 question about svm solver algorithms , in matlab specifically ( x post from machinelearning post faux-pas ) : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012question about svm solver algorithms , in matlab specifically (x post from machinelearning post faux-pas)&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;keving81sorry, this has been archived and can no longer be voted onEOQ hey everyone . so, this is my first foray into svm and i've been studying learning with kernels while trying to implement svm on some of my own data . i have some questions about some of the matlab functions used to fit the models and was hoping someone could provide some insight . i'm using fitcsvm for a two-class problem . i'm using a polynomial kernel , am trying to optimize the parameters for boxconstraint and kernelscale simultaneously , and am evaluating the generalizability of the model using leave one out cross validation . ( note: in reality this is a biological problem and i want to see how well the firing rates of ensembles of neurons differentiate certain behavioral states , so my terminology might seem off as some of it reflects more so the question and perhaps less so the standard nomenclature used in the machine learning world ) . the default solver for matlab's fitcsvm() is smo . this seems to work fine , but there's also the option of using the l1qp solver , which uses a quadratic programming algorithm to implement l1 soft-margin minimization . when i use l1qp with parallel enabled this uses about NUM of my cores ( i'm using a NUM core machine ) and takes about twice as long as the smo algorithm ; this is less than desirable because my optimization already takes a few hours with smo . furthermore the minimum found with l1qp isn't necessarily better than what's found with smo , but i haven't been able to rigorously test this . so, my question is : what are the benefits of using l1qp ? a second question is : what is matlab doing to the kernel via the kernelscale argument pair ? in the documentation it says it divides the kernel by the kernel scale elementwise and then applies the 'appropriate kernel norm' . it's this last part that i'm a little confused by-are they normalizing each column/row vector to unit length ? fitcsvm doesn't return the kernel so i can't actually look at this myself . EOA 
 predicting when a resource will run out ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012predicting when a resource will run out?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;flipper3sorry, this has been archived and can no longer be voted onEOQ i am trying to figure out what approach would be best for predicting when a resource will run out or be low and needs to be repleneished ? for example , let's say i have a salt shaker at a restaurant and am able to tell the amount of salt left in the shaker ( by weight , mass, count , or whatever means is best ) . how could i use this data shown over time to predict when it will be empty and will need to be filled back up ? i need to be able to see that the shaker may get used more on weekends , or maybe during summer months , so taking time into account is pretty important . i have been thinking about using a neural network with cos(time) or sin(time) as a time input variable , but i feel stumped as in my machine learning class two semesters ago we covered identification problems more than these kinds of problems . EOA 
 predicting when a resource will run out ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012predicting when a resource will run out?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;flipper3sorry, this has been archived and can no longer be voted onEOQ why would you need a neural network for this ? this can be easily modeled with a linear equation based on the number of customer , then a poisson or gamma distribution can be used to represent the use of the salt shaker . EOA 
 predicting customer purchasing behavior : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012predicting customer purchasing behavior&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;meeplsorry, this has been archived and can no longer be voted onEOQ i work at a dating site and i have a ton of data at my disposal . we want to be able to predict if a user will purchase or not , and for users that are likely to not purchase , offer them a free trial . i am pretty well-versed in python ( pandas , matplotlib, etc. ) and working with large datasets , but not so much with ml . where do i start ? EOA 
 predicting customer purchasing behavior : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012predicting customer purchasing behavior&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;meeplsorry, this has been archived and can no longer be voted onEOQ perhaps with coursera ml course ( link to reddit ml thread ) EOA 
 random forests : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012random forests&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;noobmlnoobsorry, this has been archived and can no longer be voted onEOQ i'm a machine learning newbie . to learn something in ml , i have implemented a random forest ( scikit ) based classifier for activity prediction based on accelerometer data . i.e the classifier predicts states like standing , running, etc based on tri-axial accelerometer output . the rf classifier performs reasonably well in predicting state , however, i would like to add something akin to a feedback loop to it . what i mean is that , if the classifier misclassified and i hand correct it . for the next run , on same dataset , there shouldn't be a misclassification . i'm using scikit . any pointers ? EOA 
 random forests : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012random forests&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;noobmlnoobsorry, this has been archived and can no longer be voted onEOQ with rf you should simply include that sample with correct ( by hand ) prediction in your training set and then retrain your classifier . alternative is to use some online learning alghoritam . EOA 
 random forests : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012random forests&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;noobmlnoobsorry, this has been archived and can no longer be voted onEOQ thanks for the replies . EOA 
 why does ufldl consider-1 to be an inactive tanh hidden neuron output ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012why does ufldl consider-1 to be an inactive tanh hidden neuron output?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eubarchsorry, this has been archived and can no longer be voted onEOQ from the stanford ufldl tutorial : informally , we will think of a neuron as being “active” ( or as “firing” ) if its output value is close to NUM , or as being “inactive” if its output value is close to NUM . we would like to constrain the neurons to be inactive most of the time. this discussion assumes a sigmoid activation function . if you are using a tanh activation function , then we think of a neuron as being inactive when it outputs values close to-1 . tanh is symmetric about the x-axis . if you flip the signs of the incident weights then an inactive neuron is effectively active again . this piece of advice s even repeated in the older version of their tutorial . is it a mistake or is there a good reason that an output of-1 is somehow less active than NUM ? EOA 
 why does ufldl consider-1 to be an inactive tanh hidden neuron output ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012why does ufldl consider-1 to be an inactive tanh hidden neuron output?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eubarchsorry, this has been archived and can no longer be voted onEOQ sigmoids were originally devised as continuous versions of binary threshold neurons . then people started using tanh because it's faster than sigmoids and has the same shape . it's just a sigmoid with a bias of-0.5, multiplied by NUM , basically. so essentially they are still thinking of tanh's as approximations of binary threshold neurons . however it's not even true for binary neurons . nns don't care whether a neuron is active or inactive . they can just multiply by a negative weight and adjust the bias , and a zero becomes NUM and a one becomes NUM . EOA 
 why does ufldl consider-1 to be an inactive tanh hidden neuron output ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012why does ufldl consider-1 to be an inactive tanh hidden neuron output?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eubarchsorry, this has been archived and can no longer be voted onEOQ i should have clarified-the article is referencing sparse autoencoders and not vanilla ffnns . saes can be trained with gradient calculations that explicitly encourage patterns of neuron activity , and the last layer of a trained sae is frequently discarded in favor of using the hidden layer activations as sparse features . EOA 
 why does ufldl consider-1 to be an inactive tanh hidden neuron output ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012why does ufldl consider-1 to be an inactive tanh hidden neuron output?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eubarchsorry, this has been archived and can no longer be voted onEOQ i'm not very familiar with sparse autoencoders , but perhaps forcing all the neurons to be mostly the same value (-1 or NUM ) might work to regularize the network . however what i said is still true , nns don't care what range the inputs are in , they can easily invert them with negative weights and biases . so choosing NUM as the inactive state would be just as effective . EOA 
 multiple kernel learning-regression : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012multiple kernel learning-regression&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;toeflspeakingsorry, this has been archived and can no longer be voted onEOQ mkls are widely used for classification , is there a software / tool for mkl regression ? i've already known shogun toolbox , smo-mkl, spg-gmkl , simple mkl .. EOA 
 difference between implementation and research : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123difference between implementation and research&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;drakenkoransorry, this has been archived and can no longer be voted onEOQ greetings all , i'm planning on applying to my university's phd program in the coming months with a focus on data mining . i've talked with various lecturers about possible topics and , given my interest in algorithm development , they have all suggested designing/improving on existing algorithms . now , while i understand the development concept , i'm at a loss as to what constitutes the difference between the implementation of an algorithm ( with a performance improvement ) and the research element inherent in a phd . i get that in order to improve an algorithm it takes research , but i feel that it's still fundamentally an implementation problem , not a research problem . am i confusing myself with semantics ? or is there some fundamental difference between the two that i'm missing ? EOA 
 [baum-welch algorithm for hidden markov models] : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012[baum-welch algorithm for hidden markov models]&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eusocialmachinesorry, this has been archived and can no longer be voted onEOQ i was going over the baum-welch algorithm for updating and ran into a puzzling question about the new values for the symbol distribution-basically that , unless every observation contains every symbol in the model , it looks like the algorithm will update the missing symbol probabilities to zero for all states . i'm probably just missing something incredibly simple here , but i'd really appreciate it if someone would point it out to me . for the sake of a common notation , here's the wiki . any updated probability of symbol o in state i [b i(o)] can be expressed as the sum of a disjoint subset of the gammas for that observation and state , so the sum overall the updated values of b i ( o ) will always be equal to one . but unless every symbol in b is in every observation the model updates on , at least one of the symbols won't have a gamma that corresponds to it for any state , so it'll get updated to zero across all states ( the numerator for b-i (k ) is zero) . assuming that the model updates on a sequence like that ( say 'a,b', where each state has a distribution over 'a' , 'b', and 'c' ) , then it can't update on a new sequence like 'b,c' with the discounted symbol in it . it'll also estimate the probability of such a sequence as zero . after updating on 'a,b', the probability of seeing c in any state is zero , which makes the the alpha/beta estimates corresponding to it also zero for all states . the denominator for gamma is the sum over states i , of [ alpha i (t )-beta i (t)] . all of those are zero , so the divisor is zero . just estimating the probability of 'b,c' runs into a similar problem-the probability of seeing c is zero ; which takes the probability of an observation with c in it to zero . hmms don't just work on permutations of their symbol sets , so i'm clearly missing something . could someone please explain what ? EOA 
 [baum-welch algorithm for hidden markov models] : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012[baum-welch algorithm for hidden markov models]&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eusocialmachinesorry, this has been archived and can no longer be voted onEOQ you should have non-zero initial emission probabilities for all symbols . this way , within each observation , the probability of the hidden states for that observation are used with the probability of observing that sequence in the first place . when your pseudo probabilities are later normalized , the unobserved symbols and states will remain non-zero . EOA 
 [baum-welch algorithm for hidden markov models] : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012[baum-welch algorithm for hidden markov models]&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eusocialmachinesorry, this has been archived and can no longer be voted onEOQ following the wp terminology : probability of sequence and state is s1 then s2 interpret this as probability of ( sequence )-( state is s1 then s2 ) EOA 
 [baum-welch algorithm for hidden markov models] : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012[baum-welch algorithm for hidden markov models]&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;eusocialmachinesorry, this has been archived and can no longer be voted onEOQ thanks for the response ! i'm a little unclear on the terminology used in the example section . everything else i've heard or read has been in terms of a lattice diagram and the bw algorithm . would you mind explaining in those terms ? EOA 
 noob question , how to deal with unequal amount of data for each class , using deep networks : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123noob question , how to deal with unequal amount of data for each class , using deep networks&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;beijingchinasorry, this has been archived and can no longer be voted onEOQ hi i am trying to do classification using a cnn , but the amount of training data i have varies for each class . what is the correct method in order to deal with this ? EOA 
 noob question , how to deal with unequal amount of data for each class , using deep networks : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123noob question , how to deal with unequal amount of data for each class , using deep networks&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;beijingchinasorry, this has been archived and can no longer be voted onEOQ i don't think it matters . it could possibly learn incorrect prior probabilities , and possibly affect performance on cases where it is very uncertain . try weighing the cases of the classes you want more of , more. i.e. putting multiple copies of them in the training set or multiplying the error so those cases matter more . EOA 
 noob question , how to deal with unequal amount of data for each class , using deep networks : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123noob question , how to deal with unequal amount of data for each class , using deep networks&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;beijingchinasorry, this has been archived and can no longer be voted onEOQ there is no one easy way to do this and it's in no way specific to cnns . first , are you classes imbalanced only in your training data with respect to the underlying population ? or is the underlying distribution imbalanced in the same way ? having a handle on the class distribution is very helpful here . if you google for things like class imbalance or covariate shift you may get some ideas . i've tested this briefly : URL but it didn't appear to have much effect . EOA 
 noob question , how to deal with unequal amount of data for each class , using deep networks : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123noob question , how to deal with unequal amount of data for each class , using deep networks&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;beijingchinasorry, this has been archived and can no longer be voted onEOQ thanks for your reply . i'll take some time to read up on this . in my case , i think the availability of the data is causing the imbalance rather than the distribution itself being that way . EOA 
 question regarding design of a cnn : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012question regarding design of a cnn&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ hi i want to predict emotions in faces using cnn's . i downloaded a canned architecture , rigged it for output as the basic emotions , and ran through my emotion data . however a strange thing is , even when i give it a non-face image it predicts something . this is to be expected as i never told it to expliclty look for a face first , it expects all its inputs to be faces . so, how can i have a system where it predicts emotions only if it is looking at a face ? what i could think of is to add an extra non face category to the prediction , and put in non-face data with the labels as this . but this seems a little daunting as i expect i shall have to provide it a lot of non-face images for it to understand to classify only faces . am i correct in assuming this ? also can i somehow use the krizhevsky-net as the starting point for this ? would appreciate your suggestions EOA 
 searching for a suitable distance measure . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012searching for a suitable distance measure.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ hi i am looking for a distance measure that looks at correlating increases and decreases of NUM vectors . i.e. suppose i am comparing vectors with a template vector [ NUM ,2,1 ] in this case l2 norm will show [ NUM ,1.5,2 ] and [ NUM ,1.5,0 ] as being at the same distance from [ NUM ,2,1 ] however [ NUM ,2,1 ] increases and then decreases , and so is closer to [ NUM ,1.5,0 ] than to [1,1.5,2]. could you please suggest a suitable distance metric for this ? also , would kl divergence or cosine distance ( of unit vectors ) be of use , and what is the difference in the properties of either ? thanks EOA 
 stochastic variational inference question : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012stochastic variational inference question&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;calcworkssorry, this has been archived and can no longer be voted onEOQ i'm trying to work my way through the hoffman , blei, wang , paisley paper stochastic variational inference ( URL ) and have gotten stuck . can anyone give me a hint on how to derive equation ( NUM ) of the paper ? thanks ! EOA 
 dropping out for convolutional rbms and convolutional autoencoders : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012dropping out for convolutional rbms and convolutional autoencoders&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ if i implement dropout in convolutional rbms or convolutional autoencoders , do i dropout entire bases ( i.e. consider say only NUM out of NUM kernels for a minibatch update ) , or do i dropout individual hidden units from the kernel activations . ( i.e. i use all NUM kernels , but within each kernel's own hidden layer i dropout half of units ) . EOA 
 dropping out for convolutional rbms and convolutional autoencoders : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012dropping out for convolutional rbms and convolutional autoencoders&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ the latter is the more common approach . but, experiment and see what works better for your problem . EOA 
 dropping out for convolutional rbms and convolutional autoencoders : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012dropping out for convolutional rbms and convolutional autoencoders&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ i have a few more questions : NUM . for single layer networks like autoencoders with weight tying/rbms how does the weight scaling at test time work ? assuming dropout probability of NUM , then i understood the original paper as prescribing scaling the outgoing weights by a factor of NUM , so for the multilayer nets the 'upward' weights from a hidden layer are NUM of the 'downward' weights ( weights coming down from the next hidden layer above it ) ? is this applicable to single layer nets as well , i.e. the encoder weights are NUM of the decoder weights NUM . theoretically, between the NUM options ( of dropping out bases or their units ) , would the way the weights have to modified(scaled) at test time remain the same ? NUM . i have tried out the NUM st option ( dropping bases ) on conv-autoencoders , and albeit not letting it run for many epochs , i think i am in general not getting very good reconstruction error . would you say dropout is a good option for reconstruction error costs ? EOA 
 dropping out for convolutional rbms and convolutional autoencoders : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012dropping out for convolutional rbms and convolutional autoencoders&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ thanks for your comments thus far , i have tons of questions that i'll keep posting , i understand if you choose not to answer many of them . with regards to point NUM , i take this as saying that convolution-autoencoder is not a good fit itself so we can't really say that it is dropout that is messing things up . i learnt a vanilla convolutional-ae on mnist with a very small set ( only NUM ) of NUM x15 bases . i found that the reconstruction error went down URL ( reconstruction at epoch NUM ) URL ( reconstruction at end ) with the output being quite clean , but the bases were uninteresting themselves . in fact at around epoch NUM , it seemed that the bases seemed to be going towards some discernible shape , URL but then this disappeared . URL currently i am trying to understand : NUM how is it that such plain filters ( whose size is NUM /4 th of the image , so aren't really small compared to the structures in the image ) are being preferred . is this a known phenomenon for ae's or conv-aes ? when you said that you've had a hard time with getting ae and convolution to work together , could you elaborate on that a bit ? is there some better dataset to understand this , maybe mnist is too simplistic for this , but then what would be good benchmark ? EOA 
 how to get data from a site ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how to get data from a site?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ i don't know if this is the right place to post this , so please correct me if i am wrong . id like to make a program that can search for recipes by the name of food item . there is a big repository of recipes at URL i have never worked with getting data from the net , but from what i understand people generally use api provided by the sites ? so how can i search for recipes from this site automatically , do i need them to provide some usable api , or can is there some all-purpose program people use to automate such tasks ? EOA 
 how to get data from a site ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how to get data from a site?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ when there is no api i normally use c#/f#/python with their standard http client libs for downloading and some regex to extract urls . you might have to play with some http headers / cookies to get sites to talk to you . sometimes selenium works well if some website tries to prevent automatic web scraping .. but it's much slower . EOA 
 how to get data from a site ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how to get data from a site?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ most likely too late for you , but it could be useful to others : udacity offers a self-paced course on mongodb . lesson NUM is on screen scraping , with example code in python-beautifulsoup library . there is no mongodb in this lesson . EOA 
 how to get data from a site ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }234how to get data from a site?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;t.hanksorry, this has been archived and can no longer be voted onEOQ thanks i'll look it through EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ i'm working on a course project to classify tweets as either interesting or not interesting . i have hand labeled about NUM tweets , and ended up with a ratio of about NUM :10 ( interesting : not interesting ) . i downsampled the not interesting tweets and got NUM /200 distribution . i use k-fold cross validation on this set and get very good performance with a naive bayes classifier . however, i feel it is a terrible waste to discard NUM of the not-interesting tweets . so the question : could i still train my classifier using the imbalanced data set , as long as my validation set is balanced ? or am i overlooking some issues with this ? EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ there's a lot of literature on imbalanced data sets . first and foremost , what performance metric are you considering ? you'll likely want to be considering area under the roc or precision or recall rather than accuracy ( you could have a NUM % accurate classifier trivially ) . the biggest issue with what you're proposing is that the class balance you're enforcing during training is not likely to persist when applied to a real stream of tweets . i'd be interested to know what happens if you split your data into two even sets with stratified classes ( i.e. NUM /900 in two sets ) , train on a NUM /100 set and then apply your classifier to the other full NUM /900 set . EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ could you explain a bit about using the area under the roc as the objective. also in this case it would be more important to not reject 'truly interesting' , rather than falsely accept 'uninteresting' into the interesting category . can the roc-metric be used in a manner to impose this condition ? EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ sure . you're actually utilizing area under the roc ( auc ) as a measure of the classifier's rank-order . peter flach gave a good talk about this . check out slide NUM of this presentation . URL in this form , auc is the probability that a randomly chosen positive class instance is ranked above a randomly chosen negative class instance . the naive bayes classifier can be used to produce a probability for each instance . the op can then use the probability and true class to find auc . auc can indeed be used to understand relative performances in the manner you articulated . EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ thank you for your reply . i'm afraid i was never at ease with auc concepts , so i was wondering if you would suggest a video resource to get up to speed on this ? i found peter flach @videolectures.net URL would you recommend it as a good start point , or something else ? EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ that's a great find ! good video . EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ a bit of a late reply , i've been busy with other projects . thanks a lot already for pointing me in the right direction ( especially with the linked presentation in the child comments ) . i am indeed using area under roc curve ( your post made me research it a lot more ) as my primary performance metric . here are my results for varying splits : training set : NUM /900, testing set : NUM /900-> ; mean roc NUM 8 training set : NUM /900, testing set : NUM /100-> ; mean roc NUM 7 training set : NUM /100, testing set : NUM /100-> ; mean roc NUM 0 training set : NUM /100, testing set : NUM /900-> ; mean roc NUM 1 there's no significant difference in performance between ratios of the testing set . i guess that i can take the NUM /900 tests as more accurate tho , due to their size , and they are in the end a better representation of real twitter data . thanks again , and if you have any other input , i'm all ears ! EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ nicely done . i have a couple other ideas , but if you want to stick with naive bayes you might get a performance gain if you try creating NUM training sets ( each combination of NUM /100 ) , train a classifier on each , and then vote their predictions on the testing set . EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ i'm still in progress of getting more data ( only have around NUM positive cases by now ) . once i got more , i'll definitely try this out , thanks ! EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ after getting some more data , i tried out your proposal of combining naive bayes and voting on the label , and got an increase of the auc.roc of around NUM , making my current mean auc.roc around NUM 6-0.88 ( from cross-validatoin ) . final test tried on my test set of NUM /1600 , and got these stats : neg recall-NUM 6 pos recall-NUM 5 pos precision-NUM 3 neg precision-NUM 7 roc.auc-NUM 5 although , from what i read , the recall/precision rates aren't that informative for imbalanced data . i did experiment with some other classifiers ( max entropy , linear and non-linear svm ) , and feel my features are fairly optimized , but can't seem to get better results . so , if you might have another suggestion , i'll be more than happy to try it out . EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ another suggestion might be to try bagging decision trees or random forest . i've had good luck with those methods producing good rank-orders in the past . EOA 
 dealing with imbalanced data set : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123dealing with imbalanced data set&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;dnawsorry, this has been archived and can no longer be voted onEOQ if you know the operating context ( i.e. the class imbalance when you are deploying the model ) then you can use roc analysis to select an optimal classifier on the convex hull . the isometric for accuracy will be a line with a NUM degree angle for balanced classes and will change for unbalanced classes . ( steeper or shallower ) . the auc gives a good measure if you do not know the operating context the model would be deployed in , but bare in mind some areas under the curve might not be realistic . in particular if you are doing an information retrieval type task ( sounds like it ) then you will be more interested in having a steep accent on the left hand side of the roc curve than the auc score . this is a good paper : URL EOA 
 how can you predict the next frame of video ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how can you predict the next frame of video?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;csp256sorry, this has been archived and can no longer be voted onEOQ lets say i have a very large series of sequential images . the images are all broadly similar , and sequential images are particularly similar . there exists some inscrutable rule that creates each image from the previous few images . i want to create a program that can approximate that rule . i want to feed my program the last few sequential images , and have it guess the next . how would i go about this ? as a novice i have been reading around trying to puzzle things out for myself , but i feel i understand less the more i read . EOA 
 how can you predict the next frame of video ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how can you predict the next frame of video?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;csp256sorry, this has been archived and can no longer be voted onEOQ this talk has a demonstration of deep gaussian processes doing exactly this . it's a very hard task though . EOA 
 how can you predict the next frame of video ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how can you predict the next frame of video?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;csp256sorry, this has been archived and can no longer be voted onEOQ thanks for the link . i watched the whole talk and am impressed he got good results with so few frames . at NUM :20 he shows how additional data points constrain the regions between them . the interpolating line beyond the last data points return to zero even though all available data looks like a downward opening quadratic . this says to me that his model is good at interpolation , but not good at extrapolation . i understand that extrapolation is very difficult without some model to govern the evolution of the system . but i know that my system has such a model , and i would in some way like to learn that model and use it to extrapolate from available data into the future . i have a more detailed question to ask , but i am still trying to make sure that i am conceptualizing all the parts correctly . until then let me simply ask : what about conditional rbm's ? could something like this be useful : URL i assume that everything i am talking about is terribly difficult , which is perfectly fine for my purposes . all i need to do is find an approach that is reasonable . i am sorry for not being more explicit in my problem specification . i usually scoff at people paranoid about others stealing their idea but this is one case where i actually can't talk about it in detail . thank you again . EOA 
 how can you predict the next frame of video ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how can you predict the next frame of video?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;csp256sorry, this has been archived and can no longer be voted onEOQ to be honest , i think the problem is so difficult i wouldn't worry about that . there are already lots of people working on it already anyway . well , i'm no expert on deep learning , but there's matlab code available for the deep gp's here:URL it's not my work though . so i can't really help you . all i will say is that integrating prior knowledge of the model of the system might actually be harder . and in deep learning methods , very complicated depending on the nature of the prior knowledge . EOA 
 how can you predict the next frame of video ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how can you predict the next frame of video?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;csp256sorry, this has been archived and can no longer be voted onEOQ i laboured through sutskever's learning multilevel distributed representations for high-dimensional sequences , which was concerned with learning NUM x20 videos of just two bouncing balls , which took NUM hrs to train at that time . since then they have shown that tricks like nesterov's look ahead gradient are indispensable to speed up training of neural nets . i think the factored conditional rbm is also around the same time , so maybe some more up to date work involving the newer techniques for sequence modelling would be more suitable ? EOA 
 how can you predict the next frame of video ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how can you predict the next frame of video?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;csp256sorry, this has been archived and can no longer be voted onEOQ thanks for the reply . i was looking for that bouncing ball paper ! those times are for a cpu ? single machine ? if you know what that up-to-date work is , please send me a link . i am quite new and am trying to soak up as much as i can . it seems to me that i might be well served to just read everything that has come out of stanford or the university of toronto in the last NUM years . EOA 
 how can you predict the next frame of video ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how can you predict the next frame of video?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;csp256sorry, this has been archived and can no longer be voted onEOQ i gave up on considering rnns for my research long time back so i am not updated on this , sorry. all i can really suggest is , have a look the papers citing these and pick up the most cited one of those . i hope current work has come up to working with real world data ( not synthetic ones like bouncing balls ) yet . i guess the course of action will also depend on whether you are looking at these models as somewhat black-boxes to apply , or looking to explore the abilities of the model itself . in the former , maybe consider simpler approaches ( hmm-ish? ) sorry lots of maybe's from me . however, i'd ask you a favor , if you do decide upon a reading list for yourself , please let us also know what papers you found interesting/well explained . thanks EOA 
 recommended way to convert a set of probability distributions into a feature vector ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012recommended way to convert a set of probability distributions into a feature vector?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;chemmklsorry, this has been archived and can no longer be voted onEOQ i splitted a number of short texts into its sentences and ran stanford's corenlp sentiment analysis on each one . this discards the words and works with the pos trees only . as a result , for each text now i have a number of probability distributions of being in one out of five possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the texts . my first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just populate it with the values of the probability distributions . does this make sense ? wouldn't i be somehow losing the representativity of the sentiment magnitude ? thanks ! EOA 
 recommended way to convert a set of probability distributions into a feature vector ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012recommended way to convert a set of probability distributions into a feature vector?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;chemmklsorry, this has been archived and can no longer be voted onEOQ i splitted a number of short texts into its sentences and ran stanford's corenlp sentiment analysis on each one . this discards the words and works with the pos trees only . as a result , for each text now i have a number of probability distributions of being in one out of five possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the texts . in addition to what you suggest , it might make sense to also include the mean as a feature . my first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just populate it with the values of the probability distributions . does this make sense ? wouldn't i be somehow losing the representativity of the sentiment magnitude ? do you mean representing the probabilities as a vector (i.e. [0.1, NUM , NUM , NUM , NUM ]) fails to take into account the fact that the scores are ordered-that NUM and NUM are more similar than NUM and NUM ? while this is a valid concern , i don't think that it will be a big deal if you have enough data . it is common to put numerical features into buckets when using linear models . this has the same issue but it works well in practice. EOA 
 recommended way to convert a set of probability distributions into a feature vector ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012recommended way to convert a set of probability distributions into a feature vector?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;chemmklsorry, this has been archived and can no longer be voted onEOQ suppose that you have k buckets x.1 , x.2, x.3 , ..., x.k and have associated weights w.1 , w.2, ..., w.k . in a bayesian framework you model the uncertainty in w , so you can make the assumption that adjacent buckets should have similar parameters explicit by assuming that : w[i-NUM ] ~ normal(mean-w[i] , variance-NUM ) EOA 
 would anyone be willing to look over my simple nn octave code ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001would anyone be willing to look over my simple nn octave code?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;jumpy89sorry, this has been archived and can no longer be voted onEOQ i'm currently taking the stanford ml course on coursera and have been more or less breezing through all the programming assignments . this includes neural networks and the back-propagation algorithm . however, i tried re-creating it on my own from memory and i just can't get it to work right . first i tried in java , then in octave , and i'm getting similar results-a bit of learning at first and then it levels off at a low success percentage . obviously something is wrong ( probably in my back-propagation algorithm ) , but i've been over and over it and can't figure out what . there are no errors when i run the code , but i'm thinking it has to be something obvious that i'm overlooking . would anyone be willing to take a look at some heavily commented octave code to see if anything looks wrong ? code is here . EOA 
 would anyone be willing to look over my simple nn octave code ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001would anyone be willing to look over my simple nn octave code?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;jumpy89sorry, this has been archived and can no longer be voted onEOQ if you still need to fix it , have a look at : URL .optimization EOA 
 problem: simulate a mechanical device that outputs numbers from NUM-100 . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012problem : simulate a mechanical device that outputs numbers from NUM-100.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;deutschluz82sorry, this has been archived and can no longer be voted onEOQ so i ve recently gotten into statistical modeling and this problem popped into my head . so here s the rest of the setup : the device outputs a list like : NUM ,34,65,88, NUM , NUM without replacements and no repetition so according to my readings this can be thought of as the classical ball and urn model and so the whole theory of hidden markov models can be used to develop a solution to this problem . so what i'd like is reassurance that yes i m on the right track with a solution to this and also what software should i use to actually try and solve this . EOA 
 problem: simulate a mechanical device that outputs numbers from NUM-100 . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012problem : simulate a mechanical device that outputs numbers from NUM-100.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;deutschluz82sorry, this has been archived and can no longer be voted onEOQ when i first read your problem i thought of hidden markov models . another approach may be recurrent neural networks . EOA 
 problem: simulate a mechanical device that outputs numbers from NUM-100 . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012problem : simulate a mechanical device that outputs numbers from NUM-100.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;deutschluz82sorry, this has been archived and can no longer be voted onEOQ thanks for the reply . so i m not full of sh-t...great thats reassuring . as to your suggestion...recurrent neural networks ? hmm...i don't know what those are but thats just a wikipedia page away . thanks. EOA 
 common mistakes for beginners ? my ann is converging poorly . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012common mistakes for beginners ? my ann is converging poorly.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jumpy89sorry, this has been archived and can no longer be voted onEOQ i'm getting into machine learning , and tried implementing a neural net in java based off of this online book . i've tested two different training scenarios-one was simply adding two numbers between NUM and NUM together , and the other was recognizing handwritten digits from the mnist data set . the problem is convergence is slow and tops out at a fairly low level ( ~70-80% correct answers for addition and NUM-35% for mnist data ) . it's obviously working somewhat because i'm getting more correct answers than i would by chance alone , but i feel like i should be getting a lot more . the addition problem is easy and the book shows a python implementation getting over NUM % correct after a single training epoch . varying learning rate and size/number of hidden layers hasn't helped much ( also i feel like i need to have a pretty high learning rate for things to get anywhere , like NUM-10 ) . i've been over and over my backpropagation algorithm many times and can't find any mistakes . is there anything obvious i may have missed or should recheck ? any help would be much appreciated . EOA 
 common mistakes for beginners ? my ann is converging poorly . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012common mistakes for beginners ? my ann is converging poorly.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jumpy89sorry, this has been archived and can no longer be voted onEOQ you may want to try the ufldl tutorial's technique for checking your feed-forward network code : URL EOA 
 common mistakes for beginners ? my ann is converging poorly . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012common mistakes for beginners ? my ann is converging poorly.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jumpy89sorry, this has been archived and can no longer be voted onEOQ you should be getting a lot more . if you are doing the same number of iterations as that online book , you should be getting the same results . for a similar architecture , you can get up to NUM % accuracy on mnist ( eg URL ) . you clearly have a bug . i would write unit tests for the pieces until you find it . good luck ! EOA 
 what class or resource do you recommend after coursera stanford ml course . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }345what class or resource do you recommend after coursera stanford ml course.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;naturallybrewedsorry, this has been archived and can no longer be voted onEOQ the main question is in the title . for the interest of keeping the post generic it's really all you need read or address . it would help if you said why recommend and what background you recommend having before starting . below the line i give some personal background . if you read this and want to offer suggestions specific to me that would be great ! i'll be completing stanford's ml learning course through coursera in the next couple weeks . i'm curious what would be the next logical thing for me to learn to deepen my knowledge of the subject . here is a break down of my background : bs in cs by the end of the year proficient in python familiar with ruby , octave, r , python-pandas, python numpy , c, c-math : linear algebra and calc NUM . here is a list of online courses i'm familiar with : p.s, i'm sure this question has been addressed somewhere else , feel free to re-direct me . i'll also keep searching . thanks for the help EOA 
 what class or resource do you recommend after coursera stanford ml course . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }345what class or resource do you recommend after coursera stanford ml course.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;naturallybrewedsorry, this has been archived and can no longer be voted onEOQ the cmu course was my favorite EOA 
 what class or resource do you recommend after coursera stanford ml course . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }345what class or resource do you recommend after coursera stanford ml course.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;naturallybrewedsorry, this has been archived and can no longer be voted onEOQ i think the unsupervised feature learning and deep learning tutorial is a good follow up . EOA 
 study note automation : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001study note automation&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;ciceronemsorry, this has been archived and can no longer be voted onEOQ hi everybody . should start out by saying that i am new to ml . i have a strong background in stats and calculus , but none in programming ( i'll be taking the coursera class in programming this june ) . for a while now , i've had an idea for a program thats been bouncing around in my head . specifically, i want to create a program that takes text ( likely in .pdf or .doc formats ) and convert it into a series of questions and answers . for example , given the following : text-the capital of canada is ottawa . converted into q & a format-q : what is the capital of canada ? a: what capital of canada is ottawa . any suggestions for starting this would be greatly appreciated . i know there are a lot of courses out there , but if someone could narrow down what areas i should focus on , then i could take a targeted approach to learning about what i need to know . thanks ! EOA 
 study note automation : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001study note automation&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;ciceronemsorry, this has been archived and can no longer be voted onEOQ two quick comments . first , you would probably want to deal with raw text rather than proprietary formats . there are tools to extract text from doc and pdf files , but they are error prone . second , this isn't so much a machine learning task as it is a syntax task . see any good nlp/linguistics/syntax textbook on the topic of wh-movement , and you'll see that there's most likely a simple rule-based solution to this problem . EOA 
 study note automation : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001study note automation&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;ciceronemsorry, this has been archived and can no longer be voted onEOQ great ! thank you ! so i imagine this would be a two-stage process ? stage NUM-conversion of material into text , stage NUM-translation of text into questions ? and do you have any recommendations for a solid textbook that covers that area ? EOA 
 study note automation : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }001study note automation&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;ciceronemsorry, this has been archived and can no longer be voted onEOQ yes for the two-stage thing . for the textbooks check out allen's and jurafsky's respective textbooks . EOA 
 question about clustering of word vectors ( x-post from /r/machinelearning ) : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012question about clustering of word vectors (x-post from /r/machinelearning)&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;confusedsoconfusedsorry, this has been archived and can no longer be voted onEOQ i just found this subreddit and i thought maybe this might be a good place to get an answer/discussion . i also posted this in /r/machinelearning . i was looking at the word2vec tool which uses deep learning to compute vector representations of words . they've mentioned that-the word vectors can be also used for deriving word classes from huge data sets . this is achieved by performing k-means clustering on top of the word vectors . what would be an application of these word classes ? when i execute the code , it seems that the tool computes word vectors from a dataset and clusters it into NUM classes . what if i wanted to use another method of clustering ? how would i measure how 'good' these classes are , or how well the clustering algorithm is working ? also , it seems to me that the word vector representation would be a very sparse vector space . couldn't the clustering be improved by pca or some method of dimension reduction ? again, how can i test how well the clustering is performing ? if i can figure out a way to evaluate , i can change the code and add pca and see if it does better . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ i've created a neural network , with the following structure : input1-input2-input layer . n0-n1-hidden layer . NUM weights per node ( one for bias ) . n2-output layer . NUM weights ( one for bias ) . i am trying to train it the xor function with the following test data : NUM NUM -desired result : NUM NUM NUM -desired result : NUM NUM NUM -desired result : NUM NUM NUM -desired result : NUM after training , the mean square error of test ( when looking for a NUM result ) {0 , NUM }-NUM , which is good i presume . however the mean square error of test ( when looking for a NUM result ) {1 , NUM }-NUM , which surely needs to be zero ? during the train stage i notice the mse of true results drops to zero very quickly , whereas mse of false results lingers around NUM . i'm using back propagation to train the network , with a sigmoid function . the issue is that when i test any combination after the training , i get a ouput result of NUM 00014 for true , and NUM 00104 for false . i'm trying to get NUM for true , NUM for false . the network seems to learn very fast , even with an extremely small learning rate . if it helps , here is the weights that are produced , with a learning rate of NUM : n0-w0-0.999, n0-w1-NUM 55, n0-w2-NUM 04 ( bias weight )-hidden layer n1-w0-NUM 74, n1-w1-0.893, n1-w2-NUM 16 ( bias weight )-hidden layer n2-w0-NUM 35, n2-w1-NUM 42, n3-w2-NUM 43 ( bias weight )-output node back propagation steps : feed forward a feature set , summing the weights x input set . calculating sigmoid per node . apply bias . calculate output node error , desired output-actual output ( sigmoid ) . back propagate error , layer above error x connecting weight . per node . adjust weights , repeat till mse low enough . apologies for the long post , but i've been scratching my header over this for awhile now , and i can't determine what is wrong with my back propagation algorithm . thanks in advance . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ can you show me your code ? its unclear what order you're doing some of the steps in the feed-forward process . and also you need to backpropogate through the whole network . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ sure , this is the main neural net code-URL , thanks for your help . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ the bias is just another input to the neuron . don't treat it special ( besides always getting an input value-NUM ) . also , does your code work for the or and and cases ? EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ i've yet to test this , i assume it will have only been taught for xor ? i will try this out and let you know . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ correct , only xor EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ i just ran xor on a NUM-2-1 nn and got these weights h1 h2 w0 NUM 933 NUM 091 //bias weight w1 NUM 253 -4.3301 //weight to input NUM w2 -0.6427 -5.3821 //weight to input NUM o1 w3 -10.3734 //bias weight w4 -5.2024 //weight to h1 w5 NUM 029 //weight to h2 throw these numbers into your code to see if your forward propagation code works . results that i get using sigmoid units are : NUM 097 // result for NUM NUM NUM 922 // result for NUM NUM NUM 871 // result for NUM NUM NUM 136 // result for NUM NUM EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ do you mean manually define the weights that you've used ? then run the tests ? i will try that soon , thanks for your help . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ instead of setting random initial values , put those values in , and see what happens to your nn . i don't know what end criteria you are using , but your nn should not go from a correct answer to an incorrect answer . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ ok , if i use those weights and just skip my training all together , testing NUM , NUM gives me NUM 4 and testing NUM , NUM gives me NUM 4. this is my feed forward function URL-can you see anything wrong ? EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ my final output sigmoid is outputting NUM . because the net input to that node is-5.077966, the exp of that being NUM . do i need to normalize this or something ? EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ sigmoid function the equation should look something like this output-NUM / (1.0-exp(-1.0-sum)) ; where sum is the inner product of the outputs from the hidden layer , and the weights for the output . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ hmm , this is my sigmoid //calculate logistic sigmoid . outputs[n]-(1.0/1.0-exp(-pnet-> ;layers[nl].neurons[n].input)); EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ the output has to be between NUM and NUM . if it's not , then it's not a sigmoid function you're using . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ hmm , i can't see any error with my sigmoid function though . surely any input to the function would result in an output between NUM and NUM ? EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ correct . it sounds like a programming error . EOA 
 difficulty training simple xor function . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012difficulty training simple xor function.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ my sigmoid appears to be outputting between NUM and NUM . NUM 02 for a true answer and NUM 09 for a false answer .. EOA 
 neural network learning fast , giving me false positives . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123neural network learning fast , giving me false positives.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ i've recently started implementing a feed-forward neural network and i'm using back-propagation as the learning method . i've been using URL as a guide . however , after just the first epoch , my error is NUM . before using the network for my real purpose i've tried with the simple network structure : NUM binary inputs , NUM , NUM , NUM , NUM . NUM hidden layers , NUM neurons each . NUM output neuron , NUM should-valid input . each training epoch runs the test input (1 , NUM , NUM , NUM ), calculates the output error (sigmoid derivative-(1.0-sigmoid)) , back propagates the error and finally adjusts the weights . each neuron's new weight-weight-learning.rate-the neuron's error-the input to the weight . each hidden neuron's error-( sum of all output neuron's error-connected weight )-the neuron's sigmoid derivative . the issue is that my learning rate has to be NUM 001 for me to see any sort of 'progress' between the epochs in terms of lowering the error . in this case , the error starts around ~30.0. any greater learning rate and the error results in NUM after the first pass , and thus results in false positives . also when i try this network with my real data ( a set of NUM audio features from sample-NUM neurons per hidden layer )-i get the same issue . to the point where any noise will trigger a false positive. possibly this could be an input feature issue , but as i'm testing using a high pitch note i can clearly see the raw data differs from a low pitch one . i'm a neural networks newbie , so i'm almost positive the issue is with my network . any help would be greatly appreciated . EOA 
 neural network learning fast , giving me false positives . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123neural network learning fast , giving me false positives.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ i really don't know enough to say for sure but it sounds like you are just overfitting . nns are very good at that . try using a smaller network or some other generalization method . EOA 
 neural network learning fast , giving me false positives . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123neural network learning fast , giving me false positives.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ randomize your initial weights . i've had good results with random numbers between-/-NUM give training sets of positive , and negative examples e.g. all instances that will produce NUM , and all instances that will produce NUM . also , each layer needs a bias weight with a unity input . EOA 
 neural network learning fast , giving me false positives . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123neural network learning fast , giving me false positives.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ thanks for the reply , i am randomising the weights during creation , i also have an extra node that always outputs NUM , on every layer . is that suitable for a bias ? EOA 
 neural network learning fast , giving me false positives . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123neural network learning fast , giving me false positives.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;jabza.sorry, this has been archived and can no longer be voted onEOQ sounds like it's overfitting the training data ... have you tried adding a regularization term to your cost function ? EOA 
 how much data do you need to do ml ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how much data do you need to do ml?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;1337hephaestus.sc2sorry, this has been archived and can no longer be voted onEOQ i'm personally most familiar with split testing on websites . but to really do some ml , what kind of data sets do you need ? EOA 
 how much data do you need to do ml ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how much data do you need to do ml?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;1337hephaestus.sc2sorry, this has been archived and can no longer be voted onEOQ the non-answer answer is that it depends . the more complicated your model is the more data you need in order to avoid over-fitting . i've EOA 
 how much data do you need to do ml ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how much data do you need to do ml?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;1337hephaestus.sc2sorry, this has been archived and can no longer be voted onEOQ thanks for the link , i'll check it out . EOA 
 how much data do you need to do ml ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how much data do you need to do ml?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;1337hephaestus.sc2sorry, this has been archived and can no longer be voted onEOQ one of the authors also has a series of lectures available on youtube that cover some of the same content . EOA 
 8-3-8 neural network does not converge . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }2348-3-8 neural network does not converge.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ hello , i'm trying to recreate the NUM-3-8 neural network . i'm having trouble getting my network to converge . it'll get close , but there is always one output that won't converge . for example : output- NUM 740 NUM 000 NUM 147 NUM 000 NUM 000 NUM 628 NUM 002 NUM 002 NUM 000 NUM 755 NUM 041 NUM 000 NUM 000 NUM 681 NUM 000 NUM 000 NUM 141 NUM 002 NUM 756 NUM 000 NUM 008 NUM 630 NUM 000 NUM 000 NUM 000 NUM 001 NUM 000 NUM 685 NUM 000 NUM 889 NUM 000 NUM 000 NUM 000 NUM 000 NUM 018 NUM 001 NUM 798 NUM 563 NUM 000 NUM 000 NUM 568 NUM 736 NUM 535 NUM 906 NUM 617 NUM 661 NUM 768 NUM 673 NUM 019 NUM 000 NUM 000 NUM 000 NUM 000 NUM 780 NUM 745 NUM 000 NUM 080 NUM 000 NUM 000 NUM 000 NUM 000 NUM 648 NUM 000 NUM 789 i'm using sigmoid threshold units , and i have one input bias node . i run NUM ,000 epochs , and i always get one or two outputs that will not converge . any advice to troubleshoot a nn would be helpful . thanks , max edit : i found my mistake . every layer needs a bias unit . NUM 813 NUM 148 NUM 006 NUM 000 NUM 101 NUM 000 NUM 145 NUM 000 NUM 117 NUM 762 NUM 029 NUM 077 NUM 000 NUM 000 NUM 000 NUM 013 NUM 024 NUM 151 NUM 758 NUM 000 NUM 000 NUM 000 NUM 142 NUM 140 NUM 000 NUM 168 NUM 000 NUM 805 NUM 103 NUM 002 NUM 000 NUM 110 NUM 122 NUM 000 NUM 000 NUM 160 NUM 834 NUM 083 NUM 031 NUM 000 NUM 000 NUM 000 NUM 001 NUM 012 NUM 069 NUM 739 NUM 191 NUM 180 NUM 127 NUM 000 NUM 220 NUM 000 NUM 039 NUM 172 NUM 712 NUM 000 NUM 000 NUM 066 NUM 165 NUM 133 NUM 000 NUM 159 NUM 000 NUM 770 EOA 
 8-3-8 neural network does not converge . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }2348-3-8 neural network does not converge.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;[deleted]sorry, this has been archived and can no longer be voted onEOQ i answered my own question :) EOA 
 how could i beat NUM with ml ? ( x-post: /r/machinelearning ) : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how could i beat NUM with ml ? (x-post: /r/machinelearning)&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;are595sorry, this has been archived and can no longer be voted onEOQ x-post from : URL there is already an ai for playing the game NUM here . it uses minimax , but i was wondering about the possibility of using machine learning to make an ai . how would i set up a neural network or something similar to play the game ? i am not very experienced with ml , so i'm looking for advice on how to set up neural network ( nodes per layer ) as well as what algorithms to use for adjusting weights . or maybe neural networks aren't the best approach ? EOA 
 how could i beat NUM with ml ? ( x-post: /r/machinelearning ) : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how could i beat NUM with ml ? (x-post: /r/machinelearning)&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;are595sorry, this has been archived and can no longer be voted onEOQ i'm currently working on doing exactly this . did anything come of your project ? EOA 
 how could i beat NUM with ml ? ( x-post: /r/machinelearning ) : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }123how could i beat NUM with ml ? (x-post: /r/machinelearning)&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;are595sorry, this has been archived and can no longer be voted onEOQ i tried an approach with reinforcement learning using a genetic algorithm to optimize a decision tree forest ( URL ) . i'm really not conviced of my method , and it doesn't do better than NUM . EOA 
 is there any way to find the 'real/effective bandwidth' of an audio file ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012is there any way to find the 'real/effective bandwidth' of an audio file?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;tzeppysorry, this has been archived and can no longer be voted onEOQ i have some previously recorded audio that i'm using for some ml projects . the audio is currently encoded with a certain compression/codec . but , before that , it may have gone through a number of other codecs/compression algorithms . for instance , maybe gsm for a cell phone call , then ulaw , then g729 . etc. is there a way to calculate the how much useful information is still contained in the audio , as compared to a reference , ( like say pcm16 ) . i don't have access to the original non-compressed files of course . seems like there should be some information theoretic approach that would work .... EOA 
 is there any way to find the 'real/effective bandwidth' of an audio file ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012is there any way to find the 'real/effective bandwidth' of an audio file?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;tzeppysorry, this has been archived and can no longer be voted onEOQ i've never worked with this , but my understanding is that useful information is synonymous with shannon entropy . URL if i were doing this , i would start with the negative sum over i of p(xi)-log.b[ p(xi) ] expression . here are a couple of approaches which do this : URL URL and : URL EOA 
 is there any way to find the 'real/effective bandwidth' of an audio file ? : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012is there any way to find the 'real/effective bandwidth' of an audio file?&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;tzeppysorry, this has been archived and can no longer be voted onEOQ the problem with shannon entropy , as i understand it , is that a totally random signal would have the highest entropy . as i'm working with speech signals , clean signals would actually have fairly low entropy . my fear ( and i should just research it ) is that some codecs add noise into the system , which would boost their entropy value . EOA 
 pattern recognition : have method , need name . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pattern recognition : have method , need name.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;anti.popesorry, this has been archived and can no longer be voted onEOQ this seems like it might be the best place to post this . i don't know any of the pattern recognition lingo in any real depth , but i need to know if there is a name for the algorithm in a program i've already finished . if there isn't , then i need to know the closest thing to compare it to . i have a simple signal of one variable vs . another. i fit a number of polynomials at various places in the signal to extract some points , and simplify the signal by some fundamental simple shapes which pass through these points . this seems to be something like a hough transform . i then calculate a few hundred attributes , based on these shapes for the set of all signals . for instance length of a side or angle between two sides and so on . i have categorized by eye a subset of these signals into two categories , good or bad . i use this training set to find upper and lower limits acceptable for these attributes , and apply a boolean test for all the attributes of all the signals using the test set limits . in other words finding the box of dimension ( # of attributes ) that the training set lives in and reporting which events of the total set are within it . this defines the output set of good signals for which there are no false negatives , and very very few false positives ( with these signals anyhow ) . this seems to be something like linear discriminant analysis with hard limits instead of dealing with probabilities . i then do various operations to find the smallest set of these cuts which return the exact same output . this seems to be like feature selection . what do you think ? is there possibly a name for this exact thing overall ? have i gotten close with the sub-method names ? EOA 
 pattern recognition : have method , need name . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pattern recognition : have method , need name.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;anti.popesorry, this has been archived and can no longer be voted onEOQ your description is a bit too vague . you say that i have a simple signal of one variable vs . another. which makes it sound like you have NUM variables in total ( i.e, you could plot all of your dataset in a NUM d plane ) but the rest of your text absolutely doesn't sound like that . i haven't got the foggiest what you mean by fitting polynomials at various places in the signal , so i'm just going to ignore that part and assume that you are dealing with a set of geometric shapes , because that's what it sounds like . then , you obviously totally confuse training and testset ( maybe just a typo ) , because apparently you find some thresholds for some attributes on a trianingset , and then apply a boolean test using the test set limits ??? this defines the output set of good signals for which there are no false negatives , and very very few false positives ( with these signals anyhow ) . i don't know why you think there are no false negatives in that set this seems to be something like linear discriminant analysis with hard limits instead of dealing with probabilities . absolutely not . first of , lda isn't dealing with probabilities either , but more importantly : what you're doing sounds nothing like lda . from what i've understood of your method , it is probably most similar to a decision tree . EOA 
 pattern recognition : have method , need name . : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pattern recognition : have method , need name.&#32;(self.mlquestions)submitted&#32;1 year ago&#32;-&#32;by&#32;anti.popesorry, this has been archived and can no longer be voted onEOQ thanks for the comment . i wanted to keep some intimate details a little vague but here it goes ... yes, it is a NUM d plane , it's essentially a histogram with non-poissionian error bars and uneven binning . this method finds the peak and end points ( without assuming a physical model and using fits of different polynomials ) through a lot of noise , finds out if that peak is far way enough from either edge , and whether there is a sufficient amount of curvature of the signal from the peak to either edge . this is done measuring edges of triangles made from these points . this is what my shorter description describes . there can't be false negatives because i'm using the limits found from the training set , and using the training set as the test set apparently . there can be false positives though . the wikipedia article on lda sure mentions probabilities URL a decision tree ( a pretty ugly one ) does appear to be close actually but that doesn't include the shape abstraction or the removal of branches . but i am just looking for , at least , things i can compare to . EOA 
 pros and cons of user-based vs item-based collaborative filtering : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pros and cons of user-based vs item-based collaborative filtering&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;mauledbyporcupinessorry, this has been archived and can no longer be voted onEOQ so let's say you have a bunch of nondescript users and a bunch of nondescript items . you want to perform collaborative filtering in order to recommend user a a new item . no metadata is used in any case . i'm trying to better understand when you'd do user-based vs item-based collaborative filtering . user-based collaborative filtering makes sense to me-given a user , you match him/her up with all other users and come up with a similarity-weighted average rating of all songs , at which point you can find the most highly-rated thing that the user hasn't seen yet . what are the pros and cons of using one vs the other ? if you construct the matrix of ratings , with let's say users as columns and items as rows , both end up being extremely similar-in user-based cf , you look at correlations between user columns ( using cosine similarity or something similar ) , and in item-based cf , you look at correlations between item rows ( using the same thing ) . what applications make sense for using one vs the other ? EOA 
 pros and cons of user-based vs item-based collaborative filtering : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pros and cons of user-based vs item-based collaborative filtering&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;mauledbyporcupinessorry, this has been archived and can no longer be voted onEOQ it seems to me that user-based or item-based are just inverses of each other . in terms of the approach that makes the most sense to me-clustering-you would have two possibilities . given m users and n items : users are a point in n-dimensional item space , where each of the n dimensions is that user's rating for the nth item items are a point in m-dimensional user space , where each of the m dimensions is that item's rating given by the mth user the former lets you find users who are similar to each other based upon their item ratings , allowing you to make predictions for the unrated items by comparing this user to their most similar users and how they rated that item , and making the assumption that similar users will continue to make similar ratings . i.e. here are the users most like you , or going one-level deeper , users most similar to you prefer these items , or for the nsa : person a is a terrorist ; he is most similar to these people in his buying or rating patterns , so check them out as persons of interest . the latter lets you easily find items who are similar to each other based upon their ratings by users , allowing you to make predictions for the users who haven't rated that item by comparing this item to the most similar items and how the user rated that item ( which is just the inverse of the previous paragraph ) , and making the assumption that similar items would be ranked similarly by similar users . i.e. here are the items most similar to this item , or going one-level deeper , items similar to this were bought by these users , or for narcotics police : item a is discovered to be a mind-altering substance ; these items are most similar based on user buying patterns , so investigate them as potential mind-altering substances as well , or perhaps item a is a mind-altering substance ; items similar to this were bought by these users , so check them out as potential drug users . it's just two sides of the same coin . edit : removed amazon examples-they weren't working out for me . :-p EOA 
 pros and cons of user-based vs item-based collaborative filtering : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pros and cons of user-based vs item-based collaborative filtering&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;mauledbyporcupinessorry, this has been archived and can no longer be voted onEOQ two things : users most similar to you prefer these items , or in amazon-speak : users who bought this item also bought these items i think these two are inverses . people like you like this is user-based collaborative filtering , but people who liked this also liked that is item-based collaborative filtering . also : the latter lets you easily find items who are similar to each other based upon their ratings by users , allowing you to make predictions for the users who haven't rated that item by comparing this item to the most similar items and how the user rated that item ( which is just the inverse of the previous paragraph ) , and making the assumption that similar items would be ranked similarly by similar users . the assumption is stronger than that-it says similar items will be ranked similarly no matter who's ranking them . your assumption combines both . do you agree ? and yeah , they're inverses in the sense you described . if users are columns and items are rows , then you end up with this sparse matrix of ratings , and the goal is for the prediction algorithm to fill in the blanks . user-based cf sweeps across columns and correlates and fills the blanks in that way ; item-based cf sweeps across rows and correlates and fills in the blanks that way . mathematically, it's simple . but conceptually , how do the results vary when you use one vs the other ? what are the strengths and weaknesses of each ? when do i want one vs the other ? do the two have different interpretations ? what does each tell you about your data ? etc. EOA 
 pros and cons of user-based vs item-based collaborative filtering : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pros and cons of user-based vs item-based collaborative filtering&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;mauledbyporcupinessorry, this has been archived and can no longer be voted onEOQ i agree with you on your points , thanks. my instinct is that both perspectives should give you the same results , since they are just mathematically inverses . ( but i may be wrong. ) EOA 
 pros and cons of user-based vs item-based collaborative filtering : mlquestions this is an archived post . you won't be able to vote or comment . EOQbody >.content .link .rank, .rank-spacer { width : NUM ex } body >.content .link .midcol, .midcol-spacer { width : NUM ex } .adsense-wrap { background-color : #eff7ff; font-size : NUM px; padding-left : NUM ex; padding-right : NUM px; }012pros and cons of user-based vs item-based collaborative filtering&#32;(self.mlquestions)submitted&#32;1 year ago&#32;by&#32;mauledbyporcupinessorry, this has been archived and can no longer be voted onEOQ the approach that won the million song dataset recommendation challenge was actually a mixture between user-based and item-based collaborative filtering , and it seems that both approaches used together worked better than item-based alone , which in turn worked better than user-based alone , for this problem . there's an overview paper that describes the details : URL ( have a look at the 'ranking aggregation' section in particular ) . EOA 
 just crashed an ssh server with a code similar to cifar-10 in tensorflow . what might have caused this ? : machinelearning yes , those were checked , it was not a power outage . since nobody else was on the server at that time , and my code was the only thing running , i am pretty sure it was caused by that . i know that is stupid , there are multiple pc's for people to work on , and not a lot of people are using them , and our uni does not have infinite funding unfortunately . but even if i could debug ( which i can , i just don't feel too good telling people every five minutes to go restart the server ) , how would i go about it ? EOQ what batch size are you using ? going from NUM x32 to NUM x424 is a massive jump in computation and memory , i suspect that's the problem . EOA 
 just crashed an ssh server with a code similar to cifar-10 in tensorflow . what might have caused this ? : machinelearning yes , those were checked , it was not a power outage . since nobody else was on the server at that time , and my code was the only thing running , i am pretty sure it was caused by that . i know that is stupid , there are multiple pc's for people to work on , and not a lot of people are using them , and our uni does not have infinite funding unfortunately . but even if i could debug ( which i can , i just don't feel too good telling people every five minutes to go restart the server ) , how would i go about it ? EOQ NUM-424 would not fit in gpu memory , maybe there was an overflow ? you can try to calculate memory usage of your model , usually people forget to readjust the size of the output of last convolution to fc connection , resulting in huge memory increases . EOA 
 just crashed an ssh server with a code similar to cifar-10 in tensorflow . what might have caused this ? : machinelearning yes , those were checked , it was not a power outage . since nobody else was on the server at that time , and my code was the only thing running , i am pretty sure it was caused by that . i know that is stupid , there are multiple pc's for people to work on , and not a lot of people are using them , and our uni does not have infinite funding unfortunately . but even if i could debug ( which i can , i just don't feel too good telling people every five minutes to go restart the server ) , how would i go about it ? EOQ i will check the output of the last convolution , thanks for the tip ! i calculated that for NUM k images , NUM x424x3 each , that would use up about NUM gb of memory . however, i don't think it writes on the gpu memory ( unless i'm not understanding something ) , since we only use cpu's for computation . the server is a mac , do not ask me why , and those do not have nvidia cards , which are right now the only ones usable by tensorflow , as far as i know . however , the pc has about NUM gb of memory free memory right now . can it be that the code uses over NUM gb of memory , if all the sizes are readjusted ? would decreasing the amount of filters change memory usage much ? right now i think they are around NUM . EOA 
 just crashed an ssh server with a code similar to cifar-10 in tensorflow . what might have caused this ? : machinelearning yes , those were checked , it was not a power outage . since nobody else was on the server at that time , and my code was the only thing running , i am pretty sure it was caused by that . i know that is stupid , there are multiple pc's for people to work on , and not a lot of people are using them , and our uni does not have infinite funding unfortunately . but even if i could debug ( which i can , i just don't feel too good telling people every five minutes to go restart the server ) , how would i go about it ? EOQ i've noticed tf.session() segfaults if all gpus are being used , but that's hardly a kernel panic or anything . worth running uptime to just check if the machine really did reboot . it's quite possible you weren't the one that crashed it . otherwise , you probably just need to litter some print statements at major points in your code , tell everyone hey , i'm debugging something that seems to cause kernel panics , so i need the machine for a couple hours when no one else is . what time is good for everyone ?, and schedule the possible downtime . EOA 
 just crashed an ssh server with a code similar to cifar-10 in tensorflow . what might have caused this ? : machinelearning yes , those were checked , it was not a power outage . since nobody else was on the server at that time , and my code was the only thing running , i am pretty sure it was caused by that . i know that is stupid , there are multiple pc's for people to work on , and not a lot of people are using them , and our uni does not have infinite funding unfortunately . but even if i could debug ( which i can , i just don't feel too good telling people every five minutes to go restart the server ) , how would i go about it ? EOQ probably ran it out of memory , or possibly out of disk . check the kernel logs if they are present /var/log/k-. if nothing there you may need someone monitoring the console next time you run to see if/why the kernel panics or starts killing things for oom . EOA 
 just crashed an ssh server with a code similar to cifar-10 in tensorflow . what might have caused this ? : machinelearning yes , those were checked , it was not a power outage . since nobody else was on the server at that time , and my code was the only thing running , i am pretty sure it was caused by that . i know that is stupid , there are multiple pc's for people to work on , and not a lot of people are using them , and our uni does not have infinite funding unfortunately . but even if i could debug ( which i can , i just don't feel too good telling people every five minutes to go restart the server ) , how would i go about it ? EOQ if this is the galaxy challenge dataset ( which i think it is based on the numbers you mentioned ) , you can downscale the images by a factor of NUM and then crop to NUM x64 and lose almost no performance . that should be much easier to handle . you can always try reducing the scaling/cropping later . EOA 
 am i misunderstanding the seq2seq model ? : machinelearning for large vocabularies people tend to use hacks like a hierarchical softmax . EOQ one resource seemed to explain that you only predict over a subset of the vocabulary at each timestep . another resource described hierarchical softmax more like a training hack as you said , that at each timestep you only have to calculate the probability for the target word , and not for every word . can you briefly explain how hierarchical softmax is used ? EOA 
 can i solve localization task as a simple regression problem ? : machinelearning do you have one type of object or more ? if it is only one , you can learn a model which just predict the NUM points and it could work . for example one of the winner in kaggle use such technique : URL ( in fact they quantize the coordinate and use softmax , but idea is similar ) . but when you want to detect multiple type of objects , this could be harder . then it is much more similar to faster-rcnn or multi-box approach . EOQ the bengio students did something similar when they won the taxi kaggle : we initially tried to predict the output position x , y directly , but we actually obtain significantly better results with another approach that includes a bit of pre-processing . more precisely , we first used a mean-shift clustering algorithm on the destinations of all the training trajectories to obtain around NUM ,392 popular destination points . the penultimate layer of our mlp is a softmax that predicts the probabilities of each of those NUM ,392 points to be the destination of the taxi . as the task requires to predict a single destination point , we then calculate the mean of all our NUM ,392 targets , each weighted by the probability returned by the softmax layer . URL EOA 
 can i solve localization task as a simple regression problem ? : machinelearning do you have one type of object or more ? if it is only one , you can learn a model which just predict the NUM points and it could work . for example one of the winner in kaggle use such technique : URL ( in fact they quantize the coordinate and use softmax , but idea is similar ) . but when you want to detect multiple type of objects , this could be harder . then it is much more similar to faster-rcnn or multi-box approach . EOQ andrea vedaldi's rcnn minus r paper pushes this approach to the extreme . previous works like rcnn did bounding box regression by correcting a previous stage algorithm's outputs . EOA 
 how do i get informative features for each classification using scikit ? : machinelearning what model are you using ? EOQ i'm using something like this , onevsrestclassifier(sgdclassifier()), but i'm thinking of using a logisticregression classifier from the linear.model . EOA 
 how do i get informative features for each classification using scikit ? : machinelearning what model are you using ? EOQ random forests and decision trees have the functionality you are asking for EOA 
 what type of neural net should i use to optimize for clicks on an image ? : machinelearning if you have the data , just a feedforward convnet , with whatever bells and whistles . if you have NUM k-images , you almost certainly have enough data to transfer learn/fine-tune from an imagenet resnet , of which there are now pretrained models becoming available . if you need to collect it , you need some sort of data collection scheme ; since you can't really take actions , perhaps one of the recent active learning techniques where you request that a human give you labels for inputs from a supervised set that you are confident on . ( where you are your model. ) EOQ this is easier than imagenet-you only have NUM label categories . start with something simple , known, and fast to train like alexnet and go up from there as needed . EOA 
 dnn-> ; rnn-> ; seq2seq-> ; rl ... coming full circle ? : machinelearning in my opinion rl solves an entirely different problem ( credit assignment ) than any deep or recurrent network . currently, rl can leverage deep or recurrent networks for the estimation of a q function . people have even started using differentiable policies to try to train rl systems end to end w/ sgd . i think of it as neural networks solving the problem of representing sensory input ( text , images, audio , etc ) and rl learning to leverage that to seek some reward . EOQ ooh , could you share the papers you've seen on end-to-end training ? EOA 
 dnn-> ; rnn-> ; seq2seq-> ; rl ... coming full circle ? : machinelearning in my opinion rl solves an entirely different problem ( credit assignment ) than any deep or recurrent network . currently, rl can leverage deep or recurrent networks for the estimation of a q function . people have even started using differentiable policies to try to train rl systems end to end w/ sgd . i think of it as neural networks solving the problem of representing sensory input ( text , images, audio , etc ) and rl learning to leverage that to seek some reward . EOQ probably NUM 6732? it uses a rl for training seq2seq . EOA 
 those of you with experience implementing real-time machine learning on the web-what is an acceptable latency in your experience ? : machinelearning to me , NUM ms is probably fine but a lot of it depends on the ux . where the request exists in the ui and how it responds while waiting is really important . for example , a dead stall for NUM ms-when i click a link might be noticeable and feel sluggish . you might already know your next steps but just in case you have a few options depending on the model and implementation : change the ux so the user knows it won't be instantaneous . only send what's absolutely necessary to complete the request and response . ( i know it sounds obvious but this is ignored so often. ) compress the request and response . the cpu cost of compression will almost always be less than the network cost . if your model is deterministic , throw memcache or redis in front of it . i recommend measuring the potential cache hits before adding this to your stack . prune your model . can you sacrifice some accuracy for speed ? hardcoding coefficients might work but ... before you do any of that : measure what's slow . is that NUM ms all network requests ? is it rendering the results ? hope this helps . EOQ the NUM ms is combined network request with inputs , model calculation , and spitting back results . EOA 
 those of you with experience implementing real-time machine learning on the web-what is an acceptable latency in your experience ? : machinelearning to me , NUM ms is probably fine but a lot of it depends on the ux . where the request exists in the ui and how it responds while waiting is really important . for example , a dead stall for NUM ms-when i click a link might be noticeable and feel sluggish . you might already know your next steps but just in case you have a few options depending on the model and implementation : change the ux so the user knows it won't be instantaneous . only send what's absolutely necessary to complete the request and response . ( i know it sounds obvious but this is ignored so often. ) compress the request and response . the cpu cost of compression will almost always be less than the network cost . if your model is deterministic , throw memcache or redis in front of it . i recommend measuring the potential cache hits before adding this to your stack . prune your model . can you sacrifice some accuracy for speed ? hardcoding coefficients might work but ... before you do any of that : measure what's slow . is that NUM ms all network requests ? is it rendering the results ? hope this helps . EOQ depends what you are doing . you should compare to other comparable apis . ibm's alchemyapi gives around NUM ms response time for a nlp app ( URL ) . NUM th percentile solr search responses are typically NUM-60ms ( URL ) EOA 
 those of you with experience implementing real-time machine learning on the web-what is an acceptable latency in your experience ? : machinelearning to me , NUM ms is probably fine but a lot of it depends on the ux . where the request exists in the ui and how it responds while waiting is really important . for example , a dead stall for NUM ms-when i click a link might be noticeable and feel sluggish . you might already know your next steps but just in case you have a few options depending on the model and implementation : change the ux so the user knows it won't be instantaneous . only send what's absolutely necessary to complete the request and response . ( i know it sounds obvious but this is ignored so often. ) compress the request and response . the cpu cost of compression will almost always be less than the network cost . if your model is deterministic , throw memcache or redis in front of it . i recommend measuring the potential cache hits before adding this to your stack . prune your model . can you sacrifice some accuracy for speed ? hardcoding coefficients might work but ... before you do any of that : measure what's slow . is that NUM ms all network requests ? is it rendering the results ? hope this helps . EOQ NUM ms is very fast . i am not sure you can reliably hit any server and return in that time ; many users' connections ( especially mobile ) have larger latencies than that , so your spec may not be realistic unless it's all done client-side . a lot will depend on the ui . if you are doing predictive text , or text completion , you need to be very fast . but for many other applications much longer times are acceptable . are you sure you mean latency or do you mean total time including latency ? also , you can often hide the computation time by returning partial or incomplete results and improving or expanding them as you go . use animations and other transition effects ( which will easily soak up NUM-200ms or more ) to pretend you have the result , even if it isn't yet available . for example , you can animate a graph growing an edge to a new vertex before you know what you will put in the vertex . but all the growing buys you time with the user . they will think you already have the answer . EOA 
 those of you with experience implementing real-time machine learning on the web-what is an acceptable latency in your experience ? : machinelearning to me , NUM ms is probably fine but a lot of it depends on the ux . where the request exists in the ui and how it responds while waiting is really important . for example , a dead stall for NUM ms-when i click a link might be noticeable and feel sluggish . you might already know your next steps but just in case you have a few options depending on the model and implementation : change the ux so the user knows it won't be instantaneous . only send what's absolutely necessary to complete the request and response . ( i know it sounds obvious but this is ignored so often. ) compress the request and response . the cpu cost of compression will almost always be less than the network cost . if your model is deterministic , throw memcache or redis in front of it . i recommend measuring the potential cache hits before adding this to your stack . prune your model . can you sacrifice some accuracy for speed ? hardcoding coefficients might work but ... before you do any of that : measure what's slow . is that NUM ms all network requests ? is it rendering the results ? hope this helps . EOQ those of you with experience implementing a real time machine , learning on the web . EOA 
 convnets that output images ? : machinelearning blurring the network output and the true color image and doing euclidean distance seems to give the gradient decent help . (i ended up averaging the normal rgb distance and two blur distances with NUM and NUM pixel gaussian kernels . it seems like this person had to jump through a number of loop holes to get the network to work properly . there were a number of things mentioned that seem a bit more nuanced than what you are doing such as extracting the tensors before the pooling layer and concatenating them EOQ have you tried sticking an adverserial network on the end of the autoencoder ? i only glanced through this the other day , i didn't read in anything vaguely resembling detail , but i think that's what it's doing . URL EOA 
 tts ( text to speech ) via nn ? : machinelearning URL EOQ alex graves only very briefly alludes to tts in his generating sequences with recurrent neural networks paper ; however, he gave a cool demo in nando de freitas's deep learning class last year . EOA 
 tts ( text to speech ) via nn ? : machinelearning URL EOQ that's actually somewhat impressive. did he publish anything on the subject ? EOA 
 tts ( text to speech ) via nn ? : machinelearning URL EOQ he never published the tts experiments , but there are likely extra tricks to get it to work-moving the basic gmm-on-top handwriting model ( conditional or not , upper triangular covariance or not ) to work on vocoded speech didn't work for us in vrnn . most tts papers i have seen recently are taking some part of the classic hmm based tts systems and replacing with a neural net and/or rnn . this isn't really and end to end neural network system but does make use of neural networks for some parts . EOA 
 looking to write a natural language query processor : machinelearning take a look at sempre , which is a tool for learning semantics from query-logical form pairs : URL turning text queries into database queries is a hairy but well-understood problem . just start reading at the papers that the sempre guys wrote and walk the citation graph :) EOQ that looks very nice ! i'm trying to decide how much this is a machine learning problem and how much could be mapped to parsing . if the input was restricted to a small set of input questions , something like this nlp parser might get me pretty close : URL EOA 
 looking to write a natural language query processor : machinelearning take a look at sempre , which is a tool for learning semantics from query-logical form pairs : URL turning text queries into database queries is a hairy but well-understood problem . just start reading at the papers that the sempre guys wrote and walk the citation graph :) EOQ if you can constrict the syntax , then the problem gets dramatically easier . you might get away with traditional ( non-nlp based ) parsing . URL is a good place to start you can also view it as a ui problem : autocomplete queries you know how to handle ( or do it field based ) . full , unconstrained natural language question answering is one of the grand challenges of nlp EOA 
 looking to write a natural language query processor : machinelearning take a look at sempre , which is a tool for learning semantics from query-logical form pairs : URL turning text queries into database queries is a hairy but well-understood problem . just start reading at the papers that the sempre guys wrote and walk the citation graph :) EOQ machine learning is like magic ! EOA 
 looking to write a natural language query processor : machinelearning take a look at sempre , which is a tool for learning semantics from query-logical form pairs : URL turning text queries into database queries is a hairy but well-understood problem . just start reading at the papers that the sempre guys wrote and walk the citation graph :) EOQ URL does a pretty ok job of this , at least as a starting point . afaik it's the best open source implementation available . sempre ( and maybe parts of deepdive ) are also worth looking at . EOA 
 what is a good alternative for matlab neural network toolbox ? : machinelearning there are a number of python libraries that might suit your needs ; i've found that lasagne is pretty decent . what in particular are you looking to do ? standard supervised classification/regression , or something more involved ? EOQ simple recurrent networks : elman network ( ma ) , jordan network ( ar ) , input recurrent . arma. arima . temporal axons : time-delay neural networks ?aka? tapped delay-line neural networks ( tdnn ) , gamma, laguarre . EOA 
 what is a good alternative for matlab neural network toolbox ? : machinelearning there are a number of python libraries that might suit your needs ; i've found that lasagne is pretty decent . what in particular are you looking to do ? standard supervised classification/regression , or something more involved ? EOQ why ? those are all ancient and outmoded . EOA 
 what is a good alternative for matlab neural network toolbox ? : machinelearning there are a number of python libraries that might suit your needs ; i've found that lasagne is pretty decent . what in particular are you looking to do ? standard supervised classification/regression , or something more involved ? EOQ if computer vision problems are of interest then matconvnet features some very good matlab implementations of cnns . URL EOA 
 what is a good alternative for matlab neural network toolbox ? : machinelearning there are a number of python libraries that might suit your needs ; i've found that lasagne is pretty decent . what in particular are you looking to do ? standard supervised classification/regression , or something more involved ? EOQ EOA 
 pre-trained speech-> ; text nns ? : machinelearning pretrained neural networks for speech are fairly uncommon ( unlike image models ) , but a few resources for either pretrained or already set up train-your-own speech recognition systems ( not just neural based ) are below : [ NUM ] end-to-end large vocabulary speech recognition , bahdanau et . al. [ NUM ] pdnn , a python toolkit for deep learning [ NUM ] pretrained models from kaldi or cmu-sphinx , such as this one i wrapped with an easy example of use with wav files . [ NUM ] there was a super great comment by someone else on how to run kaldi pretrained models with gstreamer , but i am having a lot of trouble finding it in my history . edit: /u/quirm 's comment is here . this is probably the most robust solution if you don't have access to a bunch of training data and don't feel like spending many weeks hacking up research papers . [ NUM ] baidu deep speech NUM is a pretty straighforward architecture if you have a ctc implementation around . there is a fast wrapper for torch now , with progress toward a fast theano version , and slow versions are floating around if you hit google . EOQ im guessing that even if you did find one , it would work very poorly unless your data is a close match to the data used to train the network . differences in microphone or audio characteristics can totally ruin it . same goes for differences in whats being spoken . im not sure if you were expecting something different . EOA 
 pre-trained speech-> ; text nns ? : machinelearning pretrained neural networks for speech are fairly uncommon ( unlike image models ) , but a few resources for either pretrained or already set up train-your-own speech recognition systems ( not just neural based ) are below : [ NUM ] end-to-end large vocabulary speech recognition , bahdanau et . al. [ NUM ] pdnn , a python toolkit for deep learning [ NUM ] pretrained models from kaldi or cmu-sphinx , such as this one i wrapped with an easy example of use with wav files . [ NUM ] there was a super great comment by someone else on how to run kaldi pretrained models with gstreamer , but i am having a lot of trouble finding it in my history . edit: /u/quirm 's comment is here . this is probably the most robust solution if you don't have access to a bunch of training data and don't feel like spending many weeks hacking up research papers . [ NUM ] baidu deep speech NUM is a pretty straighforward architecture if you have a ctc implementation around . there is a fast wrapper for torch now , with progress toward a fast theano version , and slow versions are floating around if you hit google . EOQ thanks for the reply and sharing the wisdom ; i've got no expectations , here! EOA 
 training an autoencoder , layerwise or full-stack ? : machinelearning if you mean the preinitialization of an autoencoder layer by layer with rbms , it's mostly done to solve the vanishing gradient problem as far as i know . in case your architecture does not have this problem and it's converging fast enough , i see no reason to preinitialize it layer by layer with an rbm and then fine-tune it as a whole . EOQ saw this paper a while ago which tries to address your question is joint training better for deep auto-encoders ? the gist :-joint training is better if it converges ( vanishing gradient )-layerwise pretraining is greedy might get you stuck in a local optimum , i.e. in the first layer you learn some representation , which is however useless for the next layer EOA 
 training an autoencoder , layerwise or full-stack ? : machinelearning if you mean the preinitialization of an autoencoder layer by layer with rbms , it's mostly done to solve the vanishing gradient problem as far as i know . in case your architecture does not have this problem and it's converging fast enough , i see no reason to preinitialize it layer by layer with an rbm and then fine-tune it as a whole . EOQ you da real mvp . thanks:) EOA 
 overfitting when combining variational ae with mlp ? : machinelearning um . you'll need to be more specific as to what integrating with mlp means . the vae is arguably an mlp , or several mlps plus some noise. EOQ well , i use vae and obtain encoded representation and use that as an input to mlp network . EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ hi there , feature selection is concerned with finding a subset of features ( from the set of all of your features in x ) which have the most discriminatory power with respect your outcome variable y . so let's say , you survey NUM people , and ask NUM quantitative questions about their lifestyle . let's say NUM of these people have diabetes and NUM don't . wouldn't it be great to understand exactly which of the NUM quantities effect the diabetic status the most ? it's highly unlikely , that if you trained a predictive model on these NUM features , all of them would be equally important . in this dummy example it's easy to see why for example 'age' , 'weight', 'hours of exercise' and 'daily sugar intake in grams' would be stronger/better indicators of diabetic status than say 'height' , 'race' or 'educational background' . this is a trivial example , but when you have a system you don't really understand , and you want to find all the features that might be relevant to your outcome variable fs algorithms can be very useful . many times we have noisy data and a good proportion of our features are irrelevant to the outcome variable or redundant in the context of others ( multicollinearity ) . there's a huge literature about feature selection ( fs ) , and there is no one-size-fits-all algorithm . as always in machine learning it's about trade-offs .. one fs algorithm might recover more of the truly relevant features but also include more false positives . another algorithm might be very conservative and only include completely uncorrelated features but miss a few interesting ones .. this is clearly not an up-to-date paper anymore but will get you started : URL EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ this sounds very much like principal component analysis ( pca ) , albeit with a conversion process to protect the orthogonal vectors back on to the original . EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ in pca you end up with weights on combinations of the original vectors instead of the orthogonal set . reprojecting would not make sense . EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ he means to project onto the response variable . think of performing a supervised pca , rather than traditional pca . EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ boruta doesn't really have anything in common with pca .. it's based on random forests ( can easily deal with non-linearity ) , and re-sampling to estimate the empirical null distribution of each features variable importance .. pca is a simple linear decomposition tool , but i'm not sure what did you mean by projecting back onto the original .. EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ sweet . you should submit a pull request to add this to scklearn.feature.selection. a suggestion : you should be able to pass in a random state to the algorithm so you can reproduce previous results . i see that in your loop you are setting a new random state for each iteration : that's perfectly fine. but you should be able to set the random state prior to the first call of np.random.randomint. that said : using sklearn's randomforest you've parallelized out a single iteration , but i bet you could run multiple iterations simultaneously for even more performance gains over the painfully slow r implementation . this would complicate implementing the kind of reproducibility i just suggested , but the performance gains would be worth it for most people ( i suspect ) . EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ thanks for the feedback ! yepp, passing a random state seems like a desirable feature for reproducibility , will add it soon . i'll hope to get this into sklearn , but it's up to the maintainers of the feature selection module .. hope they'll find it useful .. regarding your tip for further speed enhancements : unfortunately i cannot do that . the boruta fs algorithm has to run in a serial fashion , to assess in each round , which feature(s) could be excluded . in this sense it works like a recursive feature elimination process , so you cannot start multiple threads because each iteration relies on the output of the previous . also, by running random forest with n.jobs-1 you already max out your machine , so i don't think it would increase the speed any further .. thanks a lot for your feedback again ! EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ i'm gonna have to revisit that paper , i could've sworn features weren't evaluated until the end of all iterations . definitely didn't think this was more like rfe . i thought it was more like an inverted permutation test . EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ if you have a look at the r implementation , you'll see that they do get thrown away as soon as they pass the bonferroni corrected binomial test .. they are deemed rejected and left out from the consequent rounds : addshadowsandgetimp < ;-function(decreg,runs){ xsha < ;-x[,decreg!-rejected,drop-f]; if you think about what would be the benefit of keeping those features once they've shown no importance after NUM-15 reshuffle ? they just slow down the rf .. EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ thank you ! EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ nice work . EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ how do you want your implementation cited if i use this in a paper ? EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ thanks a lot for asking ! i haven't done anything novel , just re-implemented an already existing algorithm in a different language , so please cite the original authors ( paper is at the bottom of the readme.md on github ) . if this gets incorporated into scikit learn , you'll be able to cite it both ways .. cheers ! EOA 
 python implementation of boruta , an all relevant feature selection algorithm : machinelearning interesting . so if i understand correctly , your algorithm implementation is trying to find the most interesting values/inputs of x , which are most likely to output the value of y ? sorry for the simple question . i dream of doing data science one day , but the terminology is still over my head . EOQ good guy bayeslaw EOA 
 recommender engine with rich facebook data : machinelearning lots of implicit data is usually much better for recommendation systems , like what content a user has clicked on , than metadata like this . then you can run collaborative filtering to predict what other content a user will engage with . if you want to try and throw your features at a model factorization machines ( URL ) handle both categorical and real features with pairwise interactions in a cohesive way . without implicit data , for each page you will construct by hand some relationship to the user metadata . i suggest taking a probabilistic approach here where you could describe the expected engagement in terms of a prior distribution . so for a particular page you would describe what the likelihood of liking is for a user given information like their age , gender, etc . honestly NUM k users is small with the kind of sparse data i expect you will see in categories like music , movies, statuses , and checkins . you would need to project these sparse points into a coarser perspective for them to be at all useful by incorporating external taxonomy . for example hometown goes up to the country level , movies or music into genres , etc. i would ignore them to start with as making these features useful requires a lot of feature engineering with this small sample size , and i would stick to very coarse/binned categorical features of only a few variables and pay special attention to class imbalance issues that could bias predictions . best of luck and hope this gives a few ideas ! EOQ great answer ! thanks a lot ! EOA 
 rnnlm, what i did wrong ? : machinelearning the problem you're posing to the network is to predict the last word of a sentence given a sequence of all previous words , thus, you're only using one reward for about NUM words . in other typical implementations , for a sequence of NUM words , all NUM words are used as targets . let's say the we have the following example : the quick brown fox jumps over the lazy dog in your code the input would be the quick brown fox jumps over the lazy and the output dog . what you need to do is use the same input , but make the output quick brown fox jumps over the lazy dog . so that , even intermediate words such as jumps are used as targets with input the quick brown fox . EOQ hi sherjilozair , thanks for your comment . may be i am wrong , at line NUM , i have created all possible ngrams as gram.list- zip(-[all.words[i:] for i in range(t.stamp-NUM )]) , so there is a training x which jumps are used as targets with input the quick brown fox . my questions is , for example , corpus-[ a , b, c , d ] case NUM ( what i have at the moment ) : x has [ a ,b ] , [ b ,c ] y has [ c ] , [ d ] case NUM ( your suggestion ) : x has [ a , b, c ] , [ b, c , d ] y has [ b , c ] , [ c, d ] x has [ a , b ] , [ b, c ] y has [ b , c ] , [ c, d ] what is the difference for the same rnn model , do they learn differently ? thanks again . EOA 
 rnnlm, what i did wrong ? : machinelearning the problem you're posing to the network is to predict the last word of a sentence given a sequence of all previous words , thus, you're only using one reward for about NUM words . in other typical implementations , for a sequence of NUM words , all NUM words are used as targets . let's say the we have the following example : the quick brown fox jumps over the lazy dog in your code the input would be the quick brown fox jumps over the lazy and the output dog . what you need to do is use the same input , but make the output quick brown fox jumps over the lazy dog . so that , even intermediate words such as jumps are used as targets with input the quick brown fox . EOQ the average distance between output and input is longer in your case , which makes learning much harder . EOA 
 rnnlm, what i did wrong ? : machinelearning the problem you're posing to the network is to predict the last word of a sentence given a sequence of all previous words , thus, you're only using one reward for about NUM words . in other typical implementations , for a sequence of NUM words , all NUM words are used as targets . let's say the we have the following example : the quick brown fox jumps over the lazy dog in your code the input would be the quick brown fox jumps over the lazy and the output dog . what you need to do is use the same input , but make the output quick brown fox jumps over the lazy dog . so that , even intermediate words such as jumps are used as targets with input the quick brown fox . EOQ hi sherjilozair , thank you so much . case NUM has average distance between x and y NUM , whereas case NUM is NUM in the example . just one more question , how is the ppl going to be computed if i use case NUM as my input and outputs ? should i sum all log probabilities of y ( y has [b , c ] , [c, d ]) , then do average on the number if words ( which is NUM for y ) . or i just need the log probabilities of [ c ] , [ d ] in y-[ b , c ] , [ c, d ] , then do average on NUM ? thanks again for your reply . EOA 
 how to give different weightage to feature sets while training a classifier in scikit ? : machinelearning class weight are for your labels , not for features . also, your classifier is supposed to learning the weight for features . EOQ like csong27 says , the purpose of the learning algorithm is to learn optimal weights . if the model isn't generalizing well to other data , try tuning l1 or l2 regularization . if it's more that you're trying to transfer your learned model from one domain to another and need to fiddle , you can try adding noise into the descriptions and/or randomly deleting some of them . EOA 
 has anyone used an educational discount for the jetson tx1 ? : machinelearning just curious : does tensorflow run on the tx1 ? seems like i read somewhere that it didn't yet . EOQ i would try it , but i don't have one yet :(( EOA 
 what is the difference between nodes in the hidden layer ? : machinelearning the incoming weights for each unit are different . EOQ input is a vector . weights between input and hidden layer is a matrix . input vector-weight matrix-new vector hidden vector-sigmoid(new vector) sigmoid is an arbitrary nonlinear transformation on all values of new vector . a simple neural network is a chain of linear transforms(affine transforms) and nonlinear transforms . EOA 
 what is the difference between nodes in the hidden layer ? : machinelearning the incoming weights for each unit are different . EOQ each node receives the sum of all of the previous nodes multiplied by the weights . as you can see here each node has NUM weight leading to it from every NUM node on the previous layer . it is important that these weights be initialized to a random value before you start to train them otherwise all of the nodes will be getting the same value and the network will be unable to learn . EOA 
 what is the difference between nodes in the hidden layer ? : machinelearning the incoming weights for each unit are different . EOQ to expand on what other people have already mentioned : the weights are all different and the process of training a neural network involves finding good weights . often this is done via some type of backpropagation algorithm . in those algorithms the weights get changed based on where they are in the network and the error of the network on some set of examples . since the weights are different , and the inputs are multiplied by their respective weights , the nodes will not all have the same output . EOA 
 what is the difference between nodes in the hidden layer ? : machinelearning the incoming weights for each unit are different . EOQ in artificial neural networks , every layer is traditionally fully connected meaning that in fact every neuron from layer i is connected to every neuron from layer i-1 . so yes , every neuron in a hidden layer receives the same inputs and is therefore capable of generating an identical output to any other neuron in that network . however it's key to notice that the actual input to a neuron ( its activation ) is a linear combination of its input neurons and its weights . typically the input neurons and weights are regarded as vectors so that this linear combination is an inner product of a weight vector and input vector . these weight vectors are independent for all neurons and therefore allow each neuron to calculate a specific output . note that when all the weightsvectors are identical for all neurons , your initial remark is legit as every neuron is identical in every regard . illustration of this is the fact that we never initialise all the weight vectors as zeros as they would then be identical . the act of training a neural network is then to find a set of weight vectors for every layer such that resulting output is good by some performance measure . EOA 
 what is the most efficient learning algorithm ? : machinelearning lol i feel like this post was generated by an rnn using a corpus scraped from this subreddit...particularly weighted towards all the recent endless september posts . EOQ do you see simple thoughts as less powerful than complex thoughts , as if all the simple things are well understood ? EOA 
 what is the most efficient learning algorithm ? : machinelearning lol i feel like this post was generated by an rnn using a corpus scraped from this subreddit...particularly weighted towards all the recent endless september posts . EOQ ben , to take this seriously , no i don't , in fact my favorite quote from feynman was that if we aren't able to explain complex concepts in a simple manner to smart freshman , then we don't understand the concept . that being said , please watch out as machine learning and deep learning , as is so prevalent on this subreddit , are dramatically overhyped for most tasks outside intrinsically hierarchical datasets . cynically , very little is new as all these models are simply function compositions statistically estimated using the chain rule...we just have faster computers . EOA 
 what is the most efficient learning algorithm ? : machinelearning lol i feel like this post was generated by an rnn using a corpus scraped from this subreddit...particularly weighted towards all the recent endless september posts . EOQ a huge amount of ml is statistics with a fancy name . i really really hate when dbas get all wound up trying to call themselves data scientists and working with big data . yet what is the most important task in their day ..... backing up the database . cern works with big data . the power company doesn't . cern has well over NUM petabytes of crazy complicated data . that is big data that i can't even imagine how i would approach storing and processing that kind of data . the power company has enough to put on my laptop with room to spare . yet i will meet data scientists who work there . people who take themselves very seriously . i have long taken feynman's statement to heart . i will try to learn something well enough to teach it . often when i think that i have it and start to put an intro together , i will not know where to begin . back to learning but now i have a direction . EOA 
 what is the most efficient learning algorithm ? : machinelearning lol i feel like this post was generated by an rnn using a corpus scraped from this subreddit...particularly weighted towards all the recent endless september posts . EOQ lets use a slightly more complex math problem to learn , the binary digits of NUM /e which are .111111 exponent NUM ( in base2 of course ) . or we can compute NUM 00001 exponent NUM then divide NUM /that . how would an ai , given the result of that , figure out it means NUM /e ? EOA 
 what is the most efficient learning algorithm ? : machinelearning lol i feel like this post was generated by an rnn using a corpus scraped from this subreddit...particularly weighted towards all the recent endless september posts . EOQ wtf EOA 
 theory of how to imprint a neuralnet onto a human mind by associating mouse speed with screen brightness and varying brightness to cause different hand movement of the mouse : machinelearning the hell is this ? EOQ yea but who wants to sit and argue with a computer all day-] EOA 
 theory of how to imprint a neuralnet onto a human mind by associating mouse speed with screen brightness and varying brightness to cause different hand movement of the mouse : machinelearning the hell is this ? EOQ NUM ) EOA 
 forcing learning rate to zero in torch ? : machinelearning what i've done is to subclass and override the gradient update functions to do nothing . if you find a better way please share ! EOQ you can set the learning rate of certain layers to zero by overriding their updateparameters and accgradparameters to zero . you don't necessarily have to subclass , but it is cleaner . if you are building the model for the first time , you simply get the initialized layer's handle and overrride the mentioned functions . if you have a pre-trained network , and you want to freeze certain layers , you can either use the generic :apply function , that applies a closure to each layer , such as : URL or you can traverse them via traversing the model's .modules table : URL finally , here's an example freezing the first NUM layers of a net : URL EOA 
 forcing learning rate to zero in torch ? : machinelearning what i've done is to subclass and override the gradient update functions to do nothing . if you find a better way please share ! EOQ let's say you're never going to update your networks parameters and you're solely using the last output it produces . as such , there is no need for the network layers to hold their inputs . what would you need to do in order to prevent the network layers from saving their outputs . as i understand it , most of the memory is used up there . EOA 
 image difference recognition ? : machinelearning this seems like a problem where you're going to need datasets custom-designed to get started . you might be able to use ms coco somehow . i wonder what funny outputs neuraltalk would produce if you took the thought vectors for two different images and then used the difference as the input to the decoder . EOQ you should probably hire someone with expertise in feature engineering . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ thanks , that was perfect . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ theano's check.blas.py has timing results commented in the code . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ that script was a lifesaver for me when i was trying to verify that i had successfully gotten gpu acceleration set up on a machine . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ yea , i actually contributed a few times that made it into the current list . fortunately they've increased the matrix size as the old test wasn't really informative on the newer cards . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ i'm not near the computer that has my powerful gpu in it , but i remember getting something like a NUM x speedup on char-rnn . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ it depends on which cpu , and which gpu .. on my laptop , which has a rubbish mobile cpu , but a decent NUM m , gpu wins a lot :-) on a desktop i have , which has a decent cpu , and a less good gpu , cpu does ok ... EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ not a benchmark , but i have observed a NUM-15x speedup in going from cpu to gpu on a fully connected net with NUM to NUM layers . the experiments were performed on a g2.2xlarge node on aws . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ if you are lazy enough , follow URL and switch gpu true/false . at the end the training call returns the elapsed time up to milliseconds . EOA 
 cpu vs . gpu on regular non-conv networks : machinelearning i just wrote a quick benchmarking tool in torch to help you answer this question : URL it compares times for fully-connected relu nets with l2 loss between cpu and gpu , with options for batch size , number of hidden layers , and input/output/hidden sizes . on my desktop with a good cpu ( i7-4790k ) and a good gpu ( titan x ) i see speedups between NUM x and NUM x depending on the network . EOQ i think this will depend totally on the size of the network . i found some small networks run faster on cpu . if i remember , one of these was the theano mlp example on mnist . EOA 
 anyone took nlp intro course from coursera ? i have a question ... : machinelearning URL EOQ thank you ! :) EOA 
 anyone took nlp intro course from coursera ? i have a question ... : machinelearning URL EOQ why would anyone waste their time with these rubbish online courses ? EOA 
 anyone took nlp intro course from coursera ? i have a question ... : machinelearning URL EOQ because people want to learn whatever interests them and either do not have the time to go to a classroom , they are too far from the closest place offering a course they are interested in , or they do have the financial means to take a class . either way it's their choice. i take random classes all the time . EOA 
 anyone took nlp intro course from coursera ? i have a question ... : machinelearning URL EOQ yeah , the way i see it , if you're too poor for college , then you deserve to be dumb ... /s moocs and online learning are literally the democratization of education . EOA 
 does batch normalization mean that the assumption of normally distributed inputs can be relax in lecun's loss surface paper ? : machinelearning i think that's the right relaxation to make-i wonder if batch normalization was even slightly motivated by this attempt to relax that assumption . EOQ i doubt that it's a motivation .. i think that batch normalization is a very simple idea that's guaranteed to work-of course it will perform better when all your inputs are located at the regions where the sigmoid / tanh functions have the largest gradient ! EOA 
 how to train word2vec for same words differently according to context ? : machinelearning you would hope that different dimensions of the vectors correspond to different aspects of similarity , but the dimensions usually don't convey the attributes of nouns well enough for you to identify something that specific . EOQ yeah , ml is cool , all the cool kids are doing it , but no one knows a fucking thing about it . educate yourself . EOA 
 what sort of ml should i use to refine magic numbers in a program i have ? : machinelearning bayesian optimization , take a look at spearmint ( if you work with python ) EOQ okay , thanks! EOA 
 what sort of ml should i use to refine magic numbers in a program i have ? : machinelearning bayesian optimization , take a look at spearmint ( if you work with python ) EOQ i assume you are trying to maintain an optimal flow rate , both in f1 and f2 . you could build a neural network that uses m , dm/dt, f1 , f2, f1.new , and f2.new to predict your optimal flow rate . once you build a predictive model , you can then optimize solving for f1.new and f2.new needed to achieve the optimal flow rate . EOA 
 what sort of ml should i use to refine magic numbers in a program i have ? : machinelearning bayesian optimization , take a look at spearmint ( if you work with python ) EOQ yeah , sorry should have mentioned what m is and what we're trying to do . m is a measurement from a highly nonlinear biological system . we're trying to keep m inside a range ( m.min,m.max ) using f1 and f2 . f1 raises m almost instantly , f2 lowers m over a course of NUM minutes or so . the end goal is not to maintain f1 and f2 , rather to keep m in stasis . generally in the steady state that would mean f1 and f2 were also fixed , but m being very nonlinear means that it doesn't really have a steady state . EOA 
 what sort of ml should i use to refine magic numbers in a program i have ? : machinelearning bayesian optimization , take a look at spearmint ( if you work with python ) EOQ ah i see . then m is still an input , while your output becomes delta m at your next time period . set delta m to NUM and solve for the new flow rates . a simple random search or ga should work when solving for the flow rates . EOA 
 what sort of ml should i use to refine magic numbers in a program i have ? : machinelearning bayesian optimization , take a look at spearmint ( if you work with python ) EOQ i probably should also mention that m is just sampled in the real world from the actual system . dm is just m[t]-m[t-1] . there's a couple differential equations that give a good approximation for m , but it's something that's completely outside my control in the real world . all i can change is f1 and f2 , with the goal of getting m inside (m.min , m.max). does that make more sense ? genetic algorithms do seem like the best way to go for this ; i just wanted to see if there was anything obvious that i was missing before going ahead with them . EOA 
 what sort of ml should i use to refine magic numbers in a program i have ? : machinelearning bayesian optimization , take a look at spearmint ( if you work with python ) EOQ a ga could work , but be careful-if you tune it wrong , it might get stuck in a local minimum . gas are better than most optimizers at avoiding that problem in nasty search spaces , but you still have to be pretty careful . EOA 
 what sort of ml should i use to refine magic numbers in a program i have ? : machinelearning bayesian optimization , take a look at spearmint ( if you work with python ) EOQ if you have some sort of training set and you can define an appropriate loss function , then you can optimize all the continuous parameters provided that your loss function is differentiable almost everywhere w.r.t. these parameters . in your example , these would be the partition boundaries and coefficients . you just have to use reverse-mode automatic differentiation ( also known as backpropagation ) . if you work in python , you may want to take a look at autograd which should be relatively easy to apply on existing code , otherwise you could use theano or tensorflow which are more commonly used for neural networks but they will require you to rewrite substantial parts of your code . optimization w.r.t. discrete parameters , such as the number of partitions , is more difficult , probably a research-level problem . intuitively, the most obvious choices seem to be bayesian optimization or genetic algorithms . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ tri-state relus : URL EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ thanks for posting this , really like this approach ! it's exactly the kind of thing i was after . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ as an aside , be careful when reading old nn papers . a lot has changed in the field and while there's still plenty of value in reading them , it's easy way to get mislead . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ thanks for the good advice , any particular ones that i've linked to so far that i should be wary of ? EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ the vast majority of stuff published pre-2010 is obsolete . by obsolete i don't mean worthless , i mean that the results are very unlikely to transfer onto modern network architectures as a rule of thumb , count the layers . three layers or fewer probably means it was written before the high-depth/gpu/good initialization revolution . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ the cascade correlation algorithm in particular . it's a good idea for its time , but something like that just doesn't scale to millions of neurons . plus its only for depth-NUM . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ can you explain why it doesn't scale ? is it the neuron recruitment ( learning ) process ? are there any proofs showing how it really compares to backpropagation ? EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ yes , the algorithm works by adding neurons one at a time . i'd imagine such an approach wouldn't work on imagenet . perhaps it is possible to slightly modify the method to make it work on larger networks , i am not sure . the original paper showed some empirical results , but i don't think there are any proofs showing it is better than backprop . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ section NUM of ( NUM ) . they make a continuous parameterization of the network architecture , then optimize that using their hyperparameter optimization framework . the whole paper is very interesting and they apply the technique to other hyperparameters too ( not just network architecture ) . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ .. including the training set . that's right , they learn the training set that , when learned on , produces the best validation score . quite simple when you think about it , but it surprised me nonetheless . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ re : infinite rbm learning ordered representations with nested dropout infinite dimensional word embeddings re : pruning learning both weights and connections for efficient neural networks deep compression : compressing deep neural networks with pruning , trained quantization and huffman coding re : growing a continuum among logarithmic , linear, and exponential functions , and its potential to improve generalization in neural networks net2net : accelerating learning via knowledge transfer learning the architecture of deep neural networks gradnets : dynamic interpolation between neural architectures deep residual learning for image recognition training very deep networks / highway networks empirical evaluation of gated recurrent neural networks on sequence modeling / learning phrase representations using rnn encoder-decoder for statistical machine translation neural gpus learn algorithms subdivision reducing the training time of neural networks by partitioning EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ relevant question : has anyone been able to use these or any other approaches to evolve a general dnn to a convolutional architecture ? in other words , to learn the dimension of the input data ( e.g., NUM-dimensions in case of images , NUM-d in case of a mono audio , etc, etc ) EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ of the ones i've read . i think pretty much all of these techniques apply equally to convolutional architecture . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ this is not using nns , but other than that it's pretty much exactly what you describe : URL EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ please tell if you find out ! EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ are we limited to only topology of deep nets ? EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ i would say neat and hyperneat are very good examples and i should really have included them to begin with . EOA 
 interesting papers on learning automatically learning neural network topology : machinelearning this is the best type of post for /r/machinelearning imho-one that provides a convenient jumping off point for a thorough review of an interesting topic in the field and accompanying discussion . more of this ! EOQ if you like neat , i actually modified it as part of my thesis . lots of information in the specific pages of my website. if you are interested in knowing more , hit me with a pm and i will send you the url etc . EOA 
 study groups : machinelearning NUM st comment . raise ur dongers . ヽ??????ﾉ EOQ NUM st comment . raise ur dongers . ヽ??????ﾉ EOA 
 study groups : machinelearning NUM st comment . raise ur dongers . ヽ??????ﾉ EOQ could you please stop with this stuff ? EOA 
 study groups : machinelearning NUM st comment . raise ur dongers . ヽ??????ﾉ EOQ why do people downvote this kind of content ? do people not care who posted first or do they just hate dongers EOA 
 state of multimodal deep learning ? : machinelearning the biggest obstacle is data . EOQ what about films with subtitles ? i suppose someone would also have to go through and label scenes , but hopefully not individual frames ... EOA 
 differences between continuous bag of words ( cbow ) and skip-gram ? : machinelearning each of the skip-gram and cbow method , defines a method for creating a supervised learning task from a plain raw corpora ( let's say wikipedia ) . the hope is that by learning to perform well in this auxiliary task the machine will be able to learn good word vectors . that's the basic idea . the answers to how the auxiliary tasks help in learning word vectors and how cbow and skip-gram creates the auxiliary tasks are lucidly written in this paper : URL. EOQ any bag-of-words model assumes that we can learn what a word means by looking at the words that tend to appear near it . the cbow model trains each word against its context . it asks given this set of context words , what missing word is likely to also appear at the same time ? skip-gram trains each the context against the word . it asks given this single word , what are the other words that are likely to appear near it at the same time ? EOA 
 differences between continuous bag of words ( cbow ) and skip-gram ? : machinelearning each of the skip-gram and cbow method , defines a method for creating a supervised learning task from a plain raw corpora ( let's say wikipedia ) . the hope is that by learning to perform well in this auxiliary task the machine will be able to learn good word vectors . that's the basic idea . the answers to how the auxiliary tasks help in learning word vectors and how cbow and skip-gram creates the auxiliary tasks are lucidly written in this paper : URL. EOQ i think the tensorflow tutorial documentation contains a great explanation of cbow and skipgram word embedding models-see here . EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ hey i'm also doing some independent learning , lmk if you want to chat about it online EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ sure , you seem ahead of me though , as i haven't used theano yet , not sure how much help i would be to you haha EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ well , i actually haven't run a cnn for two days yet for one thing ! i couldn't get the theano cnn to work on my own dataset , so i ended up going with a simpler classifier that worked . did you implement cnn in python without any ai libraries ? sounds impressive . EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ i was quite proud when it identified NUM /10 handwritten numbers with ~98% accuracy :d two things i've learned about ml doing it the pure python way , regardless of network type , is:-you need a lot of input data , and-using cpu is verrry slow , compared to how i imagine theano would run using the gpu in fact , i ended up stopping my python cnn after NUM days as it was reaching around NUM 3 and my preset error threshold was NUM 1 and so would have taken approx . NUM more days to reach it ( i implemeted a fairly accurate prediction function that attempts to identify the end epoch , given previous performance ) . i'm sure the more experienced ml will laugh at my approach , and it is laughable to be honest !, but i like to be able to understand what exactly the function of each network component are , e.g. forward propagation , back propagation , fully connected , feature maps/kernals , pooling layers etc . so what better way than build it yourself ! the next step is a gpu based solution for sure , would you recommend a particular guide for starting out with theano ? EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ i took the same approach when i first started ( i've been playing with nns for quite a while and i'm only checking out theano seriously now ) . i wrote my own python solution but i made heavy use of numpy and then later cudarray . i don't consider this cheating as you still have implement the algorithm yourself . the only thing i didn't bother doing on my own was cnns as i just never spent too much time on them (i did do rnns , lstms, dropout , various learning rate solutions , etc...). i think doing what you're doing is a great way to learn and i would recommend it ( along with reading a lot of papers ) . ultimately when you really have an application in mind something like theano might make it much easier to prototype new ideas but if you've done stuff on your own you'll have a much better grasp of the subject . if you integrate something like cudarray into you code i think it will be more or less as fast as theano would be ( but you still would have to do everything yourself as cudarray is essentially a gpu version of numpy ) . i've also found its not too hard to add some custom functions to cudarray to do things it can't do . EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ thanks , i'll look into cudarray . i have a question , rather basic i'm afraid . when using thenao or cudarray , is it purely the arithmatic that is undertaken on the gpu , or are enture loops processed ? pseudo code , only arithmatic : for x in xrange(len(y)) : z-cudarray.sum(a-(b / (b-c))) or , vector processing : z-cudarray.processthisvector( y ) sorry if it's oh so simple , but i've never ventured into the gpu realm , so have no grasp of it yet . i'm not sure what the biggest cost of my python implementations are , the number of loops or the arithmatic involved in each loop , or both , and how how gpu would affect this . EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ only arthimatic is implemented on the gpu . if you use cudarray you have to also be careful because a lot of things might cause extra memory allocation ( which slows everything down ) . for instance , as far as i know ( i might be wrong ) , doing something like ( ca is cudarray ) : z-ca.dot(w, x)-b is very wasteful because ca.dot() creates an intermediate vector/matrix to store its results and then adds b to that and then the result is stored as a new vector z . a more efficient version pre.allocates z ( assuming you're doing many minibatches of the same size and the size of z is fixed ) and then in e.g. the nn code one does : ca.dot(w, x , out-z) z-b this directly stores the output of w.x into z and then we just add b to that without ever allocating any intermediate memory . also as a general pointer any time you're doing something in a loop ( in python ) its gonna be very slow unless the inner operators are very large and there are very few iterations . depending on what you're doing you should always try to convert it to matrix operations ( this can almost always be done ) . i should add my answer is more for cudarray . in theano anything you call with theano.function() in principle gets compiled into c or gpu code but i think normally the stuff in func can only be an arithmetic expression ( things like for loops can probably be mimiced using theano's scan( ) function) . EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ hey i'm not really a mentor but i know a bit and i'm also looking for someone to chat about it with , i've been learning for the past NUM months or so , mostly with python and some of the python libraries including sklearn and theano . EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ hey , i have similiar situation to you , guys. i'm not a mentor , but looking for some community or group to talk about problems or algorithms . i have some knowledge about ml , but i can drill down into your case if it's needed EOA 
 looking for machine learning mentor : machinelearning how far along are you ? i started a couple of weeks ago and have got the basics down for vanilla nn and just ran a cnn for two days using the mnist numerics data set with pretty good success . looking into rnn at the moment . i'm no pro but could exchange ideas . using python without any ai libraries i've found is a great way to get the concepts understood . EOQ hey guys , we should organize-maybe facebook group-any other idea ? EOA 
 how to use sympy for symbolic maths and optimization : machinelearning i've looked at sympy as a mathematica replacement , but was disappointed that it lacks even mathematica's basic wysiwyg functionality . this isn't a dealbreaker , but plain text becomes cumbersome with complicated equations , even with tab-complete . EOQ URL EOA 
 how to use sympy for symbolic maths and optimization : machinelearning i've looked at sympy as a mathematica replacement , but was disappointed that it lacks even mathematica's basic wysiwyg functionality . this isn't a dealbreaker , but plain text becomes cumbersome with complicated equations , even with tab-complete . EOQ sorry , i knew about the output formatting , but i was referring to wysiwyg input . EOA 
 didn't get any responses from r/askcomputerscience-how dependent would a ml model of our universe be on our perspective of the universe ? : machinelearning same question .... computer vision feature detectors and slam moving from hand-crafted to deep-learning ? stanford has all that NUM d model data-set work which leads towards this , but has anyone published anything interesting with practical applications of deep features and NUM d localisation ? EOQ URL i show that physical devices that perform observation , prediction, or recollection share an underlying mathematical structure . i call devices with that structure inference devices . i present a set of existence and impossibility results concerning inference devices . these results hold independent of the precise physical laws governing our universe . in a limited sense , the impossibility results establish that laplace was wrong to claim that even in a classical , non-chaotic universe the future can be unerringly predicted , given sufficient knowledge of the present . alternatively, these impossibility results can be viewed as a non-quantum mechanical uncertainty principle . next i explore the close connections between the mathematics of inference devices and of turing machines . in particular , the impossibility results for inference devices are similar to the halting theorem for tm's . furthermore, one can define an analog of universal tm's ( utm's ) for inference devices . i call those analogs strong inference devices . i use strong inference devices to define the inference complexity of an inference task , which is the analog of the kolmogorov complexity of computing a string . however no universe can contain more than one strong inference device. so whereas the kolmogorov complexity of a string is arbitrary up to specification of the utm , there is no such arbitrariness in the inference complexity of an inference task . i end by discussing the philosophical implications of these results , e.g., for whether the universe is a computer . same answer : URL we have implemented a convolutional neural network designed for processing sparse three-dimensional input data . the world we live in is three dimensional so there are a large number of potential applications including NUM d object recognition and analysis of space-time objects . in the quest for efficiency , we experiment with cnns on the NUM d triangular-lattice and NUM d tetrahedral-lattice . URL EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ yes , it's helpful ! thank you ! EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ [ deleted ] EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ it's interesting , because my supervisor is somehow sceptical about neural networks ( but i haven't asked why ) . i'm not familiar with nns ( on the level of math ) , but i'll check it . comparing two approaches is a concrete task . thank you ! EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ rna sequence tagging EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ thank you ! EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ holy shit why are there so many people in this subreddit who want help with their masters theses ? EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ most masters theses ( and phd theses ) are inspired by other people's ( usually professors' ) ideas . why would you possibly be against this ? EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ maybe because stackexchange is not for subjective questions . thus my second thought was about reddit , i don't know about others . EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ [ deleted ] EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ do you have any suggestions , where i can get help ? EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ do your own master's thesis . EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ i certainly will . i'm just asking for suitable problem to solve . maybe, someone will give an idea for me . if you're not interested in this question , why even bother to answer ? EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ so maybe you'll go away . you're supposed to find a research question on your own . EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ what's going on with you people ? can you please read my post one more time ? it wasn't my homework to find an idea for the thesis . i'm allowed to bring my own problem if it's hmm . i do not want to research coupling anymore . in particular , i want to find a topic that can be interesting for a potential employer and i guess people here can advise me on that . the last definitely requires experience which i don't have . EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ no one cares about you and your problems . that's not what this subreddit is for . EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ if you don't want to see it just downvote it and move on . no need to be an asshole . EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ masters are generally thought at the moment to offer the best balance between increased salary and tuition/time/opportunity-cost . bachelors are too entry level , and phds too long & likely to drop out . EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ machine learning library index URL list of high quality open data sets in public domains : URL take one from the first link , take another from the second link , cram them together , write what you learned and how you learned it , then write a program to do exactly that . learning how to learn how to pick the best machine learning problem : title for your thesis : neuralnetworks of neuralnetworks . take the data . write a program which uses a some machine learning principle to pick the right configuration and settings for creating a neural network ( or machine learning approach ) to get the best crossvalidation and testing set accuracy . taking the learning machines up another level of meta . learning the best strategies by which filtering , expanding, condensing , collecting, ignoring , enhancing, uncropping and fromulating data results in the best way to map from what we know , to how to go about best predicting what happens next . then build it , and name it blinky . my name's blinky , and i just want to be your friend : URL EOA 
 hidden markov models : problem to solve for master's thesis : machinelearning i'm a mere student but i get the feeling hmm are old technology now , so most of the research deals with their application to various fields . that said , i think the two of the most mathematical hot topics at the moment in ml ( at least at my uni ) is bayesian and nonparametric approaches . the idea of an infinite hidden markov model seems pretty interesting . hope this is useful ! EOQ thanks a lot for the links ! i have already some basic experience from my university and coursera . although my work should be focused on hmms and must be suitable for degree in mathematics , i think i'll use a lot of coding and testing ml approaches in it . thanks for purpose statements too . ps . haven't seen this movie before :d EOA 
 pre-processing ( center-scale , box-cox transformation ) inside cross-validation ? : machinelearning technically , you need to do it inside the cross-validation , otherwise you're not truly testing your generalization . on a new test set , you would need to center/scale using the parameters from your training data . that said , i would do it inside the cv once , and then do it over the full test set . if it's not too slow ( e.g. your data set isn't gigantic ) , then keep it . if it's really slow , then just note how much a difference it makes and then stop using it : unless your cv folds are stratified in a way that distorts their statistics , it should be a very tiny difference . as for how you do it within the cv : you just compute the mean and std on the training part of your cv fold , then subtract that same mean and divide by that std on both the training/test set . repeat for every fold . EOQ agreed . it is better to do inside each cross validation fold and there is no reason not to . however you can do any transformation that does not require labels on the full dataset including the testset . EOA 
 pre-processing ( center-scale , box-cox transformation ) inside cross-validation ? : machinelearning technically , you need to do it inside the cross-validation , otherwise you're not truly testing your generalization . on a new test set , you would need to center/scale using the parameters from your training data . that said , i would do it inside the cv once , and then do it over the full test set . if it's not too slow ( e.g. your data set isn't gigantic ) , then keep it . if it's really slow , then just note how much a difference it makes and then stop using it : unless your cv folds are stratified in a way that distorts their statistics , it should be a very tiny difference . as for how you do it within the cv : you just compute the mean and std on the training part of your cv fold , then subtract that same mean and divide by that std on both the training/test set . repeat for every fold . EOQ why can i do transformation which do not require labels on the full dataset ? center-scaling also do not require labels ... EOA 
 pre-processing ( center-scale , box-cox transformation ) inside cross-validation ? : machinelearning technically , you need to do it inside the cross-validation , otherwise you're not truly testing your generalization . on a new test set , you would need to center/scale using the parameters from your training data . that said , i would do it inside the cv once , and then do it over the full test set . if it's not too slow ( e.g. your data set isn't gigantic ) , then keep it . if it's really slow , then just note how much a difference it makes and then stop using it : unless your cv folds are stratified in a way that distorts their statistics , it should be a very tiny difference . as for how you do it within the cv : you just compute the mean and std on the training part of your cv fold , then subtract that same mean and divide by that std on both the training/test set . repeat for every fold . EOQ if i understand you , you're saying to calculate the mean and standard deviation on the training set , and reuse those same numbers for test set , right? EOA 
 pre-processing ( center-scale , box-cox transformation ) inside cross-validation ? : machinelearning technically , you need to do it inside the cross-validation , otherwise you're not truly testing your generalization . on a new test set , you would need to center/scale using the parameters from your training data . that said , i would do it inside the cv once , and then do it over the full test set . if it's not too slow ( e.g. your data set isn't gigantic ) , then keep it . if it's really slow , then just note how much a difference it makes and then stop using it : unless your cv folds are stratified in a way that distorts their statistics , it should be a very tiny difference . as for how you do it within the cv : you just compute the mean and std on the training part of your cv fold , then subtract that same mean and divide by that std on both the training/test set . repeat for every fold . EOQ correct , that's what you should be doing . but if your test data is from the same distribution as your training data , it shouldn't really matter . EOA 
 what was that youtube channel ... : machinelearning welsh labs ? EOQ bingo ! thank you . EOA 
 what was that youtube channel ... : machinelearning welsh labs ? EOQ when i saw the title i thought this was an idea to train a neural network or something to find a youtube channel based on some features you gave it . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ there's no cost attached to putting something on arxiv if you want . and people do it : the adam paper ( adam : a method for stochastic optimization , kingma and ba ) as the no more pesky learning rates one ( schaul , zhang and lecun ) . now, these people didn't necessarily do it for the recognition . then the question would be : can you do significant and original independent work ? in many cases you cannot , because you need licenses for many datasets , and training and testing models on the larger datasets ( like imagenet , which in turn is publically available ) eats up lots of compute power . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ then the question would be : can you do significant and original independent work ? in many cases you cannot , because you need licenses for many datasets , and training and testing models on the larger datasets ( like imagenet , which in turn is publically available ) eats up lots of compute power . really ? an independent researcher could buy ~10 gpus and get data from google images or public datasets like imagenet . my guess is that over NUM % of iclr papers could be done with this setup . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ ~10 and the computers to run them would cost somewhere around $15000 . said independent researcher would have to be damn sure it was worth it . getting data from google images is really slow , and imagenet isn't public , it's very difficult to get access to if you're independent . it can be done , it's what i'm doing , but it's painful . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ just looked up the no more pesky learning rates paper . any idea why that isn't used in practice ? what happened ? does it underperform nesterov ? it looks very promising , and yet the paper is years old . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ the big universities and corporate donors to ml conferences seem to have a lot of power over what gets selected for publication in major journals i'm not sure that i agree with this . most of the reviewers do work at companies or universities , but the companies and universities themselves cannot see the reviews or effect them . it would indeed be quite dangerous if say , google, were able to bribe conferences to accept more papers from google authors . reviewers also should not be able to review papers from their own institution . to my knowledge neither of these have been violated . there may be people submitting authentic , original and significant work , and not getting the recognition they deserve for their work . most ml conferences let you upload to arxiv publicly while your paper is under review . so it can still be seen even if it isn't accepted . publishing independently does not seem like a bad idea to me , if you believed your work was interesting and significant enough . at least in ml , this isn't exclusive with submitting to conferences . you can pretty much do whatever you want with your paper . have any major contributions been made via independent publishing ? i was under the impression that 'a neural algorithm of artistic style' was independently published , but it seems i might have been wrong about this . my guess is that they are planning to submit a paper to a conference or it's under review somewhere . right now ml is moving so fast that you really want to get your results out immediately , rather than waiting for the reviews to come in . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ plenty of stuff put on arxiv is influential before it is published anywhere . also reviewing for the major conferences is double blind so i don't think author affiliation is as much of a deciding factor as you suggest . it is true that most conference reviewers are at a large university or company , but that is where most of the people are so it is sort of inevitable . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ totally agree . but with regard to reading papers on arxiv : be careful and critical ! there's a lot of good stuff on there , but also a lot of bad stuff ( since it's not been reviewed ... down the rabbit hole : there's also a lot of bad reviewed stuff ) . what i am trying to say is : there's no entry barrier to publishing on arxiv , which comes with pros and cons . EOA 
 why and when would you choose to publish ml research independently ? : machinelearning people put stuff on arxiv all the time , is that what you mean by independent publishing ? as a submitter and reviewer for journals and conference i have no idea what you mean by big universities and corporate donors ... have a lot of power over what gets selected . as a postdoc in a uni , do you think i ask anyone in my university what ratings to give papers ? who has the time , just understanding what they heck the papers are saying takes too much time. for conference this is unlikely on the face of it . for journals there are editors , which could be a single chokepoint , but you'd have to give pretty strong evidence for me to buy it . EOQ this is true , but i think your warning is too strong . in practice there is a very low concentration of genuinely bad work on arxiv , and the variance in peer review is very high . EOA 
 is there any high quality research into using deep learning for cognitive augmentation : machinelearning we are nowhere near that point . people are making good headway with things like machine translation , though. EOQ no EOA 
 is there any high quality research into using deep learning for cognitive augmentation : machinelearning we are nowhere near that point . people are making good headway with things like machine translation , though. EOQ nope . EOA 
 is there any high quality research into using deep learning for cognitive augmentation : machinelearning we are nowhere near that point . people are making good headway with things like machine translation , though. EOQ sounds cool . do you have a per-neuron brain/computer interface i can plug into to try this out ? it probably won't work very well , but the reason i haven't tried it is not having the hardware . or, you know , any expectation that the hardware can be created in the near future . EOA 
 any advancements in meta learning ? : machinelearning check out URL for a recent competition . this paper URL was a recent winner . the tools of the field still seem to be search/bayesian optimization and ensembles . EOQ to add to this : i've been developing an ec-based ml pipeline optimization system called tpot . auto-sklearn is likely going to find the ideal model parameters faster , but tpot allows the optimization system to create an arbitrarily large ( or small ) series of preprocessors , feature selectors , models, etc . in my next paper , i'm planning to make tpot's search smarter and faster-partly by integrating lessons from auto-sklearn , and partly by allowing the algorithm to actually learn online how to create good pipelines . EOA 
 any advancements in meta learning ? : machinelearning check out URL for a recent competition . this paper URL was a recent winner . the tools of the field still seem to be search/bayesian optimization and ensembles . EOQ this should be your starting point . URL later you might look here . URL at the bottom you should see dozens of references , which might be useful for your further research . EOA 
 any advancements in meta learning ? : machinelearning check out URL for a recent competition . this paper URL was a recent winner . the tools of the field still seem to be search/bayesian optimization and ensembles . EOQ you can view adaptive stochastic gradient learning rules as a form of meta-learning , i guess . plenty of those to go around . EOA 
 any advancements in meta learning ? : machinelearning check out URL for a recent competition . this paper URL was a recent winner . the tools of the field still seem to be search/bayesian optimization and ensembles . EOQ have there been- EOA 
 any advancements in meta learning ? : machinelearning check out URL for a recent competition . this paper URL was a recent winner . the tools of the field still seem to be search/bayesian optimization and ensembles . EOQ /r/agi EOA 
 how significant is the cognitive overload when you are a deep learning researcher ? : machinelearning no more than it is for any other engineer . EOQ the problem is there are really few new ideas . if you're new to the field you'll be overwhelmed by endless papers , but after a while you form a map in your head , and start looking for something fundamentally new as opposed to marginal improvement . i guess you've got to develop a to filter signal from noise . EOA 
 lstm peephole implementation . : machinelearning note that the multiplication of a diagonal matrix and a vector is just an element-wise multiplication of a vector and a vector . EOQ i see your point , at mila lab implementations peephole weights are vector , but at first two implementations are matrix . i guess [ NUM ] and [ NUM ] implementations are wrong . thank you for the answer EOA 
 lstm peephole implementation . : machinelearning note that the multiplication of a diagonal matrix and a vector is just an element-wise multiplication of a vector and a vector . EOQ yes , i just took a look at the first NUM : the first one is just an experimental version for a blog . the understanding is very biased by the author the second one has extremely bad programming design , the author cannot even organise the code for a more modular and reusable way . you can also check the implementation of lasagne i am more interested in a question why the cell to gate weights must be diagonal ? EOA 
 lstm peephole implementation . : machinelearning note that the multiplication of a diagonal matrix and a vector is just an element-wise multiplication of a vector and a vector . EOQ you don't need to worry about peepholes , as they have been shown to be not so important . see lstm : a search space odyssey , greff & al . EOA 
 lstm peephole implementation . : machinelearning note that the multiplication of a diagonal matrix and a vector is just an element-wise multiplication of a vector and a vector . EOQ you wright many implementations are not including the peephole connection . but still i would like to know right implementations . thank you for the answer EOA 
 seeking your recommendation on good books to learn the machine learning , data mining , and statistical learning ! : machinelearning i'm on my phone so won't find the links for you , but: if you are using r you really can't go past introduction to statistical learning ( and the more thorough elements of statistical learning ) by hastie and tibshirani . the books are great , free ( simple google search will find them ) , and isl has a great associated mooc by the authors ( again , google will find it ) . it sounds like that is probably all you need . r documentation is usually pretty good and the sort of work you are doing has lots of tutorials . for the more advanced programming aspects of r there are several books by hadley wickham for free online that cover pretty much everything you will ever need . for the basics , with your pre-existing knowledge , r programming by deng is good and free . EOQ for your specific project i would recommend you to following this course on probabilistic graphical models . they also have a text book that is optional . URL EOA 
 seeking your recommendation on good books to learn the machine learning , data mining , and statistical learning ! : machinelearning i'm on my phone so won't find the links for you , but: if you are using r you really can't go past introduction to statistical learning ( and the more thorough elements of statistical learning ) by hastie and tibshirani . the books are great , free ( simple google search will find them ) , and isl has a great associated mooc by the authors ( again , google will find it ) . it sounds like that is probably all you need . r documentation is usually pretty good and the sort of work you are doing has lots of tutorials . for the more advanced programming aspects of r there are several books by hadley wickham for free online that cover pretty much everything you will ever need . for the basics , with your pre-existing knowledge , r programming by deng is good and free . EOQ easy . for r there are NUM very very good books . a bit more theoretical-an introduction to statistical learning : with applications in r . a bit more practical-applied predictive modeling . those are really good and i would say a must read . EOA 
 seeking your recommendation on good books to learn the machine learning , data mining , and statistical learning ! : machinelearning i'm on my phone so won't find the links for you , but: if you are using r you really can't go past introduction to statistical learning ( and the more thorough elements of statistical learning ) by hastie and tibshirani . the books are great , free ( simple google search will find them ) , and isl has a great associated mooc by the authors ( again , google will find it ) . it sounds like that is probably all you need . r documentation is usually pretty good and the sort of work you are doing has lots of tutorials . for the more advanced programming aspects of r there are several books by hadley wickham for free online that cover pretty much everything you will ever need . for the basics , with your pre-existing knowledge , r programming by deng is good and free . EOQ i am happy to help ; this question comes up very often , and i answered a related one on quora quite recently (how do i get started in machine learning , both theory and programming?). the synopsis is : consider taking an intro course like andrew ng's introduction to machine learning or pedro domingo's machine learning lectures read through introduction to data mining by steinbach , tan, and kumar ( programming language agnostic , but great to get the big picture of techniques ) read a more programming focussed-book to learn about the relevant tools and libraries and start working on your own projects to get practice ( here , ignore the python machine learning book i mentioned and substitute it by an r equivalent-haven't read an r ml book so i don't want to recommend a particular one here ) dive deeper into the math by reading any of the NUM t . hastie, r . tibshirani, j . friedman, t . hastie, j . friedman, and r . tibshirani. data mining , inference, and prediction . NUM nd edition ., volume NUM . springer, NUM . c. m . bishop et al . pattern recognition and machine learning | christopher bishop | springer , volume NUM . springer new york , NUM duda , richard o ., peter e . hart, and david g . stork. pattern classification , NUM nd edition . john wiley & sons , NUM icing on the cake : the deep learning book by yoshua bengio , ian goodfellow , and aaron courville . the release date is set around NUM , but the NUM-page manuscript is already available as as of today ( online and for free ) . EOA 
 seeking your recommendation on good books to learn the machine learning , data mining , and statistical learning ! : machinelearning i'm on my phone so won't find the links for you , but: if you are using r you really can't go past introduction to statistical learning ( and the more thorough elements of statistical learning ) by hastie and tibshirani . the books are great , free ( simple google search will find them ) , and isl has a great associated mooc by the authors ( again , google will find it ) . it sounds like that is probably all you need . r documentation is usually pretty good and the sort of work you are doing has lots of tutorials . for the more advanced programming aspects of r there are several books by hadley wickham for free online that cover pretty much everything you will ever need . for the basics , with your pre-existing knowledge , r programming by deng is good and free . EOQ for someone with a strong math background i would recommend to follow andrew ng's stanford course rather than his coursera one . EOA 
 seeking your recommendation on good books to learn the machine learning , data mining , and statistical learning ! : machinelearning i'm on my phone so won't find the links for you , but: if you are using r you really can't go past introduction to statistical learning ( and the more thorough elements of statistical learning ) by hastie and tibshirani . the books are great , free ( simple google search will find them ) , and isl has a great associated mooc by the authors ( again , google will find it ) . it sounds like that is probably all you need . r documentation is usually pretty good and the sort of work you are doing has lots of tutorials . for the more advanced programming aspects of r there are several books by hadley wickham for free online that cover pretty much everything you will ever need . for the basics , with your pre-existing knowledge , r programming by deng is good and free . EOQ very true , good point ; somehow overlooked the part about his/her math major . EOA 
 noob text classification question : machinelearning i would look at tf-idf for transforming you words into some numerical representation . then your next problem will be determining how to deal with data imbalance . if you have poorly distributed scores you might want to find more samples . if that's not possible you can oversample the scores with fewer samples , or under sample the scores with more samples to even out for score distribution . then you need to decide if you want to score by regression or classification . it sounds like you have low to high scores which probably will use a regression type model . sentences will be very sparse in tf-idf . each sentence will be represented by a vector of tf-idf values . then maybe throw a random forest regressor or just a decision tree at it for starters and go from there . EOQ a common way to deal with oddly distributed targets is to first transform them into a nicer distribution with an invertible function and then fit your model on the transformed targets . when you want to make predictions , you predict the transformed target then use the inverse function to map it back to the actual targets . if it's exponentially distributed ( and non-negative ) log(1-t) is a pretty safe bet . you can try a bunch of things-plot a histogram of the transformed targets and getting it to look like a gaussian is a good indicator that you've got something sensible to work with . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ i posted this on the other thread , but it looks like this is the one everyone is commenting on . so , figure NUM in the paper shows the scores of different games , methods, and numbers of actor-learner threads across training epochs . for some of the methods ( a3c ) and some of the games ( pong , space invaders ) , the one-thread agent performs almost as well as the NUM-thread agent . so, a single a3c agent learns successful policies , even if there is just one actor-learner ? this is really surprising to me , since the whole point of experience replay and parallel actor-learners is to decorrelate the training data . but this looks like a3c performs either way , right? or do they mean that they still used NUM different actor learners , but just trained them sequentially ? EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ very cool that they made a little wolfenstein-clone just for this . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ pretty sure that is a modification of quake NUM . edit : maybe not , but the icons remind me of it a lot ... double suprise edit : if you did want to make a raycast engine for this kind of stuff , this is the thing i have been tinkering with/reading to learn URL EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ the icons , fps counter behavior/font certainly looks very similar to the ones from quake NUM . i have a feeble memory of these , but i think the yellow icon here was the machine gun . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ my god . it's probably gonna learn unreal tournament next , and then fucking kill everyone in a deathmatch . very excited for this . also , they said they'll be tackling starcraft as well ( sc and ut have ai contests every year ) . on second thought , i don't want deepmind owning me on every single game that i love . it would be cool to see it learn some trash talk hahah human noob EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ okay so how does this get shoehorned into robotics ... collaborative robots could share across a network , however when it comes to a single agent can multiple agents be simulated in a way which maintains diversity of experiences . a matter of multiplexing each frame off to a different network ? EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ how is this different from robotic vacuum cleaners mapping and navigating a house ? some models have been doing this for years , using cameras and ultrasonic sensors . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ those systems are programmed end-to-end , each time something happens there is a code path where the outcome is determined by the programmer . this on the other hand knows nothing apart from what appears on the screen and which actions can be performed , combined with a reward for reaching the desired goal , learning end-to-end . a similar robot with existing dqn implementations ( like the system which played atari games ) would have trouble with walls and finding goals if the goal was obscured by a wall . even once trained and having experienced these kinds of things the agent may find itself in a room , constantly going around in circles . the previous algos used random actions to explore their environment ( epsilon greedy ) . if you were stuck behind a wall then it may take rolling the same number on dice NUM times in a row before you escape the trap , eat a reward , and have that decaying reward pushed back through all the actions which led to that experience . you had to either teleop or force re-rolling of the same dice over-and-over to get out of the trap and give the agent good quality experiences ( labelled dataset ) to work from . in this paper instead of random actions they use multiple simulated agents to build the diversity which yields exploration of environment , without creating bias in the dataset due to it being from only one policy . combined with lstm this enables the agents to learn about following walls , leaving rooms , keeping motion in one direction even if it seems closer the other way . without the burden of having to randomly discover these experiences after rare strings of multiple actions . a programmed robot vacuum has to have many states and switches to ensure it does an efficient job . in the near future dqn robot vacuums will be rewarded for doing a good job and will discover for themselves what actions they must take to be rewarded . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ thank you , this answer is so much more than i was hoping for ! i really appreciate you thorough explanation . if i understand correctly , the lstm-equipped dqn can develop more complex strategies , once it has access to a greater variety of success cases ? does this mean the previously simulated agents had different personalities rather than an equally random action potential ? EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ i could listen to you all day ? your way of explaining is brilliant ! so this dqn generates a macro strategy from a massive parallel dataset of successful micro strategies ? EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ speaking of practical experiments , with experience replay style dqn implementation ... URL wallworld in there is kinda a NUM d labyrinth type world with an agent which has eyes which see walls and nostrils which smell goals ( even through walls ) . you can adjust the size of the world and the configuration of the walls change as the size does . g - a with the agent having to bypass the wall to reach the goal , it will need to make a turn , as it goes along the wall it will have input which shows a wall closer to it than the goal and will need experiences which teach the agent to keep going along the wall until it is no longer a barrier , at which stage approaching the goal is doable . now when you add a trap to this wall .... g -| a | | | this is where experience replay would require a lot of dice rolls with the right sequence of results to ever have a chance at experiences which lead out of the trap and to a reward . however even if it manages to escape the trap many times there is still an issue with how this can be modelled without any memory or sequence/time based stuff . lets say the agent turns left , follows the wall , then comes up against the side trapping wall . at this stage it's fairly deterministic that there is a wall ahead , the goal is back behind you , turn around dummy . where an lstm can keep track of that . there is a wall ahead , the goal is behind , but there was a wall between us a moment ago , keep going , keep moving away from the goal EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ what's the state of the art on NUM d games ? given the large number of degrees of freedom , it seems like it'd be quite a bit more difficult than even go . how far are we from an ai fps pro or starcraft champion ? EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ regular , open ai competitions for starcraft : brood war are held one of the main issues with judging game ai is that it has to be under the same constraints as the player , or you aren't making a fair comparison . the ai packaged with rts games often relies on cheating to be challenging for humans , usually by being given more information or resources than a player would have . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ i wonder how much mileage you could get out of an approach similar to alphago-take videos of players and record ( say ) joystick movements to use as labels . train a cnn on the frames and use rl to let the ai play itself . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ i don't think that approach would work well . the space of possible screen states is far , far higher than the space of possible go boards . and even the alphago learning has encoded game states to go from , rather than photos of go boards . the equivalent data set for an rts would be a time series of a list of units/buildings , their locations and current orders , etc., rather than screenshots . one major problem with inferring game state just from screenshots is that the screen space only reflects a tiny part of the game state at any time , unlike looking at a go board . in an rts you can look anywhere on the map at any time ( even if only some of it provides up-to-date information due to fog of war ) , and those views are all aspects of a single game state that has to be responded to consistently . i think you have to directly build in abstractions like units , location, passable vs . impassable space and pathfinding , resources, production , fighting strength , etc., even if you allow the parameters to be learned from recorded games . the sscait bots are provided information like visible unit locations and types through an api . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ i agree that the approach would not be fruitful for rts since there are so many systems you have to simultaneously manage/keep track of . but for fps it might be simpler , given that local information is generally sufficient . there are end-to-end convnet systems for self-driving cars ( using the research game torcs ) that use around a dozen automobile affordances as the labels . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ i don't think an fps would be a huge challenge for an ai after it got the basics down . humans would have a hard time competing with the computer with respect to reaction time , and i suspect the ai would when NUM % of the time in any head-on encounters . i could imagine a situation where the ai just moves around randomly spinning in NUM degree circles over and over and headshotting people as soon as they come into view . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ honestly , i don't think it would be that difficult for them to make a bot that was extremely good in starcraft . the reason is that humans are apm limited whereas bots are not . i was a relatively high end sc1 player , and to beat NUM % of people in sc2 , you didn't actually need much game understanding ... you could just beat them with raw mechanics . you do a pressure build to force them to react to you , and use your apm advantage to either crush them outright , or out-macro them while the skirmishes are happening . beating top end players in a bo5 or bo7 though ... much harder , but the same idea . you can make some seriously bad decisions and still win if you can triple your opponents apm . what i would be shocked to see ... is a computer that could play with human level apm and win on strategy . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ and yet , there is no sc bot ( so far ) that can beat skilled human players , so it must not be as easy as you suppose . apm is only valuable when you have a strong grasp of what to do with it . otherwise you just do dumb things faster . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ true enough . i simply meant that a game like go forced the computer to play on a much more even playing field with humans . yea starcraft is much much harder in other ways , but there are tons of things a computer could abuse in starcraft that a human can't due to reaction speeds and apm limitations . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ i think you have a point there . the advantage a computer has in chess is larger when there is extreme time pressure to make moves , and in micro-heavy real-time games there is always time pressure . just like a human can't evaluate a million chess board positions a second , they can't maintain NUM apm . EOA 
 deepmind navigates a NUM d maze using asynchronous methods for deep reinforcement learning : machinelearning that labyrinth test was something much needed when exploring different dqn algos . EOQ no need to fear skynet if we can just teach it to play video games EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ lstm is the most sensible rnn architecture . it can be derived directly from vanilla rnn in NUM steps : don't multiply , use addition instead gate all operations so that you don't cram everything . NUM st statement means instead of multiplying the previous hidden state by a matrix to get the new state , you add something to your old hidden state and get the new state ( not called hidden , but called cell , explained below ) . why? because multiplication-vanishing gradients . now , we are capable of long term memory since we are not losing it by repeated multiplications . but is storing everything useful ? obviously no . also, do we want to output everything we have stored at each instant ? again no . there are NUM projections in a vanilla rnn : input to hidden , hidden to hidden , hidden to output . lstm regulates each one of them projections with input , forget and output gates respectively . each of these gates are calculated as a function of what we already know , and current input i.e f(h.prev , x). now our internal hidden state will become holy and the access to it becomes highly restricted . so it has a new name-the cell . only certain information , iff it's deemed relavent considering the past can get in ( use of he in a sentence means we now know the gender of the subject , we send it in . use of another he in same sentence is not useful , so throw it away ) . some of it is forgotten with time or due to certain inputs ( like forgetting the gender of the subject at the end of a sentence ) . and out of all the information we store , only some of it is sent out and this is regulated by the output gate ( we don't want to keeping telling that the subject is male , we will only do so when we have to ) all in all , instead of multiplying with a fixed matrix , you instead calculate what should change in your cell and get the change as a result of an addition step . and, you send out only some your cell as the output . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ mind-blown . lstm makes sense now . thanks man ! EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ great explanation :) i've never thought of it that way before , it's never been obvious to me . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ if you like lstms , i have a question . we use so called leaky memory , with the 'forget' operation being m-u.m'-(1-u).m, with u the update strengh . over time the memory will be lost as a sigmoid is never truly NUM or NUM . can we not increase perplexity by using a truncated sigmoid ( with perfect NUM and NUM on the sides ) ? or an additional gate lock/unlock making it impossible to modify the value of the memory ? for example , if sig(wlock x-block) > ;0.8 then lock . if < ;0.2 then unlock . and we multiply the leaky memory update by a binary value depending of the locked status of the cell . it would be more expensive and add another set of parameters , but truly long term memory could be achieved that way . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ i think this is interesting point . sigmoid is never truly NUM or NUM , but it can move as close as necessary to NUM or NUM . the situation you mentioned is very theoretical , and i think it only happen as follow : if there is no update information as the time go on from input gate and the sequence goes to infinite , lstm will constantly lost its memory . however, the assumption conflict itself , there is no point to remember infinite sequence if there is no new information to learn . let assume there exist an infinite sequence and there is always update information from input gate . to simplify the case , i assume an infinite sequence of equally important and distinguished information at each time step ( extreme long non-repeat sequence ) . this is the case lstm re-scale its memory to fill in new information : step NUM : m-NUM -NUM 999... x1 step NUM : m-NUM /2-( NUM 999... x1 )-NUM /2-x2 step NUM : m-NUM /3-[ NUM /2-(0.9999... x1 )-NUM /2-x2]-NUM /3-x3 step NUM : m-NUM /4-{ NUM /3-[1/2-(0.9999... x1 )-NUM /2-x2]-NUM /3-x3}-NUM /4-x4 step i : .... as long as the number doesn't blow up , the lstm can store infinite length of sequence . you might notice that the only case information can be lost is in the first step , but if we have a good initialization of the cell memory , lstm can add back the loss information by modifying its initial memory . i think that is also the reason some encoder-decoder lstm system initialises their first hidden and cell states by using a small network training on encoded input . and the above case only happens given our prior knowledge that all input in the sequence are equally important , hence, we don't have to learn the gating units . as a result , you can generalize the case to more informative sequence , where lstm have to balance what new information it should remember and what old memory its should forget . this leads to another difficult case : what if the new information is absolutely important and lstm has to totally forget the old memory , but the sigmoid activation is never truly NUM or NUM ? as long as the lstm has full control of its gating units ( learnable gating units ) , it can adjust the weights of new and old information to output any number ! in short , sigmoid is never truly NUM or NUM , but the combination of NUM sigmoid can be NUM . for example : we need : m-4 we have : mt-1-NUM , m'-NUM -> ; mt-NUM 08-NUM -NUM 9-NUM -NUM i think this also answer the reverse case , when lstm must totally ignore new information to keep its old memory . p/s : my math and understanding might wrong , but this is my unsupervised understanding about lstm , i am happy if you can help me realise i did something wrong EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ i am not a math expert . i ask the question because in theory the lstm can be smart , in practice it is a stupid linear dot product . the leaky memory is so good because you have a separate matrix just to learn if you should update the memory or not . this is why i was thinking of adding a separate matrix to lock the memory . more and more , we move into time dependencies that do beyond sentence understanding , to document or whole book . or to video games with actions massively delayed . lstm are just not capable of handling this . neural turing machines are a way of having a longer memory , as you can only write somewhere in special context . so it is locked by default . but i am not really convinced by ntms . it is not elegant . this cannot be the right way of handling arbitrarily long memories . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ me too , that is why i simplified it to a more stupid math . but don't under-estimate that stupid linear dot product , just because of this simple addition , they can solve the gradients exploding and vanishing problem that remained for decade . in practice , i think the main issue of lstm is not the storing memory mechanism , but because lstm only store memory in NUM encoded vector , the memory cell . hence, it has to decide to encode a very compact and abstracted information of the sequence . it is like us when reading an article , we don't remember word by word , sentence by sentence or even document by document , what we have in our mind is just simple conclusion lstm is good , lstm is not good ..., and all that concept might be wrong . i think the concept of handling memory is different among memory network , neural turing machines and lstm . intuitively, lstm : encoding the temporal pattern in a vector ( a stacked lstm can be understand as a matrix of memory cells ) . memory network : embedding all possible content in a big matrix neural turing machines : personally, i think it is a mixed between memory network and lstm , you don't directly embed content by mapping it to memory matrix but learn a function to encode the content , then learn how to read and write it in the way network understand . frankly speaking , i don't think anyone known the right way yet , that why it is fascinating EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ lstm and ntms are quite different beasts actually . ntms disassociate computation from memory . take for example the copy task presented in the ntm paper . the algorithm to perform that task is actually simple , but in order for lstm to do it , it would require a large number of cells , which is unnecessary . ntms offload this to external memory and instead learn how to use the external memory . i agree with you that no one knows a good way to have external memory . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ leaky memory units has been tried before , see temporal kernel rnns . they don't work as well as lstms EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ wonder what experienced pros like u/oriolvinyals and u/flukeskywalker think of this kind of explanation . i for one , think this got more attention than it deserves . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ or /u/egrefen i agree . colah's blog did a much better job imo . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ thanks for the compliment . i like /u/pranv's explanation , and some of . there are many good explanations of lstm cells out there , but for me , the most clear and intuitive explanation of lstm dynamics is the first three chapters of . therein , he introduces rnns , provides a concise analysis of the vanishing gradient problem , and shows how the specification of an lstm cell ( first without the forget gate , then with its addition ) naturally falls out of said analysis . it's longer than all the other explanations listed here ( well , it's a few thesis chapters ) , but if you're looking for a clear path to understanding this ubiquitous recurrent structure , i really recommend taking the time to read these chapters . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ you believe it is a bad explanation ? EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ what did you think ?? EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ i'm a bot , bleep, bloop . someone has linked to this thread from another place on reddit : [ /r/machinelearning ] fantastic explanation of the lstm architecture in this buried comment if you follow any of the above links , please respect the rules of reddit and don't vote in the other threads . ( info / contact ) EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ not really scared to touch anything , here's a good review article : URL EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ seq2seq EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ scrn : URL i had actually read that paper , but didn't remember the acronym . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ mikolov gave a talk in russia yesterday , talking a bit about scrns . you may find the video (here)[URL] ( relevant ~9m in ) EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ if you listen closely , scrm matches lstm for small number of hidden states , then starts to lose . NUM hidden units-> ; scrm wins , NUM hidden units-> ; lstm wins . see ~20m in . use lstm when your goal is state of the art , use scrm to speed up the training , especially as you're designing a complex model . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ yeah , i was on the lecture . if you listen even more closely , lstm has NUM x parameters . i'd think it would be more fair to compare the models by # of parameters , but that's my guess . EOA 
 scrn vs lstm : machinelearning sure enough . lstm looks so scary that people are afraid to touch anything EOQ i don't understand the part about turing-completeness . aren't rnns turing-complete ? ( if we ignore the infinite tape of the real utm ) EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ hello , thanks for your answer , i'm familiar with most of these concepts , however it's the first time i'm implementing something completely from scratch , the set is mnist which is then feature scaled . this is the net : URL it is trained with adagrad , NUM iterations with early stopping and NUM patience ( usually after NUM th iteration , validation performance won't improve much ) , batch size is NUM . training accuracy gets up to NUM % while validation hovers around NUM 8%, getting a kaggle test set accuracy of NUM 14% . i would like to get the best results without using other datasets or more data , but modifying the network seems to increase variance EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ try the following . NUM use adam instead of adagram NUM . then try sgd with momentum-0.9 and learning rate NUM 1 and change the learning rate to current/10 whenever the loss stays stationary for NUM epochs . NUM try using the prelu activation function . NUM increase the number of neurons in the dense layers . given the fact that it's a very small dataset you could just write a routine to try all of the above multiple times and average out the losses to see which is best . also consider ensembling your best models . EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ i actually have a wip version that didn't give better results ( the one i linked was the best so far ) that uses prelu and adam but gets NUM % in test error . i will try to use sgd ( i actually have never tried dynamically changing the learning rate ) . i did a test with NUM and NUM units in the dense layers ( but it was still with adagrad and relu ) , i might try again now . thank you a lot for your hints , i'll keep you updated if you're interested EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ if you are using cnn , here is one of the reason : URL you cannot compare training error and validation error in term of which one is bigger to see if your model overfitting ( high variance ) . first , the errors depend on the size of dataset second , there is no scale to know how much bigger is enough just check the curve of your validation error , and when you inspect the learning curve , don't care about the value , focus on the overall trend : if the validation error go down and up-> ; you overfitted the training data . if it goes down constantly , even though you training for very very long time , you probably fitted the validation set . EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ thank you a lot , so i should see it going up and down sometimes ? but not too often ? EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ it depends . when you are training with dropout , your training data changes all the time , hence, it will fluctuate , but the general trend should be go down , then, go up . if no dropout , and you do not shuffle your dataset , the curve will be smooth with the same trend , but the result probably won't be better than the dropout model however , it doesn't matter how beautiful the curve is , you should pay attention to it smallest validation error to do model selection . even though you can achieve a model with smallest validation error , no guarantee for you that it will do better in test set . in that case , they do ensemble learning . long story short , check out this course : URL . for me , it is the best online course in machine learning . EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ thank you a lot ! i will check it out ! EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ as someone who doesn't understand ml very well , i thought you'd dropped out of an ml course because the learning curve was too steep or something . EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ if you really want an equal comparison , just turn on dropout for your validation measurement . bias vs variance is kind of a useless concept for deep learning . just monitor validation performance . it's good to try to control overfitting , but even when your training loss starts going below validation , further training can still improve validation performance . EOA 
 learning curve dropout : machinelearning so firstly , what network are you training ? how deep is it ? what layers are you using ? if your net is shallow dropout won't help much . dropout is an amazing technique to use when you want to regularise your network . what you are encountering is called overfitting . fully fitting the training set which in response makes your performance of your system in new data worse . try using kfold to cut your training set in multiple subsets and train on those , and use the validation to see your performance . choose the weights with the lowest validation error , not the lowest training error . use dropout only you have a very large network and start from a low NUM and go up to NUM 5 in several training sessions to see the differences . you might also consider using l2.reqularisation or batch.normalization to help with that issue EOQ thank you for your answer , i noticed it can still improve , however it usually leads to higher validation error aswell , so i was looking for a way to get better performance . but as soon as i add something like l2 normalization it suffers bias problems ( it gets to a minimum but performance is still bad ) EOA 
 sentence/thought vs word embedding performance boost : machinelearning check out this paper ( using merely averaging word vectors and performance is not bad ) : URL EOQ thank you very much this is very interesting ! EOA 
 sentence/thought vs word embedding performance boost : machinelearning check out this paper ( using merely averaging word vectors and performance is not bad ) : URL EOQ also , if its possibly to get such good performance without taking into account word order , this must mean the models that do take into account word order have much more room for improvement ? EOA 
 sentence/thought vs word embedding performance boost : machinelearning check out this paper ( using merely averaging word vectors and performance is not bad ) : URL EOQ it really depends for which task are you using those word embeddings . large context usually gives more semantic similarity , i.e. words from the same domain , while smaller context will give a more syntactic similarity , like words with the same part of speech . EOA 
 sentence/thought vs word embedding performance boost : machinelearning check out this paper ( using merely averaging word vectors and performance is not bad ) : URL EOQ thanks ! by large/small context do you mean large/small length of sample passages ? also, by same domain/syntactic similarity , you mean the closest vectors to a given passage will be closer to the passage in domain/syntax respectively depending on whether it is a large/small context ? EOA 
 sentence/thought vs word embedding performance boost : machinelearning check out this paper ( using merely averaging word vectors and performance is not bad ) : URL EOQ i think the op was specifically referring to the word embeddings you would average together in place of training sentence/passage embeddings . i do not think there has been as extensive analysis on the size of context windows for training sentence embeddings as there has been for word embeddings ( i could be wrong though ) . we can imagine that it context window might serve a similar role , as you spell out , but i would test that for yourself , with your own specific task & corpus . EOA 
 sentence/thought vs word embedding performance boost : machinelearning check out this paper ( using merely averaging word vectors and performance is not bad ) : URL EOQ i've observed about NUM % accuracy improvement in downstream task ( wsd with few training examples ) by taking an weighted average of word vectors instead of a plain average . but certainly this depends a lot on our task . EOA 
 sentence/thought vs word embedding performance boost : machinelearning check out this paper ( using merely averaging word vectors and performance is not bad ) : URL EOQ i don't know that you can expect any performance at all from averaging a passage's word vectors . EOA 
 conditional random fields-rookie question : machinelearning generally these kinds of things are hard to code as you want to update the output based on the current output , and these kinds of constraints form a loop that may or may not converge . one way of approaching this is to encode higher-order potentials that encourage the properties you're interested in ( e.g. NUM % of the output takes label foo otherwise pay a cost of bar for each element over or under the NUM % ) . there's no easy answer for how you should do this , and it often comes down to thinking really hard and then modifying an existing optimisation technique . this paper URL made some headway with the labels should be NUM % foo kind of formulation and is well worth paying attention to . EOQ can you explain your problem more ? specifically what type of data you're dealing with , what you're trying to predict , and how much data you have ? if you use a crf for this , you'll probably end up with loops and/or higher order potentials , and exact inference will probably be impossible . but that's fine-there are a ton of methods for approximate inference . take a look at pystruct . but the question about whether crfs are most appropriate is important . depends on the data/objective/amount of data . EOA 
 conditional random fields-rookie question : machinelearning generally these kinds of things are hard to code as you want to update the output based on the current output , and these kinds of constraints form a loop that may or may not converge . one way of approaching this is to encode higher-order potentials that encourage the properties you're interested in ( e.g. NUM % of the output takes label foo otherwise pay a cost of bar for each element over or under the NUM % ) . there's no easy answer for how you should do this , and it often comes down to thinking really hard and then modifying an existing optimisation technique . this paper URL made some headway with the labels should be NUM % foo kind of formulation and is well worth paying attention to . EOQ the problem is to do more accurate sleep stage tagging from wearable device sensors . at the moment i have about NUM k epochs ( from more or less NUM datasets ) . i have run into the endless loop problem before , when i tried to scale the emission probabilities by a small factor depending on the output . i am taking a look at pystruct , that's probably what i will try next . thanks! EOA 
 conditional random fields-rookie question : machinelearning generally these kinds of things are hard to code as you want to update the output based on the current output , and these kinds of constraints form a loop that may or may not converge . one way of approaching this is to encode higher-order potentials that encourage the properties you're interested in ( e.g. NUM % of the output takes label foo otherwise pay a cost of bar for each element over or under the NUM % ) . there's no easy answer for how you should do this , and it often comes down to thinking really hard and then modifying an existing optimisation technique . this paper URL made some headway with the labels should be NUM % foo kind of formulation and is well worth paying attention to . EOQ also take a look at generalized expectation and posterior regularization : URL URL these let you regularize your learning and/or inference so that things like xx% of output labels are foo in expectation . these are sort of soft marginal formulations rather than hard constraints , but that can be a lot more tractable . also, they sometimes provide a variational distribution whose parameters can then be used for map inference . learning from measurements and constraint driven learning are also in this same vein . URL URL out of those , i think that posterior regularization is the only one that is used much for regularizing inference rather than learning . we also had some relevant work last year in doing inference with global functions of expectations : URL edit : also, you can apply a lot of this domain knowledge in the map inference regime using dual decomposition , e.g. URL EOA 
 conditional random fields-rookie question : machinelearning generally these kinds of things are hard to code as you want to update the output based on the current output , and these kinds of constraints form a loop that may or may not converge . one way of approaching this is to encode higher-order potentials that encourage the properties you're interested in ( e.g. NUM % of the output takes label foo otherwise pay a cost of bar for each element over or under the NUM % ) . there's no easy answer for how you should do this , and it often comes down to thinking really hard and then modifying an existing optimisation technique . this paper URL made some headway with the labels should be NUM % foo kind of formulation and is well worth paying attention to . EOQ crf isn't perfect , but it is certainly suitable for this and will outperform a hmm . you just need to put in some clever features and have the right training data prepared the right way . i would try a crf before you try something else , maybe you'll get results that are good enough for your application . i think crfsuite has python bindings . you might be able to roll something better on your own , but you're not likely to find an off-the-shelf implementation , and you're going to need to do some research and some complex coding to really get it working . also, the method will probably be computationally intensive enough that you'll have to turn to something besides standard python . i don't know the constraints of your problem , how much time you have to solve it , or how good the model has to be . but one of the fundamentals of machine learning is kiss-keep it simple , surely! until you know for sure that crf isn't going to give you results that meet your needs , its going to be much less work and complexity to give it a whirl as opposed to something less conventional . chances are it will be time well spent as well-having a good understanding of crfs will serve you well in the long run for these types of problems as well . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ okay , that makes sense . i have two follow up questions if you don't mind : what makes a completely fleshed out solution ? what exactly is a workshop paper ? will it be considered as an icml submission ? what is the deadline for that ? EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ a full paper generally requires a story-you can explain the background , explain the idea , test the idea with experiments , then draw some broader conclusions from the experiments . if you can't write a complete narrative around your idea yet , then you're probably better off waiting and doing more work . there's not much worse than submitting a bad and half-assed paper . workshops usually have a deadline a month or two after the main conference deadline. they're typically organized around specific topics and hold their own sessions with talks . i believe that papers in workshops are still published in the conference proceedings ( edit : this is not the case in icml ) , but they're not as prestigious as getting published in the main conference proceedings . basically, it's a second chance to present something at the conference . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ thanks for clarifying-haven't published in icml before . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ on this topic : how well-received are evolution-based optimization papers at icml ? i was thinking about submitting something on my , but didn't have time because of other deadlines . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ if you can compare against bayesian optimization / random search and show that you are better , you are in business . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ it has to compare favorably in both minimum search ( which usually reflects in the accuracy ) and in execution time though . evolution based optimizers are usually way more complex than bayesian optimization or sgd . they are very good for non differentiable functions , but other than that , is hard to make a case for them . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ also useful when local minima are few and are of poor quality EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ make sure title has words deep learning in it . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ here's an interesting scenario : you take an old algorithm and improve it ( or claim you do ) , but you also throw more layers and more computer time at it than anyone else , and get sota results . should this be publishable ? EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ yea .. kinda sucks .. good point . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ is sota setting a requirement . can we put in partially baked ideas with some results ? i think that the real goal is to stack more layers and pontificate about how the brain works . state of the art results are an unfortunate side-effect . EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ dude you rock . don't let anyone else tell you otherwise EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ no one has told me otherwise ?つ ?.? ?つ EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ one more thing . can we make a slight change in the previous published papers ( e.g. change rnn from lstm to some other relatively new gates that have not been worked on yet ) and show the results with rest almost the same . will this be publishable ? EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ it depends how you write it and how good the results are . you will at least need to show improvement over what you are replacing ; this is the kind of paper where a sota result or two will help a lot . it will also help if you have a compelling reason why you settled on the architecture you did . can you tell a story about why we should expect your version to be better ? EOA 
 [icml NUM ] [ meta ] what makes a good paper , and submission in deep learning : machinelearning general tips have been posted is sota setting a requirement no . it helps , but novelty is much more important ( i.e., if what you've done is not new/has been done before , forget about icml ) . can we put in partially baked ideas with some results ? definitely not , you'll be rejected with a nice try , come back once the idea is fleshed out and you've actually done the work . if you have any half-finished things , the best you can hope for is getting accepted at one of the workshops . EOQ no EOA 
 as far as i know , there are no training algorithms known that will train a threshold network/perceptron . is this still true , and has threshold-forward-sigmoid-backprop been tried ? : machinelearning yes . . no asymmetry but they train sign(x) . EOQ interesting that this comes to mind . i'd heard from david krueger , who is ( was previously? ) a student in the montreal group , that bengio doesn't actually think target prop works . i was originally very excited by it , but expecting it to fail plus difficulty of implementation made me move on . this analysis makes it look like it might have life left in it : URL-and if their report that it works on training sign(x) then maybe there's something interesting in the idea for controlling attention . it's especially useful because you can still use backprop as much as you could have otherwise. EOA 
 as far as i know , there are no training algorithms known that will train a threshold network/perceptron . is this still true , and has threshold-forward-sigmoid-backprop been tried ? : machinelearning yes . . no asymmetry but they train sign(x) . EOQ david was and is still a student here at u de m . difference target prop was a big step forward imo compared to the original target prop proposal , and at least partially inspired some of the hardware related stuff yoshua and some other students have been working on . it is just not clear what benefit you get from training sign(x) activations compared to ones with more freedom , and having a model based gradient for every singal layer has high computational cost , similar to other interesting ideas about multi-stage/model based error propagation such as in gsn . it also ties into a few other ideas including , reinforcement learning , and even the recent gradient noise paper from google if you look at a high level about minimizing differences in each layer . it is becoming clear that backprop through the weights is a convenient and low-memory way to send training signals , but certainly not the only way-even a very noisy version of the gradient still makes good progress in optimization . EOA 
 as far as i know , there are no training algorithms known that will train a threshold network/perceptron . is this still true , and has threshold-forward-sigmoid-backprop been tried ? : machinelearning yes . . no asymmetry but they train sign(x) . EOQ cool , thanks. those are good references . is this the gradient noise paper you're talking about ? URL EOA 
 as far as i know , there are no training algorithms known that will train a threshold network/perceptron . is this still true , and has threshold-forward-sigmoid-backprop been tried ? : machinelearning yes . . no asymmetry but they train sign(x) . EOQ yep-that's the one ! EOA 
 as far as i know , there are no training algorithms known that will train a threshold network/perceptron . is this still true , and has threshold-forward-sigmoid-backprop been tried ? : machinelearning yes . . no asymmetry but they train sign(x) . EOQ is somewhat similar . the network structure is even more 'rigid' than a threshold activation because the weights are designed to be binary . they use the training updates to adjust an underlying floating point network , which is periodically re-cast into binary form . EOA 
 as far as i know , there are no training algorithms known that will train a threshold network/perceptron . is this still true , and has threshold-forward-sigmoid-backprop been tried ? : machinelearning yes . . no asymmetry but they train sign(x) . EOQ you can , in principle , learn with binary stochastic neurons using reinforce , which is ultimately sort of similar to what you're proposing . it doesn't work terribly well in practice for large numbers of neurons , though. EOA 
 looking for a networking-ml interesting project : machinelearning i always though research on malicious packet classification using string kernels and things was pretty interesting , and people have recently moved to using deep learning for malware classification . those might be areas to look into . EOQ that sounds like it would've been done by a lot of people already . anyway , thanks for the idea ! EOA 
 looking for a cnn implementation for NUM d images : machinelearning there's no fundamental difference between NUM spatial dimensions and NUM d-time-fundamentally , you're just dealing with an nxcx(...) tensor , and so any of the existing packages that support NUM d or nd convolution and pooling ( convolution in caffe , volumetricconvolution in torch , etc ) will be fine. EOQ take a loot at c3d : URL. they analyze videos , but use 'real' NUM d convolutions instead of NUM d-1d , so time is treated as another spatial dimension . EOA 
 looking for a cnn implementation for NUM d images : machinelearning there's no fundamental difference between NUM spatial dimensions and NUM d-time-fundamentally , you're just dealing with an nxcx(...) tensor , and so any of the existing packages that support NUM d or nd convolution and pooling ( convolution in caffe , volumetricconvolution in torch , etc ) will be fine. EOQ URL EOA 
 looking for a cnn implementation for NUM d images : machinelearning there's no fundamental difference between NUM spatial dimensions and NUM d-time-fundamentally , you're just dealing with an nxcx(...) tensor , and so any of the existing packages that support NUM d or nd convolution and pooling ( convolution in caffe , volumetricconvolution in torch , etc ) will be fine. EOQ convolution already supports NUM d images in the sense of what you mean-color images are inherently NUM d , and the maps learn correlation across channels in the filters . you could explicitly encode spatial constraints but this might not be necessary . theano has some rudimentary NUM d ( NUM d i guess technically? ) support , but i have never used it . EOA 
 looking for a cnn implementation for NUM d images : machinelearning there's no fundamental difference between NUM spatial dimensions and NUM d-time-fundamentally , you're just dealing with an nxcx(...) tensor , and so any of the existing packages that support NUM d or nd convolution and pooling ( convolution in caffe , volumetricconvolution in torch , etc ) will be fine. EOQ in NUM d convolution there might be multiple input channels but there's no restricted spatial extent in the channel dimension , and no sliding the filter along that dimension . so when people talk about NUM d convolution they mean that each input channel is actually a cuboid , and each filter is a hypercube that is slid across all three spatial dimensions . EOA 
 structure for ml partner system to an application : machinelearning not sure i completely understand the scope of the problem yet , but here are some questions you can ask yourself : -is it absolutely necessary to use two databases ? postgres and mongo are both perfectly respectable databases in their own right , but using both seems excessive without a specific requirement . there is opportunity to simplify by using one db . -should i be concerned about java performance ? how comfortable are you with java ? used well , java can outperform many other languages on the jvm . used poorly , it can be a nightmare . performance has more to do with how well it's programmed . if concerned , learn best-practice java optimization methods and use a good profiler to scan for bottlenecks . -what are some things i need to worry about ? keeping two dbs in sync is always fraught with peril , especially when one is sql and one is nosql/document store . these are two different worlds , and mixing them is not recommended . as with #1 above , consider simplifying to use one db if possible . -is it necessary to have users running their own db instances locally ? if they need to cache their own client data , are there other , simpler schemes for accomplishing this ? ( the answer may be no , but it's worth looking into ) -change the format of the application and other things depending on how they use it this seems horribly vague ; could you please expand on this a bit ? prima facie , this seems like a recipe for headache . being completely honest , and not trying to be a d-ck (i swear!), from the information given this setup seems unnecessarily complex and a bit convoluted . it seems you'll be able to get away with some simple java web services and a rest api-job scheduler , but again i don't completely understand the nature of the problem . i may be missing key information . architecture diagrams far more informative and may yield better answers . good luck . EOQ thanks , so just to illuminate some things : NUM ) i'm supposed to be building this on top of an already existing application running on a cloud . the client deployment however sometimes is on premise , so access to their local mongo database isn't easy . it's possible that we could bundle the ml application and the actual application together to solve that problem , but we wanted to try a web service approach using api calls . obviously part of the issue with that is that using strictly api calls means that i need to figure out a way to get the data i need per client . but without direct access to the main db i can't think of another way without syncinc/mirroring . i don't want it to be a headache so i'll try to think of more solutions i'm comfortable enough with java , but i'm not so comfortable that i'm closed to other solutions . i've never programmed java for specifically optimization before but i figured this would be a good exercise in doing so . -i think the answer to having users running their own db instances locally is that this is the only solution . -basically the application needs to make suggestions and change the gui layout based on results from an incremental scoring algorithm and possibly deciding whether someone is good for something using k-cluster means . it's already starting to be a headache trying to work around structuring it but i'm hoping it will be a good headache and you're not being a dick , i would rather have it straight than anything else . i'm curious what the rest api solution-job scheduler in full might look like , especially since there are tasks down the road that might involve something more complex . let me know if i can pm you or something EOA 
 structure for ml partner system to an application : machinelearning not sure i completely understand the scope of the problem yet , but here are some questions you can ask yourself : -is it absolutely necessary to use two databases ? postgres and mongo are both perfectly respectable databases in their own right , but using both seems excessive without a specific requirement . there is opportunity to simplify by using one db . -should i be concerned about java performance ? how comfortable are you with java ? used well , java can outperform many other languages on the jvm . used poorly , it can be a nightmare . performance has more to do with how well it's programmed . if concerned , learn best-practice java optimization methods and use a good profiler to scan for bottlenecks . -what are some things i need to worry about ? keeping two dbs in sync is always fraught with peril , especially when one is sql and one is nosql/document store . these are two different worlds , and mixing them is not recommended . as with #1 above , consider simplifying to use one db if possible . -is it necessary to have users running their own db instances locally ? if they need to cache their own client data , are there other , simpler schemes for accomplishing this ? ( the answer may be no , but it's worth looking into ) -change the format of the application and other things depending on how they use it this seems horribly vague ; could you please expand on this a bit ? prima facie , this seems like a recipe for headache . being completely honest , and not trying to be a d-ck (i swear!), from the information given this setup seems unnecessarily complex and a bit convoluted . it seems you'll be able to get away with some simple java web services and a rest api-job scheduler , but again i don't completely understand the nature of the problem . i may be missing key information . architecture diagrams far more informative and may yield better answers . good luck . EOQ python seems to be the way to go for machine learning nowadays thanks to the excellent scikit learn library etc . why java ? django-scikit learn provide a great base for quick iterative prototypes to production . same with nlp , gensim and spacy are really good libraries , and everything is easy to integrate . if you go that route i'd use postresql and add caching as you need . really depends how long your models take to run , and how often the results are requested . EOA 
 structure for ml partner system to an application : machinelearning not sure i completely understand the scope of the problem yet , but here are some questions you can ask yourself : -is it absolutely necessary to use two databases ? postgres and mongo are both perfectly respectable databases in their own right , but using both seems excessive without a specific requirement . there is opportunity to simplify by using one db . -should i be concerned about java performance ? how comfortable are you with java ? used well , java can outperform many other languages on the jvm . used poorly , it can be a nightmare . performance has more to do with how well it's programmed . if concerned , learn best-practice java optimization methods and use a good profiler to scan for bottlenecks . -what are some things i need to worry about ? keeping two dbs in sync is always fraught with peril , especially when one is sql and one is nosql/document store . these are two different worlds , and mixing them is not recommended . as with #1 above , consider simplifying to use one db if possible . -is it necessary to have users running their own db instances locally ? if they need to cache their own client data , are there other , simpler schemes for accomplishing this ? ( the answer may be no , but it's worth looking into ) -change the format of the application and other things depending on how they use it this seems horribly vague ; could you please expand on this a bit ? prima facie , this seems like a recipe for headache . being completely honest , and not trying to be a d-ck (i swear!), from the information given this setup seems unnecessarily complex and a bit convoluted . it seems you'll be able to get away with some simple java web services and a rest api-job scheduler , but again i don't completely understand the nature of the problem . i may be missing key information . architecture diagrams far more informative and may yield better answers . good luck . EOQ java mostly because i'm comfortable with it , but if i do make the switch i guess its got to be now . i have to do some more mental work in my head but , noted EOA 
 why is the activation derivative needed in the weight update rule of the back propagation algorithm ? : machinelearning backprop is calculating the now , in backprop , you have a nested function , f(g(x))-y and this-( target ? output )-f'(input)-is basically how the chain rule works for nested functions like this . if some very simple posts EOA 
 why is the activation derivative needed in the weight update rule of the back propagation algorithm ? : machinelearning backprop is calculating the the short answer is because we use the chain rule to simplify the calculation of the gradient of the objective function with respect to the weights . for the long answer , i suggest you watch the following video : URL the part specifically related to your question starts at ~24 minutes but the whole video really is worth watching . let me know if you have questions about the video and i can try to work through them with you . in regards to setting f'(input) to a constant , this would allow only information about how the overall activation affects the objective function to be used when calculating the weight update . in other words , this would eliminate information about how the weights affect the activation ( and therefore , the objective ) for the weight update . it makes sense that it would take longer to converge if it converges at all since you are reducing the amount of information you have about how each weight affects the objective . EOA 
 mxnet in production ? : machinelearning i use mxnet for some time. here are the some pros and cons i can point ; pros;-to my knowledge , it is the most memory friendly library . even very large models like vgg can fit into NUM gb memory in trining time. it is not imaginable with caffe for instance . that's why i partially quit caffe .-i has variety of interfaces . javascript, python , r , matlab , so on and all these interfaces are backed well by the community .-it is very easy to utilize multi-gpu training by the design , interms of data or model partition over gpus .-it has a good data iterator structure where caffe is also lacking . you can load data in variety of type and apply very wide-range of augmentation techniques .-they provide good pre-trained networks on imagener . (even still waiting for resnet :). this is great for low budget guys like me .-community is very reposnive and helpful . they care any of your issue regardless of how stupid it is . thanks guys ! cons :-my big pain about mxnet is , it is likely to give different results with different compile settings . for instance one model works fine with cudnn3 but not cudnn4 or vice a versa . it posits a forgettable problem when you experiment sicen you can observer ridiculous results and cannot see the results .-it is a bit hard to write your own layers if you like to have efficiency by low level typings . mxnet uses its own backend tensor library and it is a bit weird without much of inspection . also, it falls short o documentation for coding custom layers . this is all for now , i'll add any new thing in mind later . hope it works . EOQ that was very detailed . thanks! EOA 
 did anyone download the deepmind go pdf ? : machinelearning URL i'm guessing nature wants money so they asked to take the free hosting down . EOQ awesome , thank you EOA 
 did anyone download the deepmind go pdf ? : machinelearning URL i'm guessing nature wants money so they asked to take the free hosting down . EOQ URL half the time , google scholar is the answer ; the other half , libgen/scihub is . EOA 
 did anyone download the deepmind go pdf ? : machinelearning URL i'm guessing nature wants money so they asked to take the free hosting down . EOQ i thought libgen was down ... EOA 
 did anyone download the deepmind go pdf ? : machinelearning URL i'm guessing nature wants money so they asked to take the free hosting down . EOQ it will always be up in our hearts . and other domains . EOA 
 did anyone download the deepmind go pdf ? : machinelearning URL i'm guessing nature wants money so they asked to take the free hosting down . EOQ URL EOA 
 what is in your opinion the best amazon book ( or another resource ) to learn basics about reinforcement learning ? : machinelearning that's an easy one :p URL they are also working on a second version . it's on the website as well . also, there's a mini-course and full course on rl on udacity . EOQ URL EOA 
 what is in your opinion the best amazon book ( or another resource ) to learn basics about reinforcement learning ? : machinelearning that's an easy one :p URL they are also working on a second version . it's on the website as well . also, there's a mini-course and full course on rl on udacity . EOQ there is a long literature that pre-dates the term reinforcement learning on bandit problems and markov decision processes . i'd recommend reading some of those papers to get some history and context . EOA 
 what is in your opinion the best amazon book ( or another resource ) to learn basics about reinforcement learning ? : machinelearning that's an easy one :p URL they are also working on a second version . it's on the website as well . also, there's a mini-course and full course on rl on udacity . EOQ i've heard that someone is going to write a book on amazon's machine learning platform . i would recommend learning to use that instead of this reinforcement learning , which is probably just some outdated convex optimization tool for people who are too old to learn to use deep learning . EOA 
 word2vec and vector origin : machinelearning i see ! so if you want a NUM dimensional word vector , you would need v x h-NUM ? EOQ edited . mostly correct . i think you meant to say v x h-v x NUM if you wanted a NUM-dimensional word embedding/vector . you can see it in action with tensor flow here . it's from a udacity course on deep learning taught by some googlers . EOA 
 word2vec and vector origin : machinelearning i see ! so if you want a NUM dimensional word vector , you would need v x h-NUM ? EOQ nice , i had started that course , but didn't get there yet . i think i get it . you have NUM hidden units and maybe NUM or even NUM million input nodes , ( the size of your vocab ) . but since they're one-hot vectors , only one neuron in the input fires to the NUM hidden units . so the weights from that neuron to the hidden units , that's your word vector . does this make sense ? EOA 
 word2vec and vector origin : machinelearning i see ! so if you want a NUM dimensional word vector , you would need v x h-NUM ? EOQ the inputs are one-hot encodings of words , which try to predict a one-hot encoding of another word . think of it this way . the network can not be NUM % sure about the next word . it just can assign probability to what can come after the current word . this means instead of having NUM for an word in output vector , we have a double number that shows the probability of that word . note that if you are using a v-dimension one-hot coding for your dictionary , the output layer of nn will be v-dimension . of course dimensionality reduction can be applied . EOA 
 word2vec and vector origin : machinelearning i see ! so if you want a NUM dimensional word vector , you would need v x h-NUM ? EOQ it's not true . the input vector v.in[i]-w.in-e.i , where e.i is the one-hot vector , which return ith column of matrix w.in . it's is the same for output vector : v.out[i]-w.out-e(i) . EOA 
 word2vec and vector origin : machinelearning i see ! so if you want a NUM dimensional word vector , you would need v x h-NUM ? EOQ a one-hot vector as input to a matrix multiply is secretly just a really , really slow lookup table . EOA 
 word2vec and vector origin : machinelearning i see ! so if you want a NUM dimensional word vector , you would need v x h-NUM ? EOQ i'm also fairly new to this but here is my understanding : they are a result of solving the optimization problem described in the paper . the one hot word vectors are just one of the inputs into the prediction function , these word vectors are another . the algorithm modifies them until performance converges . EOA 
 what's the most spectacular disruptions that advances in machine learning might realistically cause : machinelearning dirty humans making food with the same glove the just handled money with , will need to find a new job . either that or NUM st world countries continue their decent into NUM rd world food safety standards . EOQ autonomous weapons would rank pretty highly . EOA 
 what's the most spectacular disruptions that advances in machine learning might realistically cause : machinelearning dirty humans making food with the same glove the just handled money with , will need to find a new job . either that or NUM st world countries continue their decent into NUM rd world food safety standards . EOQ in NUM years , self driving cars will be on the roads used by regular people . not everyone will use them but some people will be . in at least the major cities to start . tesla , uber, toyota , bmw, audi , etc. are already testing their prototypes . source? just google it EOA 
 what's the most spectacular disruptions that advances in machine learning might realistically cause : machinelearning dirty humans making food with the same glove the just handled money with , will need to find a new job . either that or NUM st world countries continue their decent into NUM rd world food safety standards . EOQ [ deleted ] EOA 
 what's the most spectacular disruptions that advances in machine learning might realistically cause : machinelearning dirty humans making food with the same glove the just handled money with , will need to find a new job . either that or NUM st world countries continue their decent into NUM rd world food safety standards . EOQ lol i updated it to give some more color EOA 
 highway networks are to deep residual networks what lstms are to ...? : machinelearning it's called an lstm . that's what you will find in the original lstm paper ( URL ) . the gate on the 'context' ( called the forget gate ) was added in NUM . the core idea of the lstm was replacing new.state-f(new.info-old.state) with new.state-f(new.info)-old.state . this was called the constant error carousel ( see section NUM of the lstm paper ) . since then many components have been added/removed around this core idea . EOQ a key insight of the highway networks paper was the residual blocks-the network didn't do anything special if they didn't skip layers . i'm not sure how you do this in the temporal dimension ; maybe three different layers that you swap between each step ? seems plausible . EOA 
 highway networks are to deep residual networks what lstms are to ...? : machinelearning it's called an lstm . that's what you will find in the original lstm paper ( URL ) . the gate on the 'context' ( called the forget gate ) was added in NUM . the core idea of the lstm was replacing new.state-f(new.info-old.state) with new.state-f(new.info)-old.state . this was called the constant error carousel ( see section NUM of the lstm paper ) . since then many components have been added/removed around this core idea . EOQ it sounds like you want to know if there are gate-less lstms because skip connections worked well for solving a different problem . there are such a thing , sort of , for the temporal dimension called clockwork rnns [ NUM ] and then the gated version ( i.e. highway network ) would be gf-rnn [ NUM ] . but all of these analogies are tenuous . [ NUM ] URL [ NUM ] URL edit : temporary-> ; temporal EOA 
 highway networks are to deep residual networks what lstms are to ...? : machinelearning it's called an lstm . that's what you will find in the original lstm paper ( URL ) . the gate on the 'context' ( called the forget gate ) was added in NUM . the core idea of the lstm was replacing new.state-f(new.info-old.state) with new.state-f(new.info)-old.state . this was called the constant error carousel ( see section NUM of the lstm paper ) . since then many components have been added/removed around this core idea . EOQ see temporal kernel rnns with by sutskever and hinton . they use leaky integrators . also , there is a minor yet important difference between highway networks and resnets . highway type gating means it is a smooth but exclusive or gate . resnets are like and gates edit : the last sentence is true only for one of the variants of highway network in the nips paper , not the general formulation . EOA 
 highway networks are to deep residual networks what lstms are to ...? : machinelearning it's called an lstm . that's what you will find in the original lstm paper ( URL ) . the gate on the 'context' ( called the forget gate ) was added in NUM . the core idea of the lstm was replacing new.state-f(new.info-old.state) with new.state-f(new.info)-old.state . this was called the constant error carousel ( see section NUM of the lstm paper ) . since then many components have been added/removed around this core idea . EOQ if you're doing it in the temporal direction , then its just a window . the input to forward-prop each timestep is , say, the data available for the preceding three timesteps . EOA 
 highway networks are to deep residual networks what lstms are to ...? : machinelearning it's called an lstm . that's what you will find in the original lstm paper ( URL ) . the gate on the 'context' ( called the forget gate ) was added in NUM . the core idea of the lstm was replacing new.state-f(new.info-old.state) with new.state-f(new.info)-old.state . this was called the constant error carousel ( see section NUM of the lstm paper ) . since then many components have been added/removed around this core idea . EOQ statistics : machine learning :: starcraft : league EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ at ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i think he meant either at the code or at a more detailed explanation . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ the code is fairly standard and most of the effort goes into etl process for the data . i use historical data available for free and scrape additional stuff from websites , so a lot for normalization is involved . after this , i create artificial attributes(through slightly different methods for each league) from historical data and create decision trees . based on the betting odds , and the regression accuracy for the specific event type ( for example , home team winning in league x ) , i estimate an roi . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ it would be awesome if you provide the code anyway-) EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ what is the best sites for this kind of data ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ sites that offer data as raw as possible-you can either get it directly via http or scrape the markup . something like xscores.com seems to be easier to scare , but there are still issues . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ looks interesting . i did something similar in the past and i found matching the team names from multiple sites to each other is quite painful . basically lots of sites write team names differently . did you also experience that ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ hell yes , i have a dedicated class just for name normalization . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ interesting , i wanted to do a similar thing with tennis . what features are you using ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ do you mean learning attributes ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i would guess the question is 'what input is going into the model' ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ a series of dynamically generated attributes from historical matches like : last x matches points , last x h2h matches points , table position for each team , ispromoting, isrelegating , team wealth , average scored/conceded goals . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ how many days worth of historical data do you include ? do you do something to assign greater relevance to more recent games ? and what are preferred market , asian over/under or NUM x2 ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ great questions . i use a varying number of seasons , depending on league volatility ( for example in bpl teams change a lot , so i can't use very old data ) , usually going NUM years back . by default , all matches have identical weight , but i created a momentum attribute that takes more recent wins into account . i usually determine the most accurate outcome(that best describes the data) based on the outcome of each team behavioral model . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ have you done any backtesting ? it seems interesting , but i don't know how reliable a model like this can be . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ what do you mean backtesting ? regressing against historical data ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ exactly . evaluating the predictions for past matches . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ yes , i do regression testing against several season stages with the equivalent model . i cannot quite do full cross validation because of the chronology of events ( i cannot use future scores to predict old ones ) . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ has it made you any money ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i don't bet much , just enough to have a good overview . but overall yes , it has been profitable . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ how does your bot's predicted probability fares against the odds are betting house ? does it follow them closely ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ not always , and i intended it that way . if predict events exactly as well as the house , you end up losing in the long run . i provide an roi which is the ratio of historical odds to house odds . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ roi as in return on investment ? that's not the ratio of your odds to the bookie odds . though i get what you are saying , the approach is quite typical , you only bet on an event if you disagree with odds from the bookies EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i know , it's bad namimg and still experimental . yes, if they underestimate the odds EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ it would be very useful to present your backtesting results with some charts . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i'll make a note of that and edit this post with some results . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ what's the key to the outcome column here URL ? i'm uncertain as to the meaning of NUM , NUM x, x , x2 etc . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ is standard betting notation-NUM (home team to win) , NUM (away team to win) , x(draw) , > ; x.5 ( more than x goals ) . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ ah thanks . i was wondering if it was a betting thing , not being a betting man myself . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ so , in todays watford/chelsea game , this recommends playing both watford ( NUM 8 ) to win and chelsea ( NUM 0 ) to win in the same game ? you have about a NUM % chance to win this game . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i think this particular one is probably wrong due to a wrong expected odd (score should be around NUM ). this would mean that you have an roi of NUM % your investment . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ if i may ask ... where do you get your data from ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ www.football-data.co.uk/data.php for match history , and i scrape live score sites for current matches ( for prediction ) . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i am working on something similar atm . though the data from football-data.co.uk is a mess ... took me quite a while to preprocess them and bring them in usable format . but if i may ask , how do you calculate the team wealth ? and which sites are good to scrape actual matchdata from ? most found are hard to grab data off ... EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ i found out ea's fifa has some pretty interesting data and certaintly it has a lot of it . one can search throughout the web sites such as URL that have useful information ( this is mostly about the players but i cannot stress enough that they a lot of data down there ) EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ it seems like you are placing some bet on every single game , isn't it a bit optimistic to think you are better informed than the bookies on all games ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ a bet is not placed on every game , this has to be decided upon roi . there is difference in prediction models and hopefully your is more true to real life events that the other . you will probably never be better informed , but you might be able to use more relevant data . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ but today you do recommend bets on all NUM italian games , all NUM french games and on both premier league games , right? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ no . this is an exhaustive analysis of certain leagues . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ how does the roi relate to confidence ? when roi is higher , then confidence is higher ? from the data , it seems they are not related actually ? basically for betting we should just follow the confidence , right? if it is high , then bet , it low , then nobet . if so , what does the roi stand for ? EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ they are related , in the fact that they are based on an internal expected probability . usually, you should profit when betting on games with roi > ; NUM ( not really correct , this should be > ;0 theoretically ) . you can expect an average return ( initial bet-profit ) of roi-bet . high confidence bets are good , but really you want better odds-if you bet on a NUM % sure thing that the house expects to be NUM % sure , it's a bad investment . it's more profitable to bet on a NUM % sure thing with NUM % estimated odds . ex : if you have see an roi NUM , this means you have an roi of NUM % ( i'll change this in the next prediction ) . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ honestly , after seeing tonight's matches and your predictions , i think this approach is too simple for correct predictions , let alone for being profitable . then again , if it was so simple to win at betting , everyone would do this . however tweaking and improving your model can be a fun and educational experience . EOA 
 match prediction bot ( x-post from /r/soccerbetting ) : machinelearning can we have a peak ? :) EOQ tuning is done per-league : we had NUM % in ligue NUM and NUM % in serie a , so further inspection of this is needed . EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ i will answer the general question . yes , there is an alternative to penalty functions : constraints. you construct an acceptable region for parameters , and if you learning step pushes parameters outside this area , you pull them back to a point which is close to the desired point you tried to learn to and on or near the constraint boundary . in practice the advantage i have found is that good values of the free parameters ( e.g. size of acceptance region ) are easier to guess at than penalty function coefficients , and they stay more universal across problems and datasets . it can be harder to implement constraints other than spherical or ellipsoidal as the projection step algorithm may require some more nontrivial work or thought than inventing a penalty function . for instance , there is an algorithm to find the closest point in l2 to a l1 ( lasso ) boundary , but it is not obvious . but when it's possible , i prefer them to penalties . in the problem at hand the op is looking for sparsification , and there is likely a big literature on this problem in many domains . at a minimum a sum penalty of abs(a.i)c for c smaller than NUM may push in that direction . not sure how to implement as a constraint . EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ thanks man !, your comment was the most useful !! EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ dropout and early stopping are both examples of regularization that don't involve ( explicitly ) penalizing a cost function . EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ thanks for your answer , but i think you didn't understand what i was needing . i just don't want to regularize. i want to regularize a subset of parameters , such that only one of them is high , rest are low . EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ so regularize just those parameters ? EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ i still don't understand what you're looking for . what's wrong with adding a penalty term to the cost function ? EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ i've just been looking at adversarial autoencoders , and they would be useful to achieve things like this . ( URL ) just pick the distribution of random inputs to the discrimination network to have the distribution you require. EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ thanks for your answer i think i should have specified , no randomness . EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ i think your initial intuition could actually be the way to go . just replace your parameters w with w'-softmax(w) . now use gradient descent to learn the inner w , whereas w' are now the parameters used externally . using the softmax transform should 'nonlinearize' the sgd updates in the way that you want . you could control the temp over time as a hyperparameter controlling uncertainty . start high , end up low with a single max . you can use an l2 regularization on the w params as well , and this should transform in the right way to a different kind of prior for w' . EOA 
 are there any methods to regularize parameters other than penalizing the cost function with a metric that measures extent of unwanted behavior ? : machinelearning could you be any more vague ? EOQ thanks ! EOA 
 cntk or deeplearning4j ? what do you think will be better for a scalable image classification solution : machinelearning dl4j's main strength has been recurrent nets and nlp-the gui . the convolution support hasn't been the best strength of the framework . we are working on improving a lot of it and we are happy to hear feedback . scaling is relative. dl4j does data paralellism on spark . this works for certain problems but not all . a lot of frameworks have great support for image classification though . to be fair : the python ecosystem has a lot of great starter kits for handling etl . dl4j will be able to help with building data pipelines . dl4j's main value is how it integrates with other jvm frameworks . that may not matter for your use case . it will also depend on the model sizes : eg, imagenet winners aren't going to really run well on spark due to heap space issues . re : users. you're right we aren't used in a lot of research where most of the dl is . that's also not the core focus . other frameworks do great for research use cases . features we add/prioritize matter more to say : banks or telcos than a researcher . very different focus than the other frameworks . we try to have as many features as possible but we're not aimed at critical mass in research and we don't need to be . at the end of the day we are a for profit company not a research group . EOQ try deepdetect if you want a commodity more than a lib EOA 
 cntk or deeplearning4j ? what do you think will be better for a scalable image classification solution : machinelearning dl4j's main strength has been recurrent nets and nlp-the gui . the convolution support hasn't been the best strength of the framework . we are working on improving a lot of it and we are happy to hear feedback . scaling is relative. dl4j does data paralellism on spark . this works for certain problems but not all . a lot of frameworks have great support for image classification though . to be fair : the python ecosystem has a lot of great starter kits for handling etl . dl4j will be able to help with building data pipelines . dl4j's main value is how it integrates with other jvm frameworks . that may not matter for your use case . it will also depend on the model sizes : eg, imagenet winners aren't going to really run well on spark due to heap space issues . re : users. you're right we aren't used in a lot of research where most of the dl is . that's also not the core focus . other frameworks do great for research use cases . features we add/prioritize matter more to say : banks or telcos than a researcher . very different focus than the other frameworks . we try to have as many features as possible but we're not aimed at critical mass in research and we don't need to be . at the end of the day we are a for profit company not a research group . EOQ why not mxnet or neon if scaling is your main concern ? EOA 
 cntk or deeplearning4j ? what do you think will be better for a scalable image classification solution : machinelearning dl4j's main strength has been recurrent nets and nlp-the gui . the convolution support hasn't been the best strength of the framework . we are working on improving a lot of it and we are happy to hear feedback . scaling is relative. dl4j does data paralellism on spark . this works for certain problems but not all . a lot of frameworks have great support for image classification though . to be fair : the python ecosystem has a lot of great starter kits for handling etl . dl4j will be able to help with building data pipelines . dl4j's main value is how it integrates with other jvm frameworks . that may not matter for your use case . it will also depend on the model sizes : eg, imagenet winners aren't going to really run well on spark due to heap space issues . re : users. you're right we aren't used in a lot of research where most of the dl is . that's also not the core focus . other frameworks do great for research use cases . features we add/prioritize matter more to say : banks or telcos than a researcher . very different focus than the other frameworks . we try to have as many features as possible but we're not aimed at critical mass in research and we don't need to be . at the end of the day we are a for profit company not a research group . EOQ dl4j co-creator here . deeplearning4j's actually used pretty widely , but it's a work in progress . the most used parts of the lib right now are rnn/lstms , word2vec/doc2vec and multilayer perceptrons . the convnet architectures can use improving . caffe's probably the best place to go for that right now . that said , you can combine convnets with rnns on dl4j to work with video and so forth . it's also easily extensible . the main drawback for the moment is that we're doing a cuda rewrite , which means gpus don't work as fast as they should , so we recommend people stick with cpus . some people think dl is python-gpus , which is fine , but it doesn't solve everybody's problem , especially not on an enterprise stack , which is what dl4j was built for . the dl4j community's active and growing , although it may not be the biggest . we've got NUM devs on the gitter channel , and they're using the lib for all kinds of things . lot of fraud detection , recommender systems , time series prediction , search etc . URL EOA 
 cntk or deeplearning4j ? what do you think will be better for a scalable image classification solution : machinelearning dl4j's main strength has been recurrent nets and nlp-the gui . the convolution support hasn't been the best strength of the framework . we are working on improving a lot of it and we are happy to hear feedback . scaling is relative. dl4j does data paralellism on spark . this works for certain problems but not all . a lot of frameworks have great support for image classification though . to be fair : the python ecosystem has a lot of great starter kits for handling etl . dl4j will be able to help with building data pipelines . dl4j's main value is how it integrates with other jvm frameworks . that may not matter for your use case . it will also depend on the model sizes : eg, imagenet winners aren't going to really run well on spark due to heap space issues . re : users. you're right we aren't used in a lot of research where most of the dl is . that's also not the core focus . other frameworks do great for research use cases . features we add/prioritize matter more to say : banks or telcos than a researcher . very different focus than the other frameworks . we try to have as many features as possible but we're not aimed at critical mass in research and we don't need to be . at the end of the day we are a for profit company not a research group . EOQ dl4j is a steaming pile of shit . caffe is a pretty proven solution for production . but really scaling is mostly going to depend on other parts of your stack , since any framework will be able to saturate your gpu/cpu . EOA 
 cntk or deeplearning4j ? what do you think will be better for a scalable image classification solution : machinelearning dl4j's main strength has been recurrent nets and nlp-the gui . the convolution support hasn't been the best strength of the framework . we are working on improving a lot of it and we are happy to hear feedback . scaling is relative. dl4j does data paralellism on spark . this works for certain problems but not all . a lot of frameworks have great support for image classification though . to be fair : the python ecosystem has a lot of great starter kits for handling etl . dl4j will be able to help with building data pipelines . dl4j's main value is how it integrates with other jvm frameworks . that may not matter for your use case . it will also depend on the model sizes : eg, imagenet winners aren't going to really run well on spark due to heap space issues . re : users. you're right we aren't used in a lot of research where most of the dl is . that's also not the core focus . other frameworks do great for research use cases . features we add/prioritize matter more to say : banks or telcos than a researcher . very different focus than the other frameworks . we try to have as many features as possible but we're not aimed at critical mass in research and we don't need to be . at the end of the day we are a for profit company not a research group . EOQ what's so bad about dl4j ? EOA 
 cntk or deeplearning4j ? what do you think will be better for a scalable image classification solution : machinelearning dl4j's main strength has been recurrent nets and nlp-the gui . the convolution support hasn't been the best strength of the framework . we are working on improving a lot of it and we are happy to hear feedback . scaling is relative. dl4j does data paralellism on spark . this works for certain problems but not all . a lot of frameworks have great support for image classification though . to be fair : the python ecosystem has a lot of great starter kits for handling etl . dl4j will be able to help with building data pipelines . dl4j's main value is how it integrates with other jvm frameworks . that may not matter for your use case . it will also depend on the model sizes : eg, imagenet winners aren't going to really run well on spark due to heap space issues . re : users. you're right we aren't used in a lot of research where most of the dl is . that's also not the core focus . other frameworks do great for research use cases . features we add/prioritize matter more to say : banks or telcos than a researcher . very different focus than the other frameworks . we try to have as many features as possible but we're not aimed at critical mass in research and we don't need to be . at the end of the day we are a for profit company not a research group . EOQ can you say what kind problems arise with dl4j EOA 
 cntk or deeplearning4j ? what do you think will be better for a scalable image classification solution : machinelearning dl4j's main strength has been recurrent nets and nlp-the gui . the convolution support hasn't been the best strength of the framework . we are working on improving a lot of it and we are happy to hear feedback . scaling is relative. dl4j does data paralellism on spark . this works for certain problems but not all . a lot of frameworks have great support for image classification though . to be fair : the python ecosystem has a lot of great starter kits for handling etl . dl4j will be able to help with building data pipelines . dl4j's main value is how it integrates with other jvm frameworks . that may not matter for your use case . it will also depend on the model sizes : eg, imagenet winners aren't going to really run well on spark due to heap space issues . re : users. you're right we aren't used in a lot of research where most of the dl is . that's also not the core focus . other frameworks do great for research use cases . features we add/prioritize matter more to say : banks or telcos than a researcher . very different focus than the other frameworks . we try to have as many features as possible but we're not aimed at critical mass in research and we don't need to be . at the end of the day we are a for profit company not a research group . EOQ not sure what he's replying to here , but for this use case running image net models on spark isn't ideal . the number of params insanely large and is better for model parlallelism . dl4j mainly does data paralellism which can work great for certain classes of problems . our new c/c-core will be integrating with a param server which should help take care of a lot of that . EOA 
 are there any chess ai's that use unsupervised learning , and start from being bad at chess to becoming very good ? : machinelearning you probably mean reinforcement learning in this case . if it is allowed to learn from playing games , that isn't quite unsupervised . see tdleaf(lambda) , the chess ai improved from NUM to NUM in NUM games . URL EOQ can it learn by playing against itself though ? or against a random-moves opponent ? or is that totally ineffective for some reason ? EOA 
 are there any chess ai's that use unsupervised learning , and start from being bad at chess to becoming very good ? : machinelearning you probably mean reinforcement learning in this case . if it is allowed to learn from playing games , that isn't quite unsupervised . see tdleaf(lambda) , the chess ai improved from NUM to NUM in NUM games . URL EOQ cool , thanks. EOA 
 are there any chess ai's that use unsupervised learning , and start from being bad at chess to becoming very good ? : machinelearning you probably mean reinforcement learning in this case . if it is allowed to learn from playing games , that isn't quite unsupervised . see tdleaf(lambda) , the chess ai improved from NUM to NUM in NUM games . URL EOQ URL and URL come to mind but nothing you can play against on the web ( if that was what you were asking ) EOA 
 are there any chess ai's that use unsupervised learning , and start from being bad at chess to becoming very good ? : machinelearning you probably mean reinforcement learning in this case . if it is allowed to learn from playing games , that isn't quite unsupervised . see tdleaf(lambda) , the chess ai improved from NUM to NUM in NUM games . URL EOQ knightcap URL EOA 
 are there any chess ai's that use unsupervised learning , and start from being bad at chess to becoming very good ? : machinelearning you probably mean reinforcement learning in this case . if it is allowed to learn from playing games , that isn't quite unsupervised . see tdleaf(lambda) , the chess ai improved from NUM to NUM in NUM games . URL EOQ will check it out , thanks. EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ it depends which class . the andrew ng class is good . the hinton class is good . the data science specialization is good . the data mining specialization is great . the machine learning specialization is pointless . the program at udacity is good , but kind of shallow , just not very deep . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ the hinton class is awful . i haven't seen a more dry presentation of data since al gore released an expose of his tie rack . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ hah ok yeah i have to concede you're right about that EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ could be a good start . right after , take a look to ng's lectures in the stanford engineering everywhere online program (course code should be cs229 , but google will help you find out ;-)). EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ pay for ? well, i guess it's nice to keep coursera alive , but i shouldn't think you'd need the formal credentials if you're already working for a hedge fund . i liked it . people say it's outdated now , but i don't necessarily agree-there are still plenty of problems best solved with simple regression , and it helps to see what people have tried before . ng's course made me much better at reading ( and write ) heavily vectorized code , and taught me plenty of the basics which helps me immensely understanding e.g. torch code these days . of course , maybe you already know the basics and how to think and code with everything vectorized . in that case it may not be worth it . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ andrew ngs course is extremely helpful for someone without much machine learning/mathematical background . it illustrates concepts with easy to follow examples and labs at the end to enforce what was learned . for a more in depth version , i recommend watching his stanford lectures . you can find the , easily on youtube . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ not really . too basic , too outdated . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ what about the course offered by google at udacity . it's called deep learning i think . is it good ? EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ shallow crap . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ what other courses would you recommend that is from coursera , udacity and etc ...? EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ for a start i'd totally recommend andrew ngs coursera course . he goes into quite a few algorithms from linear models to neural networks and svms , clustering, pca , recommender systems .. and he explains quite a few best practices that'll later spare you from making some stupid mistakes . i think that's quite a good foundation to get you going .. from there just choose some ml libraries ( like sklearn , theano, keras , whatever,.. ) and read their tutorials . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ basic yes , outdated not completely . it may lack the latest advancements but the ml staple algorithms are pretty much there . EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ mah nikka EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ depends on you . why do you want to learn about machine learning ? EOA 
 coursera machine learning worth it ? : machinelearning i liked it . yes, it's basic , but for someone who never encountered machine learning , it's a great starting point . it exposes you to many important concepts and algorithms , it has you try it yourself , implement the stuff ... you can then go deeper into stuff you want to/need to use irl . and that google course-basically a tensorflow tutorial :d not bad , but not really a deep learning course . but hey , it taught me how to use tensorflow , it's the definitely the best tutorial for it i've seen thus far , so if you want to learn tf , go for it . EOQ i want to embed my brain with machine learning techniques EOA 
 best deep learning framework for deploying on ios/android ? : machinelearning mxnet is ideal . it supports android and ios both : URL android example URL ios example URL EOQ do you know what it uses for convolution and dot product ? EOA 
 best deep learning framework for deploying on ios/android ? : machinelearning mxnet is ideal . it supports android and ios both : URL android example URL ios example URL EOQ i hear tensorflow is fast on cpu , but i don't know if it specifically works well on android . related question , does anyone know if any deep learning framework allows the use of gemmlowp for efficient NUM bit fixed point neural nets ? maybe tensorflow does ? URL EOA 
 best deep learning framework for deploying on ios/android ? : machinelearning mxnet is ideal . it supports android and ios both : URL android example URL ios example URL EOQ uhm . memkite? EOA 
 best deep learning framework for deploying on ios/android ? : machinelearning mxnet is ideal . it supports android and ios both : URL android example URL ios example URL EOQ i've had this same question for quite a while. i know torch has android/ios ports . i remember the jetpac app team a while back had demo deep learning apps for mobile that were fairly fast . better question-do any of these frameworks successfully use gpu or any hardware acceleration ? this package claims to do so for ios-URL , has anyone tried it out ? EOA 
 best deep learning framework for deploying on ios/android ? : machinelearning mxnet is ideal . it supports android and ios both : URL android example URL ios example URL EOQ python EOA 
 best deep learning framework for deploying on ios/android ? : machinelearning mxnet is ideal . it supports android and ios both : URL android example URL ios example URL EOQ deeplearning4j covers android . EOA 
 how can i find papers about the composition of word vectors in order to obtain higher level representation ? : machinelearning try semantic scholar : URL or look directly on the acl anthology : URL acl , emnlp, tacl and naacl are the most likely venues . EOQ try compositional distributional semantics . EOA 
 how can i find papers about the composition of word vectors in order to obtain higher level representation ? : machinelearning try semantic scholar : URL or look directly on the acl anthology : URL acl , emnlp, tacl and naacl are the most likely venues . EOQ google EOA 
 how can i find papers about the composition of word vectors in order to obtain higher level representation ? : machinelearning try semantic scholar : URL or look directly on the acl anthology : URL acl , emnlp, tacl and naacl are the most likely venues . EOQ this is always a good first thing to try : URL URL EOA 
 what are some open/active research areas in deep learning currently ? : machinelearning check out the deep learning book by ian goodfellow , yoshua bengio , and aaron courville . it was a section on deep learning research . URL otherwise look at last year's nips or similar conferences . see what papers interest you and try to do some simple modifications . there is also a lot of opportunity at this moment to find new , clever applications for deep learning . EOQ thanks . your suggestion helped . although i am going with unsupervised learning , but got to know some stuff being worked on . EOA 
 what are some open/active research areas in deep learning currently ? : machinelearning check out the deep learning book by ian goodfellow , yoshua bengio , and aaron courville . it was a section on deep learning research . URL otherwise look at last year's nips or similar conferences . see what papers interest you and try to do some simple modifications . there is also a lot of opportunity at this moment to find new , clever applications for deep learning . EOQ for some low(-ish) hanging fruit , try model compression-reducing the size of a large neural network to something much smaller and manageable . alternately , you can look at multi-modal deep learning-images , audio, text , etc can be combined to create a combined probabilistic / generative model . ( how cool is that? ) EOA 
 what are some open/active research areas in deep learning currently ? : machinelearning check out the deep learning book by ian goodfellow , yoshua bengio , and aaron courville . it was a section on deep learning research . URL otherwise look at last year's nips or similar conferences . see what papers interest you and try to do some simple modifications . there is also a lot of opportunity at this moment to find new , clever applications for deep learning . EOQ thanks for the help . i have worked a little on multimodal networks similar to andrej karpathy's neuraltalk . model compression is also a progress nowadays but i doubt that it'll be possible in NUM-4 months . will continue it though . thanks. EOA 
 what are some open/active research areas in deep learning currently ? : machinelearning check out the deep learning book by ian goodfellow , yoshua bengio , and aaron courville . it was a section on deep learning research . URL otherwise look at last year's nips or similar conferences . see what papers interest you and try to do some simple modifications . there is also a lot of opportunity at this moment to find new , clever applications for deep learning . EOQ basically two schools of thought : NUM . stack more layers NUM . stack less layers the first school is pretty dominant right now EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ thanks for the comment . there must be some way to derive meaning from the relations between the numbers if nothing else ? for example , averaging the vectors in a paragraph might lead to something meaningful ? i understand they are just the learned weights in a deep learning process , but still they must be meaningful because you can calculate the most similar words to a given words using the distance between their vectors EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ neither model in word2vec is a deep process . they are both just forms of log-bilinear models . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ @textclassy the individual numbers in the word-vectors can't be interpreted . you can , however, calculate the distance between word vectors as the degree of similarity between two words . this is used in data mining . in terms of scaling from word vectors to paragraphs , there's a lot of research in how to do that . averaging hasn't worked for me . the state of the art seems to be to learn sentence vectors by feeding the word vectors through an rnn , then learn paragraph vectors by feeding the sentence vectors through another rnn , & c . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ this is hand waving , but you could think of it as soft clustering . typically you see people plot a sample of the words with i'm not sure a simple stddev would get you general/specific though-the network is deciding how to best use its capacity . i'd expect it to use most of that capacity on frequent words unless there are some trivial gains from less common words ( say colocations that wouldn't otherwise be modeled ) . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ interesting , im going to have to learn about t-sne . also, what does handwaving mean ? EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ i meant it to indicate that the analogy of soft clustering is imprecise or vague but that it may be a useful way to understand the vector temporarily until you explore it more and come to a better understanding . usually i've heard the expression in reference to an oversimplification . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ very short intuition pump since its late here and i'm on the train home . imagine you know nothing of the meaning of cat , kitten, and dog . count the words that occur in , say, the same sentence . you will notice that cat and kitten frequently co-occur with words like meow and yarn than dog does . based on this observation , you may infer that they are more similar in meaning , as they are used similarly . this is standard distributional semantics , and stems from work in linguistics by harris , and by firth . it also bears some semblance to ( late ) wittgenstein's theory of meaning , although in a more quantitative form . some context words ( cute , meow ) are intuitively more informative of the semantics of words than other (the , and). count based methods account for this poorly , and normalisation schemes ( tf-idf ) may not be enough to overcome this on their own . enter models like word2vec , pmi matrix factorisation , etc. broadly put , but having words predict their context ( or vice versa ) , you are attempting to jointly model the relation between words via their contexts and the informativeness of the contexts . in doing so , arguably better representation of the lexical semantics of individual words ( since words generally have meaning in context , not on their own , in practice ) is obtained over traditional distributional models . in short , it helps to consider the history of vector-based approaches to modelling meaning before trying to get an intuition about word2vec vectors . the features may not be directly interpretable , but if you look at the big picture , it's somewhat clear ( to me at least ) what information the vectors are modelling . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ i've seen these : NUM pca or t-sne to map to a NUM-d or NUM-d space NUM . k-means NUM . converting the numbers to a heat map to see the patterns in common between words like this view of vectors made using glove instead of word2vec URL EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ you can't . that's part of what's so cool about it . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ this is getting into the theory . you might ask , why have NUM numbers in the vector ? why not NUM ? or NUM ? or NUM ? in fact , why not study the effect of different sizes of word vectors on your ability to , say, classify words , etc.? study the math that's actually going on in word2vec (probability , entropy, softmax , kl divergence , etc.). what does it all mean ????? the truth is that people have created algorithms such as word2vec without really understanding ( or at least publishing ) why and how they work the way that they do . there is more theoretical work that needs to be done . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ you could take a large data set , for examle the wikipedia corpus , then draw a few NUM k samples from it , let's say and get the vector for each one , then sort the words NUM times , each time for each axis , and show the top NUM words , the mid NUM and the lowest scoring NUM words . this could give you some intuition , don't know if this will work . if it works , it would be awesome if you could post some of your results here . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ hey check this out , definitely patterns here but there seem to be several things going on in each dimension , for example the last one seems to be spanish words ecologically oriented things , as well as financially oriented things : dimension NUM most neg reviewed review strengthened aprons ensure regulator doub matche jacketed bloomberg sox octagon experianced manded ensuring occu topside matchs remeasured phr most pos sleight literal cleverness evasion anthropomorphism forgoes semantic eloquence mythology tablature biblical microtransactions godlike parables solider reticent fervor parables evasive symbolism mythological dimension NUM most neg federally s7 unbuckled causal seatbelts testable skydivers contiguous omniscient heeler priori NUM c flamenco legally appointee NUM c handheld transponder rancheria equivalant most pos jun hyun summoning emptying turbulant steamrolling chul flowering perennial settling sequoia steels decayed loading blemished returnees bay sik ripening blooming dimension NUM most neg searchs penguins suckle divorce navigating adulthood backstage bootstrapper increasingly topside rutting waders hypercompetitive transition jobbers proofreader syndication offloading breakups ladder most pos deepest patted indo violator insult gigawatts unconditionally colliders ohh unsportsmanlike portuguese incitement reanalyzed particulates fissile pools facilities hundredth penalites supercollider dimension NUM most neg refutes contravening denies yesterday falsifying alleges investigating indictment besieged refuting nder disobeyed retaliated denied scalp refute misrepresented trounced fuming dispel most pos nachos reagan coffeemakers sweeties cato ketchup pajama republicanism theyd cheeseburgers profs cuppa federalist tupperware buckeyes mother's invariable shhhhh stoners they'd dimension NUM most neg cauterising alien unknowing unfixable growingly cooties unthinking freudian inoperable intermingled enablers transient hydrolytic gullible accusative oversensitive appendage dispensable uninvolved phantoms most pos winnning penitentiary scholarship clerkship glory rb winners goalscorers kicks fame granddaddy scorer glorious paydays career scorers settlement columned competitions halfs dimension NUM most neg boomed landed scooted boom tripled platted backflips sensex flipped rocketed quadrupled remaing hoisted runway legislated glided marshals marched snagged momentous most pos swordsman sids gratuit hellspawn drinker sludging masque luddite expert alcoholic fantasist phile germaphobe untrustworthy meanwhile warlock homeopath disciple exorcist souce dimension NUM most neg siders depleated strongholds NUM th sympathizers NUM f NUM th gigging jetting loyalists stalwarts NUM h NUM rd ast nfc ames NUM h NUM th NUM th xxxxxxxxxxxx most pos story inconsequential apt subscribe impractical convinced matter excusable ascribed frame strive supposes inputed fufilled forgivable fixated endeavor trivial fixate thetime dimension NUM most neg incandescence induction citrine demi topaz diamond rejuvenation reconstructive microdermabrasion minerals amber graphite petroleum diamonds colourway zenith perm turners specialization technicals most pos deter spyware downstate redact covenant backdoors posix dissuade nefarious suss discourage covenants verbatim choicer plagiarist downloader covertly coerce entrap muffle dimension NUM most neg newcomers uganda pilates gis edibles kandi newbies nutri technic hooping vied ians upo budding eac acro fiesta gung pulses lark most pos seaplane certainty mortality reopen nadir statistically liquidated privately survivor cringing coincidence fatality completion unnoted quietest longest anniversary conclusion closed publicly dimension NUM most neg ev3 recenter strategies mobilize biography medically coordinate advises clearance assistance coordinates authorization assess devise institutions strategizing strategize multidisciplinary chaperon ssi most pos maguro clinked drizzled admist anticlimax silvers exclaiming golds hast belchers thud shrieked mishearing ventanas unseasonable akbar drenched teensy thuds dripping dimension NUM most neg slivers glimmering sup shimmers reiterate chime sparkles crackles articulates throbs coo meaningfully desires reaffirm advances archived whisperings tradeoff echos holds most pos godchildren megafirm squadron tiring callups family gunny sapper sworn smurfing backrow orienteering sons siblings canoeing stepbrother schoolmates geocaching bushcraft hitmen dimension NUM most neg wads compound fowl jaeger contagion titty dunder inadvertently undergarments even agent intruders pigs biolabs pathogens hadnt agar fogger lyme binkies most pos j2 starpower fund parallelization palladium platinum rebooted smooths directionless rubles unaligned squarish mirroring replenishes challening overground megafirm unrepresented industrials i2 dimension NUM most neg ife gett aba ely maximo ere ys pres ger lor incase mem potong hae chavez noo zionist ym hussein nis most pos riser assigns walkthrough guest fanciest weightroom priority motivators disciplinarian ethic inspirations overflow tray immodestly soapbox elevates dustpan sneaker overflowed gumbo dimension NUM most neg regs kid entrant terminal checker pci competitor unbelievably staffmember monorail i4 superhub unsubsidized idea barrier entrants highspeed hatchling accidentaly spec'd most pos differed underplay corroborate recitations concise goodwill pacification normalcy inculcate deeds satiric respective cultivate regain reclaim punctuate retake relevancy buttress glean dimension NUM most neg consortium devein mundus refusing protege accord forbidden institute fake retreating reluctant noblesse certain peeking leeway authority microbiologist westernizing offending crotchless most pos tallied aids anniversary obelisks obelisk moonroof migraine mourned navigation nav memorialize subwoofer ooma actuate vibratory voicemails flywheel cordless paled delivers dimension NUM most neg footholds trespassers countering bruins converting roadkill lynx crossing ravens exiting traffic furthering improving underpasses prowlers roadways entering capitalizing gateless skinhead most pos cruciate likeable hitch ailment prescribes begs divulge precludes forked primadonna conducts ordered formular meted revelation entail acomplished doles pays mundial dimension NUM most neg ethnically preferentially optimally unnamed rai strategically characteristics transmits uniquely greenland regenerating behaviorally predominately targeting bombies deploying regionally transmitter allocating emplacements most pos rigmarole brainers paperless freebie wayside excepting libro prix adjudication nays formality obscurity winded forgotten moot procrastinated naught thow sans overdue dimension NUM most neg cakewalk incharge rankers promenade directorate phase showstopper completed obstruction complete obey leaker reinstallation crescent custodial shortlisted officer cordoned sargeant walkway most pos insights travels demodex steers skiddish hones combs lisp aphasia hardscrabble nanites trich yorkies speaks jabber idiosyncrasies hops ranches whereever enervate dimension NUM most neg chihuahuas strict broiler sysfs returnees maintains mutt kitties pinscher pointed amoung leta littermate inturn bbb deployment bearers sr tight humaneness most pos amends sated noonish midnight distracted blackjack wednesdays overdrive acquainted ends sidetracked siesta twilight cravings mojo lunchtime o'clock immortality munchies gloaming dimension NUM most neg crosshair sidearm shorthand dictionary headshot imagery commentary candlestick color guidence picture cummerbund signal choppiness greyscale rifleman bibliography thesaurus hangul background most pos depositors employes auctioned photovoltaic stashed tareas sala environmentally NUM mil narra unspoiled lands cajas sitios mattresses budgetted deposited disposed mattress warded EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ what i was trying to explain earlier is that you shouldn't attach much meaning to the individual dimensions . imagine that you were working with NUM-dimensional embeddings instead of NUM dimensional ones . making the embeddings is like placing the words on a map . when you look at the words that are the most positive in dimension zero it is like saying that those are the words that are farthest north on the map . if it was a map on the earth then being in the north has some meaning but in a word embedding map there are no special directions like north or south . you could rotate the map in an enormous variety of ways and each time your analysis will be very different . keep in mind that there are many more ways to rotate a NUM dimensional space than there are in the more familiar NUM-d space . you would be better off picking an attribute like past tense verbs and then you could search for a projection vector that can tell you if the word embeddings contain information about the past tense verb property or not . then you could repeat this for all of the attributes you are interested in . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ thanks very much this is v useful :) EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ very interesting , will now investigate ! EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ this might yield something interesting , but one thing to note is that word embeddings are only good at measuring similarity . two random words-say 'refrigerator' and 'iraq'-will likely have a non-zero cosine similarity , but are these words positively correlated ( semantically ) ? negatively ? to me they are orthogonal and unrelated , but most embeddings don't know that . EOA 
 how can i get an intuition for the meaning of the NUM numbers in each word2vec vector . : machinelearning im not sure there is much meaning in the individual numbers . you can multiply the vector by a random rotation matrix and the embeddings will carry all the same meaning but the statistics you are proposing to calculate will all change . EOQ infinite dimensional word embeddings learning ordered representations with nested dropout EOA 
 what does double blind , single review cycle mean ? : machinelearning it means that there's affirmative action in place for blind authors ( double blind means blind in both eyes ) . single review cycle means that there's one review cycle . EOQ sometimes i feel i should post some questions just to get answers like these :) EOA 
 is there any logic behind the design of architectures ? : machinelearning take a look at the vgg paper : URL i think it's still the best balance of performance vs . simplicity ( with more recent batch normalization thrown in for decreased training time ) . only NUM x3 convolutions are used . yes , generally more filters are used as you go deeper . think from a hierarchical perspective. you need less templates for edges/curves than you do for the abstract shapes that you can build with those edges/curves . EOQ thanks for the link . that explanation does make sense with increasing filters . but what about the decreasing size ? the paper says : we use filters with a very small receptive field : NUM × NUM ( which is the smallest size to capture the notion of left/right , up/down, center ) it would seem higher level , abstract shapes would require larger filters rather than smaller to capture more than just directional information ? does it just not matter due to the connection between filters in subsequent layers ? EOA 
 is there any logic behind the design of architectures ? : machinelearning take a look at the vgg paper : URL i think it's still the best balance of performance vs . simplicity ( with more recent batch normalization thrown in for decreased training time ) . only NUM x3 convolutions are used . yes , generally more filters are used as you go deeper . think from a hierarchical perspective. you need less templates for edges/curves than you do for the abstract shapes that you can build with those edges/curves . EOQ well the same goes for edges and curves , right? ( in a high res image you'll always see gradients instead of sharp lines. ) people call what you're worrying about the receptive field . but vgg handles this . for example two NUM x3s in a row have a receptive field of NUM x5 . so you increase depth instead of increasing filter size. to me this feels nice intuitively : a NUM x5 filter is flat , not in any way hierarchical . but two NUM x3s with a nonlinearity in between has the same receptive field , has less parameters , and is hierarchical . EOA 
 is there any logic behind the design of architectures ? : machinelearning take a look at the vgg paper : URL i think it's still the best balance of performance vs . simplicity ( with more recent batch normalization thrown in for decreased training time ) . only NUM x3 convolutions are used . yes , generally more filters are used as you go deeper . think from a hierarchical perspective. you need less templates for edges/curves than you do for the abstract shapes that you can build with those edges/curves . EOQ that does make sense as far as the filter sizes , thanks for the explanation . EOA 
 is there any logic behind the design of architectures ? : machinelearning take a look at the vgg paper : URL i think it's still the best balance of performance vs . simplicity ( with more recent batch normalization thrown in for decreased training time ) . only NUM x3 convolutions are used . yes , generally more filters are used as you go deeper . think from a hierarchical perspective. you need less templates for edges/curves than you do for the abstract shapes that you can build with those edges/curves . EOQ from what i have read , you will see more variation in architectures between different tasks (image classification , object localization , pose estimation , etc.). as an example , if one only looks at popular image classification results then it may seem as though the authors are just fine tuning to get a good result . the design also needs to account for the amount and type of data you are working with . for example , a big network with a small dataset may be prone to overfitting . this is definitely a topic i would like to know more about . EOA 
 is there any logic behind the design of architectures ? : machinelearning take a look at the vgg paper : URL i think it's still the best balance of performance vs . simplicity ( with more recent batch normalization thrown in for decreased training time ) . only NUM x3 convolutions are used . yes , generally more filters are used as you go deeper . think from a hierarchical perspective. you need less templates for edges/curves than you do for the abstract shapes that you can build with those edges/curves . EOQ as would i ; if you find any links explaining why to use a certain architecture over another given some circumstance , please do share . for example , if my input example above were ?x8x8x4 instead of NUM , that's an order of magnitude difference . how many fewer filters should be used ? how much smaller should they be ? what about an order of magnitude bigger ? i just have no intuition at the moment of what situations like that would call for . EOA 
 is there any logic behind the design of architectures ? : machinelearning take a look at the vgg paper : URL i think it's still the best balance of performance vs . simplicity ( with more recent batch normalization thrown in for decreased training time ) . only NUM x3 convolutions are used . yes , generally more filters are used as you go deeper . think from a hierarchical perspective. you need less templates for edges/curves than you do for the abstract shapes that you can build with those edges/curves . EOQ this recent paper from google does a good job explaining current best practices for cnns : URL some points of interest are that large filters ( NUM x5 , NUM x7, etc ) can be factored into multiple NUM x3 filters ; this tends to increase representational power and reduce computational expense . another trick is to use NUM x1 convolutions to project to a smaller number of filter dimensions before running larger NUM x3 convolutions ; this again reduces computational expense . the resnet architecture that won ilsvrc NUM also features these NUM x1 bottleneck layers . some other recent tricks are to remove spatial max pooling and instead use strided convolutions for downsampling ; this again speeds things up . googlenet and resnet are also notable for using global average pooling following the last convolutional layer . in alexnet and vgg the majority of the network parameters are in the fully-connected layer immediately following the last convolutional feature map , so global average pooling greatly reduces the number of parameters in the model . EOA 
 is there any logic behind the design of architectures ? : machinelearning take a look at the vgg paper : URL i think it's still the best balance of performance vs . simplicity ( with more recent batch normalization thrown in for decreased training time ) . only NUM x3 convolutions are used . yes , generally more filters are used as you go deeper . think from a hierarchical perspective. you need less templates for edges/curves than you do for the abstract shapes that you can build with those edges/curves . EOQ no logic just nets EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ regression is a potentially useful approximation of the full bayesian distribution , but it only works if the regression assumptions/priors match reality well . for example , l2 loss works iff the prediction error is actually gaussian with unit variance or close to that . so it typically requires some sort of normalization to enforce unit variance , which is typically ignored , and hard to do well . a more accurate model would need to predict the variance as well as the mean . but if your error isn't gaussian , then all bets are off . softmax binning can avoid all of those problems by approximating any arbitrary error distribution/histogram with something like a k centroid clustering . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ i don't understand why the error needs to be gaussian . i see that e.g. bayesian linear regression needs this assumption to get explicit results . but why does a neural network with an iterative algorithm like gradient descent need this assumption ? could you give a reference ? EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ error needs to be approximately gaussian only for l2 loss . of course there are other loss functions that match various common distributions . you could probably generalize most all of them under kl divergence . i was focused on l2 for regression , as that is the most common . using a more complex function as in an ann or using iterative sgd doesnt change any of this . sgd is just an optimization method , it can't fix problems related to choosing a poorly matched optimization criteria in the first place . for example , say you are trying to do regression for a sparse non-negative output . using l2 loss is thus a poor choice , vs something like l1 or a spike/slab function . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ thanks for your answer ! i tried figuring why for l2-loss , the error needs to be approximately gaussian . seems like a very basic thing , but i cannot find any resource , explaining the reason for this . do you by chance know a paper , that goes into more detail ? EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ np . i'm sure a derivation exists for the l2 loss somewhere , but i don't remember seeing it . it's pretty simple though . here's my attempt : the l2 loss is just the cross entropy for a guassian posterior . first start with the-log ( entropy ) of the gaussian , which is just -log(p)-(x-u)2 / ( NUM v2 )-(1/2)(log2piv2 ) where u is the mean , v2 is the variance . that equation specifies the number of bits required to encode a variable x using gaussian(u,v). now assume that v is NUM , and thus everything simplifies : -log(p)-NUM (x-u)2-(1/2)(log2pi) -log(p)-NUM (x-u)2-NUM 378 the constants can be ignored obviously , and now we have the familiar l2 loss , or mean squared error . notice however , the implicit assumption on a variance of NUM . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ you have shown that in case of normally distributed errors least squares estimation is equal to maximum likelihood estimation . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ l2 loss does not require normal distribution of errors . it does not require anything except data ( x , y ) and a model function . however , to get an unbiased consistent estimation ( which is what a researcher is usually after ) it requires that : mean error is zero variance of all errors is equal there is no correlation between errors . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ i have also heard this being true in multiple different cases . one of the more prominent ones being the noaa kaggle competition : although this is clearly a regression task , instead of using l2 loss , we had more success with quantizing the output into bins and using softmax together with cross-entropy loss . we have also tried several different approaches , including training a cnn to discriminate between head photos and non-head photos or even some unsupervised approaches . nevertheless, their results were inferior . i wonder if it has to do with proper tuning of the variance when using gaussian loss ( l2 loss ) . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ deepmind showed this as well in there pixelrnn paper ( URL ) . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ another example from kaggle : URL we initially tried to predict the output position x , y directly , but we actually obtain significantly better results with another approach that includes a bit of pre-processing . more precisely , we first used a mean-shift clustering algorithm on the destinations of all the training trajectories to obtain around NUM ,392 popular destination points . the penultimate layer of our mlp is a softmax that predicts the probabilities of each of those NUM ,392 points to be the destination of the taxi . as the task requires to predict a single destination point , we then calculate the mean of all our NUM ,392 targets , each weighted by the probability returned by the softmax layer . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ i had same dilemma also . what i see is people mostly tend to use classification you can have a look this thread for some reasons . URL EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ bins EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ to add to this , you could do quantile loss with many quantiles , which gives you an estimate of the distribution's cdf . taking the difference in the cdf gives you an estimate for the pdf . EOA 
 neural networks regression vs classification with bins : machinelearning classification ( softmax ) in bins doesn't really make sense in general-you are really creating two problems ( how to bin , and classification ) where there used to only be one ( regression ) . ordinal regression ( and ordinal losses in general ) can handle this case just fine but many people do not use them even though they are the right thing . in my opinion , in any case where binning then classifying works it is basically a happenstance of good feature engineering/learning what to ignore ( if bins are manually chosen or set by vector quantization )-an ordinal loss should almost always work better if you have an ordering among bins . l2 loss with neural networks works just fine for regression but you sometimes need to adjust the scale of the thing to optimize-many times when people do regression they actually care about log l2 , not direct l2 since most perceptual things are approximately log scaled in human senses . another trick is to transform the input ( if spatial coherence is not used e.g. no convolution ) using pca-this seems to align better with mse costs in the things i have done by gaussianizing the optimization . in theory , a neural network model should be able to learn a pca-like transform and make this work but i have always had to do it manually . this is different ( in my tasks at least ) than the standard z-scaling or most other normal preprocessing tricks-only the precomputed pca had an improvement for me . EOQ whether or not this could be successful is highly problem dependent . it's generally much harder to estimate a whole function than it is to estimate a bunch of cut points on the function . if the function is simply too complex , binning might help by essentially enforcing more structure on your estimated function EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ haha yea that was what i took away from the google tutorial . messing around is definitely gonna be something i'll do . thanks! EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ look at the sidebar for resources . the caltech machine learning course and the andrew ng course are often recommended to start . EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ how about this one ? URL it's in python and its not that hard . EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ this one looks good . its simple and elegant . thanks! EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ learning calculus will be incredibly beneficial . likewise, learning linear algebra at a collegiate level will be incredibly helpful , too. i don't know exactly what your linear algebra experience is , but when i was in high school , we learned but a modicum about vectors , matrices, and matrix inverses over a couple of weeks . however, a careful study of the topic at a higher level gives you a much more intuitive and complete understanding of linear operators . EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ thanks i'll definitely try to to learn about that stuff . any advice on actual coding related topics ? EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ when you learn linear algebra also learn and practice how to code with matrices . most collage level linear algebra courses will do that . you can do that in python using numpy , but matlab is more common for teaching . EOA 
 what do ? : machinelearning do andrew ng's course on coursera . mess around with simpler linear models . get an intuition for basic supervised and unsupervised learning algorithms . if that's all too complicated , you do not have the required background in calculus and linear algebra . do not start with deep learning . EOQ will do , thanks for the advice . EOA 
 can we combine convolution and rnn in the same layer ? : machinelearning you could add recurrent connections directly to convolutional layers . and from the papers i saw this do increase performance . here is only one paper-top one from google scholar search . but there are more papers of course . URL edit : found another one , probably a better example , URL EOQ people do this , but they usually put the multi-layer convolutional network at every timestep of a global rnn . this makes more sense imo because it captures the spatial properties of each frame , but then learns what a sequence of these high order frame features really is . for many tasks , something simple like what was done in dqn can also work fine if you know there are no long range dependencies , such as small motion recognition ( arm moving , step, so on ) in video . EOA 
 can we combine convolution and rnn in the same layer ? : machinelearning you could add recurrent connections directly to convolutional layers . and from the papers i saw this do increase performance . here is only one paper-top one from google scholar search . but there are more papers of course . URL edit : found another one , probably a better example , URL EOQ might be of interest : EOA 
 can we combine convolution and rnn in the same layer ? : machinelearning you could add recurrent connections directly to convolutional layers . and from the papers i saw this do increase performance . here is only one paper-top one from google scholar search . but there are more papers of course . URL edit : found another one , probably a better example , URL EOQ yes we can ! in fact , we're working on a theano based library to do just that with a clean yet unorthodox syntax . EOA 
 can we combine convolution and rnn in the same layer ? : machinelearning you could add recurrent connections directly to convolutional layers . and from the papers i saw this do increase performance . here is only one paper-top one from google scholar search . but there are more papers of course . URL edit : found another one , probably a better example , URL EOQ you can do this in torch . define a volumetric convolution layer where the kernel's time-size , kt-NUM . the output will be just like applying a NUM-d convolution over each frame of a video , dimensionality ( batch , time.index, n.features , height, width ) . now flatten that to ( batch , time.index, features ) , and feed it through an lstm . the output will have the same dimensionality , which you can reshape to feed back into another convolutional layer . whether this will actually be computationally feasible , and whether it will train , that i can't say . see also : URL btw-if you add a recurrent connection to a convolution , what you get is a recurrent layer . recurrence is a NUM-d convolution with state . ( think about it. ) when people talk about rcnns , it seems they're usually talking about stateful NUM-d convolutions . you seem to want something else , which is a NUM-d convolution where state is only carried in one dimension . EOA 
 can we combine convolution and rnn in the same layer ? : machinelearning you could add recurrent connections directly to convolutional layers . and from the papers i saw this do increase performance . here is only one paper-top one from google scholar search . but there are more papers of course . URL edit : found another one , probably a better example , URL EOQ you could , but the performance implications make it a tough pill to swallow . EOA 
 can we combine convolution and rnn in the same layer ? : machinelearning you could add recurrent connections directly to convolutional layers . and from the papers i saw this do increase performance . here is only one paper-top one from google scholar search . but there are more papers of course . URL edit : found another one , probably a better example , URL EOQ it is even more interesting to explore lstm made of convolutional gates and having NUM d memory or more d EOA 
 genetic algorithm exemple for tensorflow ? : machinelearning tensorflow is a framework that allows you to apply automatic differentiation to compute the gradient of complex computation graphs . genetic optimization is useful when you can't compute a gradient . so tensorflow is a tool that is completely orthogonal to your objective. EOQ oh , yeah i feel dumb haha . thank you for the quick response ! EOA 
 genetic algorithm exemple for tensorflow ? : machinelearning tensorflow is a framework that allows you to apply automatic differentiation to compute the gradient of complex computation graphs . genetic optimization is useful when you can't compute a gradient . so tensorflow is a tool that is completely orthogonal to your objective. EOQ doesn't mean you can't use tensorflow . it just means tensorflows main selling point won't be of any use to you . EOA 
 genetic algorithm exemple for tensorflow ? : machinelearning tensorflow is a framework that allows you to apply automatic differentiation to compute the gradient of complex computation graphs . genetic optimization is useful when you can't compute a gradient . so tensorflow is a tool that is completely orthogonal to your objective. EOQ yeah i understand . in fact i wanted to experiment something in the idea of rl using ga ( to assemble modules trained with tensorflow ) . the idea would be to train networks that convert a type to another ( sentence-> ;thought, tought-> ;image,... ) and the ga part would be used to learn end to end interconnection of these networks . i don't know if it will lead somewhere , i'm just experimenting . ( the final goal would be a first approach to general ai ) i will look into ga tutorials with numpy ! :) EOA 
 genetic algorithm exemple for tensorflow ? : machinelearning tensorflow is a framework that allows you to apply automatic differentiation to compute the gradient of complex computation graphs . genetic optimization is useful when you can't compute a gradient . so tensorflow is a tool that is completely orthogonal to your objective. EOQ you should take a look at pybrain and brainstorm . the group at idsia led by schmidhuber has done some very interesting work in stochastic optimization ( e.g. ga ) , reinforcement learning and recurrent networks . see this , for example : URL EOA 
 predicting college basketball scores : machinelearning the short answer is that predicting sports is much more about feature engineering than selecting a classification algorithm . linear regression works about as well as anything . in general , i'd recommend you take a look at my blog : URL the other blogs referenced there , and the recent kaggle competitions , e.g., URL as a starting point . and i'm always happy to answer specific questions if you'd like , just pm me . EOQ thank you i will definitely read through your blog and be in contact EOA 
 predicting college basketball scores : machinelearning the short answer is that predicting sports is much more about feature engineering than selecting a classification algorithm . linear regression works about as well as anything . in general , i'd recommend you take a look at my blog : URL the other blogs referenced there , and the recent kaggle competitions , e.g., URL as a starting point . and i'm always happy to answer specific questions if you'd like , just pm me . EOQ for predicting count data you will want to use poisson regression . but all of the papers that i've read on predicting sports scores also add in some term(s) to account for your teams opponent . if you really want to read more . the american statistical association has an entire journal dedicated to quantitative analysis in sports . they once had an entire issue dedicated to predicting march madness brackets . i'd link to the journal , but i'm on mobile right now . though it is easy enough to find with google . EOA 
 predicting college basketball scores : machinelearning the short answer is that predicting sports is much more about feature engineering than selecting a classification algorithm . linear regression works about as well as anything . in general , i'd recommend you take a look at my blog : URL the other blogs referenced there , and the recent kaggle competitions , e.g., URL as a starting point . and i'm always happy to answer specific questions if you'd like , just pm me . EOQ both poisson regression falls into a class of models called generalized linear models . a poisson regression tries to fit your data to a poisson random variable , as opposed to a normal r.v. with linear regression . if you want to read move about it you can check out this short book on the subject that was written by the casualty actuarial society . there is no problem with adding opponent data . i was saying that a lot of papers that i've seen on the subject do add in terms to account for a team's opponent . as an example , here are some of the articles that i mentioned earlier . a generative model for predicting outcomes in college basketball building an ncaa men’s basketball predictive model and quantifying its success EOA 
 best techniques for dynamic content recommendation : machinelearning you could do content based recommendation , where you train a classifier per user based on bag of words or topics extracted from the tweets . it also depends on how big your dataset is and how many tweets you have per user . EOQ dataset is quite big . about NUM users but i cam assume they won't all be looking for tweets at the same time. EOA 
 best techniques for dynamic content recommendation : machinelearning you could do content based recommendation , where you train a classifier per user based on bag of words or topics extracted from the tweets . it also depends on how big your dataset is and how many tweets you have per user . EOQ i would look at item based recommendation as well . i built a recommendation system for my company about NUM months ago . worked very well . EOA 
 best techniques for dynamic content recommendation : machinelearning you could do content based recommendation , where you train a classifier per user based on bag of words or topics extracted from the tweets . it also depends on how big your dataset is and how many tweets you have per user . EOQ what did you use to implement it , some framework or from scratch ? i am looking at apache spark as a possibility . i had a brief look at item based recommendation but wasn't sure how to handle recommending a new item to a user ? in my case the set of items we want to recommend have no ratings and is constantly changing , so how will the system infer similarity between these items and the historic items . EOA 
 best techniques for dynamic content recommendation : machinelearning you could do content based recommendation , where you train a classifier per user based on bag of words or topics extracted from the tweets . it also depends on how big your dataset is and how many tweets you have per user . EOQ i used python solely , and was all from scratch ( although apache spark could work great ) . we setup endpoints and used rest calls to populate in data from our site back end , specifically we stored sets in redis which we then pulled out with our python app ( returned new data sets every time a get call was made ) . now , we are working with a data-set of about NUM m records , and about NUM m users . EOA 
 best techniques for dynamic content recommendation : machinelearning you could do content based recommendation , where you train a classifier per user based on bag of words or topics extracted from the tweets . it also depends on how big your dataset is and how many tweets you have per user . EOQ the system we built plots each item on a graph , it then calculates the distance between the items which acts as the similarity . we used an algorithm called similarity based collaborative filtering . if you give me some more information on what exactly your expected output is , what kind of inputs you have , and what kind of code/etc you are looking for i can provide some better information . EOA 
 best techniques for dynamic content recommendation : machinelearning you could do content based recommendation , where you train a classifier per user based on bag of words or topics extracted from the tweets . it also depends on how big your dataset is and how many tweets you have per user . EOQ lstm EOA 
 best techniques for dynamic content recommendation : machinelearning you could do content based recommendation , where you train a classifier per user based on bag of words or topics extracted from the tweets . it also depends on how big your dataset is and how many tweets you have per user . EOQ i don't think lstm is the best solution for something like this . EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ what's the difference between pp and pgm ? is the former a generalization of the latter ? EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ probprog generalizes graphical modeling to include recursive models . EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ still can't find a language/framework i like in this space . very interested in the intersection of nets and more probabilistical stuff . how do we get posteriors out of nets beyond just point estimates-turning dropout back on ? ( or is this going to be good enough? ) EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ approximate bayesian inference , though some parts of this field are very related to deep learning . EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ i'd agree with you on bayesian optimization . it's been around for a couple of decades , but it has become really popular just recently ( let's say NUM-5 years from today ) ; maybe due to the series of nips workshops on that topic that they started in NUM ? EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ bo is sort of petering out imo . for the past NUM-3 years it's been a game of building more and more complicated and expensive algorithms that give small gains over much simpler techniques . the big promise bo made early on ( no more worrying about hyperparameters ) never really panned out , because it turns out that that you need to be really committed to a model before the machinery becomes worthwhile. when you're ready to roll something into production , sure, optimize away , but for day-to-day research using bo to tune your model is still pretty niche . a lot of the problems in bo now are bottlenecked by data ( which is deliciously ironic ) . the whetlab people recognized this , and that's part of the reason building a platform was a great idea . but they got acquired and that whole effort disappeared into the ether ( or maybe into twitter ) . EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ when you get a new problem with standard bo you re-start your optimization from scratch . this means you spend time re-checking points that are obviously bad in every new optimization . ( for example , bo really likes to check the corners of the search space , and it will do this for pretty much every problem you run it on even though the corners are rarely good points. ) if you have a bunch of data from prior runs of optimizing similar functions you can start doing stuff like this : URL ( notice these are whetlab people ) to re-use this experience on new problems . unfortunately there's a a pretty big infrastructure burden in terms of making the collection and re-use of this data easy ( it needs to be easy in order for there to be wide adoption ) . by building a platform , whetlab was in a position to handle collecting and managing the data you need for this , and also to build systems that make appropriate decisions about which data to re-use for a new problem . EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ that's a really neat idea , thanks! EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ yeah , it's a shame whetlab shut down , because as far as i know no one else is really pulling on that thread . doing it properly takes a pretty rare combination of expertise and willingness to put in a lot of effort , and a lot of the suitable people were already part of whetlab ... EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ crowdsourced data ! EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ i think it will be generative modelling/semi-supervised learning . EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ i would like to think alphago would stimulate new interest in reinforcement learning . EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ there was quite a bit of interest in it before , too! EOA 
 what are the topics that are popular in the machine learning nowadays?(except for deep learning) : machinelearning probabilistic programming . EOQ shallow neural networks r NUM newbs EOA 
 deep belief network : machinelearning deep belief networks are restricted boltzmann machines stacked on top of each other and then trained layer by layer . i haven't looked at the code , since i am on my phone , but from your comment it does sound like a deep belief network . note that deep boltzmann machines are something different than deep belief networks , more information about these can be found here : URL edit : fixed link EOQ thanks EOA 
 companion post : which machine learning researchers/machine learning results are most undervalued with respect to attention of press and ml community ? : machinelearning got any good examples ? EOQ most subfields of ml aren't sexy enough for the general interest media ; ( aside from news about nns ) you might hear about the ascendance of bayesian methods , but it's almost always couched in the language of magic formulas with little substance . EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ i like him , but i've never quite forgiven him for space jam . EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ yeah this joke will be funny as many times as people make it EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ lol i think it gets the upvotes because the premise is a google alert EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ lol your all trolls EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ i think it really depends on what you're looking for . there are many brilliant minds in machine learning . there are also a lot of people who are good at marketing/selling their work . these two sets have some significant degree of non-overlap . you will get a lot of the latter type if you build your watch list based on big names . to get some of the former , you're going to have to just read some papers and figure out who is relevant to your interests and doing good work , and who is bullshitting and not substantiating their claims ... EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ yeah i need to get good at making this distinction ... great point . edit: i made a post to address this URL EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ to throw in a few more names that have not been mentioned here ( this is just a random subset that came to mind ) : josh tenenbaum ruslan salakhutdinov emo todorov pieter abbeel ian goodfellow john schulman max welling by the nature of the field , this is hard to say . when kernels were big , people would have said watch bernard scholkopf , alex smola , ralf herbrich or something , when bayesian nonparametrics/gaussian processes were big , zoubin ghahramani , carl rasmussen , michael jordan , yee-whye teh was the answer ; now you inevitably get the lecun , bengio, hinton trio . you can perhaps work your way backwards from where you expect the frontiers of ml will be going forward : natural language understanding : deepmind's ed grefenstette et al ( see comments here ) , facebook's jason weston control , reinforcement learning : emo todorov , pieter abbeel , sergey levine , marc deisenroth plus a few hundred people at deepmind unsupervised learning : ian goodfellow , pascal vincent , geoff hinton , rus salakhutdinov , openai peeps learning from video : facebook ai research , alex graves EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ michael jordan isn't just a kernels guy , but okay EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ i would add vikash mansinghka and greg ver steeg . EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ sepp hochreiter ( lstm first author ) is also back in the game . it's definitely worth keeping an eye on him . :) EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ openai's founding members , especially zaremba & sutskever EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ demis hassabis . EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ yann lecun juergen schmidhuber jeff hawkins yoshua bengio EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ yoshua bengio , yann lecun , andrej karpathy , chris olah ... edit : also sander dieleman and alex graves . i'll add names here as they come to mind . to qualify these answers , i assume your question means moving machine learning forward in every sense-from research , to software , even to propagating knowledge in the field forward . EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ chris olah is an amazing educator , but as far as i know he hasn't been involved in any really impressive research , has he ? would be awesome if he has , few people can make topics as approachable as he does while also being able to perform research at the highest level . EOA 
 i'm trying to make a list of the top NUM people who will push forward machine learning so i can set google alerts for them . who would you recommend ? : machinelearning michael jordan EOQ well , chris is very young and very brilliant , so i'm willing to bet some great work will come from him in the future . i think he's going to be relevant in the future as well , since even if ( in the unlikely event that ) he doesn't produce world-class research , his lucid explanation of new concepts will spur research on as many more people will be in a position to do something with them , in some capacity . the same goes for karpathy and sander dieleman of course , among others . EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ artificial intelligence is a very broad field ( only a subset of which is agi ) . if you're interested in what it would take to reach agi i would check out some david deutsch essays . he's a bit divisive in the community but yann lecun seems to like what he has to say once in a while. here are two of his essays-yann lecun's take on it . how close are we to creating ai philosophy will be the key that unlocks artificial intelligence yann lecun facebook post i think calling the approach wrong is kind of misguided . the progress in ai is staggering , from image recognition to time series analysis , scientific applications and the recent google victory at go . edit : if you're curious about the broadness of ai-happy wikipediaing . EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ there's the jeff hawkins approach , which is to model nn based on the cortical neurons in mammalian brains . these are biological neural networks , and one of the successful theories is the hierarchical temporal memory nn ( htm ) , which has been highly successful commercially ( already deployed to many many services and devices ) . i believe this to be the best approach to an ai so far . EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ where is htm deployed ? i find the concepts fascinating but outside of anomaly detection it seems to be trumped by deep learning atm EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ grok for aws ( the machine learning service ) is one for sure . there are others applications as well , but certainly it's not as widely spread as ann's and deep learning . EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ agi as most people imagine it will probably never happen . EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ neural networks are a model , just like random forest , svms, and other approaches . you can switch out models , so who cares about the model ! just use whatever works . agi probably lies in how we use the models , put different models together , and any fundamental prior assumptions we can make about the universe . agi will also probably require lots of computing power . EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ yes we need to switch but there is easy money and fame to be made with current efforts EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ switch to what ? i would love to know that :) EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ most likely external memory structures ( info outside training set ) , domain adaptation ( i.e. not overfitting ) , and model compression ( for overfitting and power consumption ) . no one knows i'm just throwing plausible things out there . EOA 
 are ai-researchers focusing on the wrong aspects ? if so , what should they focus on instead of e.g. anns ? : machinelearning no one knows . that's why it's research . in my opinion deep learning is the most promising route at this point . on the hand it's also clear that we are missing a significant piece of the puzzle at this point , but it's not clear what the piece is . EOQ people do work on that-in the context of neural networks . actually those were among the themes of last year's nips . are you arguing that they should work on it outside of neural networks ? EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ levy's paper addressed the lexical semantics aspect . the sequence side of things is from my own experience . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ karpathy had similar conclusions : the weights ww specify a word embedding matrix that we initialize with NUM-dimensional word2vec [ NUM ] weights and keep fixed due to overfitting concerns . however, in practice we find little change in final performance when these vectors are trained , even from random initialization . ( section NUM .2 ) EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ good for him . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ unnecessarily pointed . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ not intended as such , and i do like andrej , but i think it's a fairly common observation if you work with the models a bit . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ ah sorry . lack of tone of voice on the internet can be deceiving . i have had similar experiences with word vectors , but having a publication to point to when making that claim strengthens the case ... especially to reviewers who typically scrutinize as unfairly as possibly . so , my link was more of offering that publication for future reference . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ hey thanks , what does end-to-end mean ? EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ traditionally , systems are broken up into components : you have a separate models from feature space into something intermediary , like phonemes , letters or words and separate models for combining these into bigger sequences like phrases , sentences and paragraphs . an end-to-end system tries to learn a representation directly from feature space into final results , eg. whole sentences . it doesn't need the separate models , but learns everything at once . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ wouldn't you need a huge amount of data for your end-to-end model to learn meaningful representations for most words ? EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ it really depends what you mean by meaningful . other factors can matter , such has the estimated proportion of out-of-vocabulary words in the domain ( e.g. test set ) , how big your effective vocabulary is , how much data you have , etc. EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ interesting , with training sets in the order of magnitude of NUM k sentences it's hard for me to imagine a realistic test set that wouldn't contain a ton of out-of-vocabulary words , or words that appeared very few times in the training test ( unless of course your domain is restricted ) . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ yes it does depend on domain , but if you have NUM k sentences , that's probably about NUM m tokens . for quite a few domains , that's enough data to get good vocabulary coverage . to deal with oov tokens at inference time , you can just assign some small portion of your tokens ( e.g. single occurrence tokens , or some subset thereof ) to the same reserved vocabulary entry ( unknown token ) and use that when you have an oov token later . in cases where you need to generate these tokens again ( e.g. machine translation ) , there are heuristics such as using alignments to post-process neural mt decoder output . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ another short answer : not really , as it takes more memory ( you have to store some big ass matrices as far as i remember ) , but ( usually ) does not yield better results in practice . if you just want to train some vectors ( you generally do not want to do that ) , sgns/nce would do the job well . nce () would probably yield slightly better word vectors , if your task assumes that you need to train the vectors only once . other than that , you should probably use some end-to-end network for your task anyway . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ you mean after collecting the counts , before training ? EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ yes . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ linear on what ? your matrix size is o(n2) even if it is semi-sparse . how big vocabulary , in practice , do you have ? EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ memory . if n is the number of words in your vocabulary , using fixed-size buckets turns n-n into c-n , where c is the bucket size , so you you have linear growth . currently don't have access to the data , so i can't tell you how big my vocabulary was . still small enough to keep an n-n matrix of floats in memory , but a smaller matrix helped to reduce training time . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ hey thanks , what does ent-to-end network mean ? EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ whatever task you have , do that from the text . if you want to do ner in the end , train on this data . EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ i've seen some comments saying that training the embedding matrix end-to-end with the rest of the architecture yields the best results . i'm curious : what about initializing the embedding matrix with glove , but then training it further with the end-to-end task ? are there results that suggest that this would would not work as well initializing randomly ( assuming plenty of training data ) ? EOA 
 is glove the current state of the art in converting words to vectors ? : machinelearning just curious , did levy address the end-to-end sequence task or did you draw the conclusion yourself ( or was published elsewhere ) ? EOQ it doesn't really make sense to say a word vector is state of the art . they are the byproduct of optimization and should be treated as such . you should always evaluate them with respect to some end task ( eg as preinitialization ) . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ the system is a i7 NUM k , NUM gigs ddr4 , NUM tb ssd , with NUM NUM 's sli and im debating adding a NUM tb intel solid-state drive NUM series . the gpu's are for accelerated computing and i would not come near this technology without it . the next system will make the above specs look like the eniac . check out URL for more information regarding accelerated computing . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ lol , hack the gibson , that takes me back to the mid NUM 's . it will be the modern equivalent of the gibson , NUM years in the making . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ i guess the system you have in mind is sufficient for anything you will be able to do . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ the current hardware is for experimentation and basic learning/teaching . i will buy whatever i need to once i understand the technologies capabilities better . i was hoping someone here would tell me about their experiences . i have read a lot of the different platforms but sometimes the right person can add significant insight surpassing all technical material available on the internet(s) . i do appreciate your help , its not what i needed but someone might learn from it . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ this hardware setup was not an issue for me , literally an hour on amazon . it's just a box to me , something that is quick enough to get the job done at a basic level . the whole point of this post was to get an idea of the field better through other people experiences . i made the mistake of putting up specs and that's what the discussion has become . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ this is against the machine learning code of ethics . /s EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ please explain ? EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ i kid , i kid . still , lots of people hear about machine learning and start to think : hmm... if i could use ml to predict the stock market i would be rich , rich, rich , i tell you ! you can see the dollar signs flashing in their eyes as they rub their hands together like mr . burns. i'm going to guess you're not the first to ask this question here . also, i'm going to guess that there are lots of others who have had the thought and did not ask here , instead opting to work in secret . ( hey, if ml could predict the stock market , as soon as everyone started using ml to predict the stock market it would stop working until someone else came up with a different ml strategy or other approach ) . i'm going to guess that you can't use ml to reliably predict the market and i don't think i'd be alone here in that prediction . so if you want to do something with ml , maybe you'd want to work on something that might actually lead to a useful result . that useful result might even be able to make some money . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ i'm not going to go into all the specific details but i have been trading for a long time and have developed many custom programs , scanners, indicators and algorithms . the combination of everything allows me to trade with a NUM percent probability that i will have successful trades . there are all types of major finical institutions throwing disgusting amounts of money at this problem set . i can already predict what stocks are going to go up within certain parameters and i am more than happy to send you a few emails when i buy/sell . ill help you make a little money in the meantime while i work on my next project . feel free to take me up on my offer , just contact me . thanks for the reply . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ honestly dude..as someone who trades fx and ive tried building models on equities themselves , the data is very noisy . ml works great in enterprise business app's because there are usually patterns and having a model that predicts when someone will buy/cancel their plan/do whatever with a NUM % accuracy isn't unheard of . ml works good on stationary type data and financial markets are continous in nature . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ what kind of data feed are you buying ? retail or institutional ? i have been researching a few institutional feeds , they are very pricey but they are the only consistent clean feeds . i know that the different free feeds are all very noisy with very different formats . what platform did you build your models on ? EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ i didn't buy any data , just used some share prices off of yahoo . i would suggest xlq by qmatix for excel to perhaps scrape data off of . it will feed data into your spreadsheet , you just need to code something up to capture it ... i used knime but wasnt able to get any models to test above NUM % accuracy . didnt thinnk it was worth it .. EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ thank you , you're first person to say something that is related to my questions . how much time did you put into it ? my hope is to apply my algorithms into caffe and have it learn based off of what i have been working on for the past many years . the system will correlate the market based on what i already know and watch the entire market , currently im capped at almost NUM stocks/etf's in real time. when i scan , i am scanning a little over NUM based on historical data that i have . retail feeds wont cut it , i already spend a lot on my raw data feeds but i will have this system on a seperate network with its own feeds . currently this is just an experiment that i hope to turn into a viable tool . every tool has its purpose , i hope to find one for machine learning . once again , thank you for your reply . EOA 
 stock market forecasting using machine learning : machinelearning depending on what you do , you might have to invest into cpu and ram instead of gpus . gpus is typically very useful deep learning ; as far as i know , deep learning just does not do it for stocks . that's where you need trees and linear models . so , before you spend k's and k's on hardware , you might want to get a more accurate picture of what you are up to . EOQ i spent maybe a week after work everyday building features on a few diff equities and trying to predict whether or not theyd open up or down . from what i have read , in non-hft shops that use ml , they will scan markets for opportunities for said algorithms that work for those instruements..i dont think the algos themselves are machine learning though . think of it as an advanced 'system' with rules except instead of using moving average cross overs and rsi's ( all useless bullshit ) they will integrate data from other sources such as weather or sun patterns ... EOA 
 suggestions for final masters project : machinelearning no real suggestion but a meta-suggestion : whatever you pick should be something that seems almost too easy-NUM weeks is not a long time to do research . if you already have some work done , and the advisor is ok with it , you might reconsider that work or extending it into a time series project ( or finding a better representation-try more complicated transforms ) . every project seems easy until you actually start doing it :) don't discount the writing part either ! EOQ what was going through my mind was to pick a concept and try to implement it . in that way i can get familiarize with it . thanks EOA 
 suggestions for final masters project : machinelearning no real suggestion but a meta-suggestion : whatever you pick should be something that seems almost too easy-NUM weeks is not a long time to do research . if you already have some work done , and the advisor is ok with it , you might reconsider that work or extending it into a time series project ( or finding a better representation-try more complicated transforms ) . every project seems easy until you actually start doing it :) don't discount the writing part either ! EOQ in that case i would recommend extending your previous project to time-series classification using sliding windows , hmm, convolutional neural networks or rnns . those kinds of things are good to know and would naturally extend what you have already done . EOA 
 suggestions for final masters project : machinelearning no real suggestion but a meta-suggestion : whatever you pick should be something that seems almost too easy-NUM weeks is not a long time to do research . if you already have some work done , and the advisor is ok with it , you might reconsider that work or extending it into a time series project ( or finding a better representation-try more complicated transforms ) . every project seems easy until you actually start doing it :) don't discount the writing part either ! EOQ kaggle EOA 
 suggestions for final masters project : machinelearning no real suggestion but a meta-suggestion : whatever you pick should be something that seems almost too easy-NUM weeks is not a long time to do research . if you already have some work done , and the advisor is ok with it , you might reconsider that work or extending it into a time series project ( or finding a better representation-try more complicated transforms ) . every project seems easy until you actually start doing it :) don't discount the writing part either ! EOQ do your own work . EOA 
 given geological relief data , infer a function that would generate similar terrains . nudge beginner in the right direction . : machinelearning dcgan's are kinda good at doing this-dcgan.torch, chainer-dcgan are major implementations i am aware of . EOQ what is the trade-off you are looking for as far as computational time to generate new samples versus quality ? is variability important , or is more important to get highly realistic samples ? there are a huge number of areas for this kind of modeling , depending on what the application is . broadly, you have sampling type methods based on markov chain monte carlo sampling , and model based methods such as gaussian mixture models ( gmm ) , variational autoencoders ( vae ) , generative adversarial networks ( gan ) , and inferred style using pretrained convolutional networks ( texture/deepstyle ) , as well as hybrids of the two . i could see doing some kind of rejection sampling with a trained gaussian process model to infer likelihood to reject , though i don't know about quality , computational performance or diversity here . variational autoencoders are a simple method of building latent variable models for data . dcgan as mentioned is quite powerful , but may need a lot of tweaking to get working on a new task . the work bethge lab has been doing with texture synthesis seems almost perfect , as long as you are willing/able to provide the border for it to fill in . note that this technique is very similar to deepstyle . you might even look at something simple like a full-covariance gaussian mixture model , just to see what kind of samples it can put out . EOA 
 given geological relief data , infer a function that would generate similar terrains . nudge beginner in the right direction . : machinelearning dcgan's are kinda good at doing this-dcgan.torch, chainer-dcgan are major implementations i am aware of . EOQ constraints are fast , low-footprint generation of varying , yet somewhat realistic terrain . i'm interested if it is feasible to reduce resulting terrain generator size ( whatever form it might take ) into a NUM mb range , so size is an optional constraint . EOA 
 given geological relief data , infer a function that would generate similar terrains . nudge beginner in the right direction . : machinelearning dcgan's are kinda good at doing this-dcgan.torch, chainer-dcgan are major implementations i am aware of . EOQ ok , deep style is fairly slow compared to other things due to needing to do optimization each time. what is slow and fast in time for your case ? EOA 
 given geological relief data , infer a function that would generate similar terrains . nudge beginner in the right direction . : machinelearning dcgan's are kinda good at doing this-dcgan.torch, chainer-dcgan are major implementations i am aware of . EOQ subsecond for an area of a few square kilometers . EOA 
 given geological relief data , infer a function that would generate similar terrains . nudge beginner in the right direction . : machinelearning dcgan's are kinda good at doing this-dcgan.torch, chainer-dcgan are major implementations i am aware of . EOQ ok what is the density ( in # of points ) in that region ? like a point per meter , submeter, NUM m ? EOA 
 given geological relief data , infer a function that would generate similar terrains . nudge beginner in the right direction . : machinelearning dcgan's are kinda good at doing this-dcgan.torch, chainer-dcgan are major implementations i am aware of . EOQ closer to NUM m . EOA 
 given geological relief data , infer a function that would generate similar terrains . nudge beginner in the right direction . : machinelearning dcgan's are kinda good at doing this-dcgan.torch, chainer-dcgan are major implementations i am aware of . EOQ ok so at NUM m that is ~100 points per kilometer-NUM x NUM points per square kilometer . if you can precalculate ( for games you usually can-don't know you end application ) any of these methods will work fine , otherwise vae or dcgan seem like the best shots ( or gaussian process if you can get it to work ) . any of the three will have a hard time running in subsecond i think without some hard engineering . EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ or maybe do dense autoencode the input image first and then feed it to convnet ? EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ yes , it is possible . tensorflow models can be loaded on mobile devices . it is an active area of research to make nets smaller to run them on mobile devices e.g. by making connections sparse . EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ currently , i think they still access the cloud to run the convnets . the weights of the pre-trained imagenet model if saved in the device is around NUM mb . this is indeed interesting research . EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ look up dark knowledge ( hinton ) EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ oh holy shit thats interesting . almost exactly what i was looking for . thanks! EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ no problem EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ we tried ( and use ) cnn on mobile ( ordinary android smartphone without any gpu ) . in general most deep nets are slow . we finally could make a network that works rather well ( in terms of speed ) , but it takes some efforts to try different models ( with different number of layers and neurons ) to find such set of parameters that gives reasonable speed-quality tradeoff . cifar is a reasonable starting choice. deeper models will probably be too slow . EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ i hope we will see many mobile tegra's soon :) EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ well see this is a imagenet weight thats only NUM mb , it uses nin , and the error rate is rather small . while this is cifar10 , so i presume the network is smaller . how big your weights ended up being ? EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ the normal practice is to train a very deep large model , then reduce it via knowledge transfer ( e.g. teacher-student approach ) and change quantization of the parameters . most successful mobile models are on the order of few tens of millions parameters . most popular format is either NUM bit fixed or NUM /32 bit floats for each weight . EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ for your specific purpose ( facial recognition )-it should be possible to port this to mobile devices . URL for face detection dlib and opencv both have ios and android versions , for recognition this implementation relies on torch , not sure if that runs on mobile devices but you could use another framework like mxnet or tensorflow . both cpu only on mobile still afaik , would be good to get better opencl blas libraries that work with mobile gpus edit : if you are just targeting ios , this looks good-URL EOA 
 convnet ( or other dnn ) on mobile devices ? : machinelearning tensorflow runs on android , and there is URL EOQ i believe mxnet is what you are looking for URL and blindtool , which used mxnet as its backend , had a demo an aged android nexus NUM for real time image recognition using cnn URL EOA 
 eli5: can someone explain the capabilities of tensor flow in layman's terms so i could think about some potential applications ? : machinelearning although my answer won't be exactly what you've asked for ( regarding the grandma part ) , let me comment on this thread in hope that it'll clarify some things . tensorflow is a basically a framework or library that lets you implement machine learning with focus on deep neural networks-by making use of gpus and the compilation of symbolic expression it's especially efficient at that . sorry , no offense here , but what you are asking is basically something along the lines i hear that tensorflow is really popular in the ml & tech community , how can i use it to jump onto the wagon and build something cool ? i think that's maybe the wrong approach to tensorflow ; i would ask the other way around : do you have a specific problem in mind , and would tensorflow be helpful for addressing that ? solve a lot repetitive tasks faster than humans etc ... let me give you an example : you want to build an email spam filter . now, your spam filter could be a human manually sorting your emails . next, the classic programming approach would be to have someone coming up with rules ( implemented in code ) like if this email contains word x , label it as spam , else if email contains ... and so forth . machine learning algorithms , are basically writing this program for you . for example , you collect some data ( spam and non-spam emails that are labeled as such ; this is called supervised learning , but i don't want to digress too much at this point ) and let an algorithm figure out the rules for you ; basically, the algorithm comes up with a model or program for spam classification then . here's a figure that i created a while back , which basically summarizes the paragraph above . i recently answered a related question on quora here to give you some examples of applications what are some real-world examples of applications of machine learning in the field ? re-ask now , back to tensorflow . you can basically use this in NUM ways : you could use it to do research in ml ( developing and implementing novel algorithms etc. ) you can build a ( predictive ) model or product that helps you with a certain business problem ( deep learning algos are especially good at certain tasks involving image and voice recognition , natural language processing , and so forth . or take google deepmind's recent alphago ai for example ) EOQ wow . excellent answer . this is precisely what i was looking for . if you have any beginner friendly books on the topic , ( technical and non technical )-would appreciate that as well . also , side question : as far as inputs in ml , can the data be in any format ? EOA 
 eli5: can someone explain the capabilities of tensor flow in layman's terms so i could think about some potential applications ? : machinelearning although my answer won't be exactly what you've asked for ( regarding the grandma part ) , let me comment on this thread in hope that it'll clarify some things . tensorflow is a basically a framework or library that lets you implement machine learning with focus on deep neural networks-by making use of gpus and the compilation of symbolic expression it's especially efficient at that . sorry , no offense here , but what you are asking is basically something along the lines i hear that tensorflow is really popular in the ml & tech community , how can i use it to jump onto the wagon and build something cool ? i think that's maybe the wrong approach to tensorflow ; i would ask the other way around : do you have a specific problem in mind , and would tensorflow be helpful for addressing that ? solve a lot repetitive tasks faster than humans etc ... let me give you an example : you want to build an email spam filter . now, your spam filter could be a human manually sorting your emails . next, the classic programming approach would be to have someone coming up with rules ( implemented in code ) like if this email contains word x , label it as spam , else if email contains ... and so forth . machine learning algorithms , are basically writing this program for you . for example , you collect some data ( spam and non-spam emails that are labeled as such ; this is called supervised learning , but i don't want to digress too much at this point ) and let an algorithm figure out the rules for you ; basically, the algorithm comes up with a model or program for spam classification then . here's a figure that i created a while back , which basically summarizes the paragraph above . i recently answered a related question on quora here to give you some examples of applications what are some real-world examples of applications of machine learning in the field ? re-ask now , back to tensorflow . you can basically use this in NUM ways : you could use it to do research in ml ( developing and implementing novel algorithms etc. ) you can build a ( predictive ) model or product that helps you with a certain business problem ( deep learning algos are especially good at certain tasks involving image and voice recognition , natural language processing , and so forth . or take google deepmind's recent alphago ai for example ) EOQ you might need conversion/filter steps , depending on the input data . EOA 
 eli5: can someone explain the capabilities of tensor flow in layman's terms so i could think about some potential applications ? : machinelearning although my answer won't be exactly what you've asked for ( regarding the grandma part ) , let me comment on this thread in hope that it'll clarify some things . tensorflow is a basically a framework or library that lets you implement machine learning with focus on deep neural networks-by making use of gpus and the compilation of symbolic expression it's especially efficient at that . sorry , no offense here , but what you are asking is basically something along the lines i hear that tensorflow is really popular in the ml & tech community , how can i use it to jump onto the wagon and build something cool ? i think that's maybe the wrong approach to tensorflow ; i would ask the other way around : do you have a specific problem in mind , and would tensorflow be helpful for addressing that ? solve a lot repetitive tasks faster than humans etc ... let me give you an example : you want to build an email spam filter . now, your spam filter could be a human manually sorting your emails . next, the classic programming approach would be to have someone coming up with rules ( implemented in code ) like if this email contains word x , label it as spam , else if email contains ... and so forth . machine learning algorithms , are basically writing this program for you . for example , you collect some data ( spam and non-spam emails that are labeled as such ; this is called supervised learning , but i don't want to digress too much at this point ) and let an algorithm figure out the rules for you ; basically, the algorithm comes up with a model or program for spam classification then . here's a figure that i created a while back , which basically summarizes the paragraph above . i recently answered a related question on quora here to give you some examples of applications what are some real-world examples of applications of machine learning in the field ? re-ask now , back to tensorflow . you can basically use this in NUM ways : you could use it to do research in ml ( developing and implementing novel algorithms etc. ) you can build a ( predictive ) model or product that helps you with a certain business problem ( deep learning algos are especially good at certain tasks involving image and voice recognition , natural language processing , and so forth . or take google deepmind's recent alphago ai for example ) EOQ glad to hear that it was helpful . if you have any beginner friendly books on the topic , ( technical and non technical )-would appreciate that as well . i'd recommend pedro domingo's the master algorithm ( as a non-technical one ; here is a link to a q & a ) . it's relatively new and just came out a couple of months ago . although the main topic of the book is about the quest towards finding a universal master ( machine learning ) algorithm , the majority of the book discusses the NUM major tribes of machine learning , plus, he gives tons of examples regarding applications . that's probably exactly the stuff you are interested in :). now , there are NUM kinds of technical : ( NUM ) technical in studying the algorithms ( from the math perspective ) and ( NUM ) technical in terms of implementations ( e.g., the programming part ) . to start with a general introduction on the math , get some practice ( and have some fun ) with the implementation , i'd like to shamelessly recommend my own book . i think that implementing algorithms as you go along is a great way to practice the theoretical knowledge , plus, it's more motivating since you can actually start solving your problems with it . although it's a bit of a challenging read for a beginner , i'd then recommend any of the three if you want to dive into the math in more detail : t . hastie, r . tibshirani, j . friedman, t . hastie, j . friedman, and r . tibshirani. data mining , inference, and prediction . NUM nd edition ., volume NUM . springer, NUM . c. m . bishop et al . pattern recognition and machine learning | christopher bishop | springer , volume NUM . springer new york , NUM duda , richard o ., peter e . hart, and david g . stork. pattern classification , NUM nd edition . john wiley & sons , NUM ah , yes, and i . goodfellow & y . bengio are working on a great deep learning book , the manuscript is available online for free . ( it really depends , but i think the books are best read in this particular order :p ) EOA 
 eli5: can someone explain the capabilities of tensor flow in layman's terms so i could think about some potential applications ? : machinelearning although my answer won't be exactly what you've asked for ( regarding the grandma part ) , let me comment on this thread in hope that it'll clarify some things . tensorflow is a basically a framework or library that lets you implement machine learning with focus on deep neural networks-by making use of gpus and the compilation of symbolic expression it's especially efficient at that . sorry , no offense here , but what you are asking is basically something along the lines i hear that tensorflow is really popular in the ml & tech community , how can i use it to jump onto the wagon and build something cool ? i think that's maybe the wrong approach to tensorflow ; i would ask the other way around : do you have a specific problem in mind , and would tensorflow be helpful for addressing that ? solve a lot repetitive tasks faster than humans etc ... let me give you an example : you want to build an email spam filter . now, your spam filter could be a human manually sorting your emails . next, the classic programming approach would be to have someone coming up with rules ( implemented in code ) like if this email contains word x , label it as spam , else if email contains ... and so forth . machine learning algorithms , are basically writing this program for you . for example , you collect some data ( spam and non-spam emails that are labeled as such ; this is called supervised learning , but i don't want to digress too much at this point ) and let an algorithm figure out the rules for you ; basically, the algorithm comes up with a model or program for spam classification then . here's a figure that i created a while back , which basically summarizes the paragraph above . i recently answered a related question on quora here to give you some examples of applications what are some real-world examples of applications of machine learning in the field ? re-ask now , back to tensorflow . you can basically use this in NUM ways : you could use it to do research in ml ( developing and implementing novel algorithms etc. ) you can build a ( predictive ) model or product that helps you with a certain business problem ( deep learning algos are especially good at certain tasks involving image and voice recognition , natural language processing , and so forth . or take google deepmind's recent alphago ai for example ) EOQ excellent . you deserve a cold beer my friend . ( or an equivalent vice ) EOA 
 eli5: can someone explain the capabilities of tensor flow in layman's terms so i could think about some potential applications ? : machinelearning although my answer won't be exactly what you've asked for ( regarding the grandma part ) , let me comment on this thread in hope that it'll clarify some things . tensorflow is a basically a framework or library that lets you implement machine learning with focus on deep neural networks-by making use of gpus and the compilation of symbolic expression it's especially efficient at that . sorry , no offense here , but what you are asking is basically something along the lines i hear that tensorflow is really popular in the ml & tech community , how can i use it to jump onto the wagon and build something cool ? i think that's maybe the wrong approach to tensorflow ; i would ask the other way around : do you have a specific problem in mind , and would tensorflow be helpful for addressing that ? solve a lot repetitive tasks faster than humans etc ... let me give you an example : you want to build an email spam filter . now, your spam filter could be a human manually sorting your emails . next, the classic programming approach would be to have someone coming up with rules ( implemented in code ) like if this email contains word x , label it as spam , else if email contains ... and so forth . machine learning algorithms , are basically writing this program for you . for example , you collect some data ( spam and non-spam emails that are labeled as such ; this is called supervised learning , but i don't want to digress too much at this point ) and let an algorithm figure out the rules for you ; basically, the algorithm comes up with a model or program for spam classification then . here's a figure that i created a while back , which basically summarizes the paragraph above . i recently answered a related question on quora here to give you some examples of applications what are some real-world examples of applications of machine learning in the field ? re-ask now , back to tensorflow . you can basically use this in NUM ways : you could use it to do research in ml ( developing and implementing novel algorithms etc. ) you can build a ( predictive ) model or product that helps you with a certain business problem ( deep learning algos are especially good at certain tasks involving image and voice recognition , natural language processing , and so forth . or take google deepmind's recent alphago ai for example ) EOQ who would mind a cold beer in the after work hours ;). glad that i could help ; diving into ml , you definitely have some exciting times ahead . happy ( machine ) learning ! EOA 
 eli5: can someone explain the capabilities of tensor flow in layman's terms so i could think about some potential applications ? : machinelearning although my answer won't be exactly what you've asked for ( regarding the grandma part ) , let me comment on this thread in hope that it'll clarify some things . tensorflow is a basically a framework or library that lets you implement machine learning with focus on deep neural networks-by making use of gpus and the compilation of symbolic expression it's especially efficient at that . sorry , no offense here , but what you are asking is basically something along the lines i hear that tensorflow is really popular in the ml & tech community , how can i use it to jump onto the wagon and build something cool ? i think that's maybe the wrong approach to tensorflow ; i would ask the other way around : do you have a specific problem in mind , and would tensorflow be helpful for addressing that ? solve a lot repetitive tasks faster than humans etc ... let me give you an example : you want to build an email spam filter . now, your spam filter could be a human manually sorting your emails . next, the classic programming approach would be to have someone coming up with rules ( implemented in code ) like if this email contains word x , label it as spam , else if email contains ... and so forth . machine learning algorithms , are basically writing this program for you . for example , you collect some data ( spam and non-spam emails that are labeled as such ; this is called supervised learning , but i don't want to digress too much at this point ) and let an algorithm figure out the rules for you ; basically, the algorithm comes up with a model or program for spam classification then . here's a figure that i created a while back , which basically summarizes the paragraph above . i recently answered a related question on quora here to give you some examples of applications what are some real-world examples of applications of machine learning in the field ? re-ask now , back to tensorflow . you can basically use this in NUM ways : you could use it to do research in ml ( developing and implementing novel algorithms etc. ) you can build a ( predictive ) model or product that helps you with a certain business problem ( deep learning algos are especially good at certain tasks involving image and voice recognition , natural language processing , and so forth . or take google deepmind's recent alphago ai for example ) EOQ tensorflow is just a tool . this is like asking a carpenter what a the capabilities of a hammer are . EOA 
 has microsoft released code for their NUM-layer cnn that won ilsvrc NUM ? : machinelearning URL EOQ not sure , but there are many reimplementations . here's one for lasagne : URL EOA 
 has microsoft released code for their NUM-layer cnn that won ilsvrc NUM ? : machinelearning URL EOQ here's one in torch : URL EOA 
 has microsoft released code for their NUM-layer cnn that won ilsvrc NUM ? : machinelearning URL EOQ keras : URL EOA 
 has microsoft released code for their NUM-layer cnn that won ilsvrc NUM ? : machinelearning URL EOQ just a fair warning that this keras example uses dropout instead of batch normalization . EOA 
 has microsoft released code for their NUM-layer cnn that won ilsvrc NUM ? : machinelearning URL EOQ caffe prototxts and pretrained models ( from the paper authors ) : URL EOA 
 how to represent a directed acyclic graph as a vector for input into a neural network ? : machinelearning you might pick up some tricks by consulting some of the in this area . EOQ you could try converting the adjacency matrix to a vector . this sounds like an interesting problem ! EOA 
 how to represent a directed acyclic graph as a vector for input into a neural network ? : machinelearning you might pick up some tricks by consulting some of the in this area . EOQ you could use kernel methods with graph kernels to perform classification , regression etc . without being dependent on an explicit representation of your dag instances as feature vectors ( instead , one uses a similarity measure between dags ) . nonetheless, some graph kernels even provide explicitly computable feature vectors of graphs , e.g. weisfeiler-lehman graph kernels . see URL EOA 
 how to represent a directed acyclic graph as a vector for input into a neural network ? : machinelearning you might pick up some tricks by consulting some of the in this area . EOQ depending on what you are doing chances are you can create a embedding matrix where each row will represent a node in your graph , and a weight matrix where each row will represent an edge in your graph . you can than learn functions that use the directed graph explicitly instead of implicitly . EOA 
 how to represent a directed acyclic graph as a vector for input into a neural network ? : machinelearning you might pick up some tricks by consulting some of the in this area . EOQ URL EOA 
 how to represent a directed acyclic graph as a vector for input into a neural network ? : machinelearning you might pick up some tricks by consulting some of the in this area . EOQ well you can always linearize them , because they're dags . but you lose some information in the process . EOA 
 how to represent a directed acyclic graph as a vector for input into a neural network ? : machinelearning you might pick up some tricks by consulting some of the in this area . EOQ a recursive neural network may be of some use . perhaps something like convolution over sequences , but the sequences are the inputs to a particular node , where the inputs are the outputs of the previous node's conv . though that does have a variable size problem . perhaps conv-> ; recurrent summarizer ? EOA 
 parallelized mutual information based feature selection module : machinelearning does your mutual information estimator deal well with categorical features ? if yes , how? i don't think kraskov's estimator can deal with discrete variables as described in the article . i don't think it's difficult to come up with modifications but if you already implemented them , this would be nice. EOQ hi , i used this method : URL and extended it so it can calculate joint mutual information , that is : i(x1,x2;y) EOA 
 parallelized mutual information based feature selection module : machinelearning does your mutual information estimator deal well with categorical features ? if yes , how? i don't think kraskov's estimator can deal with discrete variables as described in the article . i don't think it's difficult to come up with modifications but if you already implemented them , this would be nice. EOQ how computationally expensive is the feature selection process ? for instance , how long would it take to run on a dataset with NUM k observations and NUM features using a standard high-end laptop ? i've had the issue with mi that it was too computationally expensive for my kind of data . EOA 
 parallelized mutual information based feature selection module : machinelearning does your mutual information estimator deal well with categorical features ? if yes , how? i don't think kraskov's estimator can deal with discrete variables as described in the article . i don't think it's difficult to come up with modifications but if you already implemented them , this would be nice. EOQ the mi estimator is based on knn so the methods scale as scikit-learn's knn algorithm would . if i had NUM k samples i'd definitely consider sub-sampling my data , to about a NUM and running it multiple times , or running something that's less computationally intensive , like l1 regularised regression ( lasso ) or svm . EOA 
 parallelized mutual information based feature selection module : machinelearning does your mutual information estimator deal well with categorical features ? if yes , how? i don't think kraskov's estimator can deal with discrete variables as described in the article . i don't think it's difficult to come up with modifications but if you already implemented them , this would be nice. EOQ unimportant note : you misspelled feature on line NUM of your example code ( both in the readme and the blog post ) EOA 
 parallelized mutual information based feature selection module : machinelearning does your mutual information estimator deal well with categorical features ? if yes , how? i don't think kraskov's estimator can deal with discrete variables as described in the article . i don't think it's difficult to come up with modifications but if you already implemented them , this would be nice. EOQ thanks a lot for letting me know , will correct it ! EOA 
 parallelized mutual information based feature selection module : machinelearning does your mutual information estimator deal well with categorical features ? if yes , how? i don't think kraskov's estimator can deal with discrete variables as described in the article . i don't think it's difficult to come up with modifications but if you already implemented them , this would be nice. EOQ that's really cool-i find it hard to find feature selection methods that can be parallelised . i'd be interested to see the bench-marking when you get round to it . thanks! EOA 
 any open source framework for sliding window analysis : machinelearning have you checked biopython ( URL ) ? is open source and does a lot biostatistics related stuff EOQ time series analysis concerns itself with these types of problems . statsmodels and pandas in python have a bit of this iirc . from a programming perspective , toeplitz and circulant matrices are probably relevant to whatever your end goal is . what are you trying to do ? EOA 
 any open source framework for sliding window analysis : machinelearning have you checked biopython ( URL ) ? is open source and does a lot biostatistics related stuff EOQ i believe caret ( the r package ) has something for splitting data into continuous time chunks for cross-validation . not sure if this is similar what you're describing , but maybe worth a look . that package is really well designed from a software architecture point if view . EOA 
 any open source framework for sliding window analysis : machinelearning have you checked biopython ( URL ) ? is open source and does a lot biostatistics related stuff EOQ sparkstreaming has windowed operations . it only supports scala and java at the moment though . EOA 
 any open source framework for sliding window analysis : machinelearning have you checked biopython ( URL ) ? is open source and does a lot biostatistics related stuff EOQ if you can get the data into elasticsearch , their kibana product is really great for doing some standard sliding window analysis . URL EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ what is the advantage doing this ? a curriculum strategy where the easier samples are fed first ? EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ similar to how humans learn : from easy to hard . EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ ok , thanks for the idea , i will try that later tonight EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ yes , i have that in there i didn't word that very well . so my last hidden state of the last layers is projected onto the output layer of size 'hidden.size x num.labels' ( two in this case ) . then i apply the softmax to that output . weights-tf.variable(tf.random.normal([hidden.size,self.num.classes], stddev-0.01)) bias-tf.variable(tf.random.normal([self.num.classes], stddev-0.01)) self.y-tf.matmul(last.state, weights)-bias self.y-tf.nn.softmax(self.y) EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ i think you might be right about me not getting the last hidden state , but i'm not sure it's to do with that NUM . the NUM refers to the first dimension which goes from NUM to batch.size-1 . saying NUM , and the corresponding-1 should give me all the batches in a slice ( correct me if i'm wrong though! ) rnn.rnn() returns 'outputs , state' state is a list of size seq.length , of tensors of size 'batch.size x hidden.size-num.layers-NUM ' ( i verified through some trial and error testing ) . this seems to indicate to me states[-1] should be the last state where it is the output and hidden.state of each layer concatenated (which is where i got the batch.size x 'hidden.size-num.layers-NUM ' from . so , what i tried was changing that line to : last.state-tf.slice(self.states[-1], [0 , hidden.size-(2-num.layers-1)], [-1 , hidden.size]) i'm thinking that will give me the last 'hidden.size' length slice in each row of states[-1] , which i thought would correspond to the final hidden state of the final layer . unfortunately, after training for a few epochs that does not seem to be the case , the network still isn't learning . i do however think you are correct , and the mistake probably lies somewhere there . thanks! EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ here's what i have in mine : attns-attention(tf.slice(new.state, [0 , cell.output.size-(num.dec.layers-1)], [-1 , cell.output.size])) i apologize but it was my mistake . i think the NUM is okay . however, i don't think the hidden.size is the right thing to multiply by . instead, why not use the cell output.size ? so take the lstm output.size and use that as the indexing number instead of hiddne.size. i think the hidden.size is the concatenation of the hidden size of all the layers . so the hidden size is not an accurate number to go off of . don't mean to mislead you as i make mistakes all the time. EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ thank-you , i think that did the trick ! it is now learning ( finally ) , i can study for my midterms in peace . i also changed the way i was feeding data in , and re-wrote the model code to clean it up a bit . i will update my original post with the new code and short summary just incase anyone was curious/future people have similar projects . thanks again ! EOA 
 tensorflow sentiment analyzer help : machinelearning i didnt read your code but one thing i did when building a similar model is to first train a naive bayes classifier on the same task . then i pretrained the lstm on only the samples where the naive bayes classifier is correct with high confidence . EOQ awesome glad it worked for you ! that last hidden state part had confused me for a while as well-p EOA 
 hypercolumns and pixel classification : machinelearning each hypercolumn / feature vector / fiber has a receptive field : some region in the image that can affect the values in this fiber . if you don't use pooling or strided convolutions , moving between adjacent fibers corresponds to a shift of the receptive field by one pixel in the image . thus a NUM x7 output feature map gives information about a NUM x7 patch in the center of the image in this case ( using the surrounding pixels as context ) . for every time you use a NUM x2 pooling or stride-2 convolution , the stride of the output is multiplied by NUM . that is , a single step in the output will move you by NUM n pixels in the image ( where n is the number of stride-doubling operations ) . upsampling doesn't make much sense to me ; i would guess that a better approach is to apply the network NUM {2n} times , each time shifting the input by NUM pixel in x or y direction . the resulting feature maps can then be interlaced to give you a feature map that is the same size as the input ( minus border pixels lost to 'valid'-mode convolution ) , which has stride NUM . this could also be implemented without interlacing , by inserting zeros between pixels in the filters and using 'spaced' and non-strided poolings . EOQ it sure is much faster . but computing the full high-res feature maps might work better . also , upsampling would not give you full translation equivariance ( shift the image by NUM pixel-> ; the feature maps shift by one pixel ) EOA 
 hypercolumns and pixel classification : machinelearning each hypercolumn / feature vector / fiber has a receptive field : some region in the image that can affect the values in this fiber . if you don't use pooling or strided convolutions , moving between adjacent fibers corresponds to a shift of the receptive field by one pixel in the image . thus a NUM x7 output feature map gives information about a NUM x7 patch in the center of the image in this case ( using the surrounding pixels as context ) . for every time you use a NUM x2 pooling or stride-2 convolution , the stride of the output is multiplied by NUM . that is , a single step in the output will move you by NUM n pixels in the image ( where n is the number of stride-doubling operations ) . upsampling doesn't make much sense to me ; i would guess that a better approach is to apply the network NUM {2n} times , each time shifting the input by NUM pixel in x or y direction . the resulting feature maps can then be interlaced to give you a feature map that is the same size as the input ( minus border pixels lost to 'valid'-mode convolution ) , which has stride NUM . this could also be implemented without interlacing , by inserting zeros between pixels in the filters and using 'spaced' and non-strided poolings . EOQ interesting , are there any papers you would recommend that take this approach ? intuitively it seems like it could be rather slow , especially for pixel classification . EOA 
 hypercolumns and pixel classification : machinelearning each hypercolumn / feature vector / fiber has a receptive field : some region in the image that can affect the values in this fiber . if you don't use pooling or strided convolutions , moving between adjacent fibers corresponds to a shift of the receptive field by one pixel in the image . thus a NUM x7 output feature map gives information about a NUM x7 patch in the center of the image in this case ( using the surrounding pixels as context ) . for every time you use a NUM x2 pooling or stride-2 convolution , the stride of the output is multiplied by NUM . that is , a single step in the output will move you by NUM n pixels in the image ( where n is the number of stride-doubling operations ) . upsampling doesn't make much sense to me ; i would guess that a better approach is to apply the network NUM {2n} times , each time shifting the input by NUM pixel in x or y direction . the resulting feature maps can then be interlaced to give you a feature map that is the same size as the input ( minus border pixels lost to 'valid'-mode convolution ) , which has stride NUM . this could also be implemented without interlacing , by inserting zeros between pixels in the filters and using 'spaced' and non-strided poolings . EOQ i thought this paper took an interesting approach , not sure if it's exactly what tscohen is suggesting but maybe in the ballpark URL EOA 
 hypercolumns and pixel classification : machinelearning each hypercolumn / feature vector / fiber has a receptive field : some region in the image that can affect the values in this fiber . if you don't use pooling or strided convolutions , moving between adjacent fibers corresponds to a shift of the receptive field by one pixel in the image . thus a NUM x7 output feature map gives information about a NUM x7 patch in the center of the image in this case ( using the surrounding pixels as context ) . for every time you use a NUM x2 pooling or stride-2 convolution , the stride of the output is multiplied by NUM . that is , a single step in the output will move you by NUM n pixels in the image ( where n is the number of stride-doubling operations ) . upsampling doesn't make much sense to me ; i would guess that a better approach is to apply the network NUM {2n} times , each time shifting the input by NUM pixel in x or y direction . the resulting feature maps can then be interlaced to give you a feature map that is the same size as the input ( minus border pixels lost to 'valid'-mode convolution ) , which has stride NUM . this could also be implemented without interlacing , by inserting zeros between pixels in the filters and using 'spaced' and non-strided poolings . EOQ i've got another related question-can some spatial localisation of objects be gained from hypercolumns directly , doing classification and localisation in one go , without having to use something like r-cnn for localisation ? EOA 
 machine learning algorithm : machinelearning i'm not sure i follow . what do you mean by 'prepare meals' ? as in , generate new recipes that are similar to the ones you've shown it ? if that's what you're hoping for , you should look at generative models as they are used to generate novel examples from a distribution . which, in this case , would be a new recipe. though , it seems like it will likely be a difficult task if you are just using the words in the document . if you're actually hoping for useful recipes that will taste good , i don't see much hope for it . but if you're just hoping for something that looks like a recipe and uses recipe-like words and formatting , then you should look up one of the NUM rnn examples posted on this subreddit where people use a rnn to generate new examples . EOQ what i actually mean is a system that stores specific instructions and can recall those instructions . like a dictionary stores specific definitions for words . in this case you'll feed the model a document and it will extract all of the how tos from that document so that for example you can query it like : how do you make a veggie salad ? and it will present all of the instructions it is has on making veggie salads , be it NUM or NUM examples . EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ there are per-frame alignments of the timit corpus in microsoft's cntk repo here . also, they use the NUM state-level classes , like used in most papers on timit ( you can also find the bigram lm in the repo elsewhere ) , but you can easily reduce it down to the NUM phonemes . btw , i'm working on some notebooks for mlp using keras-also using timit . i wanted to publish them on github soon . may i ask , what you are working on ? EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ i am not the op , but a keras notebook would be helpful to me too . could you please aslo include one of the end-to-end architectures using the ctc loss function in your notebook ? EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ yea , i'll do that . i'm still kinda working on it , but i'll let you know via message when i have everything up . i'd like to get some feedback on it . i do want to do ctc , but i'm not sure how end-to-end i'll manage to get in this short time. for now , i'm recognizing phonemes only . one day , maybe i'll get to decode words . so far , i'm reproducing everything from alex graves' phd . i'm working to get as accurate ( or better ) results as he did there . EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ hi , can you kindly suggest how you have managed to get results close as those quoted in alex graves phd . EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ i used the hand-made alignemnts available in timit to generate a sequence . i'll post some code very soon-maybe today . i'll let you know . EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ ok , here is the file in question . look for the tosequence method . this is what i use to convert the hand-made alignments in timit into a classification for each frame . the reason i use textgrids is because i lost my original corpus years ago and that is all i have now . i can fix the notebook if someone gives me a hand ... the algorithm for determining the class for each frame is as follows : i iterate over all the frames ( determine their beginning and end ) i match the frame to each segment in the file : if the frame fits fully in a segment-i assign that segment's class as the frame's if the frame is half in one and half in another segment , i choose the one where most of the frame sits in if it's exactly half in one and half in another , it doesn't matter , so i generally choose the first ( thanks to < ;-) EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ just did the first example in the notebook here . lemme know if you have any suggestions . i'm off to do the other things now ... EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ here's the notebook of the first result . i'm working on the rest . i'd be glad to get some feedback . thanks! EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ amazing ! i'll have a look at it in a day or two and let you know . thanks for putting up so much work on this EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ hey , i also added the context-times-10 notebook as well . i'm not NUM % sure about it , but i guess it's fine. make sure to update with the latest code , cause i forgot to commit something last time and the previous notebook won't work without it either . EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ i guess with a temporal quantification of NUM ms you will have always an error < ;-NUM ms for your borders , it is inevitable . NUM frames-NUM ms , when rounding you will have a size of either NUM ms or NUM ms EOA 
 mfcc frames to phoneme alignment in timit data : machinelearning just round . when training gmm-hmms the training procedure ( baum welch algorithm ) uses the labels as a starting point but it gets aligned during training so the final alingnment may not be the one specified . when training using recurrent nets ctc loss function is used which does the same alignment procedure . if you re just using standard mlp just round to nearest frame , getting a state of the art speech recogniser going is suprisingly difficult . EOQ i am trying to do a simple classification problem . i am trying to predict the phoneme associated with each frame . for the time being i am trying to reproduce the results which alex graves published in his thesis 'supervised sequence labelling using recurrent neural nets' . in chapter five of his thesis he showed results of frame wise classification using a simple mlp with NUM sigmoid hidden units . however i have not been able to reproduce similar results . i feel there is nothing wrong in my implementation of the mlp , the only thing that might be a problem is the phoneme-frame tagging . EOA 
 why don't we use gaussian density to preprocess machine learning data per default ? : machinelearning often , the gaussian assumption does not hold . also p(x) will in generally not be invertible . EOQ i see , thanks for the info . professor made it sound like it mostly holds true ! EOA 
 why don't we use gaussian density to preprocess machine learning data per default ? : machinelearning often , the gaussian assumption does not hold . also p(x) will in generally not be invertible . EOQ you're describing an with a softmax on the output layer . EOA 
 why don't we use gaussian density to preprocess machine learning data per default ? : machinelearning often , the gaussian assumption does not hold . also p(x) will in generally not be invertible . EOQ thanks , will look into it :) EOA 
 why don't we use gaussian density to preprocess machine learning data per default ? : machinelearning often , the gaussian assumption does not hold . also p(x) will in generally not be invertible . EOQ thanks , will look into it :) EOA 
 is there any task where deep neural nets fail or can't do as well as other algorithms ? : machinelearning there are plenty of problems where linear models are enough and neural nets with many hidden layers make little difference or perform slightly worse . single hidden layer nets or gaussian process regression can often be great . but deep learning is an approach to machine learning , not a single technique or method . a deep learning researcher might use a shallow neural net or a linear model . deep learning is about selecting the right depth for the problem and explicitly acknowledging and optimizing ( at least through model selection ) the width vs depth tradeoff . there are also some types of problems where totally different models are appropriate . sometimes for very small datasets , for instance , one should prefer a carefully constructed bayesian model . sometimes that will be something non-parametric or sometimes it will be a bayesian neural net . or even on some problems something like transformation based learning ( a type of greedy rule learning ) can be the best . or decision trees ! anyone who says one method is always the best thing is exaggerating . what i do believe is that experts can get their machine learning paradigm of choice to do something reasonable on almost any problem with enough effort . i am a deep learning researcher and love training neural networks , the deeper the better . however, you will find that most experienced researchers are highly pragmatic and know that if one wants to get good results , one must be open minded . EOQ deep nns are going to suck big time every time when you don't have enough data . if all you have is NUM examples , knn or an svm with a suitable kernel will do wonders while sensible nn architectures will flop around just overfitting the few examples they have . having said that , unsupervised feature learning with nns ( assuming you have tons of unlabeled data ) will do wonders to set up your knn with a more suitable input representation . EOA 
 is there any task where deep neural nets fail or can't do as well as other algorithms ? : machinelearning there are plenty of problems where linear models are enough and neural nets with many hidden layers make little difference or perform slightly worse . single hidden layer nets or gaussian process regression can often be great . but deep learning is an approach to machine learning , not a single technique or method . a deep learning researcher might use a shallow neural net or a linear model . deep learning is about selecting the right depth for the problem and explicitly acknowledging and optimizing ( at least through model selection ) the width vs depth tradeoff . there are also some types of problems where totally different models are appropriate . sometimes for very small datasets , for instance , one should prefer a carefully constructed bayesian model . sometimes that will be something non-parametric or sometimes it will be a bayesian neural net . or even on some problems something like transformation based learning ( a type of greedy rule learning ) can be the best . or decision trees ! anyone who says one method is always the best thing is exaggerating . what i do believe is that experts can get their machine learning paradigm of choice to do something reasonable on almost any problem with enough effort . i am a deep learning researcher and love training neural networks , the deeper the better . however, you will find that most experienced researchers are highly pragmatic and know that if one wants to get good results , one must be open minded . EOQ [ deleted ] EOA 
 is there any task where deep neural nets fail or can't do as well as other algorithms ? : machinelearning there are plenty of problems where linear models are enough and neural nets with many hidden layers make little difference or perform slightly worse . single hidden layer nets or gaussian process regression can often be great . but deep learning is an approach to machine learning , not a single technique or method . a deep learning researcher might use a shallow neural net or a linear model . deep learning is about selecting the right depth for the problem and explicitly acknowledging and optimizing ( at least through model selection ) the width vs depth tradeoff . there are also some types of problems where totally different models are appropriate . sometimes for very small datasets , for instance , one should prefer a carefully constructed bayesian model . sometimes that will be something non-parametric or sometimes it will be a bayesian neural net . or even on some problems something like transformation based learning ( a type of greedy rule learning ) can be the best . or decision trees ! anyone who says one method is always the best thing is exaggerating . what i do believe is that experts can get their machine learning paradigm of choice to do something reasonable on almost any problem with enough effort . i am a deep learning researcher and love training neural networks , the deeper the better . however, you will find that most experienced researchers are highly pragmatic and know that if one wants to get good results , one must be open minded . EOQ [ deleted ] EOA 
 is there any task where deep neural nets fail or can't do as well as other algorithms ? : machinelearning there are plenty of problems where linear models are enough and neural nets with many hidden layers make little difference or perform slightly worse . single hidden layer nets or gaussian process regression can often be great . but deep learning is an approach to machine learning , not a single technique or method . a deep learning researcher might use a shallow neural net or a linear model . deep learning is about selecting the right depth for the problem and explicitly acknowledging and optimizing ( at least through model selection ) the width vs depth tradeoff . there are also some types of problems where totally different models are appropriate . sometimes for very small datasets , for instance , one should prefer a carefully constructed bayesian model . sometimes that will be something non-parametric or sometimes it will be a bayesian neural net . or even on some problems something like transformation based learning ( a type of greedy rule learning ) can be the best . or decision trees ! anyone who says one method is always the best thing is exaggerating . what i do believe is that experts can get their machine learning paradigm of choice to do something reasonable on almost any problem with enough effort . i am a deep learning researcher and love training neural networks , the deeper the better . however, you will find that most experienced researchers are highly pragmatic and know that if one wants to get good results , one must be open minded . EOQ yes , but i meant the pre-trained models have been trained already with a lot of data so in principle deep neural networks still need a lot of data . EOA 
 is there any task where deep neural nets fail or can't do as well as other algorithms ? : machinelearning there are plenty of problems where linear models are enough and neural nets with many hidden layers make little difference or perform slightly worse . single hidden layer nets or gaussian process regression can often be great . but deep learning is an approach to machine learning , not a single technique or method . a deep learning researcher might use a shallow neural net or a linear model . deep learning is about selecting the right depth for the problem and explicitly acknowledging and optimizing ( at least through model selection ) the width vs depth tradeoff . there are also some types of problems where totally different models are appropriate . sometimes for very small datasets , for instance , one should prefer a carefully constructed bayesian model . sometimes that will be something non-parametric or sometimes it will be a bayesian neural net . or even on some problems something like transformation based learning ( a type of greedy rule learning ) can be the best . or decision trees ! anyone who says one method is always the best thing is exaggerating . what i do believe is that experts can get their machine learning paradigm of choice to do something reasonable on almost any problem with enough effort . i am a deep learning researcher and love training neural networks , the deeper the better . however, you will find that most experienced researchers are highly pragmatic and know that if one wants to get good results , one must be open minded . EOQ i think integrating data over big heterogeneous sensor networks , like this URL EOA 
 is there any task where deep neural nets fail or can't do as well as other algorithms ? : machinelearning there are plenty of problems where linear models are enough and neural nets with many hidden layers make little difference or perform slightly worse . single hidden layer nets or gaussian process regression can often be great . but deep learning is an approach to machine learning , not a single technique or method . a deep learning researcher might use a shallow neural net or a linear model . deep learning is about selecting the right depth for the problem and explicitly acknowledging and optimizing ( at least through model selection ) the width vs depth tradeoff . there are also some types of problems where totally different models are appropriate . sometimes for very small datasets , for instance , one should prefer a carefully constructed bayesian model . sometimes that will be something non-parametric or sometimes it will be a bayesian neural net . or even on some problems something like transformation based learning ( a type of greedy rule learning ) can be the best . or decision trees ! anyone who says one method is always the best thing is exaggerating . what i do believe is that experts can get their machine learning paradigm of choice to do something reasonable on almost any problem with enough effort . i am a deep learning researcher and love training neural networks , the deeper the better . however, you will find that most experienced researchers are highly pragmatic and know that if one wants to get good results , one must be open minded . EOQ deep learning isn't a dominant technique because it's optimal for every conceivable problem . it's a dominant technique because it works for nearly any problem with a sufficiently large amount of training data , without requiring a huge amount of domain knowledge to build . it also tends to beat everything else for the really unpleasantly complex problems that we encounter in real life ( like machine vision ) . but it's very easy to come up with a toy problem with a simple structure and very little training data where a simpler and more finely tuned architecture will get the answer faster and more reliably . EOA 
 is there any task where deep neural nets fail or can't do as well as other algorithms ? : machinelearning there are plenty of problems where linear models are enough and neural nets with many hidden layers make little difference or perform slightly worse . single hidden layer nets or gaussian process regression can often be great . but deep learning is an approach to machine learning , not a single technique or method . a deep learning researcher might use a shallow neural net or a linear model . deep learning is about selecting the right depth for the problem and explicitly acknowledging and optimizing ( at least through model selection ) the width vs depth tradeoff . there are also some types of problems where totally different models are appropriate . sometimes for very small datasets , for instance , one should prefer a carefully constructed bayesian model . sometimes that will be something non-parametric or sometimes it will be a bayesian neural net . or even on some problems something like transformation based learning ( a type of greedy rule learning ) can be the best . or decision trees ! anyone who says one method is always the best thing is exaggerating . what i do believe is that experts can get their machine learning paradigm of choice to do something reasonable on almost any problem with enough effort . i am a deep learning researcher and love training neural networks , the deeper the better . however, you will find that most experienced researchers are highly pragmatic and know that if one wants to get good results , one must be open minded . EOQ deep neural nets can't really do this , and if it can will probably use a lot more computing resources than what was available to karl sims back then . URL EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ thanks for the help ! what about deepcl ? and is it possible to use cuda on arm mali gpus ? EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ this request seems familiar somehow :) URL both cltorch URL and deepcl URL are based on clblas NUM , which is mostly opencl NUM compatible . cltorch is probably more complete , but deepcl can be used from python . lua, as in , torch, is really lightweight , and might work well on mobile devices . EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ i haven't used deepcl but it's from the same author as the torch opencl backends . i'm not aware of mali specifically but nvidia announced limited arm support for cuda a while ago and jetson supports cudnn but i doubt that support extends to non-nvidia devices . is this for inference or training ? you might be able to get away with a fast matrix lib for opencl to handle inference . it would be some work though . EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ well , the opencl arm is only for pass throughs once the weights and parameters have been locked . the training can happen on cuda . EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ caffe cl just made it as a branch of the official caffe bvlc repository . EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ actually , this is a framework i'm still working on ( no docs yet so i'm sorry ) . trough arrayfire it supports opencl directly as well as cuda . repo: URL EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ veles supports opencl . benchmark results for alexnet and comparison to torch EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ that seems like it's written in python and limited to NUM neurons/nodes , correct? EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ it is definetly not limited to NUM neurons . otherwise alexnet would be impossible to implement . python is used as a front end language . core is written in c-/opencl/cuda . as far as i remember it has means to run trained model without python EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ huh , good to know , maybe i didn't look into it enough . my bad . thanks for the find ! EOA 
 best opencl deep neural network framework ? : machinelearning theano sort of supports opencl[0] via gpuarray[1] but its pretty buggy . torch also has a few projects[2] . but honestly , opencl will feel like a second class citizen in deep learning and should generally be considered a last resort until support improves (which isn't likely in the next few years?especially with amd adopting cuda[3]) . [ NUM ] URL [ NUM ] URL [ NUM ] URL [ NUM ] URL EOQ URL EOA 
 what's the best doc2vec tutorial ? : machinelearning URL EOQ understand word2vec first . doc2vec is a surprisingly trivial extension of word2vec EOA 
 cannot generalize a simple feed forward ann problem : machinelearning typical methods to combat overfitting would be reducing the size of hidden layers , adding l2 regularization , or adding dropout EOQ how many training samples do you have in each fold ? if you're using a few thousand sparse features then you're going to want a lot of training data . EOA 
 cannot generalize a simple feed forward ann problem : machinelearning typical methods to combat overfitting would be reducing the size of hidden layers , adding l2 regularization , or adding dropout EOQ my sample size is NUM ,000 EOA 
 cannot generalize a simple feed forward ann problem : machinelearning typical methods to combat overfitting would be reducing the size of hidden layers , adding l2 regularization , or adding dropout EOQ two things i can think of doing here : use additional layers ( NUM-4 total ) and to use relu instead of sigmoid . relus are an important part of the better performance of cnns in image classification . EOA 
 cannot generalize a simple feed forward ann problem : machinelearning typical methods to combat overfitting would be reducing the size of hidden layers , adding l2 regularization , or adding dropout EOQ relus help because they make it easier to pass gradient back through a deep net ( larger segment of space is non-infinitesimal gradient ) . deeper nets help capture more complex correlations . the problem op has is overfitting , so neither of these things is likely to help him/her . EOA 
 cannot generalize a simple feed forward ann problem : machinelearning typical methods to combat overfitting would be reducing the size of hidden layers , adding l2 regularization , or adding dropout EOQ what would you recommend to op ? EOA 
 cannot generalize a simple feed forward ann problem : machinelearning typical methods to combat overfitting would be reducing the size of hidden layers , adding l2 regularization , or adding dropout EOQ NUM ) smaller network NUM ) l2 regularisation NUM ) dropout EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ what date are they probably going to start replying ? EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ soon . EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ any idea what kind of stuff they're going to ask about in the interview ? EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ i received my rejection email today ( sunday jan NUM st ) . people should start hearing back soon . good luck to all who applied . EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ where are you from ? i've got mine too , feb NUM st ( around NUM :30 am gmt ) . i'm from eu . has anyone not from usa proceeded for interviews ? EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ i just got mine too . EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ friend got an email to schedule on site interviews EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ got a mail on NUM th january about dates/deadlines , and a link to a google docs form to add additional info for the application . nothing after that so far . EOA 
 has anyone heard back from google brain residency program ? : machinelearning i doubt anyone has heard back yet : ) EOQ haven't heard back yet either EOA 
 optimizing a complex non-differentiable loss function : machinelearning differential evolution . EOQ subgradient methods , if your function is subdifferentiable . EOA 
 optimizing a complex non-differentiable loss function : machinelearning differential evolution . EOQ a common trick is to use a surrogate loss . that is , a loss function which is a lot easier to optimize while somewhat leading to a similar minimizer/result . for example : in binary classification the actual loss one is interested in is the zero-one loss ( every misclassified example has a loss of one , and every correctly classified example has a loss of zero ) . however, that loss function is non-differentiable and non-convex . a common surrogate loss for the zero-one loss is the logistic loss which is a lot easier to optimize ( convex and twice differentiable ) and works pretty well in practice for non-separable data . in other words : it might be worth your effort to try and come up with a nice surrogate . switching to global optimization will most likely lead to bad performance . EOA 
 optimizing a complex non-differentiable loss function : machinelearning differential evolution . EOQ this is pretty difficult to do in my situation . the function i want to optimize is very complicated , and as a result has pretty high accuracy compared to simpler , differentiable solutions . one idea i had is to simply do binary classification , which doesn't require the function to be differentiable-just linearly separable ( and with new techniques , this is no longer the case ) . my loss function returns a result between NUM and NUM , and i could just label everything below NUM as wrong , and above NUM as correct . do you think this would result in good performance ? EOA 
 optimizing a complex non-differentiable loss function : machinelearning differential evolution . EOQ it may be possible to use an auto-differential system , like URL EOA 
 optimizing a complex non-differentiable loss function : machinelearning differential evolution . EOQ genetic algorithms EOA 
 simple seq2seq example in tensorflow ? : machinelearning this one is easy to follow . URL EOQ thank you ! this is a great find ; seems to be exactly what i need to get a foothold in tf's seq2seq paradigm . much appreciated . EOA 
 simple seq2seq example in tensorflow ? : machinelearning this one is easy to follow . URL EOQ thanks /u/deep.rabbit . glad you liked it . i try to keep my code clear and concise. :) EOA 
 simple seq2seq example in tensorflow ? : machinelearning this one is easy to follow . URL EOQ check out skflow package-URL the examples there are pretty easy to follow . language translation model example is coming soon . EOA 
 simple seq2seq example in tensorflow ? : machinelearning this one is easy to follow . URL EOQ thanks , though i was hoping to stay within vanilla tensorflow rather than learning another framework . EOA 
 part time ml jobs ? : machinelearning shameless self-promotion but hopefully useful : we are creating a platform for hosting ml trained models where we are also integrating a marketplace where non-it companies can hire freelance data scientists . the goal is that you and other machine learners will have access to a constant flow of projects and work at your own pace on the projects you choose . the marketplace has no activity yet because we are currently finishing other features of the platform , but feel free to register as a data scientist and you will start receiving requests from companies very soon . url : URL EOQ wow . thanks. i surely will . EOA 
 part time ml jobs ? : machinelearning shameless self-promotion but hopefully useful : we are creating a platform for hosting ml trained models where we are also integrating a marketplace where non-it companies can hire freelance data scientists . the goal is that you and other machine learners will have access to a constant flow of projects and work at your own pace on the projects you choose . the marketplace has no activity yet because we are currently finishing other features of the platform , but feel free to register as a data scientist and you will start receiving requests from companies very soon . url : URL EOQ ever take a crack at the markets ? if you can find something..you can make lots of $$ and it's entrepreneurial EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ wonder how cloaked in bias these statement are . will be interesting to see if all this hubris is smashed with similar results and the not grandmaster enough stuff turns out to be not much more than denial . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i think in their belief that alphago cannot sufficiently improve to grandmaster level you can question if that's not bias . but when it comes to the playing strength it displayed against fan hui i suggest taking them at their word . myungwan kim gave very specific and clear examples of the mistakes made by alphago , as wel as examples of where it played very well . there's just no way for me to get that specific while staying accessible to non-go players . but it would be very ungenerous towards myungwan kim to suggest he was giving a biased analysis and not his honest professional opinion of alphago's current strength . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ it'd be interesting to have go professionals analyze a game btween alphago and a human master without knowing that either is a machine. i suspect the analysis might look rather different EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ if you bring one or two top go players in the same room to analyze a game played at this level , they would be certain one of the players was a bot . you could still fool them and have the game be played between NUM bots . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ you could also give them a large set of games to analyze , most of which are human vs . human, so they can't make any assumptions about which games are automated play . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ sounds like a statistician/cs guy's job , not a go masters EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ on of the criticisms of alphago in the video is that it doesn't play creative or surprising moves but rather conservative moves . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ that's the thing , is it only searching as far as it needs . will conservative shift to be just barely enough to beat whatever input is coming in . will a lack of creativity and pure value based approach be enough to beat even the most devious human player . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ not sure why people keep on repeating this self playing thing without reading the paper . the rl trained nn is not used to generate moves as it is found to be inferior to the human only model . self playing is used to remove bias from the human model when training the value network . think of it this way : recorded human games are not statistically representative as moves that humans evaluate in their heads and rejected are not recorded . however the value network need to be able to evaluate all possible positions during search , like those played out in experts' heads . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ imo , the thing is not the number of games at this point , but it should be given such games that are played differently or more intuitively at the top-level which can give alphago more creativity and variations . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ it doesn't have to be playing against itself . for example , they could train alternate instances of the program on only korean , chinese, or japanese games and have an ai of each style . they could train an ai only on a set of games where lots of captures happened , or where few captures happened , and have an ai that likes to fight and one that doesn't . they could train one on games that spend a lot of moves in ko fights , etc. once they have these alternate versions of the ai , they can all play against each other , and learn from one another and the strongest can go on to face humanity . one other thing that wasn't considered in the summary is that google can just plug more computers into this algorithm . if what we've seen is alphago with a brain running on one hundred computers , what will alphago look like running on ten thousand computers ? if google is willing to commit those resources , which they do have available , then alphago may be able to make gigantic leaps in ability at will . google could play game one with a hundred machines online , lose and decide to power up alphago for game two . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ not necessarily it will play against itself . most likely it will be variations of itself and ml will be employed to evolve the best alphago they can . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ is having variations really necessary here ? in nature it makes sense because of factors like speciation ( branching to several species to exploit different niches ) , group collaboration ( each individual has a sightly different ability so overall they perform better ) , or threat prevention ( a virus does not wipe out the whole population because some carry congenital resistance ) . for a game player though you're talking about a totally ordered skill set : a beats b , b beats c implies a beats c . that's necessarily true for a given tournament structure , but not necessarily true across different tournament formats ( b might be better in one and a in another ) . conclusion: it seems to make sense to focus on the best specimen only ; or is it possible that it's training algorithm might get stuck on a local maximum ? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i am not really knowledgeable about go but my intuition is , that by focusing on the best specimen you are just creating a program that is good at beating itself.1 let's say that the program makes consistently a mistake in the NUM th round , which it can not exploit itself , then i am not sure were the information that it is a bad move should come from . NUM very loosely speaking , i am aware that for any program playing itself the win probability is .5. EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ yea i'll check out the paper when i have more time and comment back , but your intuition makes sense . i was hoping they could modify the chosen moves after a game was lost ( for a single player ) and reinforce the moves that lead to victory , without the need for the additional player , but i guess it depends on the details . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ if a program trained against itself has sufficient learning capacity/training , then you'd expect the global minimum to be the nash equilibrium . no one ; human or other could do better than play an equivalent nash equilibrium strategy against such a program , while a non nash equilibrium strategy would be exploitable by a competent adversary . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ iirc the global minimum is necessarily a nash equilibrium , however if you have some true evaluation function g and some approximation g' of your program , then just using games between the program would only find nash equilibria in g' . i would even expect that usually you can find series of nash equilibria in g' which are not even local minima in g . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ variations are very important , check out genetic programming . a beats b , b beats c does not imply a beats c . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ true , but it's rate of learning is way slower than a human , which diminishes the value of each game . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i think alphago is still gearing up and probably does not have a hard upper limit on improving game play . keep the electricity on and it can still be playing go in six hundred years . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ for the record , they were very well aware of go computers playing sub-optimally from winning positions and spend next to no time discussing such moves . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ ai is not their field of expertise. would you trust the assessment of a handwriting expert on the kinds of errors an ocr system makes if they're given only five samples , especially when actually good ocr systems were a new thing ? i'd take that with a grain of salt . still , i think i'm going to actually watch that video . usually i find videos a waste of time ( text is so much faster to read ) , but this one sounds like it's worth it . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i'd say that , in this particular case , the video is superior mainly because they're rapid-fire marking up diagrams of the game state . [ edit ] oh , i didn't see the comment below where you said you are a high-level go player , so you probably knew that this might be in the video . ( unfortunately , the pace of conversation can get slow , perhaps because one of the participants doesn't seem to be totally fluent in english . but that's hard to avoid , i suppose. ) EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i think the pace is slow because myungwan kim actually thinks a few times about the position and not because of his english abilities . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ given enough time , you can solve the game completely . however, there are practical limits , which are difficult to evaluate at this stage . the limits of this technology could also be below the grandmaster level . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ that's not true . you're assuming that a grandmaster strategy can be learned from the data . usually, for games , you need more than data ( at least than the type of data that can usually be acquired ) to reach the top level ( e.g. cfr for poker , mcts for go ) . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ if you have the data of every change in board state , you know every move . how would you need anything more ? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ alphago probably won't . mcts simply doesn't seem equipped to actually rise to the class of best of the best . it's good for making pretty strong bots , but i'm not sure if it can ever understand the concept of aji , for example . without concept of aji , you simply don't beat best human players . that's just too big a handicap you're giving to humans . even with god-tier ability to read ahead . if alphago really , really improves its reading ability , i could see it winning one or two matches against top human pros . but then humans would adapt . the errors in reading humans made , they would learn from those , but alphago won't learn concept of aji , so it keeps giving humans ridiculously large handicap while its advantage in ability to read ahead would shrink away . you either need to be slightly better and not do any particular mistakes , or if you do mistakes , you need to be really , really much stronger than humans to compensate for those mistakes . alphago by design will make really , really big mistakes . thus i don't see it ever being better than humans . better than me , sure. good enough to teach top pros thing or two about go ? hopefully, yes . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ fwiw you may be getting down votes because of the seemingly arrogant tone of the comment . that's independent of being right . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i wouldn't use those ratings for any real analysis . the problem is that there simply aren't enough games between the different regions in go . east asia is so far beyond europe and north america that only special exhibition games are played ; cjk pros are constantly playing competitive matches among themselves . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ there's no way to know alphago's rating , other than to say that it's probably significantly stronger than NUM . the program didn't lose a single game , so the rating of NUM is essentially meaningless , except as a lower bound . based on the outcome of these five games alone , alphago may well be as strong or stronger than lee sedol . after all , alphago achieved exactly the same score against fan hui as one would expect lee sedol to achieve . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ if i understand correctly the process of anchoring their scale , they used the full sample of NUM games against fan hui to anchor the metric , ignoring the differences in time controls ; alphago won the formal matches by NUM-0 , true, but it won the informal matches with short time controls by NUM-2 , so they do have an approximate reference point that way . lee shouldn't lose NUM % of his games against a NUM p pro . they also ran NUM-stone handicap matches against zen and crazystone programs , that should be around kgs-6d amateur strength ( before handicap ) and winning percentages against them from an internal tournament . its all very approximate , but at least its clear they themselves belive they need and can make advances on what they have before the lee matches . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i'm not sure how much games at shorter time controls say about how games at longer time controls should play out . i don't know how alphago's strength scales with search time. based on the outcomes of the formal games , however, it's impossible to give anything but a lower bound on alphago's playing strength . it's also important to keep in mind that these games were played several months ago , and that development of alphago is likely very rapid . i'm very skeptical of the confidence many go players are expressing that lee sedol will trounce alphago . i think the match will shock a lot of people . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ fair point re impact of time controls ; i havent'a clue either . and def , i'm pretty confident the team has a lot of low-hanging fruit to pick in order to improve this system . like, i was just commenting on r/baduk , its mind-blowing to me that this system was never exposed to a single pro game in training at all ( which facebook used exclusively and got sota results ) , or that they had a fairly substantial dataset used in training just one component-fast rollout-but also not used for training move prediction at all . even though they state even small improvements on move prediction netted them significant improvements in play strength . or that they used only NUM day of training ( and NUM 8 million games ) for reinforcement learning from self-play . getting to amateur NUM d rank . used that to produce a dataset of NUM million games ( !!! ) to train position evaluation on , picking just NUM positions from each and throwing the other ~150 away , for using all moves lead to overfitting . can they choose NUM positons and still avoid overfitting , increasing the dataset by an order of magnitude ? can they maybe bootstrap from alphago they have now , and create a dataset of NUM million positions from games played NUM elo points stronger than their puny NUM dan player , and learn positional evaluation on that ? etc. EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ its not denial , not based on the size of the mistakes that are being made in the games . if you read reviews of ke jie and lee sedol , the reviewer will be like , he made those NUM mistakes , each lost him NUM points , he lost the game by NUM points . the mistakes being made by both players in these games are like NUM-15 pt mistakes , which is rare at the highest level . its pretty objective in that sense . the computer may improve enough , lee sedol could get sick or have a terrible few games , but at its current level , based on the data we can see , it is not the best go playing entity in existence . NUM month ago , go computers were about as strong as NUM 's chess computers , today they are around the level of NUM 's chess computers , beating lee sedol would require the computer to play at the level of NUM or NUM chess computers . i think it may be a mistake to challenge lee sedol currently , because this development , accelerated go computers by a decade , now people will fail to focus on that insane accomplishment and instead focus on how we didn't jump NUM decades . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ no...thats not how it works . the size of the mistake has almost nothing to do with who you are facing or what data/logic was used to make the move . all that matters is how it could be best punished . so unless you see the review and feel that the optimal non-mistake continuation has some flaw that the reviewer missed , then its still a NUM pt mistake . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ for mistakes of that size to turn into much smaller mistakes in this context would mean something more along the lines of re-writing our understanding of go in a deep way because our current ability to analyze games after-the-fact is fundamentally flawed . given the end-state of a game for such analysis , it's not a difficult thing to point to the impact that individual plays along the way have in arriving at that outcome . like most problems , working backwards from the completed solution is easy , it's working forwards that's hard . we're good at the former , and it's probably safe to assume that alphago isn't going to be so good at the latter that we have to change the way we analyze games just to understand how good it is-especially considering that it does make mistakes . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i have no idea what you are talking about . either a) when you say 'input' you mean the training data and the board , in which case , the only 'mistakes' could be within the training data which isn't going to become perfect any time soon so i'm unsure how thats relevant . or b) the input is the board , which can't have mistakes in it because it is simply a board state . at first i assumed you didn't really understand go , but understood how machine learning worked...im much less sure now . if you design an algorithm to make good moves , and it doesn't make good moves in certain situations , it is not the situation which is wrong , it is the algorithm . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ it makes good enough moves , i'm assuming that when presented with higher skill board state , good enough will shift somewhat and may well cover a higher ranked player . tactics , devious creativity , may not be required . there is a chance that reacting to board state is enough and given board state generated by play with a higher ranked player , the responses from the current implementation and training data , may be sufficient to win . there is only one way to findout , while it will be very interesting whichever way things go . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ you are confusing its behavior in general versus its behavior when ahead . it isn't programmed to make 'good enough moves' what it is programmed to do is to make good moves , i.e. moves that increase its chance of winning . when it is ahead , its play is conservative and plays moves that are 'suboptimal' in terms of points , but are used to clarify the board state . i,e, trading winning margin for clarity , but always leaving itself advantaged . this is different from making a move that could swing the game on its head , which is what the mistakes are . if you seriously think it is making moves that it deems sub optimal , then i have nothing more to say . the reality is that it is always trying to make the optimal move , but optimal for the program is different from optimal for the reviewer . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ people are sorta cheering for alphago if anything . the matches didn't really give much reason to suspect lee sedol would have any trouble winning his match in march , but maybe alphago just hid its full strength during those games ? be it how it may , the games seemed like ones played by really strong players , but such that both players did make mistakes . can't really call it a bias . alphago won , but jump between fan hui and lee sedol is ridiculously large . there simply isn't much anything to suggest lee sedol would run into any problems winning the games presented there . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ uhh , i don't think anyone is in denial . you'd probably not say this if you understood go and watched myungwan's analysis EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ of course people are in denial to some degree in various ways .... half the quotes from ml industry side and go community side are i didn't expect to see this so soon . this is a shock for many . denial is a given . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ that's almost the opposite of denial . people in the go community aren't playing this down or claiming that it's not notable because it's fan hui and he had an off day . people in the go community would have been shocked by a bot beating fan hui when he was on an off day and only half trying . this is definitely an incredible acheivement and leap forward , regardless of whether it can come close to lee sedol . i've seen plenty professionals saying it might be NUM /50 between alphago and lee . no-one would have said anything like that last year . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ ladder problems are one of the simplest , since they don't usually branch much but have many moves in them . take this for an example : URL, and imagine staring at the first diagram and visualizing the result of the last diagram in your head . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ if they didn't pre-build in some structure for working out ladders then it's essentially the same thing as them handicapping their own design . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ on a pure they'll make that move , then i'll make this move , then they'll make that move pro players will routinely read one or two dozen moves ahead . in rare instances more . but what i meant with limited is something a bit more complicated . which is that , as a human , what you can do is you look at one part of the board , say the upper right corner . and you can now create a mental image where you say , okay, let's presume this part of the board stays the same , now let me guess how the rest of the board will look like NUM moves from now . this is something pro players do all the time ( and a skill that is way above my play level so take my explanation with some grains of salt ) . they will have a sense how the rest of the game is likely to play out , not move-for-move but as in white will likely get a strong formation of stones here , black will likely get a strong formation of stones here , and then they can think about how that will affect that formation of stones in the upper right corner . and they can decide to make a move , or not make a move , based on these predictions . iirc one direct example of alphago being weak at this was that it would make moves , which were the proper 'professional level' move , but it would make those moves way too early . it would play them immediately as the specific local shape appeared , when the pro's only play those moves much later in the game when the situation on other parts of the board made them necessary . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ good descriptions . one thing humans do is to imagine a future board position and see how that can be arrived at , using backward chaining . ladder is an extreme case . alphago only does forward searches , which could be its achilles heel . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ could this weakness be linked toward the japanese style that you said seemed to be favored in alphago's database ? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ unlikely , they mention in the video that the japanese style is not innately worse than other styles . it was my own addition to note that the japanese have been internationally weak over the past few decades , because that makes the end result being a japanese style surprising . but that didn't mean the japanese style itself is weak . though truly understanding the difference between japanese and chinese/korean style is way above my play level , so i can't explain it any more than this . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ no . here japanese style basically means that alphago doesn't play aggressive but often more conservative moves . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i'm an amateur NUM d and i routinely read at least NUM-30 moves ahead per variation tree . when i play without time limits , reading NUM moves ahead is almost a minimum for me during 'high variation moments' especially in the middle game . pros easily out-read me , meaning that they read even further ahead or instinctively know the variations that far out . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ the way i like to think about it is that , metaphorically, go is like playing chess if you had to create all of your pieces from scratch every game one move at a time out of much lower-level interactions . different structures of pieces can behave in different ways , and that behavior can be changed over time with the addition or removal of further pieces ( i also liken it to protein folding in the way it can be oddly sensitive ) . and so good players aren't just about evaluating the value of the next n moves , but are also evaluating in a more general sense what kinds of influence those moves will have on the structures on the board , many of which won't come directly back into immediate relevance until much later on in the game . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ do we know if this is the same version of alphago that they are going to use against lee sedol or if it will be an improved version ? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ we assume it'll be an improved version . the matches against fan hui were played months ago already , so of the top of my head i believe they'll have had like five months from those matches to the match against lee sedol . the pro's were sceptical the program could become a grandmaster in such a short time. but of course , while they're masters of go they don't know that much about ai , so they could very well be wrong on that account . no one except the deepmind team knows how much stronger alphago will be come march . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ ok , because reading the publication on the algorithm really made it sound like it was a work in progress that they put against fan hui . there are tons of improvements that they can do , and with all the publicity they attracted , they can also just borrow a datacenter to crunch numbers and make different versions of alphago play against themselves to generate even more data . comments like yours about the lack of depth in forward thinking may also be invaluable to them . they use a rather classical tree search that i suspect may be biased toward exploring a lot of options close to the situation rather than exploring a few long likely threads . i suspect it would be just tweaking for them to change this behavior radically . unfortunately, they only know how grand masters ever played , not how they explored the space of probabilities , so they can't use deep-learning for that . it probably won't exactly play like a human , as the psychological part and the exchange with masters about how they explore the possibilities are just not there , but it will get a better intuition and quite possibly a hugely improved search algorithm in its next game . so i would say that all bets are off ! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ | they use a rather classical tree search that i suspect may be biased toward exploring a lot of options close to the situation rather than exploring a few long likely threads . actually it seems the other way around-the algorithm plays out some trees out to game end ( win/loss ) . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ could it be that they want to challenge the top player just to get the best data ? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ a few games won't help much . machine learning usually needs thousands of examples to learn something new . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ still it's pretty cool that a dumb'machine can play so well , just based on a history of patterns and book play . makes me think if they taught it initiative , aji, ko et al , it would beat grandmaster level . i'm praying the humans win , but it does seem alphago can be trained better !!! i mean as it stands will it beat NUM % of players ???? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ way more than NUM percent . fan hui , who it readily beat in five games out of five , is ranked something among the thousand best players in the world iirc . so at best/worst , it can only be beaten by a thousand people of many millions of players . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ based on the description though , i think fan hui would likely win the next best of five. it seems he spent some moves testing the machine , plus he's used to teaching amateurs and made a mistake or two from that . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ it's easy in hindsight to say just don't make those mistakes then ? but the difference in skill between two players , which takes years of practise to bridge , is how often they make mistakes and how grave they are . to stop making those mistakes fan hui would need years of practise. during which time alphago would get even stronger ... i , too, think fan hui could play stronger-we saw that in the first game-but beat the computer a best of five ? highly doubt it . even if they are evenly matched there's only a NUM % chance he'll win it . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ there's a big difference between learning a skill for the first time and getting a skill back that's gone out of regular usage . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ wow even less than i thought ! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ now that it's in the news , and there's the upcoming match with korea's #1 , i think we can expect google to put considerable resources into alphago in the mean time. they may've only spent x computer time on the program to see how it would do in the uk match . in other words , i wouldn't expect alphago's strength to be the same at its next match . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ the speculation around japanese playstyle seems to be a hint that the professionals expect alphago to ingest a boatload more data and improve . i'd be really interested to know if that speculation is grounded in reality , and whether they would get improvements simply by feeding her more diverse data . ( or , perhaps, they could just be conjuring up a hypothesis that happens to kinda fit these five games , but isn't quite what's actually going on. ) EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ what go experts don't know is the speed at which computers can evolve in three months of self play and how much of difference will be to add NUM x more computing power at it . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ read the paper . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ fascinating , thanks a lot . my level at go is probably similar to yours , perhaps lower ( NUM kyu at my best ) . my own take as someone who knows not that much about go , and is not intimately familiar with the ai techniques employed : ko , sente and aji : these three aspects are a bit similar . they're all simple but very high-level concepts that allow humans to see very far ahead in the game without having to branch into every likely variation ( as a rollout does ) . the algorithms used are most likely incapable of discovering and exploiting these concepts as such ( probably too abstract ) . it is quite possible that monte carlo fails to see the long term benefits of moves in those contexts , in particular in some cases in which humans can . for instance , if other moves early on in the sequence rollout look very good , alphago could get sidetracked and miss the correct sequence . creativity : the paper states that the [ supervised learning ] policy network performed better in alphago than the reinforcement learning] policy network , presumably because humans select a diverse beam of promising moves , whereas [ reinforcement learning ] optimizes for the single best move .. in other terms , any creativity ( understood as exploring a diverse , rather than focused , beam of possible actions ) is taken from human games . consider how the rollouts work : at each time-step t of each simulation , an action at is selected from state st , such that at-argmaxa ( q(st , a )-u(st , a) [ nb:i'm using superscripts instead of subscripts . q(s, a ) is the estimated value of playing move a in state s] so as to maximize action value plus a bonus u(s , a)-p(s , a) / (1-n(s , a)) that is proportional to the prior probability but decays with repeated visits to encourage exploration . this will explore all of the most promising moves , in order of how promising they are . but it is quite possible that almost all of these moves correspond to a single overall strategy , and therefore the information obtained from exploring each of those moves in turn might be redundant . a creative player would of course focus on the most promising strategy , but would also investigate several diverse strategies , thus widening the beam of explored strategies , possibly coming up with a surprising , novel strategy . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ thanks , it's great to have the perspective of someone versed both in go and the programming side of alphago ! i'm thinking that if it does end up incapable of breaking the lee sedol barrier , that might be because the stronger you get , the smaller the database of human plays at that level . so if it needs many tens of thousands of human games to form its base level , that might limit its ability to surpass the absolute top human players ( of whom there's only a much smaller sample of games ) . but i could be completely wrong on this ! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ ha ha , it's a bit of a stretch to say i'm versed in either , but thank you . i think it'll fail against sedol because it's simply not creative enough due to using flat reinforcement learning . but i am extremely biased since that's my own area of interest ;-) also , i think i want go to remain an exciting problem for ai , since that would encourage people to create more interesting ai advances . it is also possible that sedol will be able to create situations which exploit alphago's ( apparent? ) weaknesses when it comes to aji , sente and kos ( although the latter remains to be proved ) , thus devising a specific anti-computer strategy . and finally , it's possible that the sheer amount of training might suffice for alphago to beat sedol , even if there remains some bias in its play . humans have a kind of biological maximum , so one would expect the difference between the top humans to not be gigantic . whereas computers probably have a different maximum and thus a much bigger margin for progression . we've seen that with chess-nowadays computers are miles ahead of any human competition , and a half-decent smartphone can beat any human on the planet . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ while neural nets definitely can learn some unexpected abstractions , i would say that most of the things that successful neural nets learn are the things they were designed to learn . the systems that really work aren't just randomly-wired miscellaneous computing nodes ; the nodes perform calculations that are likely to be useful to the representation . for example , in computer vision applications , we can say this node is for detecting diagonal edges because it's learned a convolution operation that detects diagonal edges , because the entire layer it's part of is designed to learn convolution operations , because those are a useful operation in image processing . in my own field , nlp, i feel like we don't have the right sorts of layers yet . even in vision , where deep learning is working great , i think there are abstractions it's not learning , like deepdream doesn't seem to learn the appropriate number of eyes and noses for a dog to have . looking at the alphago paper , it looks like their first layer is designed to recognize things such as adjacency , captures, and ladders , and layers above that are convolutions like you'd use for computer vision . there may be high-level strategic considerations that aren't going to be well represented by convolutions . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ the policy network is trained to output an action ( i.e. a NUM x NUM mask with a single entry NUM where the next stone should be placed ) given a NUM x NUM ternary input ( e.g. NUM : no stone , NUM : your stones ,-1: the opponent's stones , or a similar coding ) , using a large database of expert moves . that means that the network learns a given move at time t without knowing the moves at time t-1 , t-2, … and so on . i think , to learn high-level strategies from games it might be necessary to consider the previous moves since otherwise it's probably hard to infer what a player is up to . in addition to that , when the network comes up with moves to take it is also forgetful , i.e. it will not remember some sort of strategy that it decided to follow in the previous moves . there probably will be some correlation of reoccurring high-level patterns that end up as statistics in the network weights , but it definitely seems to operate rather in local strategies rather than global ones due to the forgetfulness and rather simple tree search heuristics which encourage exploration . in a nutshell : it's doubtful high-level strategies can be efficiently learned just from looking at different unrelated positions . to circumvent that , we would probably need a recurrent neural network so that the program can learn and reproduce sequences , which, however , would likely be much , much harder to train ; or perhaps it would require a different approach entirely ( e.g. one without , or more use of restricted tree search ) . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ very good . people who keep on salivating over millions more of self plays really need to read the paper . the roll out can produce creative plays , as human model ( used as prior ) gets a decay factor during simulation . but with a probabilistic simulation the moves requiring long sequences are less likely to be discovered ( ladder is found using special searches for example ) so long range planning seems to all come from the position value estimates . this would explain the balanced feel of its game strategy ( it's trained on kgs games , not particularly japanese as speculated. ) EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ after myungwan kim's comments the consensus here seems to be that alphago won't really have a chance in march , and people are wondering why deepmind chose to challenge lee sedol . i think it may be possible to shed some light on this question by looking at figure NUM a in the EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ to me , it sounds as if they may be looking to test the accuracy of an internal elo like algorithm . if it successfully predicted and ranked these games and participants , maybe testing whether it can do so in a match where alphago is expected to lose is also valuable : they can then internally assess when it is ready to win , and move into a later showcase match relatively assured of victory . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ possibly . though with all the publicity this is getting it feels like the stakes are pretty high . i'm not sure they would have challenged sedol publicly unless they were reasonably confident by the numbers . they could just play a bunch of top professionals under nda , as kenkotoko suggested . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ this a million times i am NUM % confident they have hired top go pros under nda to help them improve deepmind & it was with in discussion with them that they chose to challenge sedol . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ this is great , you should cross post this to r/baduk ! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ it's a shame that /r/everythingscience seems to have stopped doing the . they were a great little resource . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ those came from /r/futurology which still does it : URL EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ this is really awesome , thanks for post . very much appreciate the insights in go as an eager follower of machine learning and ai in general . from a game theory perspective , i wonder what it would take to start truly approximating a nash equilibrium strategy in go . obviously the true strategy is intractable bearing some absolute breakthrough , but getting 'close' would be awesome . i'd be very disappointed if top humans can be beaten without coming close to this optimum , which is why i hope for a human victory to push the frontier of ai further . hope to hear anyone more knowledgeable thoughts on the matter . in particular , given that the strategy/move space of go is highly sparse , i have a hunch that compressive sensing could play a significant role in optimising the weights of the monte carlo tree search . unfortunately i can't articulate this better currently . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ well it's clear it's based on patterns and book play . maybe stuff like initiative , aji etc .. will give the advantage to humans ? is its ko strength based on knowledge of patterns , i.e. is ko algorithmic ? so looking forward to march that will be amazing to see what happens !!!! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ they actually weren't sure about its ko strength , because there was only one case where it really came into play . but in principle a ko can be very straightforward . to try and explain what it is : without explaining the whole game : a ko is a situation in which there is the potential for an infinite loop . the board starts in situation a , then one player makes a move changing it to situation b , but then the other player can make a move reverting the board to situation a . thus creating a potentially infinite loop . to prevent this , the most basic ko rule simply says that it is illegal to do a move which returns the ( whole ) board to a position identical to one it already had before . so then why do ko's still exist ? because, imagine a situation where there's not just the potential for an infinite loop , but where both players would want to continue the infinite loop . so a case where situation a is really good for white , and situation b really good for black . black would want to keep changing the board to b , and white would keep wanting to change the board back to a . now with a ko rule they can't do that . you are white , black plays their turn and changes the board to b , now you're not allowed to revert the board back to a . but there's a solution . what you do as white , you make a different move , somewhere else on the board away from the ko situation . and you make a move that black has to respond to or lose many points . so black responds . now the board has changed , it's no longer b , it's b2 . so white goes back to the ko , and makes their move , reverting the situation to a locally , but not the whole board which is now a2 . the above tactic is called 'making a ko threat' . so one player threatens to do something which will gain them even more points than winning the ko , and the other player has to respond , thus allowing the threatening player to take back the ko . but, of course now the other player can do the same . at its core , all ko's are a simple arithmetic : whichever player has the most ko threats should win the ko . in practice , this opens up all kinds of difficult situations . for example sometimes there are two moves which do the same basic thing , but one also creates potential for a ko threat for yourself to use later . even if there's no ko at that moment in the game , optimal play is still doing the move which creates the ko threat because if you do end up in a ko later you can make use of it then . it's the kind of long-term planning that is common in top go play . tl;dr: i think you understand why i refrained from trying to explain ko in my main post , haha > ;. < ; EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ that was a very good explanation . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ interesting , seems to have spotted some key elements of how rl behaves . at times too obsessed with following common patterns , when the specific situation might require creative deviation from those patterns . also explained earlier . i wonder if another net which hallucinates training examples would help bring some of this in . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ there's a concept i've played with in my mind for a while , that i haven't been able to put into words . but i'll try to refine my thinking here , because it's relevant to that quote . imo , current machine learning techniques lack the ability to learn from stupidity . there is a quote that goes along the lines of the most dangerous opponent to the best swordsman in the world is not the second best swordsman in the world , but the unpredictable novice amateur . might not be true literally , but the sentiment is what i'm interested in . to that sentiment , i would add that i believe expert swordsmen have some ability to learn by watching novices , and noticing the ways in which their decisions confuse or manipulate or exploit the experts . or, to give a different example for your intuition to latch onto , it's a common trope in movies and television shows for one character to say something idiotic , only for a smarter character to overhear and says you're a genius ! and then manage to reinterpret their idiotic idea into something creative and useful . the closest thing to learning from stupidity that currently exists is boosting techniques , but those more like eking out small bits of intelligence from a large pile of stupidity than directly incorporating the wisdom of stupidity . what i'm thinking about is more along the lines of finding the symmetries that underlie the structures of problems , and finding good moves indirectly by thinking about what good ideas exist in the neighborhood of bad ideas , or what bad ideas exist in the neighborhood of good ones . again , this is very difficult to put into words . but i think it's one of the major missing pieces in ai research , although i have seen some papers here and there whose authors also seemed to be reaching toward a similar tool . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ kind of , although that seems more like the ability to learn from mistakes and avoid them in the future than the ability to use mistakes to infer things about the problem structure or the ability to take inspiration from mistakes . from what i've seen in the past , many of those adversarial networks are using tiny deviations in order to trick the classifier into misclassification , whereas i'm looking for something less trollish and more substantive. it's in the same neighborhood as the ideas i am reaching toward , but not quite right . if you find one adversarial example , or an arbitrary collection of adversarial examples , that is not substantive. if you generate principled rules for producing adversarial examples , then used those rules to reason about the way the problem's true answers must look , that would be . i came across a paper a couple months ago , i think google's researchers were involved , that had something to do with inverse problem structuring , mathematical symmetry , and image classification , and that was the closest example i've ever seen of this idea . unfortunately, my computer crashed shortly after that , and i haven't been able to recover the bookmark . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ look at the paper he posted . it's kind of like that . they train one network to produce images that look real , and another network to try to distinguish real images from the fake ones . both nets are essentially fighting each other and trying to learn from each other's mistakes . there's at least some similarity to what you are talking about . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ that sounds kind of like the how young songbirds inject noise into their fathers' songs as they learn them . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ thanks for this ! this is the type of commentary we need to understand these models heuristically . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ thanks for this great summary ! i wonder if it's possible to hand the game records to an expert go analyst without informing him/her that one player is alphago , and asking for an analysis of each player's strengths and weaknesses . it strikes me that the comments about alphago's play ... great at tasks requiring lots of computation and standard play lacks creativity and insight ... are almost exactly the comments one would expect to be made about a computer player , before one ever observes the computer's play . so , how much are the comments influenced by the prior ? it might even be more interesting to inform an analyst that both players are human , and see what the resulting analysis yields about alphago . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i think there might be some bias in how they put it , but at its core they weren't like speculating but really founding the analysis with clear and repeated examples of alphago's play . the lack of 'creativity' is something that is also a problem for many human players . it is the difference between being told how to play by players better than you , and actually understanding why you're doing certain moves on your own . the former can get you to do the right move in NUM % of the cases , but you need the latter to recognise the NUM % when you need to deviate from standard play . it should also be noted that , as they said in the beginning , they were focussing their analysis on the things which felt bot like. they zeroed in on those instances were alphago was making mistakes unlike those a human player would make at that level . in general their response and that of many pro's is that alphago played very much like a human , and if they'd just been given the game notes they might not've guessed it was an ai playing . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ the one angle which is most interesting to look at is form . if you take fan hui's comments seriously he did not consider the computer a threat coming into the match . likely he did would not have taken a large amount of time to mentally prep for the machine beat down he expected . the machine beat a low level pro who probably spent very little time preparing for the match . now you have a top pro with NUM months to get in top shape for a set of matches with a huge purse . which guarantees the pro will be in top form . the physical an mental preparation for high level board game play for human players is difficult to understate . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i haven't kept up with this for a while , but historically , japan dominated go throughout twentieth century . they had very old go-masters battle each other for whoever might be the best of the best . they also sorta sent go-aid to korea and china , where go was not in the best shape ever , and provided opportunities for go players there . however, by NUM , korea and china started to emerge as very potent go countries , with very strong emphasis put on practicing go problems and raw ability to read ahead . young kids started to dominate the go world , and their style was increasingly aggressive and fight-oriented . by comparison , japanese style , with much tradition behind it , sought to establish very elegant balance between two sides , with relatively simple sequences chosen mostly throughout games . this style however simply wasn't good enough , korea and china with their child prodigies just passed by japan . modern study seems to indicate that actually twentieth century great japanese players in fact did often misread sequences , and had much weaker local reading ability than modern pros . to some extent , they made up for it by having extremely good sense of balance and flow of the game , but ultimately raw reading ahead ability seemed to dominate . korea especially was known for their players that would choose extremely complicated moves , and make really difficult fights happen . this is the korean style you'd so much hear about , and lee sedol was the one that managed to prove that this brutal and inelegant style could in fact beat the more elegant styles held by older pros , lee changho in particular . lee changho was known as the stone buddha in NUM 's , he was the strongest player on the planet , with very calm moves , and very calm appearance during games . there were plenty of players trying to beat him with this korean style , but lee sedol was the one that finally took him down . sedol was kinda posterboy for the reckless fighting style of modern korea . i don't really know how china fits in all of this . their style seems closer to koreans , but it's not really as flamboyantly reckless . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ awesome read , thank you ! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ no , sorry, that's way above my play level . i only know that in general , japanese style is more passive and chinese/korean style more aggressive. but nothing with more detail . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ good enough for me ! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ this is my layman's understanding of the styles from pro games i've seen and what i've heard , take it with a grain of salt : the main difference is that korean and chinese styles are based heavily on aggression , local reading , and quickly gaining strong , local positions . the japanese style is based more on a global awareness of the board and influence , which is the ability of the various local structures around the board to globally benefit each other for significant gains in the mid-game , despite giving less points to the player locally ( if at all ) . in recent years the korean and chinese styles can safely be said to have dominated the japanese style , though this was not always the case . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ very cool ! EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ sweet , thanks for the link to the review . have been looking forward to seeing what pros think about alphago's style . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ great read , thank you . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ who wants to place bets on the lee sedol-alphago match ? i will make odds . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ whoa , that's cool , a go player on the hs subreddit , strange that i didn't see that post , or maybe it didn't get to front page ? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ i'm a bot , bleep, bloop . someone has linked to this thread from another place on reddit : [ /r/depthhub ] if you follow any of the above links , please respect the rules of reddit and don't vote in the other threads . ( info / contact ) EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ word of warning : i'm a crap go player and i don't know many of the details of how the ai in this case actually works . this makes sense from what i know of go and the complications of creating an ai to play the game . in go there are always a ton of different moves ( compared to a game like chess which has a lot of permutations , but only a few moves per turn ) and a lot of ambiguity . this is often what makes it such an intuitive game : you can't really say there's any one reason why you prefer one move over another since there's so many moves where each is only slightly more beneficial than another . the ai seems to have the sorts of setbacks that i would imagine it would have . they've managed to make it general enough that it can recognize general patterns and positions on the board into a sort of family-resemblance of games and choose from its repertoire plausible archs to go for . so it sounds like it's playing on a level that's above tactics , but still not quite the high-level strategy that really good go players play with . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ why is alphago trained only against itself , and not against other ais ? is it a lack of training data , or a lack of available computing power ? EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ the other ais aren't good enough to make it worthwhile . EOA 
 synopsis of top go professional's analysis of google's deepmind's go ai : machinelearning upcoming match against lee sedol : myungwan kim says that with all his respect to the google team , he thinks alphago as it played against fan hui will have no chance against lee sedol . he says all pro's who've looked at these games generally agree that alphago would need a one or two stone handicap against lee sedol . myungwan kim actually says he thinks alphago when it faces off against lee sedol might be as strong as he is . in which case he still predicts lee sedol will win every game . they think that lee sedol won't make the mistake of playing overly aggressive like fan hui did . they feel google is overplaying their hand somewhat challenging lee sedol this quickly . they say it's an amazing accomplishment to get an ai this strong , but as is it's not yet at grandmaster level . so by moving the goalpost from beating a low-level pro to beating a grandmaster this quickly they're somewhat cheapening their own accomplishment in beating fan hui . my thoughts : just keep in mind even at my best when i played a lot i was never more than a middle-ranking amateur ( not even a dan rank , just a high kyuu rank ) . but what i find interesting is what seems like a polar opposition between alphago and deep blue . deep blue worked , afaik, because it could read much further ahead than any human chess player . but alphago actually has as its weakness that it doesn't read ahead that well at all ( and one difference between chess and go players is that go players can in limited ways read dozens and dozens of moves ahead ) . deep blue worked by being a superhuman calculator . alphago works by having a kind of superhuman intuition about what are good and what are bad moves , but it still makes mistakes because it doesn't really understand why a move is good or bad . it'll be very interesting whether the program as is can ever compensate for that flaw . since i don't understand anything about the program my first guess would just be yes , but if it turns out to be no , that would actually be even more fascinating . from a go perspective and from a programming perspective , i'd wager . :) EOQ holy shit , you know alot about go EOA 
 fine-tuning semantic segmentation : machinelearning the vgg path is very good , and seems fairly straightforward . the authors also share code , so you could finetune that whole model with only the NUM focus classes . EOQ i'm in the middle of the paper you linked .. the finetuning process is analog to fine-tuning for image classification , with real-valued output ? EOA 
 fine-tuning semantic segmentation : machinelearning the vgg path is very good , and seems fairly straightforward . the authors also share code , so you could finetune that whole model with only the NUM focus classes . EOQ yes , just doing an extra few epochs of training but only with your data . EOA 
 fine-tuning semantic segmentation : machinelearning the vgg path is very good , and seems fairly straightforward . the authors also share code , so you could finetune that whole model with only the NUM focus classes . EOQ the fcn models use pretrained vgg weights . i have had good results following their same training procedure but with my own classes . EOA 
 please suggest good beginner tutorials ( or papers which can be read as starting points ) for neural nets with attention : machinelearning thanks for the awesome list . this helped a lot . EOQ recurrent model of visual attention in torch : URL EOA 
 please suggest good beginner tutorials ( or papers which can be read as starting points ) for neural nets with attention : machinelearning thanks for the awesome list . this helped a lot . EOQ note that this is the hard attention variant of attention-most papers are using soft attention since it is easier to optimize ( but doesn't provide computational gain ) because you don't need reinforce for the gradient . i covered a bunch of the soft mechanisms in my post , but hard attention is also neat because it also has potential to reduce computational complexity in our algorithms by actually removing unnecessary information , rather than just downweighting . EOA 
 please suggest good beginner tutorials ( or papers which can be read as starting points ) for neural nets with attention : machinelearning thanks for the awesome list . this helped a lot . EOQ URL EOA 
 please suggest good beginner tutorials ( or papers which can be read as starting points ) for neural nets with attention : machinelearning thanks for the awesome list . this helped a lot . EOQ URL EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ is pascal titan released yet ? how much more powerful is the pascal titan than titan x ? EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ its not released yet so we don't know the specifics of how much better it is . speculation is that it will be quite a bit better than the current titan x when you consider : hmbv2 memory , NUM x bandwidth over titan x(i think) node shrink , so denser/more circuits good fp16 support EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ my impression was pascal was coming out in june , but who knows ? also what is the expected price for it ? EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ i've built this box a couple times and i quite like it : URL titan x's are definitely the way to go . there isn't much speed boost over the NUM ti but the extra memory is really nice to have . EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ URL EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ i read that before i made the thread . it says NUM for the NUM . what i didn't figure out is if the NUM is cudnn v4 compatible ? EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ it is . most of the current work on cudnn seems to be improving performance on maxwell , and the NUM is a maxwell-based card . it's a good choice. depending on your budget you might also want to look at the NUM ti and the titan x ( if you expect you'll need a lot of memory ) . EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ thank you for the info ! EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ i own a NUM ti . it's a great device for ml ( and gaming ) . however, take care that you have a sufficiently powerful psu ! mine still apparently isn't powerful enough ( NUM or NUM watts , not sure , too lazy to look ) and i keep experiencing sudden reboots when doing ml tasks on the gpu . gonna replace the psu once i have the required bucks ... EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ good point . i'd always recommend at least NUM w when doing gpu work . and well over NUM w if using two . URL EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ it depends on the brand of psu as well as the other hardware you have in there . for a NUM ti or a titan x you need to reserve about NUM w , so NUM w is cutting it very close (the cpu also uses a sizeable chunk , as well as any hard drives you may have-and don't forget the fans!). for a regular NUM it's much less though , about NUM w . so a NUM w or NUM w psu would probably be plenty in that case . EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ yes , the exact numbers depend on many things in your hw setup . but with full throttle on both cpu and even a standard NUM card , NUM w is a guaranteed outage . EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ if you use the psu that came with your case , then yes :) but if you buy a decent-brand psu that really shouldn't happen , the tdp of a NUM is only NUM w . EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ if you are doing something like rnns or very deep networks , you'll want to consider memory . definitely something with more than NUM gb is ideal for experimenting ; nothing is worse than finding out your model doesn't fit in ram . :( i'd recommend the NUM or the NUM ti . there are comparisons between the NUM ti and the titan x that report very similar performance but the NUM ti is far cheaper . the only use case i can see for a titanx is if you need the NUM gb of ram . pascal is coming out this year , and the performance bump expected is supposed to be huge ( much faster memory bandwidths , new ops for dnn , and a die shrink ) if this is for fun , i'd recommend getting something cheap to play with for now-perhaps a used card on ebay if you can find one and save up for a nice pascal card layer . EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ NUM is sufficient for beginning . it's dozens of times faster than learning on a cpu which is the speedup you are looking for . EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ URL EOA 
 which gpu for deep learning ? : machinelearning pascal titan > ; titan x > ; NUM ti > ; NUM > ; NUM . NUM is still viable . EOQ i already found links from NUM and NUM ( the linked article among others ) . i was trying to verify that the information is still up to date with the new tools released that use cudnn v4 etc . EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ hopefully not in r ! EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ how come ? EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ hi , i am indie android app developer and completed my course of basic machine learning on coursera , i have NUM years of android development experience and would like to fiddle around some real ml EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ hi , i'm a python programmer and i'm interested in machine learning . EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ great , have you heard of kaggle ? check it out and let me know if you'd be interested in working on some of the problems with me ! EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ i have an account in kaggle .. lol. i'm thinking to play with some problems NUM weeks later ( i'm a chinese guy , and i'm gonna enjoy my lunar new year. ) EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ what competitions are you looking at doing right now ? EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ i haven't done that yet . i'm just reading something there now . EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ i'm interested in the home depot product search competition EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ this competition looks great . EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ i'm interested as well . got a university computer science and engineering background . EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ hi there , i'm interested too . i have familiarity with keras , sklearn and other python packages for ml . EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ hey , i'd be down ! been learning ml models for the past year , and familiar with pandas , sci-kit learn , theano, torch . EOA 
 hey everyone , i'm looking for fellow casual data hobbyists who want to team up to enter some kaggle competitions : machinelearning @itchybumbum-i'm entering my first kaggle this round with prudential . i'd like to find a group who can be a regular team . there are a few coming up that accept teams . im me . i'm pretty sophisticated in r , and also build neural nets . EOQ hello , i am interested in kaggle too , just tried mnist on kaggle . my interest focuses on computer vision and deep learning , familiar with python and deep learning frameworks . EOA 
 more epochs vs more data in a given time period : machinelearning more data . take the extreme case : would you rather train NUM m epochs on NUM datapoint or NUM epoch on NUM m datapoints ? obviously the later , the former would lead you nowhere . EOQ agreed . the concept of an 'epoch' is somewhat artificial anyway : if you use on-the-fly data augmentation your network will never see the same exact example twice , so then you might as well start randomly sampling data points for each minibatch and 'epochs' become meaningless . the ideal situation is where you have infinite data and are never able to complete an epoch in the first place . EOA 
 more epochs vs more data in a given time period : machinelearning more data . take the extreme case : would you rather train NUM m epochs on NUM datapoint or NUM epoch on NUM m datapoints ? obviously the later , the former would lead you nowhere . EOQ if your validation set is more similar to the smaller training set than it is to the larger one , it might do a better job on the validation set having been trained more on the smaller training set . otherwise i think the larger training set should be better because it will reduce overfitting . but at some point your data set will be large enough in proportion to the number of parameters in your network that overfitting won't be a concern , and i doubt increasing the size of your data set would improve things at that point . i'm not an expert in machine learning but i have played around with char-rnn a whole bunch :) EOA 
 more epochs vs more data in a given time period : machinelearning more data . take the extreme case : would you rather train NUM m epochs on NUM datapoint or NUM epoch on NUM m datapoints ? obviously the later , the former would lead you nowhere . EOQ i believe the concept you are touching on is curriculum learning which is when a model is trained on easier problems before it moves on to harder problems . URL EOA 
 more epochs vs more data in a given time period : machinelearning more data . take the extreme case : would you rather train NUM m epochs on NUM datapoint or NUM epoch on NUM m datapoints ? obviously the later , the former would lead you nowhere . EOQ it depends . is NUM books enough to generalize the dataset ? did you choose those NUM books out of the NUM wisely ? is your neural network deep enough to not under-fit NUM books , let alone NUM books ? training with enough epochs is important if you decrease the learning rate once in a while. the network may be gravitating around a local optimum b/c its learning rate is too large . so you can try to decay the learning rate within the NUM epoch of NUM books . just make sure that before each decay event , the next series of mini-batches you pass in is similar to the previous series (e.g. i.i.d. sampling , stratified sampling , etc.). try NUM vs NUM books , and checkout the difference in performance . EOA 
 anyone know of a good tutorial on hmms ? : machinelearning look at this EOA 
 anyone know of a good tutorial on hmms ? : machinelearning look at this if your looking for an implementation you can just use that's a different issue EOA 
 anyone know of a good tutorial on hmms ? : machinelearning look at this EOA 
 anyone know of a good tutorial on hmms ? : machinelearning look at this EOA 
 anyone know of a good tutorial on hmms ? : machinelearning look at this think of it as using bayes rule to enter into a markov model . that is kind of why we call it a hidden markov model . because we need inference to place us somewhere in that state diagram . EOA 
 anyone know of a good tutorial on hmms ? : machinelearning look at this EOA 
 anyone know of a good tutorial on hmms ? : machinelearning look at this this tutorial for accord is just awesome URL EOA 
 how to apply ai to play computer games ? : machinelearning and board games . you can probably find more stuff out there if you look hard enough too . or make your own games :) EOQ [ deleted ] EOA 
 how to apply ai to play computer games ? : machinelearning and board games . you can probably find more stuff out there if you look hard enough too . or make your own games :) EOQ thanks . yeah not planning on commercializing it , just want to see what i can apply it to . EOA 
 how to apply ai to play computer games ? : machinelearning and board games . you can probably find more stuff out there if you look hard enough too . or make your own games :) EOQ i am quite keen to learn more on this subject ; would you be able to direct me towards some good links ? what you mention ( pacman , gridworld, a learning ) seems quite related to this link : URL EOA 
 how to apply ai to play computer games ? : machinelearning and board games . you can probably find more stuff out there if you look hard enough too . or make your own games :) EOQ yeah that's what i did , but was looking to apply it to something more complex . EOA 
 how to apply ai to play computer games ? : machinelearning and board games . you can probably find more stuff out there if you look hard enough too . or make your own games :) EOQ i find myself pretty much in the same spot as you right now and instead of doing rl which is what i wanted to do , i find myself working on a f# compiler for vgdl ( video game development language ) in order to internalize the gvgai library that uses the aforementioned language to simulate various atari style games . you should definitely check out the library if you are interested in rl . i did not find it hard to get it running , though figuring out the source code for it is an entirely different matter ... i saw an interview with demis hassabis where he said his algos would see the pixels of the screen to learn to play atari ? that seems extremely complicated ... actually just representing all the states as pixels would make things quite a bit easier from a coding perspective , though without a neural net to compress them , you would be looking at an exponential blowup in the number of states you would need to store with tabular methods . an easier alternative to the java gvgai library would be to adapt these java engines from the ai games site that was posted just recently . also there is the much slower python version of gvgai library . EOA 
 how to apply ai to play computer games ? : machinelearning and board games . you can probably find more stuff out there if you look hard enough too . or make your own games :) EOQ i'm working on a similar project in my research . for python i would suggest you look at my library URL . it has examples of dqn and an already created interface into the arcade learning enivornment ( with pre-compiled dlls if you're on windows ) . the library is still in development phase but should give you a good start . i am still in active development so most examples use old functions . but the breakout.dqn.py here ( URL ) works and gets about NUM points ( clipped reward ) during training . so it's still far from the original paper but it's getting there EOA 
 feature engineering : machinelearning the 'black magic' of ml . id suggest taking a look @ kaggle comps and see what people have done to extract/capture features . for one of the car competitions ( forget what it was exattly ) but some dude would generate features by using a knn algo on throttle position sensor data ... EOQ as others are saying , this is typically the core part of an ml problem that requires domain expertise. there aren't really any feature engineering algorithms that can perform well across many domains . that said , there are some algorithms that are pretty good at learning ( unsupervised ) features for some data . a good example is word2vec , which generates word embedding vectors ( i.e. feature vectors with some nice bonus properties ) from a corpus of text documents . EOA 
 is there a minimum corpus size for lda : machinelearning not really a practical bound . note you can substitute documents with sentences , if your corpus is small . EOQ there is no general rule . my experience is that it works better for longer documents than for shorter documents . and, the more data you have the better it gets . another very important part is also the preprocessing of the documents ( e.g. tokenization , stopword removal ) . this can lead to very different results . EOA 
 predicting spatiotemporal data ( non-video ) ? : machinelearning have you tried kriging aka gaussian process regression ? EOQ look here URL EOA 
 autotagging with a predefined set of concepts : machinelearning i'd start with word2vec , get the average vector and compare with that with your tags . after people have tagged content for a while you can use those vectors with any classical supervised learning algorithm to the tags to match better . ( probably no need to go fancy here , any linear algorithm should do a good job ) EOQ like andaag said , word2vec would be a good start because it can give you the most similar words . but you should use some stemmer if you want to avoid the same tags but with different forms ( car , cars for example ) . the best you can use , in my opinion , is to use a knowledge graph , like wordnet . it would gives you the words that are related in a way or another and you could give tags that are closer to the text or more abstract ones given the k-th ancestor of a node in your graph . EOA 
 autotagging with a predefined set of concepts : machinelearning i'd start with word2vec , get the average vector and compare with that with your tags . after people have tagged content for a while you can use those vectors with any classical supervised learning algorithm to the tags to match better . ( probably no need to go fancy here , any linear algorithm should do a good job ) EOQ i m not sure if by api you mean service that can do this for you . but in ml , what you want is referred to as topic modeling which in summary assigned one or more topics to any document in a corpus . topics can be predefined or the algorithm can decide on topic by it self . beside word2vec that is suggested by others , i would suggest to look for lda ( latent dirichlet allocation ) which has a proven record in unsupervised topic modeling . in very naive summary it works as follows : NUM-automatically creates a set of k topics ( k is an input to the algorithm ) . topic ( a.k.a basket ) is defined as a distribution over all words . usually the highest probable word in each topic give a good definition for that topic NUM-for each document , it assigns a probability for that document to a specific topic . so you can suggest a order list of topics ( based on probability ) for each document . EOA 
 autotagging with a predefined set of concepts : machinelearning i'd start with word2vec , get the average vector and compare with that with your tags . after people have tagged content for a while you can use those vectors with any classical supervised learning algorithm to the tags to match better . ( probably no need to go fancy here , any linear algorithm should do a good job ) EOQ do you have any references for doing unsupervised topic modelling with a predefined list of topics ? EOA 
 autotagging with a predefined set of concepts : machinelearning i'd start with word2vec , get the average vector and compare with that with your tags . after people have tagged content for a while you can use those vectors with any classical supervised learning algorithm to the tags to match better . ( probably no need to go fancy here , any linear algorithm should do a good job ) EOQ an approach to unsupervised topic modeling is the lingo algorithm : URL you can see the kinds of results it generates by looking at the carrot search page : URL EOA 
 autotagging with a predefined set of concepts : machinelearning i'd start with word2vec , get the average vector and compare with that with your tags . after people have tagged content for a while you can use those vectors with any classical supervised learning algorithm to the tags to match better . ( probably no need to go fancy here , any linear algorithm should do a good job ) EOQ if some of those documents are already classified ( or you can organize to get some classified ) , then this is pretty much multi-class classification NUM . URL is pretty much the full application . EOA 
 classification in geospatial/time series data : machinelearning smart feature engineering should be the key here . you have lat/long which means you can get zip code , which means you can get census data , average area income , climate, etc for each training row . you also have time , which means you can get day of week , whether it's summer spring or fall , a holiday , morning noon or night , etc. think about the fundamental activity driving these purchases in the real world , then start engineering features that seem like they should matter . does it matter if it was raining ? pull it in . random forests are sufficiently sophisticated that you won't see many big gains by trying delve into more advanced algorithms , so i wouldn't waste time going in that direction . EOQ what exactly is your task , what are you trying to predict here ? EOA 
 classification in geospatial/time series data : machinelearning smart feature engineering should be the key here . you have lat/long which means you can get zip code , which means you can get census data , average area income , climate, etc for each training row . you also have time , which means you can get day of week , whether it's summer spring or fall , a holiday , morning noon or night , etc. think about the fundamental activity driving these purchases in the real world , then start engineering features that seem like they should matter . does it matter if it was raining ? pull it in . random forests are sufficiently sophisticated that you won't see many big gains by trying delve into more advanced algorithms , so i wouldn't waste time going in that direction . EOQ check the edit , i added the problem definition . EOA 
 classification in geospatial/time series data : machinelearning smart feature engineering should be the key here . you have lat/long which means you can get zip code , which means you can get census data , average area income , climate, etc for each training row . you also have time , which means you can get day of week , whether it's summer spring or fall , a holiday , morning noon or night , etc. think about the fundamental activity driving these purchases in the real world , then start engineering features that seem like they should matter . does it matter if it was raining ? pull it in . random forests are sufficiently sophisticated that you won't see many big gains by trying delve into more advanced algorithms , so i wouldn't waste time going in that direction . EOQ if times tamp and location are the only two features you got , i would say it is a really hard task . EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ you mean like word2vec ? that's solving a different problem , it just treats words as tokens . i'm trying to get something that captures the letters within a word . EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ your stated goal was to transform words into NUM d vecs . if that's not your goal perhaps you can re-phrase ? EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ that is my goal , but there is a fundamental difference between what i'm trying to do and what word2vec does . word2vec transforms words into vectors using the context each word appears in within a large corpus of training data , but the actual letters that make up the word are irrelevant , word2vec only cares about the context of the words . i'm trying to create vectors based on the letters within each word and the order in which they appear within the word . so , for example , for me the vectors for spite and spike might be quite close , but they would likely be distant for word2vec . EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ i'm afraid i'm not understanding what your actual goal is . EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ ok , what part of my explanation didn't make sense ? EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ what is your goal here ? to represent an NUM-character word using an alphabet of NUM letters , you only need NUM bits per letter , so you can get away with an explicit encoding of the word using NUM bits-NUM bytes . depending on your dictionary of words , standard compression codings may be able to make that smaller . so i wonder what the point of this project is if the result will compress a word into two float32s or even two int16s . EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ the goal isn't simply to represent the value in the most compact way , the goal is to represent it in a way that the neural network should find it easy to interpret . this is why one hot encoding is often used in machine learning to encode categorical data . EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ i'm not talking about your input representation , i'm talking about your output representation . what is the purpose of learning a NUM-d vector to represent a word ? EOA 
 need some suggestions re : configuring an autoencoder : machinelearning why not use one of the many word embedding approaches ? an autoencoder seems like a miss-match here . EOQ heya , i was revisiting this blog post on rnns and they have a section which uses rnns to generate baby names . it seems really similar to what you want to do . there is also some code available on github for these examples , which you might not really even need to modify . just in case you're interested in that approach . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i want to see the writeup here . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ-1 , you should really finish this :) EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ that really speaks volumes about the writing quality that characters are so uniquely identifiable EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ you could probably get some cheap wins based on proper nouns-e.g. daenarys probably doesn't talk about snow much . but i agree . i want to read the writeup ! EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ if you tested it on fanfic , daenerys might talk about snow , though. EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ-1 . this would save me gads of time on my dissertation . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ so i finished that write up ! you can see it here . i tried to write it in a way that would be maximally helpful for me when i did this project a year ago-i remember reading a lot of tutorials that assumed , because they were about machine learning , that i knew quite a bit about programming . this one is for people who have only a few months of python under their belts . i have no idea how reddit works , so i'm hoping this lights up the batsignal for people who were interested . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ oh , i'm interested in this , too.-1 for the write up ! EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ awesome ! but this is just using ml as part of a bigger ml project ( studying the network )-not really solving a practical problem for yourself :) EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ one personal problem i've had is unemployment , so i publish machine learning papers to one day get a real job . so i have yet to solve this . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ seems like a np-hard problem to me . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ it could be worse , you could be using a guitar . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ it could be worse , he could be writing papers about guitars . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i wish you all the best ! it seems like very much a non-trivial problem . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ if you are able to get machine learning papers published at a good venue , then it shouldn't be hard for you to find a job-even without publishing those papers . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ what sort of research do you do ? would you be able to link some of your publications ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ maybe train something to recognize which companies are more likely to respond and focus your time that way . then talk about it in the interview EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ what were your topics of publication ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ maybe you're using the wrong input values in your objective function . ;) EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ okay , this is stupid , but bear with me : wanted to figure out how the captionbot for /r/adviceanimals works , worked it out with URL ( nn-based ocr ) , because all other ocr packages i tried produced shitty results ( after preprocessing ) and/or had amazingly bad/outdated documentation ( looking at you tesseract ) . i trained it on ( i think ) NUM or so samples ( transcribed manually , took some time , but relatively reasonable NUM-2 hours ) . it works pretty well for losslessly compressed images ( png ) . then i noticed that the bots just scrape the captions off the image hosting sites . oh well . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ this is actually helpful , i'm planning on doing a /r/subredditsimulator type of thing but trained only on memes posted to advice animals . to do this i need to identify the meme and scrape the text in the meme , and i've been having a ton of trouble with tesserect . i never even though of scraping the originating site ... EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ want to collaborate on that ? i've got a dataset of about NUM m of them , a lot of them already tagged with what they are and a bunch of ( bad ) python scripts for scraping . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ that sounds really fun but i'm supposed to be working by myself as it's for school . maybe i'll reach out for some advice and let you know how my work goes ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ ha ! cool story , bro. EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i use machine learning for my music collection ( NUM ,000-songs ) . i use support vector machines to classify the music into NUM different genres . and i have another support vector machine to do a regression to estimate how much i will like the music . it works very well at classifying . but i am still working on getting it to predict music i will like. right now it's only accurate about NUM % of the time . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ recommender systems can be really tricky . if you get this right , you can probably sell/give it to any music streaming service . but i chime in with the other commenters : what features do you use ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i'm reducing the data so that every song is represented by NUM floating points . to get it to NUM floats i go through a couple steps including fft and pca . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ that would've been my approach as well . although i may not have initially chosen pca but perhaps some bag of frequencies and tf-idf and then try k-nn . when you can run that on NUM k-songs , i may add that you have a formidable music collection . do you have some special setup to run a huge dataset like that through fft , pca and then train a svm ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i use the sklearn package . but i'm not using any special database to manage the data . just python and numpy . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ isnt NUM % pretty good doe ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ what was the data set that you used ? i would love to read about how you did it . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ his music collection . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ yes . i have about NUM ,000 songs tagged by genre , like and dislike . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ so the analytics were based on tagging ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ can you share the code ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ you can use extreme learning machines to drastically improve training and testing time . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ so i tried extreme learning machines on my dataset . they are pretty fast . i used this library : high-performance implementation of an extreme learning machine ( URL ) i trained the svm and elm on NUM songs and tested against NUM ( y-0.5 for songs i like and y-0.5 for songs i hate ) . for the elm i used NUM rbf neurons ( this was the best settings i could find of the NUM variations i tried ) . here are the results : time to train elm : NUM seconds time to train svm : NUM seconds error elm : NUM 7017061 error svm : NUM 8249665 i was never able to beat the svm performance with the elm . but i have added to my list of models to test in the future . i'll play around with it some more . thanks again for pointing me in this direction . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i have used elm in matlab with vanilla coding and it always trained faster than svm , so the results pretty much shocked me . for the performance , i used mean differentiation and scaling the values between-1 and NUM as preprocessing . i 'changed' the elm code a little bit and added rectifier activation function besides the sigmoid . from what i seen , sigmoid has better performance on normalised values , but rectifier function does seem to perform pretty good on raw data . could you put the dataset you work on somewhere so that i can test it with libsvm and elm in matlab ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ support vector machines are much faster to train than any other complex model i've tried . i can train a svm classifier to NUM ,000 songs in less than a second . it takes more time to read the data from the hard drive . do you have a specific library you are referring to . because i've tried three different neural net libraries including two from google . they are very slow to train . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i'm not training to the wave form . i'm reducing the data so that every song is represented by NUM floating points . to get it to NUM floats i go through a couple steps including fft and pca . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ you can do that reduction in under a second as well , or is the < ;1 second training not including the preprocessing ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ all the songs are preprocessed and stored on the hard drive. reading the mp3 file , converting to a wav , and doing the transform is the slowest step . but once the preprocessing is done and the data is in an x,y matrix . the svm takes around a second to compute . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ it takes about a day to read and convert all the files . so i don't change my pre-processing algorithm too often . i use several methods including fft , fft over frames and beat detection . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ extreme learning machines doesn't do gradient descent to learn its weights . they just do it in one iteration and they always seem to perform faster in both training and test cases in my usages(2000 features with NUM size) . you can find the implementation and details here : URL EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i'll give it a try . thanks . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used ml to detect hacks in punkbuster screenshots , to help admins detect and ban cheats on servers . it's fairly unusual to have bright colours in games like battlefield , and you don't need anything as advanced as a neural network to effectively do it . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ how were you able to detect hacks ? does the mention of it's fairly unusual to have bright colours ... mean you were detecting abnormal colors and that signifies a hack ? i'm interested in video game ai / anti-cheat methods , and was curious for some more details on what you were using/looking for EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ hacks can do a lot of things , but the most basic ones often swap the textures on enemy models for ( usually ) fluorescent colours , draw extra crosshairs ( as sometimes the gun won't have filled in crosshairs ) and/or turn on all player labels ( instead of them fading over distance ) . punkbuster takes a screenshot periodically as you are playing , and uploads a miniaturised version to a server . basically i was a lazy admin and rather than looking at these screenshots manually i wrote a program that output colour statistics on regions of the screenshots , then ran it through a ml algorithm . if the probability of a visual hack was high , then it would pop the images into a separate folder for me to look at later . the algorithm was trained on known ok vs known hack screenshots . if the hack was advanced , often the screenshot would be entirely black , always the same for the player , or not recorded . we would often put in a watch for those players and monitor them when they came online . there are a variety of other things you can do to detect hacks , but most will need to be part of the game client . tl;dr: i was lazy so coded an algorithm to classify images rather than go through them by hand . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ this is genius ! EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used a markov model to generate a company name , but i'm not really sure if that counts as ml or a personal project . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ can you generate a list of NUM company names and post them on here ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ totally . approximate gist is here : URL company list is here : URL list of names : nywex jacol tilin zionco morac poratio fatial icorpora cinoloas corcor hodse velesarath athello intorporati inelogio sanand zorphorpons inatas ananacories corinc worth noting that i took some artistic liberties with this . mostly truncating sensibly when the generated sequence ended in a weird spot . the code is also kind of gross and about NUM years old , but there ya go . i've certainly come up with worse company names on my own . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ sounds like a task for char-rnn . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i did that some years ago with crunchbase : URL ( there are a couple other files in the same directory for the complete process ) EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ is a markov model necessarily markov chain ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i don't think so , but it's some approximation of a markov chain here . just a silly little NUM min script i put together . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ asked that because i was unaware markov chains belonged to ml stack . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ how do you feel about monte carlo simulation or finite automata ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ markov random field models can have more complex structures than a simple chain . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i wanted to know what anime was popular in the current season but couldn't find any site with that info , so i scraped and aggregated info from torrent sites . i was ranking them by the average number of downloads per episode but when a new episode is only out for a day that drags down the average and would make the rankings unstable , particularly early in the season when i'm looking for stuff to watch . so for newer episodes i did some ml to extrapolate to estimate the number of downloads it'll have NUM days from release . it's available usually for a small/short-lived project though it's just not worth the effort beyond simple statistics . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ why didn't you just look at the median instead of the mean ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ the system was mostly useful when there are only NUM-3 episodes . in those cases taking the median isn't different than just taking the mean excluding new episodes . but usually i'd see tons of people trying out shows in episode NUM and each show had a different rate of dropoff from NUM-> ;2, NUM-> ;3 etc . so i really wanted to capture any little info i could from the newer episodes . the other issue is that i needed to interpolate to a fixed time for older episodes because the download count never really stops decreasing ; it's just on a log scale . so the median of current dls per episode of a long-running show will tend higher than for new show and i wanted to compensate for that as well . at the time fitting curves and tuning the number of free parameters/features seemed like a nice unified solution to both extrapolation and interpolation . later on i learned that linear interpolation was good enough and much faster so i only used ml for extrapolation . all that said , i mostly wanted to see shows rise and fall more continuously . a part of my original goal was to be able to say show x really tanks in episode y and identify that quickly from that data . the data was unfortunately too noisy for me to do that in most cases though . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ that actually sounds a lot cooler than what i initially thought . hope you found some good stuff to watch ! EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ yeah it helped , especially when trying to find something to watch with friends that noone's seen before . though i learned that my interests aren't too closely aligned with the average torrenter . so i started to look into polling the torrent tracker info and using ip addresses to train a recommender system but the engineering seemed messy and i wasn't sure what would happen if i had heroku polling a torrent tracker every few seconds ... and honestly i had a bad feeling about how much effort it'd take to get working correctly . the whole process could be ported for regular movies/tv but for anime almost everything goes through a single torrent tracker and the torrent names are more standardized so the data acquisition and normalization was a lot easier . there was a company called semetric that may have done something like this for music/tv/movies but i can't find much info because they were acquired by apple early last year . anyways , it was a fun project and i was happy to have an excuse to do some ml . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used max entropy to classify news into NUM topics from a newsbot which crawls hundreds of newspapers . i am probably the only reader of my google news clone . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i trained a char-rnn on a german dictionary and found that it embraced our love for compound nouns . i'm using it to generate novel insults to annoy my friends . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used optimization to . i still use this algorithm from time to time when visiting a new city or area and want to design a walking tour for it . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ this sounds useful as a road trip planner based in yelp reviews . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ yep ! i usually use tripadvisor as a source to pick spots as well . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used a random forest to calculate a forecast for the bitcoin price , based on past indicator data . in theory it works , but in my simulations the trading fees would have eaten up all the profit . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ trade on a site without trading fees ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ with fee i also mean slippage EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ have you tried just using it for direction prediction ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i've used a regression forest to predict how much it will go up or down in the next day EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ it wasn't any useful for a discretionary punt ? i trade fx and am not too familiar with bitcoin trading . what sorts of features did you include..if you dont mind me asking ..? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ what's a discretionary punt ? i'm not a native english speaker and have never heard this term . i basically used all the functions from ta-lib , see here : URL EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i stumbled onto machine learning from the bitcoin bot direction . now i've been reading a lot of ml papers . i initially thought about running the talib functions , but it seems that feeding a rnn raw tx data from an exchange will be more useful . i plan on processing the data into second-level bars with volume and price indicators . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ punt-bet EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i was interested in the next-likeliest letters to follow a given string entry ( e.g. oran ... g is pretty likely to follow that entry ) . so, i implemented an m-order markov chain letter prediction , which works pretty nicely . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ have you seen /r/subredditsimulator ? EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used kernel density estimation to confirm for myself that , yes, the local bikeshare station was emptying out about half hour to an hour before i would normally try to pick up a bike and it was , therefore, not worth getting a membership at the time . i've also done a ton of analysis of the structure of the reddit community , which has revealed a lot of interesting subreddits ( and communities of subreddits ) to me that i might not have known about otherwise . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ not using it yet but ive used it at my prior jobs . im planning on trying to use a classifier to predict whether or not a company will reject me for a job . i have a massive spreadsheet of every job ive applied to for the past year or so with details such as the company , job title , date, location ( city/state ) and some other meta data such as the job source (indeed , craigslist,etc). hoping to optimize my job search and not apply to jobs that will reject me . the problem i have is..not every job i apply to gives me any feedback and the 'prediction' class would be too much of a minority and i dont want to use synthetic over sampling . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ did you try predicting whether or not an application would lead to a phone interview ? it seems like that's a good first iteration , especially since you know which applications led to a call/no call . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ haven't done anything yet . i do have a bi-dashboard i whipped up in excel that gives me a statistical breakdown of a bunch of metrics , averages, by region ( i apply to jobs all over the us ) , etc ... i have had quite a bit of phone interviews but when i used ml at my last job for direct marketing at an insurance company , i found that the advantage of ml was just cutting out chaff who wouldn't buy our products and not so much getting more people to buy . i think i've applied to about NUM jobs since october and actually just got an offer on a short term contract . the problem is..having the data points to be predicted are whats important . if i've only had NUM phone interviews and NUM-500 applications i'd have to cut down the negative class in the data set quite a bit and it wouldnt be a good model , imo. EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ you might have a richer dataset if you , say, tracked visits to a personal website. then you could look at domain names , ips, ip locations , time of day , etc. to identify characteristics of people who are interested in your work . this would be more like a marketer tracking a conversion funnel . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i dont use linked in or have a portfolio or anything..im a data analyst by trade and not really a data science guy but know how to build basic models and whatnot . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ buy a domain and hosting , setup a couple webpages , create an about page , add your resume , and add google analytics for tracking . so many data science jobs involve the web and web analytics , it would be good to learn web basics and use ml on analytics data . also , learn python , not excel . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ ive used ga for reporting but how is it used for machine learning ? or how can it be used ? im assuming that its most common use is to make changes to webpages based on certain demographics or whatever EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i haven't worked with ga in a while , but you'd have to download the data . there is probably an api that you can programmatically interact with through python . i would tag all pages and track specific user actions , such as downloading a resume . also , you could add a tracking code to links on your linkedin page and emails , to see if those links are sending people to your website. once you start analyzing the data and finding things that work/don't work , you can optimize , a/b/mv test , identify keywords that people use to find your webpage , find out where your best visitors are coming from (silicon valley , nyc), etc . use ml to find the combination of parameters that optimize your objective function(s) . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ you might do better just trying to predict whether or not you even get an interview . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ it just comes down to the data . most 'real world' problems from what i have observed are dependent on the data and not so much the model itself . atleast in the business world..the more scientific stuff everyone here talks about is another story . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ that's the crux of my suggestion . i'm sure you receive job offers a lot less frequently than interviews , so at least you'll have data on job applications that led to interviews . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ most one-person practical problems aren't really ml-appropriate . you don't generate enough data ! it's like trying to cut your steak with a chainsaw . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ when i was living in a squat with heroin junkies , i used deep neural networks to predict stock price movements . now i live in the hamptons . seriously though-no , i have not used ml for anything practical yet , but i work at a start up developing enterprise ai software , so that may change soon . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ someday , someday i will use ml to make max cash in runescape with item flipping . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ do or do not do . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ team effort , but we designed a system to extract risk of bias sentences from clinical literature called robotreviewer also i'm thinking about building a system that predicts where and when to fly to for best aurora borealis chances , not strictly ml though , more a ( linear ) optimization problem . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ coughshamelessselfpromotioncough URL how can i create a world class demagogue , without being one myself ? moreover, how can i know how the donald feels about immigrants , muslims and president obama ? i can't really think of any other way ... EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used mcmc and kernel density estimation to decide if it was a good idea applying for a scholarship . i had to pay for my studies in advance and i would know if would win it only later in the year , so i needed some high level of confidence that i would get it before blindly spending a lot of money . of course the scholarship was to finance a masters degree in ml ;) EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ not personal stuff , no. i am using a prebuilt deep cnn for an inhouse product , though, and will be working on improving the model's performance by varying the preprocessing steps for the images . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used a decision tree to classify arousal in music realtime via a microphone . the objective was to change the ambience based on the eq of the music being played . it had about a NUM % accuracy at the end of it . here it is in action : URL EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used weka to explore and model data that i scraped off a domestic used car website , in order to determine a price range in which i can expect to sell my car . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ how to trade skilled labor for income . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ i used machine learning to train an agent to make this reply to your question . EOA 
 have you ever successfully used ml to solve a practical problem for yourself ? : machinelearning i used conditional random fields to predict who was speaking a line of dialogue from a novel , so i could study the social network of a song of ice and fire . i haven't written it up yet , but it was really gratifying to get NUM % accuracy and save dozens of really tedious hours tagging . EOQ [ deleted ] EOA 
 best introduction to causal inference ? : machinelearning in terms of bayesian causal networks , i would recommend starting with pearl's [ causality ] . another old but still relevant text is by glymour , spirtes, and scheines . in addition , cosmos shalizi addresses causal graphical models in his book in chapters NUM and up . compared to the other two resources i mentioned , this book approaches the topic from a more applied statistical point of view rather than a pure math pov . as for different flavor of causal inference , economists have developed their own causal inference tools in econometrics , based on the . in terms of libraries and code , there's a library in r called learning lunch talk at the data science for social good program a couple of years ago , and the other fellows really liked it , so hopefully it's a nice introduction into the field . EOQ neyman was a statistican and rubin is a statistician . econometricians seem to be the originators of the concept of instrumental variables , though. EOA 
 best introduction to causal inference ? : machinelearning in terms of bayesian causal networks , i would recommend starting with pearl's [ causality ] . another old but still relevant text is by glymour , spirtes, and scheines . in addition , cosmos shalizi addresses causal graphical models in his book in chapters NUM and up . compared to the other two resources i mentioned , this book approaches the topic from a more applied statistical point of view rather than a pure math pov . as for different flavor of causal inference , economists have developed their own causal inference tools in econometrics , based on the . in terms of libraries and code , there's a library in r called learning lunch talk at the data science for social good program a couple of years ago , and the other fellows really liked it , so hopefully it's a nice introduction into the field . EOQ besides the nice works mentioned by the other poster i found this one by buhlmanns group pretty easy to follow if you want to avoid the philosophy ftp://ess.r-project.org/manuscripts/buhlmann/causal-review-2013v3.pdf EOA 
 best introduction to causal inference ? : machinelearning in terms of bayesian causal networks , i would recommend starting with pearl's [ causality ] . another old but still relevant text is by glymour , spirtes, and scheines . in addition , cosmos shalizi addresses causal graphical models in his book in chapters NUM and up . compared to the other two resources i mentioned , this book approaches the topic from a more applied statistical point of view rather than a pure math pov . as for different flavor of causal inference , economists have developed their own causal inference tools in econometrics , based on the . in terms of libraries and code , there's a library in r called learning lunch talk at the data science for social good program a couple of years ago , and the other fellows really liked it , so hopefully it's a nice introduction into the field . EOQ this paper has some really good examples of using genetic instrumental variables , adapting mendelian randomisation to a full hypothesis search much like a gwas . millard , louise ac , et al . mr-phewas: hypothesis prioritization among potential causal effects of body mass index on many outcomes , using mendelian randomization . scientific reports NUM ( NUM ) . URL EOA 
 best introduction to causal inference ? : machinelearning in terms of bayesian causal networks , i would recommend starting with pearl's [ causality ] . another old but still relevant text is by glymour , spirtes, and scheines . in addition , cosmos shalizi addresses causal graphical models in his book in chapters NUM and up . compared to the other two resources i mentioned , this book approaches the topic from a more applied statistical point of view rather than a pure math pov . as for different flavor of causal inference , economists have developed their own causal inference tools in econometrics , based on the . in terms of libraries and code , there's a library in r called learning lunch talk at the data science for social good program a couple of years ago , and the other fellows really liked it , so hopefully it's a nice introduction into the field . EOQ anybody recommending koller & friedman's probabilistic graphical models for this ? EOA 
 best introduction to causal inference ? : machinelearning in terms of bayesian causal networks , i would recommend starting with pearl's [ causality ] . another old but still relevant text is by glymour , spirtes, and scheines . in addition , cosmos shalizi addresses causal graphical models in his book in chapters NUM and up . compared to the other two resources i mentioned , this book approaches the topic from a more applied statistical point of view rather than a pure math pov . as for different flavor of causal inference , economists have developed their own causal inference tools in econometrics , based on the . in terms of libraries and code , there's a library in r called learning lunch talk at the data science for social good program a couple of years ago , and the other fellows really liked it , so hopefully it's a nice introduction into the field . EOQ i think probabilistic graphical models offer one viewpoint of causal learning , but i'm not sure that they are the be-all and end-all of it . it might be a good reference text , but i think it goes a bit too in-depth if all you wanted was causal learning . EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ so i have only done analysis directly on ga . what would be the fundamental analysis on ga via r ? what's the most common analysis that you run on r ? campaign analysis ? do you have any good resource for beginners like me ? i don't even know what packages to use . EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ there's a library called rga that allows you to connect directly to google's servers with r . i usually use that to get data i needed . hadley wickam's libraries ( especially ggplot2 and dplyr ) are incredibly useful to me when i'm doing plotting , data manipulation , and cleanup . lubridate is useful for handling dates . most of the time i would look at how effective a page , section, or marketing campaign is on our site. this was split between time-series analysis when you're looking at things that changed ( new marketing campaign started ) , and category analysis when you have a list of things to compare ( like site sections or traffic channels ) . usually, googling how to do x in r is my approach to figuring out what i need . happy to answer more questions if you have any . EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ i have . i work for an ad-agency and connect to ga through python and r all the time. what do you want to know ? r for exploratory analysis and python when i am doing etls . EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ so i have only done analysis directly on ga . what would be the fundamental analysis on ga via r ? what's the most common analysis that you run on r ? campaign analysis ? do you have any good resource for beginners like me ? i don't even know what packages to use . EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ you can use rgoogleanalytics , i use it a bit when doing simple exploratory type stuff but now i just have python/luigi productionalize it and get it daily . i don't think there is really anything that can be considered a common analysis with using just your ga data . most ga only analyses are done right in ga . the only reason i ever do anything with ga data outside of ga is to match it up with the data that we generate through our software products or other apis/exports/scrapes . it all depends on the analysis being done . what question are you trying to solve ? EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ i'm not too familiar with google analytics but can't you export the data into an excel file ? EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ that's true . good call . it might be too much work though EOA 
 using r to analyze web traffic ? : machinelearning i mostly work in r and have spent a couple years doing analysis for a startup and a marketing consultancy . what would you like to know ? EOQ while running through google analytic's cert , was curious into seeing applications for blending r and ga too . there's a few blogs out there that walks through blending r shiny with ga . however, beyond shinier graphics , am curious too to see applications of r and ga that adds value beyond what already exists in ga . given that users are assigned id , i can imagine cluster analysis being applied on clickstream data to see similar browsing patterns . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ do you know any successful examples when such combo works in production ? EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ sounds like a dirty combination . i would use python for training weights and use the hdf5 export in c-( i.e matrix multiplications ) . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ so you have to implement the network twice , once in python , then in c-? in the case of nontrivial rnn ( for example bidirectional multidimensional lstm ) it takes too much additional effort EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ tensorflow has a c-interface . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ agree , it's an option . but rnn's there are too basic comparing to rnnlib also i'm not sure how easy can i install tensorflow on any machine ( for example old NUM-bit linux ) . does it require python for its runtime or it's possible to have only c-libraries ? EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ i've used rnnlib previously and it's rather aged by modern standards . on most things that matter tf has more functionality for rnns . the main cool thing about rnnlib is ctc and the mdl regularization stuff . i think you can run it entirely in c-, although graph construction is easier with python . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ use caffe forks such as : URL they are ideal for prod . dont ship python into prod , you'll have nightmares . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ yes , caffe is cool , i'm using it in production for standalone systems that must work in realtime. and i have serious doubts that integrating python will be a good idea . the problem with lstm in caffe is that they are too basic . rnnlib has much more advanced features . it seems currently only torch has something similar . i heard torch/lua is rather easy to integrate into c-, but yet never did it EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ i maintain the torch-android and torch-ios libraries , which showcase how to embed torch into c-and java systems . it is fairly easy to integrate torch into c-, as lua is very simple machinery ( ~15k lines of code ) . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ i wouldn't ship anything based on theano into prod , but there's nothing inherently wrong with python in prod , as long as it's not doing any super heavy lifting and you're aware of its limitations ( mainly the gil ) and how to structure things so that they aren't an issue . of course , if you have no other business case for using python , then acquiring the expertise to make python work isn't really a proposition with a lot of value . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ that's fucking retarded . python is used in production all over the place . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ sorry but python production systems at amd/pfizer/cinemark or random startups don't count . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ that's a weird set of businesses for sure . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ hi , sorry to have offended you , but we ship nn models into production at massive scale and we've evaluated many options . we use our own backends and caffe . at a micro-scale , python is okay , but if you want to do prod systems , it's a no-go . EOA 
 does anyone use rnnlib or currennt libraries for rnn training ? : machinelearning why not theano/keras using boost::python? EOQ i'm not going to argue about particular frameworks , and there's certainly nothing wrong with caffe . but as a blanket statement dont ship python into prod , you'll have nightmares . is just silly and contradicts the reality of many huge companies . EOA 
 recommendations for geographic ( city ) classification ? : machinelearning the method you're using seems relatively reasonable , and is basically just partitioning the map using voronoi cells . there's no learning that can be done under that procedure , but it might work well enough for the application you're interested in , so you might check if the error is good enough . partitioning the map does seem to be a good way to approach the problem , so here's one possible way you can modify the set-up and learn a better assignment rule . give every city an energy function around it , and to do assignment , use the largest energy in the immediate area for classification . if the classification is right , do nothing , but if the classification is wrong , increase the range of the correct city and decrease the range of the incorrect city . an example force might be exp(-k r) , where r is the difference between the city and the person , and k is the learnable parameter . this approach could be extended with more complicated force functions . this would be reinforcement learning instead of stochastic gradient descent , but it's what i came up with . EOQ you can use the haversien formula to calculate the distance between the person and the city center . ( call this hd ) calculate the radius of each city (r-sqrt(a/pi)) then : error-if ( hd < ; r return NUM ) else return ( hd-r ) that works unless you have to deal with overlapping cities EOA 
 what are some interesting results in statistical learning theory , relevant to deep learning ? : machinelearning there's a paper from last year called the loss surfaces of multilayer networks , which looks into the problem of convergence to local optima . after some simplifying assumptions they conclude that shallower models , while improving the probability of convergence to the global optimum , have much poorer local optima . that is to say , if a model is sufficiently deep then the chance of converging to the global optimum approaches zero , but whatever local optimum it does find will probably be very good anyway . URL EOQ is this the key insight to dnn's ? EOA 
 what are some interesting results in statistical learning theory , relevant to deep learning ? : machinelearning there's a paper from last year called the loss surfaces of multilayer networks , which looks into the problem of convergence to local optima . after some simplifying assumptions they conclude that shallower models , while improving the probability of convergence to the global optimum , have much poorer local optima . that is to say , if a model is sufficiently deep then the chance of converging to the global optimum approaches zero , but whatever local optimum it does find will probably be very good anyway . URL EOQ this is really interesting . thanks for posting ! EOA 
 r/machinelearning on slack : machinelearning why i don't support public slack channels : you can't hide nor delete your email address . it's trivial to download the email address of everyone that has ever joined the channel-and most people are not aware of this . EOQ yeah . why not just have a subwide irc channel , where pseudonymity can be maintained ? EOA 
 r/machinelearning on slack : machinelearning why i don't support public slack channels : you can't hide nor delete your email address . it's trivial to download the email address of everyone that has ever joined the channel-and most people are not aware of this . EOQ URL /join #machinelearning irc ftw ! EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ what role do convnets exactly have here that is crucial over a regular fully-connected feedforward net ? EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ convnets are used to 'see' the board . using them ( over fully connected ) makes plenty of sense , since small scale patterns could appear anywhere on the board and are basically the same . the supporting quote that gets thrown around a lot is that a go master makes a move because it 'looks good' , hence incorporating elements of computer vision seem to be paying off . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ ah yeah i should have realized this . thanks! EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ are non-neural network machine learning approaches a waste of time now ? should we all be using neural networks instead of things like svms , random forests , or ( gasp! ) linear regression models ? EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ in the real world you need to deliver business value , whatever approach accomplishes that is what you should use . i can train a linear svc faster than a neural net ( depending on data size of course ) , and if it works well then i might not feel as much pressure to try something else . for things like alphago , you're gonna need neural nets . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ not really . sometimes , simple approach just works , and one thing i learned following all those years tech trend is that the success of new things doesn't necessary means the death of old approaches , although people often claim so . rather, mature methods will fall into different domains , preferred under certain requirement . only that people will stop using them for something with obvious subpar performance compared to state-of-art . linear model/random forests/svms will be popular in their own right , e.g. better understood and supposedly easy to tune . but they will retire from cv/speech recognition related fields where nn outperform them by a large margin and prediction is the main focus . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ algorithms are tools . use them wisely to solve your problem . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ to me this argument is in the same flavour as why do people still travel by train when they could take a plane instead ? EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ we did eventually give up horses and boats for the most part . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ exactly , for the most part ;) we still use them for what they are worth now , and i think it will be the same in machine learning . personally , i still make extensive use of linear regression and decision trees because they suit my needs and avoid awkward conversations w/ my boss on how i spent weeks training a deep neural net for something i could have achieved in NUM hr with something much simpler EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ use whatever you want . all of those algorithms have their place still . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ pretty much this . some showed results that would get , i don't remember exactly , say NUM ish elo points above the non-neural version of the same program ( ayamc comes to mind specifically ) , which is already a pretty big deal . but to jump over NUM elo points over sota ? and what , more than NUM over previous neural-net results of facebook and google ? it's like we jumped NUM years in the future . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ i think a key insight is that this is fundamentally different from the way computers beat chess . chess as a problem was beaten with absurd scale : brute force search and huge look-up tables . however, because the go search space grows exponentially faster , it's beyond the reach of these brute force methods for the forseeable future . so achieving super-human performance at go requires 'strong'-er ai methods . most computer scientists haven't really internalized how fast cnn and rnn performance is advancing , so they're caught off-guard when a problem is solved that was not within reach of the more conventional methods they are familiar with . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ it's time for them to jump on the bandwagon . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ yes . that silly gary marcus article pointing out that alphago is using tree search just like he and steven pinker predicted ! ha ha ha ! misses the fact that they are doing extremely conservative heuristic tree search and for a short window into the future , using pattern recognition all over the place . the whole thing comes crashing down without being able to rapidly identify the promising candidate moves . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ yup . in a sense , humans also do tree search ( that's what we're really talking about , when we think of looking so many moves into the future ) . the trick is knowing which branches are worth considering , which is where the neural networks come into play . the neural networks are what's actually doing the heavy lifting . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ to borrow a quote : it's one thing to believe it , it's another to see it . i think a lot of people in the field did expect go to be cracked sooner or later , but we are more excited that it's actually happening . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ because of assumptions ( which turned out to be incorrect ) , intuitions ( which turned out to be misguided or ungrounded ) , and in many cases , simply lack of familiarity with the current state of ml . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ is anyone in idsia capable of not being bitter ? are they putting something in the water there ? stay safe bro EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ scientists are all guided by some intuitions and assumptions since everything is not known . they can often turn out to be wrong . what is bitter about acknowledging this ? EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ most computer scientists know very little about how machine learning works in my experience . they never really thought any neural network architecture could have worked well for a problem like this . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ well , but is this true for the field of computer go really ? the traditional way to get sota results in computer go was in fact using a machine learning technique-minorization-maximization and its variants . remi coulomb's ( author of the previous-sota program crazystone ) classic description : URL aja huang from alphago's team worked on previous strong programs ( erica ) as well with this technique ( and in some collaboration with coulomb i think ) . details of another previous-sota program zen are yet unpublished , but i gather the author's thesis is near completion . furthermore , simple neural nets have been used as move predictors-a rather modern architecture if not a modern neural network-in for example in steenvreter , another pretty strong system . when convolutional neural nets demonstrated sota results on the move prediction task , there was a flurry of interest and activity , as you can see on the computer-go mailinglist ; oakfoam is using it to modest results for now , and a new version of ayamc got significantly stronger using such a technique . given the modest size of the field , and merely a year of time , i'd say they were positively enthusiastic , rather than skeptical as you suggest . and of course then came the results of facebook ( which like previous google's results were quite promising but quite some ways weaker than the sota ) . i'd could've looked as if this technique can bring a solid boost to the strength of some program-say NUM-400 elo , think ayamc got something like that , and that's rather massive as far as a new twist would go already . i think you need to recognise just how mind-blowing alphago's success really is-in comparison to both the sota of computer go , and especially versus previous neural network results , the asynchronous evaluation system also from deepmind a year ago , and facebook's system . these neural network approaches were NUM-2 dan amateur only ( though facebook's system improved rapidly too , now around NUM d on kgs , ie competitive with previous-sota ) , then alphago jumped over NUM elo points over them , and even NUM elo over actual sota engines . i mean look at this graph-it's ridiculous : ( facebook's and previous-google results are around fuego and pachi strength , and ayamc is say between pachi and the next one ) URL i don't think a comparison to alexnet blowing the competition out of the water in NUM is inappropriate here-it is exactly such an incredible jump . wish i had a graph of how the elo rankings of sota programs evolved over time , but this would be a massive spike on it no quesion ; it seems NUM years ahead of the time . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ most ai classes are taught from artificial intelligence : a modern approach , which presents solutions to chess playing and presents go as more or less hopeless . the reason for the hopelessness is because go is grouped together with games that are solved via minimax search . maybe minimax search could begin to perform well with NUM years of moore's law . at a high level i'd say the root issue is that search algorithms and machine learning have been historically treated as very separate topics within ai classes . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ it sounds like that at the heart of their approach is still the minimax algorithm . it has just been augmented with neural networks to help reduce the breath and depth of the search tree . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ they use monte carlo tree search . which is of course also related to minimax , but it's unreasonable to just call it minimax-neural nets EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ yeah that's my take on it too . just that those two neural networks were such a dramatic improvement over the previous heuristics that it worked well . kinda like the alexnet in NUM changing imagenet forever . EOA 
 why were so many computer scientists surprised by deep minds go performance ? : machinelearning so the cracks were starting to show in go over the past year , if you had been following the papers it made sense that convnets would play a role in moving things forward , but nobody was expecting them to progress that far so quickly . i'm also impressed as to the scale of what they used , it isn't exactly trivial to wrangle that many gpus together in a usable fashion . i don't think any single part of the alphago setup is that crazy , but it ends up feeling greater than the sum of its parts . after a single read through of the paper , i also think this system is essentially capable of surpassing human performance , i don't see any obvious blocks that would prevent it from happening . at a minimum they can just expend some more engineering time to expanding the computation power , which should continue to improve its performance . EOQ to be fair , that's a pretty reductive sentence . it doesn't really speak to the relative amount of work that the neural net is doing as compared to the search tree . EOA 
 how can adding more features make my random forest classification accuracy lower ? : machinelearning random forests choose a random subset of features to consider at each branch . if you dilute informative features with bunch of uninformative ones , the informative ones won't be able to influence your model as strongly . EOQ it is not clear to me if the added features are informative or noise . if they are informative , it just gives more information to the forest , and hence increase accuracy . if they are noise , you basically added more randomness to the decision process of the model . extremely randomized trees shows that adding more randomness to the forests algorithm , can improve generalization performance , but usually : the more uninformative features added , the higher the bias . is the accuracy decrease statistically significant ? rf's are pretty variant : simply changing the seed can give slightly different results . rf's use bagging : they train many decision trees on differently sampled data . like others said : do proper cv instead of oob . i think breiman empirically showed that oob gives as much use as a test set the size of the train set , but later work has shown that oob should be trusted less than cv : cv is based on the entire forest , classic oob only on a NUM /3 subset . EOA 
 how can adding more features make my random forest classification accuracy lower ? : machinelearning random forests choose a random subset of features to consider at each branch . if you dilute informative features with bunch of uninformative ones , the informative ones won't be able to influence your model as strongly . EOQ are you training it on one data set and testing on another ? if so , it may be overfitted to the training data set . EOA 
 how can adding more features make my random forest classification accuracy lower ? : machinelearning random forests choose a random subset of features to consider at each branch . if you dilute informative features with bunch of uninformative ones , the informative ones won't be able to influence your model as strongly . EOQ sorry i should have specified , thanks for pointing out ! i'm measuring accuracy by oob score . EOA 
 how can adding more features make my random forest classification accuracy lower ? : machinelearning random forests choose a random subset of features to consider at each branch . if you dilute informative features with bunch of uninformative ones , the informative ones won't be able to influence your model as strongly . EOQ i think there are several factors here that play a role . for a fixed-size training set , adding more features will result in a more complex model (i assume you are not pruning the trees?); thus , you may have some sort of curse of dimensionality going on here . maybe try to increase the number of trees at least , which should add some bias to the model to reduce the variance in your estimates . you features can also be simply junk features ... redundant or meaningless information . generally, it is hard to tell from feature importance estimations using random forests whether features are not redundant or not . e.g., if you have NUM completely identical features , you'd expect sth close to a NUM :1 ratio in their feature importance . EOA 
 how can adding more features make my random forest classification accuracy lower ? : machinelearning random forests choose a random subset of features to consider at each branch . if you dilute informative features with bunch of uninformative ones , the informative ones won't be able to influence your model as strongly . EOQ more features-> ; more trees EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ yeah , but how can i sample if it doesn't fit into memory ? the moment i read the file from the disk to sample a part from it , it gets all loaded into ram , but a NUM gb dataset won't fit into NUM gb of ram EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ use memory mapping . numpy has a method for this called memmap . EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ oh thank you a lot ! i didn't know about it ! it should come in really handy ! EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ i just did it , however, i don't know how the set is formatted , and so what shape to specify for it EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ you can read , like, half of the file , not the full file ? EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ one strategy you could employ is manipulating the standard NUM k image training dataset to effectively get a larger number of samples . here is some code i wrote to pixel-shift all of the images in each direction for an effective NUM x increase in training samples : URL it ends up being about NUM gb and can increase test prediction accuracy . EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ thank you a lot ! i considered this option previously , i proceeded like this : split the set into training and validation ( NUM %-50% ) , so that in validation there are only original images , and later enrich the training set with slightly rotated images ( random from-4 to-4 degrees ) and i noted that it lead to worse validation and test results . i however read in a microsoft paper that translation and elastic transformations are beneficial to the data , however i wasn't really able to implement them on my own . i'll give a try to your library ! thanks a lot ! EOA 
 bigger mnist dataset ? : machinelearning can't you just sample from infimnist or whatever other bigger dataset version ? EOQ can you share more info about your goal ? svhn isn't a bigger mnist but it might be what you want , since it's still digit classification and has NUM k digits . however , the digits are much more detailed than mnist digits ( because they come from house numbers ) . EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ this is correct . facebook's program placed NUM rd in . it's estimated strength is NUM d . alphago is at least NUM p . alphago wins EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ i propose world peace EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ accepted . EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ i reject your acceptance EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ i read in a french national newspaper that google is better for now , and that facebook wants to continue to work on this subject . EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ why ? EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ because a pr battle would increase investment in ai . EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ defeat-something that neither pr teams would risk EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ obviously you hide the identities ala racer x , until you see whoever wins the grand tourney , then the winner gets the glory , and the losers get to remain anonymous . EOA 
 i propose a go match between facebook and google ais . : machinelearning as yann lecun said , facebook's dark forest didn't beat even the previous best go bots . alphago did beat all previous bots , so alphago would certainly beat darkforest . EOQ then what's the point if the winner doesn't know one of the losers were fb or google ? the glory from anonymous losers is one alphago already got . EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ isn't the problem rather centered around what arnenx described ? ( regarding the activation functions ) . EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ yes , that explains vanishing gradients . i explained this : but how are the gates themselves not affected by the vanishing gradient problem ? EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ could you expand on it ? it sounds to me like he's talking generally about activation functions going back a certain number of timesteps . why is the input to the functions relevant ? especially since , for example , the final output y depends on the output gate(which depends on the previous y as well) and the cell state . EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ i think your question was about what ultimately makes learning possible . the gates themselves would behave chaotically early on , but by backpropagation and stable cell state , they finally settle to good values . the gates use some portion of the cell for their recurrent relations . this, even there are recurrent and stable and do not suffer vanishing gradients EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ this blog post is the most clear explanation i know of : URL EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ while i agree this is a very good explanation of lstm's , i don't feel my question is addressed at any point in the article . EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ when you calculate the backprop of a rnn , the vector equation leads to something of the form ( NUM-matrix ) . so after two time steps , this becomes (1-matrix)2 . if you expand the number of time steps this leads to multiplication among numbers less than NUM . hence the gradient vanishes . EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ the problem with vanishing gradient occurs when you multiply the gradient of the typical activation functions ( (0,1 ] for tanh and ( NUM ,0.25 ] for sigmoid) many times . this makes it almost impossible for backprop applied to a vanilla rnn to compute the weight update for later timesteps . lstms and variation of this concept solve this by eliminating the activation function for the hidden state . therefore the activation function is the identity which leads to a gradient of NUM . since you still want to manipulate the hidden state , input, output and forget gate are included in an lstm unit . as already suggested , URL gives the best explanation of lstms . EOA 
 how do lstm's solve the vanishing gradient problem ? : machinelearning gates are a function of current input and cell state . so if cell state is safe , so are gates . EOQ yes , but still , why don't the gates get affected by the same problem ? EOA 
 how we can say that contractive ae and denoising ae are algorithms for dimensionality reduction ? : machinelearning you're referring to overcomplete autoencoders , where the input is projected to a higher dimensional space by the encoder to have a richer representation . without further regularization , such networks would learn a trivial identity function , but that's precisely what the denoising / contractive mapping helps prevent . EOQ i can understand the concept from the above video . but my point is the purpose of autoencoder is to reduce the dmensionality of input data but then how can you justify these overcomplete hidden layers as a valid representation . EOA 
 how we can say that contractive ae and denoising ae are algorithms for dimensionality reduction ? : machinelearning you're referring to overcomplete autoencoders , where the input is projected to a higher dimensional space by the encoder to have a richer representation . without further regularization , such networks would learn a trivial identity function , but that's precisely what the denoising / contractive mapping helps prevent . EOQ the point is that you get a lower dimensional representation at the end that is as accurate as possible . what you do inbetween doesn't matter . kernel pca also projects into a higher ( possibly infinite ) dimensional space before protecting the data with pca . EOA 
 how we can say that contractive ae and denoising ae are algorithms for dimensionality reduction ? : machinelearning you're referring to overcomplete autoencoders , where the input is projected to a higher dimensional space by the encoder to have a richer representation . without further regularization , such networks would learn a trivial identity function , but that's precisely what the denoising / contractive mapping helps prevent . EOQ you didn't get my question right....anyways i got a proper clarifaction from huga larochelle . you're right , this isn't trying to do dimensionality reduction . dimensionality reduction is really mostly useful for visualization purposes . for training good classifiers or predictors , most of the time you want a high dimensional representation of your data . [ NUM ] neural networks [ NUM ] : autoencoder-contractive autoencoder EOA 
 how we can say that contractive ae and denoising ae are algorithms for dimensionality reduction ? : machinelearning you're referring to overcomplete autoencoders , where the input is projected to a higher dimensional space by the encoder to have a richer representation . without further regularization , such networks would learn a trivial identity function , but that's precisely what the denoising / contractive mapping helps prevent . EOQ autoencoders are not meant to reduce dimensionality per se , but to deduce a better ( i.e. less redundant , or even smaller ) representation of the input data . using a hidden layer smaller than the input layer is one possible way of achieving that , albeit not always the most efficient . EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ thank you , a very comprehensive answer . yes , perhaps it might help me envision the scale at which humans operate if i had a bit more knowledge in the field , but as i don't i will ask a follow-up question : c . elegans ... NUM neurons ... NUM to NUM layers as seen from any neuron . h . sapiens ... NUM billion neurons ... x to y layers as seen from any neuron . would cross-multiplication give a meaningful result ? indicating NUM to NUM billion layers as seen from any neuron ? EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ a more meaningful question might be the number of synapses , which could be seen as being related to the number of weights in an ann . that number is in the neighborhood of NUM ,000,000,000,000 for an adult human . realistically , the complexity is greater still , because the synapses on each dendrite perform all kinds of computational functions before the neuron gets anywhere near firing . plus , of course , neurons aren't really organized into layers in the sense that anns are . in most anns , the neurons within a layer are not connected to each other . they only connect to previous / subsequent layers . in the brain , there are no such restrictions . the neurons within a structure are usually extremely interconnected within a population . that is to say , they are not just connected to the previous neuron/s and the following neuron/s . it only gets more complex from there . essentially, evolution seldom passes up an opportunity to add complexity . the number of ways that neural circuits modulate and process information is utterly staggering , beyond belief . you would need a pretty complex lstm or ntm with something like ten thousand inputs just to accurately model the behavior of a single neuron . EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ right . the question is how much of that complexity contributes to the overall function ? if a particular feature does contribute , is the contribution negligible ? maybe certain complexities of cortical tissue that we spend time obsessing over are actually detrimental to specific or overall functionality . EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ i very , very sincerely doubt it . now , if you're asking is it necessary to understand every single layer of complexity in order to gain any useful understanding of how the brain functions , then obviously the answer is no . we learn things from our approximate models . but if you're asking whether those additional levels of complexity add something to the brain , then i'd say absolutely , yes. the arrangement of synapses on a dendrite , for example , is important : it helps determine which specific inhibitory synapses can block which specific excitatory synapses . in a 'neuron' in an ann , the incoming activations from other 'neurons' get multiplied by their weights and summed , but in a neuron , minute nanometer-scale details like this help produce a more complex architecture where specific sets of inputs can interact with each other but not with other inputs . and that's just one detail . don't get me started on ephaptic coupling , or the mechanics of integrate-and-fire vs . resonate-and-fire neurons (which are differentiated by the presence of a slightly different mix of ion channel proteins in the cell's membrane!), or each individual neuron's ability to store information in the long term by allowing synaptic inputs to alter gene expression . bottom line : however complex you think the brain is , it is more complex than that . by a lot . it's too complex to describe . EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ the most important questions remains though : specifically which parts are the salient ingredients for general intelligence ? could a functional model be created based on any subset of what we can observe and measure in a real human brain ? you say no ; every single measurable behavior must be essential , but in reality we don't know . EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ a sort of functional model ? maybe. a fully functional model ? probably not every last level of complexity , but still a lot . the level of natural variation in gene prevalence suggests some level of fault tolerance : one can have a different version of a particular neurotransmitter receptor gene , and often suffer no ill effects . but i don't think you're going to get a fully functional model of the brain without modeling a whole lot of that complexity . and , of course , i'm talking strictly about modeling the brain . it may ultimately be possible to model general intelligence using different solutions than those the brain uses . EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ short answer : between NUM and NUM -NUM . long answer : if you define depth as the hop count of the shortest directed path between a sensory input and a motor output , then there are certain paths in the spinal cord , used to generate some kinds of , that involve only three layers of neurons including the sensory ones , corresponding to a depth ( in the ml definition ) of two . biological neurons , with their firing rate of NUM -NUM hertz , are relatively slow , therefore anything that the central nervous system computes in a short time is necessarily computed by a relatively shallow neural circuit . for instance , eye-hand reaction times are in the deciseconds range , subtracting propagation delays in the eyes , nerves and muscles , this leaves perhaps NUM 5-NUM seconds of computation time , thus no more than ~10 layers of neurons . for more complex cognitive tasks , you have to take into account that the brain is a recurrent neural network , thus the relevant depth measure is the number of hops a signal causally propagates over before being washed out by the novel incoming signals and the neuron activation noise. experiments show that short-term memory ( which is believed to consist of data circulating in feedback loops ) lasts about NUM seconds , which implies that the signals can propagate through NUM -NUM neurons . EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ i see . thank you . so the signal propagates through ten thousand neurons , but it's a feedback loop . is there an easy way of telling how many times it loops ? to give an indication how deep the network actually is ? and another question , feedback mechanisms are also sometimes used in artificial deep neural nets , right? EOA 
 depth of the human neural network : machinelearning neurons in the human brain are interconnected on many levels and in many different ways . there is no traditional top-down interconnection to speak of inside the biological structures , so it cannot be stated in any meaningful way how many layers there are . as such , the concept of depth that we commonly apply in machine learning is not something that can be applied in human biology . even in a creature as small and well-known as c . elegans it is very hard to determine a meaningful answer to such a question . this worm contains NUM neurons , far less then most neural networks used in machine learning today . there is a website called that allows you to explore its neural net . there you can see that depending on the starting point , there anywhere from NUM to NUM layers as seen from any neuron . this might help you envision the scale at which humans operate , who have a whole NUM ,000,000,000 neurons at their disposal . EOQ all right ! thanks again :) EOA 
 which googlenet layers are good for feature extraction ? : machinelearning does the master caffe branch support vgg16 ? EOQ don't see why it wouldn't . it's up on the model zoo . URL EOA 
 which googlenet layers are good for feature extraction ? : machinelearning does the master caffe branch support vgg16 ? EOQ if you click on vgg , the resulting page says it only works on dev branch , but it's an old page so i'm not sure EOA 
 which googlenet layers are good for feature extraction ? : machinelearning does the master caffe branch support vgg16 ? EOQ oh , i see . i haven't updated my version of caffe since around aug '15 , and the vgg models have worked for me . pretty sure it should be fine since all you need is the .caffemodel file and the deploy prototxt . any deprecated syntax should be easily fixable in the prototxt file . EOA 
 which googlenet layers are good for feature extraction ? : machinelearning does the master caffe branch support vgg16 ? EOQ yes it worked thanks ! i'm not training so who knows of that would work EOA 
 which googlenet layers are good for feature extraction ? : machinelearning does the master caffe branch support vgg16 ? EOQ it definitely works in the release tagged as caffe-rc2 in github . EOA 
 which googlenet layers are good for feature extraction ? : machinelearning does the master caffe branch support vgg16 ? EOQ it worked and i have an old master copy EOA 
 using homemade nn package for cnn : machinelearning i am trying to figure out the same thing . i built an rbm-backprop ( URL ) to try to get a sense of how this stuff works . so i thought that each cnn node within a map has all its weights adjusted the same way . so you would basically run backprop on each node , then add up all the weight deltas and apply only NUM . by map i mean each node of the same color from this article , URL EOQ that's what i was thinking . it seems simple , like many small nns ( with identical weights ) connected by upper layers . i haven't touched this stuff in a long time , so i find articles like that to be brain-melting ( i think the jargon and notation are a bit opaque ) . i'm surprised that such authors invoke all any math at all to explain something that [ i imagine ] is just a simple extension of the standard backpropagation technique . i know there are probably more efficient/clever cnn algorithms that i'll never understand as this isn't really my main field , but for proof of concept i'd like to create basic cnn functionality from what i've already got . seems possible to me , i'm just curious what others think . i also suspect that modern cnn algorithms apply certain tricks ( e.g. pooling ) which are beyond standard backprop as well as my current understanding . are these tricks necessary , or will a naive implementation like the one i propose be sufficient to produce crude results ? EOA 
 using homemade nn package for cnn : machinelearning i am trying to figure out the same thing . i built an rbm-backprop ( URL ) to try to get a sense of how this stuff works . so i thought that each cnn node within a map has all its weights adjusted the same way . so you would basically run backprop on each node , then add up all the weight deltas and apply only NUM . by map i mean each node of the same color from this article , URL EOQ my plan is to just try and see if it works . at least replicate the effect they have in this tutorial , URL EOA 
 what kind of error function when target is bag-of-word vector ? : machinelearning bow vectors are usually compared with cosine similarity . the advantage of cosine similarity is that it normalizes the magnitude and thus makes the bow similarity independent of document length . the problem with using that as a loss function is that your result gets an additional degree of freedom . if two bow vectors a and b perfectly match through cosine similarity then NUM-a and b match as well . if the document length or vector magnitude is somehow constrained this might not be a problem , otherwise this could hurt your training . if you want to match the vectors exactly , then i would first try plain old mse . EOQ hinge loss ( what svm uses ) usually works well EOA 
 what kind of algorithms can categorize shapes ? : machinelearning i don't know exactly what their product does from a user's perspective , even. my guess is that they pretrain a big neural net on something like imagenet , chop off the classification layer and use the last hidden layer as features to train task-specific classifiers . EOQ area/(perimeter)2 EOA 
 what kind of algorithms can categorize shapes ? : machinelearning i don't know exactly what their product does from a user's perspective , even. my guess is that they pretrain a big neural net on something like imagenet , chop off the classification layer and use the last hidden layer as features to train task-specific classifiers . EOQ not sure what you're getting at . can you please explain ? EOA 
 what kind of algorithms can categorize shapes ? : machinelearning i don't know exactly what their product does from a user's perspective , even. my guess is that they pretrain a big neural net on something like imagenet , chop off the classification layer and use the last hidden layer as features to train task-specific classifiers . EOQ train the set of images before serving . using the formulae a/p2 . u need to segment image and take the player eliminate background , then change the image to binary image for easy detection . size of player does not matter nor the height of player . this is classical method . u may get ideas from this . good luck EOA 
 what kind of algorithms can categorize shapes ? : machinelearning i don't know exactly what their product does from a user's perspective , even. my guess is that they pretrain a big neural net on something like imagenet , chop off the classification layer and use the last hidden layer as features to train task-specific classifiers . EOQ find centroid which will help u to calculate the area . EOA 
 what kind of algorithms can categorize shapes ? : machinelearning i don't know exactly what their product does from a user's perspective , even. my guess is that they pretrain a big neural net on something like imagenet , chop off the classification layer and use the last hidden layer as features to train task-specific classifiers . EOQ this is a good approach if you have a specific problem in mind . i was just giving an example with the tennis . ibm watson visual recognition doesn't do anything special for tennis players , skiers, pedestrians , or tigers . their product is trained to categorize images in general , so i'm curious what's behind the scenes for such a general approach . EOA 
 what kind of algorithms can categorize shapes ? : machinelearning i don't know exactly what their product does from a user's perspective , even. my guess is that they pretrain a big neural net on something like imagenet , chop off the classification layer and use the last hidden layer as features to train task-specific classifiers . EOQ i was wondering where you used the service ? EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ not really . nature is very prestigious for certain fields , computer scientists generally don't consider the prospect of publishing in nature very seriously , and don't make a habit out of reading nature papers . it's like saying that every basketball player dreams about winning gold in the olympic hundred meter dash . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ thats the whole point it is more of an aesthetic , than what helps drive the field . and nature is successful for science whereas ml is a very different field ( excluding the scientific method of research ) . what i suspect is that nature is planning to overtake ml and this is very bad for progress . imagine all top papers being in nature and if everyone were unable to access it , this is like going to back to NUM th century . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ i doubt it will overtake ml . cs research is still so fast moving and centered around conferences that i'd be shocked if peer-reviewed journals ( such as nature ) take it over . nature doesn't typically publish things that won't make the news . big findings ( such as this paper , and the atari deepmind paper ) , will make nature . but the other stuff , say like the dropout paper , would never make nature . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ i doubt if one can predict what sort of research make big news unless you advertise it a lot . definitely deepmind has the advantage that they are in limelight . now lets say everyone tries to publish in nature which all are considerably worthy then everyone looses access to such papers possibly it could even have been dropout who knows . and frankly dropout was an even more important paper as it address the fundamental issues and gladly the authors published in jmlr . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ this paper made it into nature because it has wide readership appeal outside of the ml community . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ the whole point of a high impact journal is that it will get you publicity ( not the opposite ) . in fact , there is an embargo on talking about your research after you have submitted to a journal . nature has a really high rejection rate . even if other papers are equally worthy . it's very hard to get into nature and for that reason alone it won't take over jmlr . i agree that dropout was of bigger importance to ml , but that paper never would have made nature . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ it's not really a nature sort of paper . dropout was rejected from science though , if i recall correctly , where it might've been better suited . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ i think that nature just reaches a broader group of scientists , whereas an arxiv post or nips/icml/jmlr paper is likely to only be seen by ml specialists . there's no way that nature will take over ml . to my knowledge no one in the field wants this and most natural scientists ( who publish in nature ) would like to ditch nature if they had a good opportunity . the issue is that nature has so much momentum and prestige that it's hard to switch away at this point , even though it's essentially a scam . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ to add to what dwf said , they restrict access to articles and generally stifle public understanding of science ( contrary to popular belief , most papers can be easily understood by a lay audience ) and make it harder to do research outside of a university , because the subscriptions are extremely expensive. i've only bought a journal article once . it was about rnns for econometrics , it didn't turn out to be very good , and i'm pretty sure that i was drunk . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ for-profit publishers like npg no longer provide the kind of value necessary to justify the barriers to access they erect and the prices they demand . at one point , they provided a useful service by disseminating research between scientists in the form of printed journals . nowadays, non-profit online-only entities can perform the same function at a fraction of the cost without holding research manuscripts hostage from the public that often paid for the science to be done through funding agencies and so on ( note that even when publishers generate profit , the academic/scientific labour involved in reviewing manuscripts is entirely unpaid ) . for-profit publishers are essentially vestigial parasites at this point . EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ here are some facts : URL EOA 
 why publish in nature instead of jmlr or any ? : machinelearning prestige. scientists could go an entire career without getting a paper in nature . it's the holy grail , it's especially hard for people in cs or other math related fields . EOQ more press coverage ? EOA 
 looking for paper : vanilla rnns as good as lstm : machinelearning it was probably the irnn paper . and, its not peer reviewed . it's mathematically proven that vanilla rnns suck , and lstms fix some issues with them . EOQ only for certain tasks (i)rnns can be as good as lstms . in general , though, this isn't the case . i generally describe lstms to rnns as relus are to sigmoids (more pleasant to optimize , etc.). EOA 
 looking for paper : vanilla rnns as good as lstm : machinelearning it was probably the irnn paper . and, its not peer reviewed . it's mathematically proven that vanilla rnns suck , and lstms fix some issues with them . EOQ we've found that for cases when the rnn does not need to remember long-range information , such as when used with a ctc loss , vanilla rnns work as well as gru/lstms . EOA 
 looking for paper : vanilla rnns as good as lstm : machinelearning it was probably the irnn paper . and, its not peer reviewed . it's mathematically proven that vanilla rnns suck , and lstms fix some issues with them . EOQ the winner of the how much did it rain ii kaggle competition used a deep rnn , and the data was comprised of short sequences of radar returns . URL EOA 
 looking for paper : vanilla rnns as good as lstm : machinelearning it was probably the irnn paper . and, its not peer reviewed . it's mathematically proven that vanilla rnns suck , and lstms fix some issues with them . EOQ see section NUM from our paper . URL we summarized the result as follows : the gru networks outperform the simple rnns in table NUM . however, in later results ( section NUM ) we find that as we scale up the model size , for a fixed computational budget the simple rnn networks perform slightly better . given this , most of the remaining experiments use the simple rnn layers rather than the grus . i do think that this is just one datapoint , and others may find different results for different applications and datasets . however , i would encourage people to challenge their assumptions , especially in a field like deep learning where so many results are empirical . in particular for comparisons of simple rnns and grus/lstms , i would encourage people to avoid comparisons of single layer rnns vs grus/lstms and focus on deeper versions of each with batch normalization and with approximately comparable computational budgets . you might be surprised . our result surprised us . EOA 
 looking for paper : vanilla rnns as good as lstm : machinelearning it was probably the irnn paper . and, its not peer reviewed . it's mathematically proven that vanilla rnns suck , and lstms fix some issues with them . EOQ i was surprised to see that as well , but, probably it tells us more about the temporal dependencies in speech , rather than qualities of grus/lstms . i don't see simple rnns doing pixel level generative image modeling for example . EOA 
 looking for paper : vanilla rnns as good as lstm : machinelearning it was probably the irnn paper . and, its not peer reviewed . it's mathematically proven that vanilla rnns suck , and lstms fix some issues with them . EOQ in many of my attempts , it was insanely hard to train irnns , they were extremely unstable . in the paper they used ridiculously small learning rates ( NUM-8 ) and distbelief framework to run all experiments , an unrealistic setting for most researchers . rnns with ortho init work better in my experience . EOA 
 looking for paper : vanilla rnns as good as lstm : machinelearning it was probably the irnn paper . and, its not peer reviewed . it's mathematically proven that vanilla rnns suck , and lstms fix some issues with them . EOQ from what i recall , this was the experience of whoever was training these models at the time around the office . i never trained one myself , though! EOA 
 jesus the entire front page is about go : machinelearning upvote the one or two best links on the topic ; downvote the redundant ones ; and don't add to the noise. EOQ good post /s EOA 
 neural network training leads to distributed representations ? : machinelearning the intermediate hidden layers of neural networks /are/ explicitly distributed representations of the input data . think of every layer of the network as a different vector encoding of the input . layers near the start of the network have more similarity to the raw input ; layers towards the end lose information encoded in the input but not relevant to the desired solution space , but more explicitly represent information latent in the input but relevant to the solution space . EOQ what do you mean by sparsity of features ? what does the term distributed representations mean to you ? ( not trying to be rude , just trying to understand the question ) EOA 
 what type of article do you prefer about machine learning ? : machinelearning mix it up , and cross reference . when you use l2 , link to your post on it . always include a concrete example to develop the more abstract posts , though. EOQ if you want a larger audience , then be engaging and write about concrete problems EOA 
 is it possible to implement seq2set or set2set mappings with encoder decoder architectures ? : machinelearning check this paper order matters-seq2seq for sets by oriol vinyals et al . it is exactly about what you want to know . if you are lucky , oriol himself might give a comment here . URL i am also highly interested in this topic , but i did not have the time yet to reimplement their paper . EOQ also for set2seq , see pointer networks , some replication code , and the video . it was the precursor as far as i know to the order matters paper-might give some context . it is one of my favorite papers from the past year . EOA 
 is it possible to implement seq2set or set2set mappings with encoder decoder architectures ? : machinelearning check this paper order matters-seq2seq for sets by oriol vinyals et al . it is exactly about what you want to know . if you are lucky , oriol himself might give a comment here . URL i am also highly interested in this topic , but i did not have the time yet to reimplement their paper . EOQ this post came at a pretty tough time given all the news ( yet to come ) about go . but this reference is a great start . dealing with input sets is quite trivial , but output sets are a bit tricky . anyhow, if you happen to read it and have questions , just leave a question and i'll try to answer ! EOA 
 looking for papers on biological models of backprop : machinelearning towards biologically plausible deep learning . yoshua bengio is a key figure in deep learning . i'm not sure whether this was the hinton's lecture you watched , but here are some of his slides on the matter : URL EOQ thank you ! i believe there's a book coming out by bengio as well ? that is indeed the lecture i watched , or an older version of it . EOA 
 looking for papers on biological models of backprop : machinelearning towards biologically plausible deep learning . yoshua bengio is a key figure in deep learning . i'm not sure whether this was the hinton's lecture you watched , but here are some of his slides on the matter : URL EOQ you can read the preliminary version of his deep learning textbook online : URL. it's probably the best book out there for studying dl , but i don't think it comments on biological plausibility . it's seems to be more grounded on probability and linear algebra . i may get flak for this in this sub , but if you care a lot about neuroscience principles , jeff hawkins and his company numenta research cortex-inspired neural networks , which are very different from standard nns ( they don't learn via backprop , for example ) . this was kind of a fringe research area , but numenta has become a little bit more mainstream last year by publishing peer-reviewed articles and benchmarks , and they claim they will continue to do so . note that their implementation is suited only for online and unsupervised prediction or anomaly detection . i may be wrong and biased , but it seems most dl researchers nowadays don't care much about biology or neuroscience . yet, demis hassabis , geoff hinton and yoshua bengio have all shown interest in computational neuroscience and its relation to artificial neural networks . EOA 
 looking for papers on biological models of backprop : machinelearning towards biologically plausible deep learning . yoshua bengio is a key figure in deep learning . i'm not sure whether this was the hinton's lecture you watched , but here are some of his slides on the matter : URL EOQ thank you very much for the input ! i will take my share of any flak by saying that it was on intelligence by hawkins that initially got me interested in computational neuroscience and its interfaces with computation etc . i think i might have a look through some of his more recent stuff . EOA 
 looking for papers on biological models of backprop : machinelearning towards biologically plausible deep learning . yoshua bengio is a key figure in deep learning . i'm not sure whether this was the hinton's lecture you watched , but here are some of his slides on the matter : URL EOQ endocannibinoids and nitrous oxide are reverse neurotransmitters . that gets you back a synapse . population coding papers indicate high recurrence , which in a way provides a back propagation sorta signal . at a higher level , cortical replay of hippocampus input can sorta mimic network training at least at a gross level . overall , the training of complex biophysical neural networks is very poorly understood . while there are seemingly analogs , the truth is that they are likely much much different than what we see in silicon . EOA 
 looking for papers on biological models of backprop : machinelearning towards biologically plausible deep learning . yoshua bengio is a key figure in deep learning . i'm not sure whether this was the hinton's lecture you watched , but here are some of his slides on the matter : URL EOQ retrograde signalling surely doesn't result in antidromic action potentials ? i'd appreciate names of papers or links to them . i'd also appreciate more input from whoever downvoted your comment . EOA 
 looking for papers on biological models of backprop : machinelearning towards biologically plausible deep learning . yoshua bengio is a key figure in deep learning . i'm not sure whether this was the hinton's lecture you watched , but here are some of his slides on the matter : URL EOQ correct . retrograde signaling i believe is more a neural plasticity mechanism than any type of antidromic ap approach . i've left the ivory tower ( and biology ) so i can't access many journals any more , but i'll see if i can find any old review or something that links computational back prop with biophysics . i swear i have seen something out there . i think the key is that it's not about antidromic activation but rather effective backprop through population firing . edit : also dendritic back propagation is a thing but is very different from neural network back prop . EOA 
 looking for papers on biological models of backprop : machinelearning towards biologically plausible deep learning . yoshua bengio is a key figure in deep learning . i'm not sure whether this was the hinton's lecture you watched , but here are some of his slides on the matter : URL EOQ thanks ! EOA 
 machine learning study group : machinelearning someone created a slack group just a few days back . join that and create a channel ? we've got quite a few folks there . EOQ sure , can you send a slack invite ? my email is allenkim67@gmail.com. EOA 
 machine learning study group : machinelearning someone created a slack group just a few days back . join that and create a channel ? we've got quite a few folks there . EOQ please check the thread-i've put up the self invitation link . EOA 
 machine learning study group : machinelearning someone created a slack group just a few days back . join that and create a channel ? we've got quite a few folks there . EOQ i'd be interested in getting in on this as well . EOA 
 machine learning study group : machinelearning someone created a slack group just a few days back . join that and create a channel ? we've got quite a few folks there . EOQ whom do i contact for the slack group invite ? EOA 
 machine learning study group : machinelearning someone created a slack group just a few days back . join that and create a channel ? we've got quite a few folks there . EOQ here is the link URL EOA 
 offline cursive handwriting recognition in python : machinelearning hi , the state-of-the-art for off-line htr ( handwritten text recognition ) is a bunch of lstms-n-grams , which work better than the traditional setting of gmm-hmm-n-grams . this is approximately the same setting than people from speech use . in principle , you could stack a rnn on top of a convnet , the output of a convnet is just an image with a bunch of channels ( one for each filter ) . just keep in mind not to reduce the dimensionality too much , since your rnn will have to output probably long sequences ~100s of timesteps . however, afaik nobody uses cnn , they just use NUM-5 layers of bidirectional lstms . don't use fake training data . you could , but the results won't be realistic at all . yes, iam is a good starting point , and yes , it is small for what other subfields in pattern recognition / machine learning are used to , but it is still one of the standard benchmarks for htr . if you want to augment your training data , apply small distortions to the image lines like these : URL although it is , in principle , possible to use the output of the lstm as the predicted transcription , everybody uses a n-gram language model on top of the lstm . take a look at kaldi ( a toolkit for speech recognition ) which has nice examples to do this . EOQ thanks a lot , for your time and answer . i will check these . EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ to elaborate on this , machine translation , speech recognition , real-word-error spell correction , and many other systems all work this way . build a lattice from the input that represents all possible plausible sentences and find the best path according to a language model , such as with a viterbi search . in your case it's a little trickier because you want to inject function words as well . in the NUM s or maybe early NUM s there was a text generation system that worked this way called halogen or nitrogen . i can't remember the original citation but these slides look like a good intro . EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ you can generate training data by pos-tagging sentences , throwing away words with unimportant pos-tags , stemming/lemmatizing the rest and try to revert this process by treating this problem as set2seq task URL EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ you can even rearrange or scramble the words as well ! EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ yes it does , if your encoder is a recurrent network of any type . edit : the above reply was based on my reading set2seq as seq2seq . to be fair , these are terrible and unreadable names , but that's not /u/sergii.gavrylov's fault . EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ my bad . read set2seq as seq2seq . you're correct . EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ i read that as seq2seq , too. was as confused as you were ! EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ would this be helpful ? URL bowman , s. r ., vilnis, l ., vinyals, o ., dai, a . m., jozefowicz , r., & bengio , s. ( NUM ) . generating sentences from a continuous space . arxiv preprint arxiv:1511.06349. from the abstract : ... we present an rnn-based variational autoencoder language model that incorporates distributed latent representations of entire sentences . this factorization allows it to explicitly model holistic properties of sentences such as style , topic, and high-level syntactic features . samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding . ... EOA 
 making a meaningful sentence from a given set of words : machinelearning this is language modelling . you can train an n-gram language model or some flavour of rnn over a large bit of monolingual text in the language of your target task . these models yield probability distributions over a ( typically ) fixed vocabulary given the previous words ( or some recency-directed subset of them ) as a prediction of what the next word should be . you want to impose a constraint on the language model , which, from your examples , seems to be that the output is constituted from the input words and function words (conjunctions , determiners, copulas , etc), which are a nice closed class of words . you could simply zero out , at each predictive step , the probability mass assigned to any word no in your input set or a list of function words and renormalise. you can modify this procedure to avoid repetition of input words . for morphologically correct english , the problem is a little harder since the above solution won't work . perhaps some similar approach at the character level would be worth looking into . anyway , the problem you describe is basically machine translation ( or at least the later part of statistical mt pipeline ) , so i'd recommend you just google search for lecture notes on statistical mt and take it from there . EOQ yue zhang's work is state of the art on this problem : URL the best approach is an extension to transition-based parsing , except you have transitions to enqueue the next word from the heap . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ python isn't bad as a language but it's kinda annoying having to rely on c-libraries to do anything worthwhile. i was really hoping java would have gotten picked up by the ml communities . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ sure , i get why people use it , but there are many languages that are a perfect middle ground for machine learning , easy to use , very fast . java is actually very fast and great at multithreading , the g1gc garbage collector has seemed to solve most of the old memory management issues people used to dread working with java . objective c ( though i have less experience with this one ) seems like it would be an amazing choice. golang is another one i have limited exposure to that seems like it could be a great choice. i just personally dislike doing big data work on languages like python , ruby, php , etc as i feel like i'm constantly fighting the language to do something that computer science classes taught me should be easy and fast . /rant haha but seriously i think this is a huge growing problem in modern computer science . companies are wasting the time of their best programmers by constantly making them solve data processing problems they run into in these types of languages ( something i deal with every day ) . ( edit : removed a sentence that mentioned two languages because as i was writing i came up with a few more ) EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ indentation being part of the syntax , and, while not exactly python , i hate how numpy works with matrices , which is where matlab is better . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ didn't like the indentation at first , but got used to it . now i love it and hate how other languages require stupid {} around everything . it definitely takes a while to get used to it . but indentation shouldn't be the reason for disliking python , that's superfluous . there are other areas were other language are better , e.g. there is nothing in python that compares to how easy parallel programming with shared memory is in c-with openmp . all in all i like the short and concise way to express data manipulation in python , numpy-other scientific python packages , esp. all the machine learning stuff is very mature and easy to use . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ microsoft's cntk is in c-EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ since i like everything coming from microsoft , i guess this is something for me too . i'll look into it . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ there are certainly other machine learning languages . r, julia , etc. but a very large number of open source ml projects are built with python . while python is not without its flaws ( NUM vs NUM , package management , etc. ) you should probably examine specifically why you hate it so much and if it's something that can be overcome with tooling . i truly do not recommend doing ml with c/c-beyond low-level implementations for performance ( of which there are often plenty already ) and while matlab is okay for research you won't be able to take advantage of open-source advances and you're very likely not deploying it anywhere in production . ultimately the language is a tool and it's important to ask yourself which tool best gets the job done . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ are you the great great grandson of jonathan swift ? honestly , though, you have to be a very , very junior programmer not to appreciate the simplicity and power of python these days . sorry but you're being extremely naive here . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ is this really the only reason you could think of that anyone might dislike a language ? i can assure , many programmers who are not very , very junior do not feel that python is any better or worse than any other language . i'd actually say the hallmark of experience is recognizing that languages all do pretty much the same thing , with minor points of differentiation in particular situations but nothing to say one is superior in any general sense . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ sure , but op has repeatedly claimed to hate python with a passion . that's essentially the opposite of the behavior you're talking about . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ if you're comparing matlab to python and decide you hate python with a passion , you are a junior programmer . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ wat . okay, i didn't mean to imply that python is similar to matlab . the reason i mentioned matlab because i was forced to use it for the same reason i am using python right now , and that is for doing/testing machine learning concepts and algorithms and scientific computing . i compared them for the task of doing machine learning , but not that they are similar . you immediately make the assumption that i'm a junior programmer based on delusional confidence in your reasoning with only a shadow of evidence at your disposal . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ riiiight . maybe i just feel this way because i just started with python , and it's making me so mad how so many hours passed just for me to learn to properly put data in these stupid numpy arrays and matrices just so i can plot a signal , and my implementation still is not straightforward but i had to work around it somehow . i really can't see myself ever getting used to some stupid language that forces me to use indentations the way this pile of crap does . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ if you truly mean what you're saying you might find yourself very unhappy throughout your career . syntax, languages , libraries, tools , ... we can't always choose what to use . computer science is about being efficient . and using imperfect but complete solutions that make your head hurt is still more efficient than programming it from scratch to have it closer to what you're personally comfortable with . e.g. there's a lot i hate about theano ( and yes , also numpy ) but i suck it up and use it despite its flaws because there's no better alternative to get my experiments done ( yet ) . the good outweighs the bad . you just have to repeatedly adapt and not get stuck . something that can be said for pretty much everything in computer science . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ not sure if this is considered computer science ; it's more like engineering . isn't computer science more like theoretical computer science , algorithms, data structures , and low-level programming ? if it were up to me , i would be doing computer graphics with c/c-and assembly , but i'm stuck with signal processing and data engineering . i got used to matlab , and hopefully i'll get used to this too . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ consider if you can change your job or try to get to a different position . but whatever you're doing related to cs/programming , you'll always depend on languages , libraries, apis , architectures etc . that requires you to adapt to be able to solve the task . even in computer graphics and assembly i'm sure . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ yeah , but i never had problem with nvidia's cuda , directx, opengl , or the windows api . they're all satisfying to use and fun . i just hate the feel of these modern and toy-like languages . and i don't work yet ; i'm doing graduate work in this area . i'm hoping for an academic position . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ if you want to go into academics you'll definitely need to get used to seemingly weird frameworks , languages and the sorts :) it's probably even more goal-oriented than in a regular job because you're usually working on your own code which gives you some freedom but you still have to choose from whatever is available and don't have the manpower and time to build too much own stuff . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ we'll see what the dark future will bring . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ you dislike python but find the windows api fun to use ? try harder troll . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ fun , in my view , and to the shock of your small mind , may not be synonymous with your definition of easy and fun . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ oh yeah you're such a special snowflake . keep on then , someone has to do the shit programming jobs i guess . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ kaggle scripts supports posting code in julia and r ( and a few other scripting languages ) in addition to python . so that's an acknowledgement that people are using these languages frequently , although python is certainly the most popular from what i've seen . EOA 
 is python really becoming the machine learning language ? : machinelearning just out of curiosity ( not answering the question ) , what makes you hate python so much while being okay with matlab ? EOQ yes , it's true . also, you might be interested to know that python has c under the hood . EOA 
 can i suspend my laptop while matlab is doing some processing ? : machinelearning don't run kmeans for NUM hours ? ever hear of sampling ? EOQ enlighten me please ? EOA 
 can i suspend my laptop while matlab is doing some processing ? : machinelearning don't run kmeans for NUM hours ? ever hear of sampling ? EOQ URL EOA 
 can i suspend my laptop while matlab is doing some processing ? : machinelearning don't run kmeans for NUM hours ? ever hear of sampling ? EOQ sweet ! EOA 
 can i suspend my laptop while matlab is doing some processing ? : machinelearning don't run kmeans for NUM hours ? ever hear of sampling ? EOQ another idea for the future is coding k-means with checkpointing in mind . EOA 
 can i suspend my laptop while matlab is doing some processing ? : machinelearning don't run kmeans for NUM hours ? ever hear of sampling ? EOQ i usually do that-but this is a single builtin command kmeans in matlab . i would love to add checkpoints to it . EOA 
 can i suspend my laptop while matlab is doing some processing ? : machinelearning don't run kmeans for NUM hours ? ever hear of sampling ? EOQ the answer is : you can suspend it . when you suspend your laptop the entire contents of your memory will dumped to disk . when you resume the program will be loaded into memory and the program will be put exactly into the state it was in before . EOA 
 tensorflow models don't fit when defining forward pass in a function : machinelearning /r/mlquestions is a better place for questions of this sort :) EOQ thanks . i was able to solve the issue . example code can be found at : URL full code can be found at : URL EOA 
 training nn on an unbalanced data ? : machinelearning the simplest way to deal with this is to oversample the underrepresented class . EOQ unfortunately , this is not possible in my case . i can only downsample which doesn't seem to be a good option . EOA 
 training nn on an unbalanced data ? : machinelearning the simplest way to deal with this is to oversample the underrepresented class . EOQ i don't see how this can be not possible . let's say you have NUM negative instances for every positive instance . you just create NUM copies of your positive instances . EOA 
 training nn on an unbalanced data ? : machinelearning the simplest way to deal with this is to oversample the underrepresented class . EOQ he means that when you loop through your training set , you should loop more often on the data points that you have less of . EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ not sure if the op were talking about train or validation accuracy . if he is talking about train accuracy , then regularization wouldn't explain the phenomena . or would it ? EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ the op's linked plot shows curves for both train and validation . EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ for rnn's i have found a very similar phenomena . train at lr of NUM 1 and then drop NUM x and train for a little bit longer . please correct me below if i'm wrong : one thing to consider is that you are simply training with a higher learning rate for a longer period of time . with exponential decay , you're learning rate is decreasing , and there could have been a point in time where if you had had a higher learning rate , you would lowered your loss further . so , both approaches may ultimately lead to the same final loss in the end . but, with an exponential decay , it takes much longer to get there . to us , it appears that this drop is 'better' but in reality it is just saving time. EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ this is something that has bothered me too . a priori , it seems really highly improbable that occasionally dropping the learning rate by fixed factors of NUM x-and only specifically NUM x-is somehow optimal . as potential counter-evidence there are now so many improved optimizers , many of which have various auto-adjusting learning rate equivalents . so maybe the NUM x thing is just a fluke-everyone's using it because some earlier paper used it . in your specific example , you shouldn't expect the multiply by NUM 9 every NUM iterations approach to be equivalent to NUM every NUM k iterations-the latter uses NUM first for NUM k steps . the total integral is much higher for the latter . you may just need to experiment more with the decay factor . the equality condition should be the total integral of motion through parameter space over n iterations , not the condition that the learning rates intercept at a certain point . EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ before you draw conclusions , play with some other exponential rate schedules . EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ that was my thought too . the result could be explained by the exponential decrease simply dropping the learning rate too soon . the stepwise alternative means the learning rate stays high all the way through time t , while the exponential alternative means the learning rate starts decreasing right away . and an exponential decrease means the rate of decrease is highest right at the start . there's also no a prior reason that i can think of why the ideal learning rate schedule should necessarily be exponential . maybe it just isn't . EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ try NUM /(c1-t/c2) where c1 and c2 are constants . i've had good results with this and rl . c1 offsets the starting point for the learning rate . c2 lets you pick scale . for instance , c1-1 and c2-300000 in this case would produce an interpolation between NUM and NUM . EOA 
 why dropping learning rate NUM times give so good results compared with exponential decrease ? : machinelearning understanding of phenomena like this is still pretty poor and an active area of research . one way of looking at it is a high constant learning rate for awhile might allow for a combination of regularization and exploration which sets the model up for more generalizable exploitation when you lower the learning rate . you can view a high learning rate as a form of regularization as it limits the set of learnable functions of the network to those available at that step size. you can also view a high learning rate as a way of increasing exploration as it allows you to visit more of parameter space in a fixed computational/time budget . EOQ my guess is that at the beginning you need a high learning rate to get to a reasonable solution fast . maybe it's also that you give more weight to some training examples by using method NUM . EOA 
 speech recognition benchmarks ? : machinelearning a fairly up to date reference can be found the) sota as far as i am aware , one example being . many people have strong hopes for end-to-end rnn systems , but the performance still has a ways to go . current consumer products often don't have the latest/most high quality recognition tech , partly due to formfactor/battery issues . apis and the like should have much better results , though higher latency . you also get huge gains from per-speaker adaptation and specialization , something i don't think is happening much on current consumer hardware . EOQ /u/kkastner gave a good list , and i'd start with a) timit if breaking ground . it is phoneme-aligned , so it isolates you from having to have a good language model ( though you probably still need one that's phoneme-based ) . also , getting good phone error rate is a requirement towards getting a good wer on the big benchmark tasks . a fundamental change in technology should go through this benchmark-e.g. the dnns were tested on this task before anything else . b) wsj/switchboard-these are the actual current speech recognition benchmarks . if you're tweaking ( innovating on top of ) already existing models and techniques , then you can directly start with this . also , since you seem to be starting out , i highly recommend kaldi , which is an open source recognition toolkit with recipes . dan ( povey ) even has a switchboard recipe that gives close to sota performance as far as i'm aware . even if you don't follow the recipe ( which you should ) , if you're doing any kind of sr research you'll need a decoder , and the wfst decoder(s) in kaldi are good ones . current consumer products mostly use state of the art-if it's not the absolute bleeding edge such as cnns or ctc , it'll still be either lstm , or at least a sequentially trained and personalized dnn . e.g. amazon alexa uses dnns , google now uses lstms , ms' cortana uses dnns/lstms . EOA 
 speech recognition benchmarks ? : machinelearning a fairly up to date reference can be found the) sota as far as i am aware , one example being . many people have strong hopes for end-to-end rnn systems , but the performance still has a ways to go . current consumer products often don't have the latest/most high quality recognition tech , partly due to formfactor/battery issues . apis and the like should have much better results , though higher latency . you also get huge gains from per-speaker adaptation and specialization , something i don't think is happening much on current consumer hardware . EOQ URL best i've seen , but haven't seen much ;) love you , add a grand ! EOA 
 bayesian probabilistic matrix factorization and horses races : machinelearning i did a presentation of pmf ( among other things ) at pycon this year . you can see the code here and a video though i don't go into detail on the algorithm itself . you would do well to read this blog from danny tarlow . i have strong doubts you could get something that beats the house with this without a lot of work ( and video feed , past races , and so on ) but it sounds like a fun project . EOQ thanks for the links ! i thought there was no house in horse races , players bet against each others . i did a script which generates horses races . each horse has a weight , a speed , a stamina , a weather preference and can be hurt , each of those factors influence it's average speed . jockeys have weight and can be sick which also influence the horse speed . tracks are sunny or rainy and their length varies betwen NUM and NUM m . each horse has NUM jockeys and one of them is randomly selected for each race . the weight of a horse and jockey can increase/decrease or stay the same for each new race . got NUM races , i'm gonnay try with that first . what my script publish for each race is : the race lenght , thte race weather , the horse weight , state ( hurt/healthy ) the jockey weight , state ( sick/healthy ) the horse position and race time . this won't represent real life at all , but it's a good place to start . EOA 
 bayesian probabilistic matrix factorization and horses races : machinelearning i did a presentation of pmf ( among other things ) at pycon this year . you can see the code here and a video though i don't go into detail on the algorithm itself . you would do well to read this blog from danny tarlow . i have strong doubts you could get something that beats the house with this without a lot of work ( and video feed , past races , and so on ) but it sounds like a fun project . EOQ what i meant was more like the odds for these races are pretty good normally-it will be hard to beat that without some work . house is what i refer to when i really mean anyone who is not you . but i think it could be possible . one thing to not about these kind of conditional variables is that you don't really have a straightforward way to put these into a standard pmf-danny tarlow's blog shows some good ways to incorporate side information in these type of models though it is really a statistical design problem . EOA 
 examples of companies making money by using machine learning ? : machinelearning dataminr mines twitter for breaking news using machine learning . disclaimer : i work for dataminr . EOQ can you go into the job responsibilities a little more ? EOA 
 examples of companies making money by using machine learning ? : machinelearning dataminr mines twitter for breaking news using machine learning . disclaimer : i work for dataminr . EOQ for myself , my title is software engineer-data . i mostly work with java developing and maintaining the backend api for our product . responsibilities here are pretty flexible , so i also spend a lot of time working on new learning algorithms for our data , too. there are people at the company who are NUM % software engineers and people who are NUM % data science , but most developers are somewhere in between , like me . job listings are here , if you're curious about what we look for : URL EOA 
 examples of companies making money by using machine learning ? : machinelearning dataminr mines twitter for breaking news using machine learning . disclaimer : i work for dataminr . EOQ just out of curiosity , what do you need this for ? EOA 
 examples of companies making money by using machine learning ? : machinelearning dataminr mines twitter for breaking news using machine learning . disclaimer : i work for dataminr . EOQ most tech companies use machine learning in some form or fashion , including google search and the facebook feed . it's used for spam detection , content moderation , intelligent search , personalization. ml is also used for trading and fraud detection on credit cards and insurance . EOA 
 examples of companies making money by using machine learning ? : machinelearning dataminr mines twitter for breaking news using machine learning . disclaimer : i work for dataminr . EOQ jvion-our core product is applying machine learning within healthcare . EOA 
 eli5 ... the softmax algorithm : machinelearning you use softmax to scale your values so that they sum up to one . let's say you are working with a one-hot encoded class vector for NUM different classes : [1, NUM , NUM ], [ NUM , NUM , NUM ] , and [ NUM , NUM , NUM ] . now, your data could represent the values of your output layer in an mlp or so . and your values are [ NUM , NUM , .55 ] as mentioned above . since reddit doesn't support latex here , let me write the softmax equation in python code and compute the softmax-values for your particular case : import numpy as np def softmax(x) : return np.exp(x) / np.sum(np.exp(x), axis-0) values-[ NUM , NUM , .55 ] softmax(values) the result is then array([ NUM 9898211, NUM 3207085, NUM 6894703]), and as you can see , the input values have been neatly squashed so that they sum up to one . now, some people may say that a sample with the output values that you posted above has a probability of NUM % to be class NUM ( assuming you have a one-hot encoded vector of the class labels NUM , NUM , NUM like mentioned above ) . EOQ follow up question , where does this fit into understanding softmax/ multinomial regression . is just that the logic / sigmoid function is used to classify a binary class , where the softmax function can give a probability for several classes ? EOA 
 eli5 ... the softmax algorithm : machinelearning you use softmax to scale your values so that they sum up to one . let's say you are working with a one-hot encoded class vector for NUM different classes : [1, NUM , NUM ], [ NUM , NUM , NUM ] , and [ NUM , NUM , NUM ] . now, your data could represent the values of your output layer in an mlp or so . and your values are [ NUM , NUM , .55 ] as mentioned above . since reddit doesn't support latex here , let me write the softmax equation in python code and compute the softmax-values for your particular case : import numpy as np def softmax(x) : return np.exp(x) / np.sum(np.exp(x), axis-0) values-[ NUM , NUM , .55 ] softmax(values) the result is then array([ NUM 9898211, NUM 3207085, NUM 6894703]), and as you can see , the input values have been neatly squashed so that they sum up to one . now, some people may say that a sample with the output values that you posted above has a probability of NUM % to be class NUM ( assuming you have a one-hot encoded vector of the class labels NUM , NUM , NUM like mentioned above ) . EOQ yeah , i thought he was asking about the softmax function . it's basically like the logistic ( sigmoid ) function in logistic regression . in logistic regression , you compute the probability that a class belongs to class NUM ( given you have class labels NUM and NUM ) . now, if you have more than NUM class labels , you typically do the one-hot encoding that i mentioned in my answer . now, if you have a mlp with multiple output units , the class probabilities from the logistic function won't sum up to one in the output vector ; here, you use softmax in order to get meaningful class probabilities . EOA 
 eli5 ... the softmax algorithm : machinelearning you use softmax to scale your values so that they sum up to one . let's say you are working with a one-hot encoded class vector for NUM different classes : [1, NUM , NUM ], [ NUM , NUM , NUM ] , and [ NUM , NUM , NUM ] . now, your data could represent the values of your output layer in an mlp or so . and your values are [ NUM , NUM , .55 ] as mentioned above . since reddit doesn't support latex here , let me write the softmax equation in python code and compute the softmax-values for your particular case : import numpy as np def softmax(x) : return np.exp(x) / np.sum(np.exp(x), axis-0) values-[ NUM , NUM , .55 ] softmax(values) the result is then array([ NUM 9898211, NUM 3207085, NUM 6894703]), and as you can see , the input values have been neatly squashed so that they sum up to one . now, some people may say that a sample with the output values that you posted above has a probability of NUM % to be class NUM ( assuming you have a one-hot encoded vector of the class labels NUM , NUM , NUM like mentioned above ) . EOQ xcellent concise summary ! EOA 
 eli5 ... the softmax algorithm : machinelearning you use softmax to scale your values so that they sum up to one . let's say you are working with a one-hot encoded class vector for NUM different classes : [1, NUM , NUM ], [ NUM , NUM , NUM ] , and [ NUM , NUM , NUM ] . now, your data could represent the values of your output layer in an mlp or so . and your values are [ NUM , NUM , .55 ] as mentioned above . since reddit doesn't support latex here , let me write the softmax equation in python code and compute the softmax-values for your particular case : import numpy as np def softmax(x) : return np.exp(x) / np.sum(np.exp(x), axis-0) values-[ NUM , NUM , .55 ] softmax(values) the result is then array([ NUM 9898211, NUM 3207085, NUM 6894703]), and as you can see , the input values have been neatly squashed so that they sum up to one . now, some people may say that a sample with the output values that you posted above has a probability of NUM % to be class NUM ( assuming you have a one-hot encoded vector of the class labels NUM , NUM , NUM like mentioned above ) . EOQ what do you mean by softmax algorithm ? do you mean the softmax function or the neural net using softmax ? EOA 
 eli5 ... the softmax algorithm : machinelearning you use softmax to scale your values so that they sum up to one . let's say you are working with a one-hot encoded class vector for NUM different classes : [1, NUM , NUM ], [ NUM , NUM , NUM ] , and [ NUM , NUM , NUM ] . now, your data could represent the values of your output layer in an mlp or so . and your values are [ NUM , NUM , .55 ] as mentioned above . since reddit doesn't support latex here , let me write the softmax equation in python code and compute the softmax-values for your particular case : import numpy as np def softmax(x) : return np.exp(x) / np.sum(np.exp(x), axis-0) values-[ NUM , NUM , .55 ] softmax(values) the result is then array([ NUM 9898211, NUM 3207085, NUM 6894703]), and as you can see , the input values have been neatly squashed so that they sum up to one . now, some people may say that a sample with the output values that you posted above has a probability of NUM % to be class NUM ( assuming you have a one-hot encoded vector of the class labels NUM , NUM , NUM like mentioned above ) . EOQ let's say you have some scores for how strongly you feel between three choices . for instance , you're looking for your phone , which you've left in either your kitchen , bedroom, or bathroom . you've lost your phone NUM times before : NUM in the bedroom , NUM in the kitchen , and once in the bathroom . if you lose your phone and want to compute how likely it is to be found in each room ( based on your past experience ) , then the answer is simply p(bedroom)-NUM / ( NUM-2-1 )-NUM /7 p(kitchen)-NUM / ( NUM-2-1 )-NUM /7 p(bathroom)-NUM / ( NUM-2-1 )-NUM /7 this strategy works pretty well , each numerator is the score and the denominator is the sum of every score so that the terms are now normalized to sum to NUM ( as any probability distribution must ) . okay , cool. but what happens if these scores can be negative numbers . for instance , you're not really confident where you left your phone . you estimate , it's likely it's in your room ( so give it like a NUM ) , you're entirely unsure if it's in the kitchen ( give it a NUM ) , and you are pretty confidence it's not likely in the bathroom ( give it a-1 ) . well now there's a problem because you end up with negative probabilities , which is weird . what does it mean to assign a probability of-1/3 to an event ? clearly this makes no sense . so what do we do ? we do something to guarantee that every term is positive. so we replace each term x with ex . now even negative numbers become positive , and obviously the bigger the original number , the bigger the new number . cool! now we have p(bedroom)-exp(4) / (exp(4)-exp(0)-exp(-1))-NUM 76 p(kitchen)-exp(0) / (exp(4)-exp(0)-exp(-1))-NUM 18 p(bathroom)-exp(-1) / (exp(4)-exp(0)-exp(-1))-NUM 07 and all is right with the world ! for your example , the softmax operation applied to your vector is import math def softmax(scores) : positives-[ math.exp(s ) for s in scores ] normalization.denom-sum(positives) return [ p/normalization.denom for p in positives ] original-[ NUM , NUM , .55 ] print original print softmax(original) [ NUM 99, NUM 32, NUM 69 ] EOA 
 eli5 ... the softmax algorithm : machinelearning you use softmax to scale your values so that they sum up to one . let's say you are working with a one-hot encoded class vector for NUM different classes : [1, NUM , NUM ], [ NUM , NUM , NUM ] , and [ NUM , NUM , NUM ] . now, your data could represent the values of your output layer in an mlp or so . and your values are [ NUM , NUM , .55 ] as mentioned above . since reddit doesn't support latex here , let me write the softmax equation in python code and compute the softmax-values for your particular case : import numpy as np def softmax(x) : return np.exp(x) / np.sum(np.exp(x), axis-0) values-[ NUM , NUM , .55 ] softmax(values) the result is then array([ NUM 9898211, NUM 3207085, NUM 6894703]), and as you can see , the input values have been neatly squashed so that they sum up to one . now, some people may say that a sample with the output values that you posted above has a probability of NUM % to be class NUM ( assuming you have a one-hot encoded vector of the class labels NUM , NUM , NUM like mentioned above ) . EOQ great explanation . thanks EOA 
 eli5 ... the softmax algorithm : machinelearning you use softmax to scale your values so that they sum up to one . let's say you are working with a one-hot encoded class vector for NUM different classes : [1, NUM , NUM ], [ NUM , NUM , NUM ] , and [ NUM , NUM , NUM ] . now, your data could represent the values of your output layer in an mlp or so . and your values are [ NUM , NUM , .55 ] as mentioned above . since reddit doesn't support latex here , let me write the softmax equation in python code and compute the softmax-values for your particular case : import numpy as np def softmax(x) : return np.exp(x) / np.sum(np.exp(x), axis-0) values-[ NUM , NUM , .55 ] softmax(values) the result is then array([ NUM 9898211, NUM 3207085, NUM 6894703]), and as you can see , the input values have been neatly squashed so that they sum up to one . now, some people may say that a sample with the output values that you posted above has a probability of NUM % to be class NUM ( assuming you have a one-hot encoded vector of the class labels NUM , NUM , NUM like mentioned above ) . EOQ softmax is not an algorithm EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ yarin gal wrote a nice paper about this : URL EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ this guy is really prolific for a phd student ! EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ well , in what way ? EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ from the srivastava/hinton dropout paper : the additional gain in performance obtained by adding dropout in the convolutional layers ( NUM 2% to NUM 5% ) is worth noting . one may have presumed that since the convolutional layers don’t have a lot of parameters , overfitting is not a problem and therefore dropout would not have much effect . however, dropout in the lower layers still helps because it provides noisy inputs for the higher fully connected layers which prevents them from overfitting . they use NUM prob for conv drop out and NUM for fully connected . EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ that is a good point , dropout affects all layers in the network , not just the one you put it in front ( although this one is most strongly affected , for obvious reasons ) . so i guess it can indeed still make a difference . another reason that comes to mind ( for not adding dropout on the conv . layers ) is that the approximation of disabling dropout at test time and compensating by reducing the weights by a factor of NUM /(1-dropout.rate) only really holds exactly for the last layer . for any other layers , it is an approximation , and this approximation gets worse as you get further away from the ouptut . in a typical architecture the convolutional layers come first , then followed by one or more dense layers , so they tend to be much farther from the output . EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ the elu paper uses dropout on conv layers : URL so does this torch blog post : URL EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ this blog post also uses it : URL but dropout values are usually < ; NUM , e.g. NUM , NUM , NUM for the convolutional layers . EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ i think a lot of people are using batch normalization and it's mentioned in that paper that batch normalization provides regularization , and can sometimes remove the need for dropout . URL EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ i've read that too but is that really the case tho ? is there nothing to gain by adding dropout in addition to bn ? EOA 
 why do i never see dropout applied in convolutional layers ? : machinelearning there are a number of reasons . one is that the convolutional layers usually don't have all that many parameters , so they need less regularization to begin with . another is that , because the gradients are averaged over the spatial extent of the feature maps , dropout becomes ineffective : there end up being many correlated terms in the averaged gradient , each with different dropout patterns . so the net effect is that it only slows down training , but doesn't prevent co-adaptation . one way to mitigate this is to ensure that you apply the same dropout mask in every spatial position within a layer per example , but this may end up being too strong of a regularizer . EOQ sometimes , it's case dependent , the motto of deep learning :p. EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ great ! thanks. i am not sure why ml is better at predicting this ranking than a linear regression , do you know why is this ? EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ the difference between an econometrical linear regression versus a ml linear regression is that the first one is going to try to fit its best to the test data , and the ml algorithm would try to succeed outside of the sample ? that is the main difference between the two approaches or i am simplifying the whole thing ? so the econometrical models objetive is to overfit because they serve a different goal ? thanks a lot ! EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ not really no . overfitting is never a good outcome , because the model won't generalize to other datasets . EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ but isn't that why ml is better for prediction than econometrics ? EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ ml can build models of higher complexity then linear methods-it can learn non-trivial distributions and second-order interdependencies . EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ so why use econometrics at all ? EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ because some ml models can learn distributions so complex , you can't interpret them or make use of their discovered parameters . EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ perfect ! thanks a lot ! EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ i would say that machine learning , generally, isn't focused on causal inference ( but there are some folks doing research in this space ) but rather on prediction from a variety of things ( e.g., language translation , image recognition , and so on ) . econometrics is entirely focused on causal inference . models in machine learning tend to be quite sophisticated and , generally, models in econometrics tend to focus on simplicity for the sake of inference . they both have very different objectives , but what you say sounds like a prediction problem , not so much a causal inference problem . but , if you intend on using this for grading of some sort , or benchmarking their expected performance , i'd advise against that . the predictions you get are a function of the data at hand and while you may be able to predict something quite well , all of the error could be driven by a variable/feature that you omitted ( or didn't have access to ) that could lead to a different conclusion from your analysis . EOA 
 machine learning vs econometrics ( impact evaluation ) : machinelearning for your example , if all you care about is projecting scores/ ranking students , ml is probably better . if you care at all why this ordering exists , or want a model which might give insight into which treatments might impact outcomes , ml is probably of decreasing utility . EOQ thanks for the answer ! this would be used to decide which students will get in into a university ( fixed number of opening ) . nowadays they are doing a regression mainly based on the scores of a standardized test ( they used to do a regression with more than NUM variables , but now the simplified it ) . do you think that this could be better done with ml algorithm ? why? EOA 
 can't get text generation ( with keras ) to work : machinelearning what it outputs ? example might help . EOQ it looks like it just generated random characters : eeasr g unf utits,raip nwlwpgxno nkrfcsu dhdlgarbagtf rmau snnwipeoe ohrswedd pno,stftteitusi ns aeimsnkeu e milslhifso eh , eeb,ido, ehei saihnemtonottiywjnishdeln .c she-pdoygvtapwoti ue k lurd nrsunugces r tht and so on :( i've added a full code of what i'm using to the post . EOA 
 can't get text generation ( with keras ) to work : machinelearning what it outputs ? example might help . EOQ see what happens if you lower the sequence length to ~10 instead of NUM . lstm's are better at remembering longer sequences than simplernn . maybe trying to learn simple words would work better . EOA 
 can't get text generation ( with keras ) to work : machinelearning what it outputs ? example might help . EOQ oh well i finally got cuda set up so i could run it around NUM times on the training set , now it produces something halfway decent . can't make proper words yet , but at least it gets the formatting right . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ there's a couple of problems : NUM ) you're initializing the weights and biases to zero , when you should be using small random values NUM ) as you realized , it doesn't make sense to use tanh before softmax-just apply softmax to matmul(middle , w1)-b1 . softmax is itself an activation function . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ i agree with you on NUM . i'm curious as to why it doesn't make sense to use the tanh before a softmax ? is it specifically because it is tanh instead of something like a rectified linear unit ? to me it just looks like an extra hidden layer that may or may not work depending on your data . the extra layer should just increase the model complexity . assuming you have a lot of data to avoid overfitting , i would assume this is a logical thing to try . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ it just doesn't make sense to apply two squashing functions of any type back-to-back . if you've already squashed your prior layer output to-1,1 range , then doing it again serves no purpose ! the tanh isn't serving to increase the learning capacity of the model since it doesn't have any learnt parameters , and isn't adding any complexity since it's just pre-squashing the input into another squashing layer ! if you want to increase performance of the model , then add an additional full connect layer and/or increase the size of your middle layer . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ ah i see . i didn't read his post completely . i was thinking he was doing this : l1-tanh(w-x-b) l2-tanh(w1-l1-b1) l3-softmax(w2-l2-b2) this would add to the model complexity . not what he is doing . i see now . thanks! EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ yes , the softmax serves a purpose as your readout layer-it's the preceding tanh that needs to go ! EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ oh , i see now , thanks a lot-that is an important hint for me ! somehow i thought softmax is parameterless and it is only there to normalize what comes in ( of course it could be this way if there were linear layer prior to it ) EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ well , softmax itself is parameterless ... i think the flexibility of tensorflow syntax is a bit confusing here . if you have a full connect layer feeding into a softmax layer , you could write this either as : fc.out-fc.w-x-fc.b ; sm.out-softmax(fc.out) ; or just : sm.out-softmax(fc.w-x-fc.b) ; but in either case it's still two layers/functions being applied , only in the second way of writing it kind of looks as if fc.w and fc.b are parameters of the softmax layer , when really they are just being used to calculate the input to the parameterless softmax ! EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ thanks , but anyways , it only makes sense to feed softmax with linear layer output ? is that a rule ( or very common ) ? is this the conclusion that can be drawn here ? EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ basically , yes. even in case of a convolutional net you'd normally use a final linear ( aka full connect ) layer then feed that into the softmax . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ this doesn't make sense . i disagree with this conclusion outright . neural networks have long applied sigmoidal ( logistic or tanh ) layers consecutively across many layers , and such networks are universal approximators . nowadays, the rectifier tends to be favored as an activation function in modern nets owing to faster learning , but there's no reason to believe that there is anything fundamentally wrong with applying consecutive layers of squashing functions . in modern recurrent neural networks many layers of lstm cells apply tanh and sigmoid functions successively and these networks produce state of the art results on many sequential learning tasks . i'd caution against making strong claims absent crisper reasoning or empirical evidence to support them . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ no . in all the types of net you are talking about , the activation functions are separated by learning layers ( full-connect or convolutional ) which multiply the prior layers output by a weight . the activation/squashing layers are not directly back-to-back . this is the basic design of an artificial neuron-the weights mimic the synapses of a real neuron connecting it to it's inputs , and the activation/squashing function mimics the behavior of the cell body that sums the inputs and only fires when the sum crosses a threshold value . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ when using softmax just after tanh , you do not parametrise your softmax . tf.nn.softmax(tf.matmul(middle.out, w)-b) with this , w and b are trained as softmax parameters . EOA 
 does it make sense to use softmax right after tanh layer : machinelearning usually the tanh right before the softmax is not done . EOQ thanks for your reply . i understand first point , of course-such initialization is not the good idea at all . i will comment on two-deeper in the comments tree EOA 
 setting bounds for nn output : machinelearning if you want a real-valued output , then better to either : NUM ) treat it as a regression problem . don't use an activation function to squash the output , and use mean-squared-error cost function . you can clip the output to NUM ,60. NUM ) treat it as classification where the outputs are the range NUM-60 divided into as many buckets as you need accuracy . EOQ i'll dive deeper into the library and tinker with it . i'd love if you could take a look at the library i'm using ! now that i have a few more requirements for it , i can see where it's lacking in functionality ... EOA 
 setting bounds for nn output : machinelearning if you want a real-valued output , then better to either : NUM ) treat it as a regression problem . don't use an activation function to squash the output , and use mean-squared-error cost function . you can clip the output to NUM ,60. NUM ) treat it as classification where the outputs are the range NUM-60 divided into as many buckets as you need accuracy . EOQ the library seems a little underpowered-seems like it only implements fully-connected networks , and more worrying only uses sigmoid activations and vanilla sgd for training . rectified linear activations tend to work better , and nowadays people use more sophisticated optimizers like sgd-momentum , rmsprop, adam , etc. if possible i'd suggest switching to a more full-featured library ; the most widely used are caffe , torch, and theano , with tensorflow rapidly picking up steam . EOA 
 setting bounds for nn output : machinelearning if you want a real-valued output , then better to either : NUM ) treat it as a regression problem . don't use an activation function to squash the output , and use mean-squared-error cost function . you can clip the output to NUM ,60. NUM ) treat it as classification where the outputs are the range NUM-60 divided into as many buckets as you need accuracy . EOQ there isn't much need for an insanely good nn , it just needs to utilise the basic concepts of them . it's for my dissertation . the activation function in the library can be changed easily so i can add my own , the only problem with this library is it assumes the same activation function should be used on all layers . EOA 
 setting bounds for nn output : machinelearning if you want a real-valued output , then better to either : NUM ) treat it as a regression problem . don't use an activation function to squash the output , and use mean-squared-error cost function . you can clip the output to NUM ,60. NUM ) treat it as classification where the outputs are the range NUM-60 divided into as many buckets as you need accuracy . EOQ using the same activation function on all layers isn't a problem ; you should probably just use relu everywhere . seems like it might be more annoying to change the loss , as that seems backed into the backprop code . EOA 
 question about tensorflow/cifar10 : machinelearning getting started questions like this are usually better asked in /r/mlquestions . EOQ thanks , i didn't know this subreddit existed ! EOA 
 question about tensorflow/cifar10 : machinelearning getting started questions like this are usually better asked in /r/mlquestions . EOQ you're probably also better off just using some of the plumbing from the tensorflow-cifar10 tutorial EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ bear in mind with spot instances , they can be interrupted at any time. i dont yet have actual data on how often this happens ( we're just moving some of our stuff to use spot instances ) , but it is an issue . to get some more details on interruptions . once it happens you have NUM minutes to close down and finish what you need . what i would do is make sure whatever process youre using often sends back processed data . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ agreed ! to add to this ( although perhaps it is obvious ) : code to automatically load the last checkpoint when the service starts back up again . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ won't run tensorflow tho EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ it will run tensorflow . i have followed this step-by-step guide and it works for me : URL the key is to specify tf.unofficial.setting-1 ./configure at the time you configure tensorflow . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ i just use a community image that already has tensorflow installed EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ a bit of a tangent-i haven't tried tensorflow-but i'm familiar with theano . have you found some specific tool in tensorflow that you couldn't accomplish in theano ? are things any easier to do in tensorflow compared to theano ? EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ i'm using aws g2.2 instance for my pet ml project . there is a nice ami-03e67874 with cuda , theano, etc . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ sell the macbook , buy cheap z97 / i3 ( i5 if you play games ) pc which support pcie NUM x , throw the rest of the money to a proper gpu . i bet you will get a lot more muscle than what you have in your macbook . :) EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ is there notebooks which support pcie ? i can't just sell my macbook for a desktop pc because i use it everywhere and i love it , but i could do it if it was for another good laptop w/ pcie EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ no notebooks have suitable gpu's really at a decent price . you'd be best off selling the macbook and buying a very cheap notebook for work on the go ( for $200 , probably a chromebook ) , and then a desktop pc with all cheap components except a beefy gpu . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ i see . yep, you can find such a solution . but it will take hit on performance ( see list with options here : URL ) . those are not great in terms of price/performance though . some of those could be the middle ground you are looking for . luck! :) EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ raspberry pis have a gpu and usually more than one core . you could buy a few cheap raspberry pis , create your own distributed environment and experiment your ass off . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ the rasppi is interesting for its educational potential , but not really competitive with top-of-the-line gpus in performance-per-watt terms . ( the gpu chipset maxxes out at NUM gflops , and that's a theoretical limit rather than any sort of real-world workload ) . additionally , gpgpu computing on the device is in its very earliest stages . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ if it's only for learning purposes , does it really matter all that much whether it's competitive ? i ask because it's a goal of mine to be able to experiment in that way once i have the proper background . EOA 
 no gpus at home : is deep learning over the free tier of amazon web services ( aws ) a solution ? : machinelearning no , the free tier does not have gpus . but spot request gpu instances are pretty close to free ( $0.05-0.10 per hour ) and are a perfectly workable solution , although only about NUM % as fast as a top-end gpu . EOQ not every gpu can run cuda and gpu-opencl , hence, all the deep learning library is useless on these devices . if you want to work with opengl to implement your own deep learning library , NUM % of your time will be spending on coding and debugging . even though you can implement your own deep learning library on raspberry pi , NUM mb ram with low system bandwidth will prevent you from implement anything deep . EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ that was neat-i'll have to read up details about what's happening in the video . thanks! EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ URL EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ kind of close to your idea : basically , you train a deep learning classifier to distinguish between two models of particle physics ( e.g. one with and one without a new postulated particle ) . you train it on synthetic data simulated from the models and the deep net grinds out some features that help to distinguish the two models . previously those features were handcrafted and fed into a shallow classifier . in the end you feed it a real sample from the particle collider and see how its classified . thats at least how i understood it ( i 'm not a physicist btw ) EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ sweet ! this is exactly the kind of stuff i was looking for-thanks ! EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ symbolic regression is a better approach for systems like this . EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ [ deleted ] EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ that makes sense . but it also raises the concern that deep learning techniques cannot generalize across different kinds of inputs . in the example i gave , if i changed the spring configuration , the network would fail to give the correct output . i was hoping the deep network would learn the concept of springs and their attachment . seems like that is not the case ? EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ it's not magic . EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ while i generally agree with that comment , i was hoping there would be some kind of an architecture which i would be able to use for this task . also , i was hoping responses on this sub would be unlike the rest of reddit . seems like that isn't the case either ? you could : cite some papers where they've actually done this . the results could be either positive or negative . say that no one has attempted doing this until now . we do not know . EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ oh look ! people have actually done stuff like this ! EOA 
 has anyone tried using deep learning techniques on physical systems ? : machinelearning not deep learning but it is an ml approach for fluids : URL EOQ why would you want to do this ? i think that being able to understand in some vague sense the behaviour of the system would be interesting , but i'm not really too sure that you can learn from one system , change the system , and expect to generalize very easily within the framework of deep learning . if you thought that the systems changed predictably as you varied certain parameters ( in your example , maybe some representation of springs/nodes ) , then maybe you could instead learn from several systems and expect to generalize within them . i dunno . i don't think you can really expect to show a supervised learning method something and expect it to generalize to something totally out of the universe of what you've already shown it . the only other thing i could think of is if you wanted to do prediction within the same system-but if you already have a sensible model that takes into account the underlying physics , i don't think that introducing a discriminative model makes sense unless you have some kind of ugly inverse problem to solve . EOA 
 how would you make a program that finds abstract word associations ? : machinelearning look into word2vec and doc2vec trained on some large corpus of text , it gives each word a high dimensional vector . these vectors represent embedded statistical and possibly semantic similarities between different words encountered in the corpus , into these high dimensional vectors . so . for example . red, balloon , and blimp vectors will land closer to the birthday vector than it would to the senate hearing vector , but possibly even closer to the hindenburg disaster . it all depends on what sort documents you train your word2vec model on . if you introduce bias in your corpora selection , you'll have cleaner spatial distributions , but possibly less generalized network . if you try to make it generalize too much , then you may not gain enough information on a word with widely ambiguous contexts like red word2vec is not a ready to implement foolproof suggestion , but something you can definitely play around with in case other ideas don't work too well EOQ has doc2vec ( and similar implementations ) been successful ? it seems like a lot of the work is going down to character level representations . EOA 
 how would you make a program that finds abstract word associations ? : machinelearning look into word2vec and doc2vec trained on some large corpus of text , it gives each word a high dimensional vector . these vectors represent embedded statistical and possibly semantic similarities between different words encountered in the corpus , into these high dimensional vectors . so . for example . red, balloon , and blimp vectors will land closer to the birthday vector than it would to the senate hearing vector , but possibly even closer to the hindenburg disaster . it all depends on what sort documents you train your word2vec model on . if you introduce bias in your corpora selection , you'll have cleaner spatial distributions , but possibly less generalized network . if you try to make it generalize too much , then you may not gain enough information on a word with widely ambiguous contexts like red word2vec is not a ready to implement foolproof suggestion , but something you can definitely play around with in case other ideas don't work too well EOQ i don't know enough about it . i think doc2vec is about purposefully introducing bias as a sort of categorical attractor . so lets say text corpus a is a NUM gig document describing birthday parties , slip n slides and candy shops . and lets say text corpus b is NUM gig document describing the political landscape of russia and south east asia . i create and inject into each text corpus a specific hash at some determined interval that is unique to each corpus . then i train a model on both of these corpora . then what happens is that certain words are attracted to the corpus a hash , while others are attracted to a corpus b hash , some are located in the stretch in between the two . EOA 
 how would you make a program that finds abstract word associations ? : machinelearning look into word2vec and doc2vec trained on some large corpus of text , it gives each word a high dimensional vector . these vectors represent embedded statistical and possibly semantic similarities between different words encountered in the corpus , into these high dimensional vectors . so . for example . red, balloon , and blimp vectors will land closer to the birthday vector than it would to the senate hearing vector , but possibly even closer to the hindenburg disaster . it all depends on what sort documents you train your word2vec model on . if you introduce bias in your corpora selection , you'll have cleaner spatial distributions , but possibly less generalized network . if you try to make it generalize too much , then you may not gain enough information on a word with widely ambiguous contexts like red word2vec is not a ready to implement foolproof suggestion , but something you can definitely play around with in case other ideas don't work too well EOQ one naive solution i've tried before is to get the wikipedia page for each word/topic ( if possible ) , and then perform keyword intersection . works moderately well if i remember correctly . EOA 
 how would you make a program that finds abstract word associations ? : machinelearning look into word2vec and doc2vec trained on some large corpus of text , it gives each word a high dimensional vector . these vectors represent embedded statistical and possibly semantic similarities between different words encountered in the corpus , into these high dimensional vectors . so . for example . red, balloon , and blimp vectors will land closer to the birthday vector than it would to the senate hearing vector , but possibly even closer to the hindenburg disaster . it all depends on what sort documents you train your word2vec model on . if you introduce bias in your corpora selection , you'll have cleaner spatial distributions , but possibly less generalized network . if you try to make it generalize too much , then you may not gain enough information on a word with widely ambiguous contexts like red word2vec is not a ready to implement foolproof suggestion , but something you can definitely play around with in case other ideas don't work too well EOQ i've had good results by applying tfidf and selecting a few ( say , NUM or NUM ) words with the highest score from each document . it's easy and takes only a few lines of code , works almost as good as word2vec and is much faster because you don't need to train a model for hours . rare words that appear together often result by counting the selected words that appear together with the target word . it will find for example government and president together with obama . another way to do it is to apply brown clustering on a corpus of text . there are packages that do that out of the box . it will find words that have similar surroundings , for example , locating all the colors in a cluster and all the politics related terms in another . it seems great for exploring the keyword space of a topic . EOA 
 how would you make a program that finds abstract word associations ? : machinelearning look into word2vec and doc2vec trained on some large corpus of text , it gives each word a high dimensional vector . these vectors represent embedded statistical and possibly semantic similarities between different words encountered in the corpus , into these high dimensional vectors . so . for example . red, balloon , and blimp vectors will land closer to the birthday vector than it would to the senate hearing vector , but possibly even closer to the hindenburg disaster . it all depends on what sort documents you train your word2vec model on . if you introduce bias in your corpora selection , you'll have cleaner spatial distributions , but possibly less generalized network . if you try to make it generalize too much , then you may not gain enough information on a word with widely ambiguous contexts like red word2vec is not a ready to implement foolproof suggestion , but something you can definitely play around with in case other ideas don't work too well EOQ what about using the google ngram dataset ? it's available online-you can use this python module-URL-i suspect words like birthday balloon or red balloon would show up in that corpus . EOA 
 how would you make a program that finds abstract word associations ? : machinelearning look into word2vec and doc2vec trained on some large corpus of text , it gives each word a high dimensional vector . these vectors represent embedded statistical and possibly semantic similarities between different words encountered in the corpus , into these high dimensional vectors . so . for example . red, balloon , and blimp vectors will land closer to the birthday vector than it would to the senate hearing vector , but possibly even closer to the hindenburg disaster . it all depends on what sort documents you train your word2vec model on . if you introduce bias in your corpora selection , you'll have cleaner spatial distributions , but possibly less generalized network . if you try to make it generalize too much , then you may not gain enough information on a word with widely ambiguous contexts like red word2vec is not a ready to implement foolproof suggestion , but something you can definitely play around with in case other ideas don't work too well EOQ copying vagrant , take a look at mikolov's word2vec or stanford's glove . these are all word embeddings , and arora has shown how all these embeddings relate to each other , and he demonstrates an easy to implement analogy process that beats the typical cosadd or cosmul similarity metrics . the paper is latent walk in discourse space .... should be the top result on google in arxiv . EOA 
 how would you make a program that finds abstract word associations ? : machinelearning look into word2vec and doc2vec trained on some large corpus of text , it gives each word a high dimensional vector . these vectors represent embedded statistical and possibly semantic similarities between different words encountered in the corpus , into these high dimensional vectors . so . for example . red, balloon , and blimp vectors will land closer to the birthday vector than it would to the senate hearing vector , but possibly even closer to the hindenburg disaster . it all depends on what sort documents you train your word2vec model on . if you introduce bias in your corpora selection , you'll have cleaner spatial distributions , but possibly less generalized network . if you try to make it generalize too much , then you may not gain enough information on a word with widely ambiguous contexts like red word2vec is not a ready to implement foolproof suggestion , but something you can definitely play around with in case other ideas don't work too well EOQ URL EOA 
 does anyone know where to download the mnist database ? : machinelearning URL you could download individual digits from here . surprising why this got downvoted , though. EOQ thanks a lot EOA 
 does anyone know where to download the mnist database ? : machinelearning URL you could download individual digits from here . surprising why this got downvoted , though. EOQ URL assuming these are the same files . haven't tested it ... EOA 
 does anyone know where to download the mnist database ? : machinelearning URL you could download individual digits from here . surprising why this got downvoted , though. EOQ the page is back . EOA 
 sentiment analysis on bag of words ? : machinelearning naive bayes would work well here . try using tf-idf normalized features . it sometimes helps to balance the two classes you're trying to predict . pay attention to the features you're creating because they matter more than the algorithm . if you need other ideas then there are many academic papers out there on sentiment analysis . bag-of-words isn't the most accurate model out there for sentiment analysis but it's simple . EOQ bag of n-grams ! that way you'll capture things like not good correctly . pay attention to how you smooth the model though , because with bag of n-grams you will start seeing examples in the test data that you've never encountered in the test data . EOA 
 sentiment analysis on bag of words ? : machinelearning naive bayes would work well here . try using tf-idf normalized features . it sometimes helps to balance the two classes you're trying to predict . pay attention to the features you're creating because they matter more than the algorithm . if you need other ideas then there are many academic papers out there on sentiment analysis . bag-of-words isn't the most accurate model out there for sentiment analysis but it's simple . EOQ and you could always try recurrent neural networks , just because they seem to work for everything else . but then you'll want a very large number of training cases . EOA 
 sentiment analysis on bag of words ? : machinelearning naive bayes would work well here . try using tf-idf normalized features . it sometimes helps to balance the two classes you're trying to predict . pay attention to the features you're creating because they matter more than the algorithm . if you need other ideas then there are many academic papers out there on sentiment analysis . bag-of-words isn't the most accurate model out there for sentiment analysis but it's simple . EOQ oops , should've clarified-i'm only given the bag of words , not the actual text . also , there are only a few thousand documents in the training set , which might be too small for rnns-i'll try it out though . EOA 
 sentiment analysis on bag of words ? : machinelearning naive bayes would work well here . try using tf-idf normalized features . it sometimes helps to balance the two classes you're trying to predict . pay attention to the features you're creating because they matter more than the algorithm . if you need other ideas then there are many academic papers out there on sentiment analysis . bag-of-words isn't the most accurate model out there for sentiment analysis but it's simple . EOQ rnns make sense when you can exploit known structure in the data ( the sequential nature of text ) . that is lost with the bag of words transformation so an rnn is no longer a good fit . EOA 
 sentiment analysis on bag of words ? : machinelearning naive bayes would work well here . try using tf-idf normalized features . it sometimes helps to balance the two classes you're trying to predict . pay attention to the features you're creating because they matter more than the algorithm . if you need other ideas then there are many academic papers out there on sentiment analysis . bag-of-words isn't the most accurate model out there for sentiment analysis but it's simple . EOQ ah ok . rnn's probably won't work either there , although you could just use straight up neural nets , but i'd still put my money on the random forests . there's also gradient boosted trees , which seem to give better results than random forests . they often appear at the top of kaggle comps . one other thing , you can probably build a tf-idf type feature , by doing counts over of words all the documents , although i'd guess this won't actually help with sentiment analysis , but perhaps it's worth a shot ? EOA 
 sentiment analysis on bag of words ? : machinelearning naive bayes would work well here . try using tf-idf normalized features . it sometimes helps to balance the two classes you're trying to predict . pay attention to the features you're creating because they matter more than the algorithm . if you need other ideas then there are many academic papers out there on sentiment analysis . bag-of-words isn't the most accurate model out there for sentiment analysis but it's simple . EOQ could someone enlighten me why this question was down voted ? seems like a legit question . EOA 
 sentiment analysis on bag of words ? : machinelearning naive bayes would work well here . try using tf-idf normalized features . it sometimes helps to balance the two classes you're trying to predict . pay attention to the features you're creating because they matter more than the algorithm . if you need other ideas then there are many academic papers out there on sentiment analysis . bag-of-words isn't the most accurate model out there for sentiment analysis but it's simple . EOQ bag of words are not really suited for that kind of classification since you want to handle negativity and keep orders of words in a sentence ( example from URL : white blood cells destroying an infection and an infection destroying white blood cells are two different sentences with two different polarity but still , the same bag of words ) . randomforests are not performing so well when you have a lot of features , so bag of words is not really good here because of the bag of words ( most of the time you will use a one-hot vector to describe a text or a sentence ) . what may interesting you is to first check the features you want to keep . like getting the most interesting words and forget about the ones that are not giving information . among those interesting words you can then try to label them as positive or negative , give them weights and then try to use some basic linear regression to find how much it can be positive or negative . if you really want to use bag-of-words , as said by the others you would achieve better results with a bag of n-words , but if you can get that , i suppose you could get the whole sentences and not apply bag of words .. anyway , don't expect to get good results with a bag-of-words model if a human can't do the job neither . EOA 
 neural style experimentation on laptop ? : machinelearning even if you managed to fit the model in your also you can try to use less layers using parameter-content.layers . this setup should produce one image in about NUM-20 minutes . good luck . :) EOA 
 neural style experimentation on laptop ? : machinelearning even if you managed to fit the model in your EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ i've run into similar repetition when doing character level generation from rnns . more training examples is always a good thing , but you could also stochastically sample in generative mode to try and break out of the 'loops' . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ my impression is that what's happening in my case is that the model is not listening to the encoded question vectors for more than a few steps during generation . the greedy generation seems to take over . this is why it actually surprises me that seq-2-seq learning is usually implemented this way . i think inputing the encoded vector repeatedly could work better . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ well if you only use a few training examples , i don't think the model will have much pressure to learn long term dependencies . using an attention model in the seq2seq would eliminate this concern . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ this is interesting-why would a small training set be harder to overfit than a large one ? my assumption was that if it could map NUM sequences exactly , i know it's working and then i could train on bigger data . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ i just don't think you will get high quality output even when it is trying to memorize a few sequences if there are only a couple training examples . i think your current output shows that it is functional enough to expand the dataset . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ the most suspicious thing to me is that the quality of predictions on training points deteriorates after more training . the sequences are actually predicted better on earlier epochs . i would expect predictions on unseen data to deteriorate , but not on the seen data . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ keep in mind that you're trying to make your seq2seq model learn grammar along with relevant content . it takes a child to be exposed to millions of sentences before grammar is learned to a functional level . so you need at least NUM mil samples before you even start to get somewhere EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ i have explained what the problem is in this thread : URL there were NUM guys there with same problem . here is looks like few more-may be this should be added to faq somewhere ? ;) EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ i have already tried implementing beam search , but this is not the problem :) look at these two examples for example : this is a this is a this is a this is a this is a this is a this this is a nice i help you with you with you with you with you with you with you after this is a , the model predictions vary . they go down a different route . this is because the question embeddings are different . in theory-if this seq2seq lstm theory is actually good for anything-it should be able to predict the correct sentence because of the features in the encoded question embedding . rather than language models , compare my problem to a translation model to understand a little better what i mean . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ problem you most likely is facing have nothing to do with a beam search , but with how you choose next character/embedding . are you using argmax for it ? you should not pick most likely char ... EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ yes , a beam search does not pick the most likely word , it picks a selection of candidates and waits until the sequence is finished . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ section NUM of this work is something that you have implemented ? URL EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ yes . i just pick x initial candidates ( words ) , run x predictions on each one . now i have x2 candidates . i choose the top ones , and so on . EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ googling around , repetition problem with beam search seems to be linked to high beam width ... may be try b-2 or something ? EOA 
 discussing training of lstm encoder-decoder networks for seq-to-seq learning : machinelearning i also have faced same repetition problem for char-level lm when i was using argmax for next char prediction . in my situation , i found useful to sample after each whitespace character and to use argmax when obtaining in word predictions . this ensured diversity and spelling plausible results in my case . but for encoder-decoder setting , this behaviour is quite strange . maybe you should try to use attention . EOQ yeah i've been trying different beam sizes :) actually , i just put a normalization layer on my word embedding table and this seems to have helped . i'm going to train overnight on NUM sequences and see in the morning if anything positive happened , i'll let you know . EOA 
 batch normalization when stochastic testing : machinelearning the network isn't meant to be stochastic in evaluation mode . after training , compute the means and stdevs over the entire training set and use those values in your forward prop . EOQ yep , that's what the paper says , although i believe there's a fairly common practice of just keeping an exponential moving average during training and using that as a close enough approximation that doesn't need the additional step . EOA 
 how to extract numbers from sentences and use them as input for a neural net ? : machinelearning i've been running into this problem lately as well , and haven't yet found a good answer . my problems involve more than just numerical deduction , so the samples can't be reduced to simple math questions . a non-end-to-end-trainable approach may be to set it up in NUM stages : ( NUM ) a tagger to identify numbers and other important regions ( NUM ) a model that sees both the tags and their contents , and then does computations on those . not exactly sure yet of the best end-to-end trainable approach for this . from reviewing some of the neural programmer models this year , i suspect that training it end-to-end could require a very specialized curriculum style training procedure . EOQ depending on the complexity of your task , i've had some success with converting numbers into a series of digits , with a symbol for each digit , so NUM would just be NUM NUM . of course if your operations are at all complex , it's probably not the right way to go , but it seems to work fine for things like understanding dates , rough order of magnitude for sizes , etc. however i seriously doubt such a network could learn to multiply , and there are no doubt more useful ways to turn numbers into embeddings . EOA 
 how to extract numbers from sentences and use them as input for a neural net ? : machinelearning i've been running into this problem lately as well , and haven't yet found a good answer . my problems involve more than just numerical deduction , so the samples can't be reduced to simple math questions . a non-end-to-end-trainable approach may be to set it up in NUM stages : ( NUM ) a tagger to identify numbers and other important regions ( NUM ) a model that sees both the tags and their contents , and then does computations on those . not exactly sure yet of the best end-to-end trainable approach for this . from reviewing some of the neural programmer models this year , i suspect that training it end-to-end could require a very specialized curriculum style training procedure . EOQ what are you trying to show ? that a nn can perform arithmetic operations or that a nn can translate a specific sentence into an arithmetic expression ? the former requires some work . check out wolfram alpha . when you type in a sentence it looks for an operator , looks up its parameters and constructs a feature vector which is then evaluated by mathematica . so for what's NUM plus NUM ? your preprocessing module has to find the word plus , translate it to-, look up a table of operator rules to know that there are arithmetic expressions on the left and right side of it , then translate it to the vector (-,10,10). so at this point of course there's no necessity for a nn cause you might as well just eval that . however , if it's your aim to get arithmetic expressions out of the net , you shouldn't really do any preprocessing . i'd try an rnn with labels such as NUM-10 for what's NUM plus NUM etc . but this network will never actually do the evaluation . if you really want that as well , you'll need a completely new network to learn math ( which you shouldn't really do because you can just as well evaluate the expression ) . EOA 
 is it only more computing power why we can now train deeper networks ? : machinelearning [ deleted ] EOQ[?]brandf&#32;2 points3 points4 points&#32;15 days ago-&nbsp;(0 children)EOQ this is exactly why machine learning competitions are so important . it forces folks to develop practical systems on today's computing resources , and encourages teams to re-evaluate old ideas under a new lens . if your concern is winning credit , fame, glory , etc then publishing papers describing how you won a competition is a great way to go about it . EOA 
 is it only more computing power why we can now train deeper networks ? : machinelearning [ deleted ] EOQ[?]brandf&#32;2 points3 points4 points&#32;15 days ago-&nbsp;(0 children)EOQ i did some tests on mnist for a mini lecture i held last year , to show some of the advancements with examples . i had some surprising results : you can get really good results on mnist training > ; NUM layers with vanilla sigmoid units if you use adam . relu vs sigmoid was not a big deal anymore at this point , but relu was still a little better . so i would definitively say that the optimization tools also got much better . another point is initializations , where we can now do better than purely random . but i would encourage you to play around with e.g. the mnist example in keras to get a feel for this . what you can't show on mnist however is the immense size of some of the recent datasets and that deep learning scales pretty well on larger data , some of which would have been already difficult to store in NUM . EOA 
 is it only more computing power why we can now train deeper networks ? : machinelearning [ deleted ] EOQ[?]brandf&#32;2 points3 points4 points&#32;15 days ago-&nbsp;(0 children)EOQ i only know the initialization in URL-are there other papers which describe how to do better than uniform small random intialization ? EOA 
 is it only more computing power why we can now train deeper networks ? : machinelearning [ deleted ] EOQ[?]brandf&#32;2 points3 points4 points&#32;15 days ago-&nbsp;(0 children)EOQ there is glorot initialization , which is standard in many deep learning frameworks . he initalization is also popular together with relu units and cnns . then there is also orthogonal initialization , which is often used with rnns , because applying the same uniformly random matrix over and over again will diverge quickly to the point where backprop becomes difficult . EOA 
 is it only more computing power why we can now train deeper networks ? : machinelearning [ deleted ] EOQ[?]brandf&#32;2 points3 points4 points&#32;15 days ago-&nbsp;(0 children)EOQ if you're using relu , then it's better to use kaiming he's sqrt(6/n) which accounts for the asymmetry of relu vs xavier glorot's sqrt(3/n) . it only makes a difference for deep nets where the multiple multiplications by poorly scaled weights could get you into trouble . URL EOA 
 is it only more computing power why we can now train deeper networks ? : machinelearning [ deleted ] EOQ[?]brandf&#32;2 points3 points4 points&#32;15 days ago-&nbsp;(0 children)EOQ the size of available image ( and other ) datasets has also increased massively . EOA 
 is it only more computing power why we can now train deeper networks ? : machinelearning [ deleted ] EOQ[?]brandf&#32;2 points3 points4 points&#32;15 days ago-&nbsp;(0 children)EOQ this is what i meant with more data ( internet ) EOA 
 how can i find the optimal window size for a running average on a time-series dataset ? : machinelearning optimal in respect with what ? if you have some objective function ( like prediction error ) you can use a validation set or this cross validation technique to test several sizes and select the best . else you should define what you consider low level and high level variations and derive a window length more theoretically . EOQ [ deleted ] EOA 
 how can i find the optimal window size for a running average on a time-series dataset ? : machinelearning optimal in respect with what ? if you have some objective function ( like prediction error ) you can use a validation set or this cross validation technique to test several sizes and select the best . else you should define what you consider low level and high level variations and derive a window length more theoretically . EOQ your not /u/juergenschmidhuber's ama account ... what is this ? EOA 
 how can i find the optimal window size for a running average on a time-series dataset ? : machinelearning optimal in respect with what ? if you have some objective function ( like prediction error ) you can use a validation set or this cross validation technique to test several sizes and select the best . else you should define what you consider low level and high level variations and derive a window length more theoretically . EOQ elaborate trolling EOA 
 how can i find the optimal window size for a running average on a time-series dataset ? : machinelearning optimal in respect with what ? if you have some objective function ( like prediction error ) you can use a validation set or this cross validation technique to test several sizes and select the best . else you should define what you consider low level and high level variations and derive a window length more theoretically . EOQ thank you for the link , i'll read the paper-but just on a cursory glance , it seems like quite a complex approach . could you elaborate on why it would be applicable to selecting an optimal window size ? EOA 
 how can i find the optimal window size for a running average on a time-series dataset ? : machinelearning optimal in respect with what ? if you have some objective function ( like prediction error ) you can use a validation set or this cross validation technique to test several sizes and select the best . else you should define what you consider low level and high level variations and derive a window length more theoretically . EOQ [ deleted ] EOA 
 how can i find the optimal window size for a running average on a time-series dataset ? : machinelearning optimal in respect with what ? if you have some objective function ( like prediction error ) you can use a validation set or this cross validation technique to test several sizes and select the best . else you should define what you consider low level and high level variations and derive a window length more theoretically . EOQ i would start with a look at the time correlation function of your data . the goal may be to find a time scale that averages over much of the high frequency noise but is shorter than the long time length scale of your correlation function . EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ thank you ! i started reading the overview and i really like the use of terminologies in the preface ! EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ [ deleted ] EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ fyi , that was not juergen schmidhuber , but some troll-account . EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ [ deleted ] EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ [ deleted ] EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ not my field , but if you can find a single review article on the topic you like , you can mark all the references in there that you think are relevant and go through those too . i've used this approach to learn the basics of a new field before , and it worked really well ! EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ check out keras it is a deep learning framework with interface to both theano and tensorflow and you can create deep networks within minutes thanks to its design . EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ for lstm you can look at this link . it has a nice step-by-step guide toward understanding the lstm . also this link has an example which train an lstm for sentiment analysis . the code is written in theano . also you can look at lasagne which is a library to build and train neural networks in theano . other useful links : a curated list of resources dedicated to rnn the unreasonable effectiveness of recurrent neural networks EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ are you trying to find your thesis topic ? deep learning is a huge area and many people that work with rbms don't know many details about lstms and vice versa , because you're usually going deep into one specific topic ( and also they're completely different paradigms , i.e. generative vs discriminative ) . for similar reasons it's not really possible to recommend libraries unless you say what specific models you want to work with . if you're still a bit unsure about the differences , you should probably just read about everything you can put your hands on . EOA 
 hi! i'm starting to do deeplearning research and i have NUM months for literature review . need advice. : machinelearning [ deleted ] EOQ[?]newbiethrownaway[s]&#32;1 point2 points3 points&#32;15 days ago&nbsp;(0 children)EOQ there's a really nice topic-wise list of recent papers given here : URL EOA 
 about camera calibration . : machinelearning your post didn't format well . perhaps try editing it to show up better and add more detail , or maybe put the code and tables on pastebin or github and link from your post instead ? EOQ usually , no one responds until the post is easy to read and understand . EOA 
 about camera calibration . : machinelearning your post didn't format well . perhaps try editing it to show up better and add more detail , or maybe put the code and tables on pastebin or github and link from your post instead ? EOQ my program . clc ; %gen cworld and cimage points fov-NUM ; foc-NUM ; u0-NUM ; v0-NUM ; dx-NUM ; dy-NUM ; ptnum-NUM ; d2r-pi / NUM ; gworld.vector-NUM ; gworld-zeros(169 , NUM ); gcamera-zeros(169 , NUM ); gimage-zeros(169 , NUM ); gr.mat-[ NUM NUM NUM ; NUM NUM NUM ; NUM NUM NUM ] ; gt.mat-[0 NUM NUM ]' ; for f.deg-3 : NUM : NUM for p.deg-3 : NUM : NUM gworld(ptnum , NUM )-gworld.vector-cos( f.deg-d2r)- sin( p.deg-d2r ) ; gworld(ptnum , NUM )-gworld.vector-sin( f.deg-d2r) ; gworld(ptnum , NUM )-gworld.vector-cos( f.deg-d2r)- cos( p.deg-d2r ) ; gcamera(ptnum , NUM )-gr.mat(1,1)-gworld(ptnum , NUM )-gr.mat(1,2)-gworld(ptnum , NUM )- gr.mat(1,3)-gworld(ptnum , NUM )-gt.mat(1,1); gcamera(ptnum , NUM )-gr.mat(2,1)-gworld(ptnum , NUM )-gr.mat(2,2)-gworld(ptnum , NUM )- gr.mat(2,3)-gworld(ptnum , NUM )-gt.mat(2,1); gcamera(ptnum , NUM )-gr.mat(3,1)-gworld(ptnum , NUM )-gr.mat(3,2)-gworld(ptnum , NUM )- gr.mat(3,3)-gworld(ptnum , NUM )-gt.mat(3,1); gimage(ptnum , NUM ) -foc-gcamera(ptnum , NUM ) / ( gcamera(ptnum , NUM )-dx )-u0 ; gimage(ptnum , NUM ) -foc-gcamera(ptnum , NUM ) / (gcamera(ptnum , NUM )-dy )-v0 ; ptnum-ptnum-NUM ; end ; end ; calculate this camera parameters.why is t mat not zeros ? i just set them zeros . EOA 
 suggestions for text based services : machinelearning i think we need more information : specifically what algorithms have you tried , what was your input data and what about those results did you find disappointing ? i am not aware of any services that can be bought and i suggest a home-grown solution might be more flexible . is integration with your it system a goal ? personally i would try to find the simplest model that is easier to maintain and explain to management . for example taking the top NUM tf-idf terms would be a good start . you may want to tweak the input by rating book and chapter titles higher ( or simply repeating them in the source text ) . EOQ all good questions . the results are not disappointing per see , the difference between the keyword extraction per the two tools i linked to ( not sure what's happening under the hood , i think one is rake and the other is mallard ? or something like that ... i'm not at my work machine ) and the human effort is stark . i'm putting together a small experiment , one of the indexers is feeding me articles , their keywords and the subject headings . i intend to feed the text into a magical black box , and having undertaken some basic analysis ( percent words the same between the two etc ) invite the indexers to think about if there is value in doing something a little more formal . in the long term the plan would be to integrate the working black box into our preservation repository as a typical enrichment step-that part would be pretty straightforward as we are already poking digital things as they come in . content wise , the types are broad and varied . i could narrow down into scholarly articles or news reports , or any of the various digital forms we encounter . it really is my little project to get wrong at this point ! EOA 
 a good starter computer for ml ? : machinelearning i have read several articles on ml gpus which say titan x is a good , high end , high memory choice. EOQ have you considered renting time on external servers ? EOA 
 a good starter computer for ml ? : machinelearning i have read several articles on ml gpus which say titan x is a good , high end , high memory choice. EOQ [ deleted ] EOA 
 a good starter computer for ml ? : machinelearning i have read several articles on ml gpus which say titan x is a good , high end , high memory choice. EOQ especially have a look at spot instances on aws , they are like NUM-15 ct / hour for gxlarge NUM gpu instances . EOA 
 a good starter computer for ml ? : machinelearning i have read several articles on ml gpus which say titan x is a good , high end , high memory choice. EOQ this is interesting , do you have a link for a news article or similar , from which where we can try to glean some more details on the new gpu ? EOA 
 a good starter computer for ml ? : machinelearning i have read several articles on ml gpus which say titan x is a good , high end , high memory choice. EOQ yes i've been using aws for now , but it only offers compute capability NUM . so i haven't been able to use some key software . ( i think it was the package fbcunn that didn't work ) . i guess the consensus here is that i should not buy the craigslist guy's gaming pc , with titan x , at $2700 . thanks for your advice . EOA 
 a good starter computer for ml ? : machinelearning i have read several articles on ml gpus which say titan x is a good , high end , high memory choice. EOQ titan x > ; NUM ti > ; NUM > ; NUM , but a NUM is still viable . EOA 
 what's a real-world example where mcmc might be the only option ? : machinelearning first , we should remind ourselves what the goal of mcmc-to sample from an intractable distribution . so, you should first remind yourself why this is an important problem . it seems as though you may not understand exactly what is going on , though, since you offered two methods as alternatives but neither claims to do what mcmc does ( sample from a distribution ) . how could i possibly use knn directly to sample from a distribution ? ( note: i'm sure knn has a place in some sampling method , so i'd ask people who know what is going on not to respond by being pedantic. ) EOQ i looked up intractible distribution-URL a few concrete answers i got there were : distributions involving integrals or derivatives that cannot be solved analytically high dimensional functions in some specific form ( i guess? ) EOA 
 what's a real-world example where mcmc might be the only option ? : machinelearning first , we should remind ourselves what the goal of mcmc-to sample from an intractable distribution . so, you should first remind yourself why this is an important problem . it seems as though you may not understand exactly what is going on , though, since you offered two methods as alternatives but neither claims to do what mcmc does ( sample from a distribution ) . how could i possibly use knn directly to sample from a distribution ? ( note: i'm sure knn has a place in some sampling method , so i'd ask people who know what is going on not to respond by being pedantic. ) EOQ mcmc let's you sample the distribution . why would you want to do this ? you can use the sampled points to approximate integrals over the distribution . for example , if the parameter vector is theta , you can use mcmc to approximate the kth moment of theta as : < ;theta ^ k > ;-\int p(theta) thetak ~-\sum.{i-1}n theta.i k , where \theta.i is the ith sample of n total samples . this will let you estimate the mean value , the variance , etc. EOA 
 what's a real-world example where mcmc might be the only option ? : machinelearning first , we should remind ourselves what the goal of mcmc-to sample from an intractable distribution . so, you should first remind yourself why this is an important problem . it seems as though you may not understand exactly what is going on , though, since you offered two methods as alternatives but neither claims to do what mcmc does ( sample from a distribution ) . how could i possibly use knn directly to sample from a distribution ? ( note: i'm sure knn has a place in some sampling method , so i'd ask people who know what is going on not to respond by being pedantic. ) EOQ pretty much any bigger non-trivial bayesian model unless : NUM ) the dependence graph is a tree and explicit marginalization is tractable , NUM ) you have a clever variational approximation , NUM ) map is enough for you , NUM ) you can use loopy-bp and it works . barring one of those options , you're pretty much stuck with mcmc . to give one explicit example : big hierarchical models with lots of heterogeneous variables and different groupings are difficult and most of the time mcmc is your only option . EOA 
 what's a real-world example where mcmc might be the only option ? : machinelearning first , we should remind ourselves what the goal of mcmc-to sample from an intractable distribution . so, you should first remind yourself why this is an important problem . it seems as though you may not understand exactly what is going on , though, since you offered two methods as alternatives but neither claims to do what mcmc does ( sample from a distribution ) . how could i possibly use knn directly to sample from a distribution ? ( note: i'm sure knn has a place in some sampling method , so i'd ask people who know what is going on not to respond by being pedantic. ) EOQ both replies so far haven't answered the question EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ thanks ! so, as i understand it , in the case of image classification a deep network uses a lot of resources to accurately learn the relative probabilities of very unlikely categories , whereas a shallower network trained off the deep one focuses on accurately relating only the likeliest ones for a given image . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ i , too, find this intriguing so i am commenting to remember about this later . also i just want to feel included . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ how can the latter idea beat training on the original labels ? EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ iirc , it doesn't perform as accurately , but the behavior of another network is easier to learn to mimic in the ( much ) smaller network than the actual data is . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ the accuracy of the smaller net never reaches the accuracy of the teacher net . but the accuracy of the smaller net exceeds the accuracy of a net the same size trained on the hard labels . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ it's basically another layer of compression so it won't be able to beat it but will have less parameters . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ saved EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ google does a lot of research in this . URL EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ have you justified the need to learn features rather than engineering them ? EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ i'm a bot , bleep, bloop . someone has linked to this thread from another place on reddit : [ /r/artificial ] [ /r/computervision ] if you follow any of the above links , please respect the rules of reddit and don't vote in the other threads . ( info / contact ) EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ take a look at URL the paper introduces a three stage pipeline : pruning, trained quantization and huffman coding , that work together to reduce the storage requirement of neural networks by NUM x to NUM x without affecting their accuracy . on the imagenet dataset , this method reduced the storage required by alexnet by NUM x , from NUM mb to NUM mb, without loss of accuracy . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ compressing the net's storage space probably isn't the issue , but the computing power required to run it . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ isn't test time just a series of matrix multiplications ? EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ everything is just a series of matrix multiplications . it can take several minutes to run a single forward pass on a low power device like that though . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ so overall the energy efficiency is better due to lower storage , and faster computation through layers . the other thing in you can now fit the entire model on-chip memory rather than sd-ram . so you will see better computational time due to faster access and more efficiency . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ well then you have to undo the the huffman coding and quantization each time you want to run it . the pruning might help , but it makes it impossible to use simd instructions . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ when you say run it do you mean training or testing ? training yes . as this scheme only works for pre-trained networks . testing i dont think so . check section NUM and NUM . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ well yes this is obviously only for testing . all this does is a fancy way of doing lossy compression on nn weight matrices . however decompressing still needs to be done before you can run the thing . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ what is preventing you from bringing the robot online ? EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ people offer good pointers , but i'd argue that these things are completely useless if you're not implementing the network yourself . this is actually what's crucial when you're working on specific embedded systems . and your post doesn't mention any goals . if you work on embedded systems , you need to have some goals defined : the system needs to perform x classifications per seconds to work , something like that . then get the cpu spec and make a gross estimate how long a single classification should take or implement a dummy programm to benchmark it . this should give you an idea of what's possible . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ why not use a tegra k1 ? comes with NUM gpu cores and is ultra low power . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ yeah something like that would work well , it was pretty much designed for this kind of application as far as i can tell ( provided the nn can be converted into opencl ) EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ i think he's restricted by the design of the robot ? EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ what would be beneficial would be a deep learning library tuned specifically for embedded devices ( ie . rasp pi , beagleboard, etc ) that utilizes arm-neon instructions to speed the forward pass . training can be done on a desktop/cluster , while the forward pass is done on the embedded device. anything like this ? EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ as for the rasppi , most of the computational power is in its videocore4 gpu . the gpu internals are documented ( quite unlike other embedded parts ) so general processing on the device is quite feasible in principle , but much of the effort so far is going towards implementing a fully free opengl stack . other things will have to wait a few months at the very least , possibly more . EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ actually , i just read this blog post that claims to be able to perform image classification on the deep belief image sdk using the gpu . he attached code ! EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ wow-i wonder how much of a boost he got from the qpu . did either of you try running it ? EOA 
 deep network on a machine with low cpu power ( nao robot ) ? : machinelearning there's been some interesting work on making low power nns . i can't remember any citations but the main bits have been : they're surprisingly accurate with even just NUM bit floating points , so there's speed up there train a large , deep nn for your task ; then train a shallower , thinner, faster nn to make the same predictions as the large one ( don't train on data , train on the other model's predictions ) . EOQ i'm going to try it when i get to that point in my software . i'll try to remember to report back when i do . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ i started as a physics major in undergrad ( through the first half of year three ) and then switched to mech e , and then finally doubling in math-cs . honestly, i think you'd be better off doing classes in most stats departments . physics is fantastic and all , but classes generally involve a hell of a lot more very specific domain knowledge than general real world modeling techniques . i'm just not sure how knowing the difference between a fermion and a boson would be useful to the typical machine learning practitioner ... and that's what you'll be learning once you get past the all undergrads take this physics courses . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ oh , this is golden . as a physics undergrad applying to computational sciences grad programmes , i am going to use this in my motivational letter . thank you ! EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ he's really arguing for having a domain to implement your system for ; a use to target your tool at if you will . physics would be a good choice of course , but any computationally intensive field will do : genetics, chemistry , and neuroscience all quickly come to mind . pick the field you think you'll enjoy and would like to work close to in the future . this is good advice for computer science ( and statistics and the like ) in general . you are learning a tool , and it will be far more useful-and you far more employable-if you also know a problem domain where you can apply it . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ i may be partial to it because i'm a physicist , but few courses make you sweat more your math and make you absolutely comfortable with making long calculations as a good mathematical methods for physics course . after taking such a course page long equations won't frighten you and you'll find that sometimes your wrist is able to do some calculations autonomously for you , while your head might be someplace else . i miss mr . morse and mr . feshbach. and mr . schwartz. and ms . mary boas . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ take physics . not only might you want to apply ml to physics problems one day , but many of the equations ( like entropy , wave equations ) are applicable to machine learning . information is physical . take an info theory class . if you want to understand quantum computing , it will help to understand quantum mechanics . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ i don't think just it's applicable it's a strong argument . everything, to some point is applicable still , almost every machine learning algorithm and method has some statistics or mathematical theory behind it . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ i don't remember anything from my intro physics classes . taking more math classes like linear algebra and complex variable would be better ( in my opinion ) . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ you might not remember any of it now , but if you ever need to revisit physics , your prior knowledge will get you back up to speed much faster . EOA 
 how important is it to take physics if i am interested in machine learning research ? : machinelearning yann lecun : advice to students : if you are an undergrad , take as many math and physics course as you can , and learn to program question : what is the rationale for taking more physics courses and how are concepts in physics related to deep learning , ai, and the like ? i understand that experience in physics will make you more comfortable with the math involved in deep learning , but i'm not sure why it would be more advantageous than taking , say, more math and statistics courses ( speaking as someone who is majoring in math/statistics ) , though i'm not too familiar with deep learning . lecun: physics is about modeling actual systems and processes . it's grounded in the real world . you have to figure out what's important , know what to ignore , and know how to approximate . these are skills you need to conceptualize , model, and analyze ml models . EOQ take a lot of statistics , linear algebra , and analysis . EOA 
 starting an ml project . looking for a little bit of guidance . : machinelearning sounds like a good k-nearest neighbors task to me . EOQ or an information retrieval approach EOA 
 starting an ml project . looking for a little bit of guidance . : machinelearning sounds like a good k-nearest neighbors task to me . EOQ thanks , i'll take a look . i'm very vaguely familiar with this approach . EOA 
 when to add ? when to multiply ? : machinelearning not an expert ( xd ) , but i can try . a refers to multiplying the density functions from simpler probability distributions and renormalise. each expert covers a part of the problem ( provides constraints on some dimensions ) and you combine them to get your answer ( covering all the dimensions of the data )-the final probability distribution . there's a good explanation in the link i provided ; but the general idea is that in a sum of experts ( mixture model ) , we can have an x with high probability when only a single expert assigns high probability to that event ; in a product of experts , all the constraints must be approximated-hence we get sharper distributions . &nbsp ; the assumption you make in a mixture model ( weighted sum of densities ) is that you have model in which each sample is generated by ( NUM ) choosing one of the individual generative models and ( NUM ) allowing that individual model to generate the data . you can use these models if it's tractable to fit them using , for instance , expectation maximization ( em ) . but they are not that good ( inefficient ) for high-dimensional data ; mixture models produce a more vague distribution since the posterior distribution cannot be sharper than the individual models in the mixture , and each expert would need to be be somewhat adjusted to all the dimensions in order to produce a useful final sharp distribution . see on the subject . &nbsp ; with a poe , you have the problem of the partition function ( needed to re-normalize ) being intractable to compute , but as hinton showed , can get you there . i think you can find some additional motivation in the papers . i hope this helps :) EOQ assuming you are talking about adding/multiplying prob dists , multiplication is like the probabilistic generalization of logical and , whereas addition is more like or ( in both cases assuming some sort of post op renormalization ) . EOA 
 when to add ? when to multiply ? : machinelearning not an expert ( xd ) , but i can try . a refers to multiplying the density functions from simpler probability distributions and renormalise. each expert covers a part of the problem ( provides constraints on some dimensions ) and you combine them to get your answer ( covering all the dimensions of the data )-the final probability distribution . there's a good explanation in the link i provided ; but the general idea is that in a sum of experts ( mixture model ) , we can have an x with high probability when only a single expert assigns high probability to that event ; in a product of experts , all the constraints must be approximated-hence we get sharper distributions . &nbsp ; the assumption you make in a mixture model ( weighted sum of densities ) is that you have model in which each sample is generated by ( NUM ) choosing one of the individual generative models and ( NUM ) allowing that individual model to generate the data . you can use these models if it's tractable to fit them using , for instance , expectation maximization ( em ) . but they are not that good ( inefficient ) for high-dimensional data ; mixture models produce a more vague distribution since the posterior distribution cannot be sharper than the individual models in the mixture , and each expert would need to be be somewhat adjusted to all the dimensions in order to produce a useful final sharp distribution . see on the subject . &nbsp ; with a poe , you have the problem of the partition function ( needed to re-normalize ) being intractable to compute , but as hinton showed , can get you there . i think you can find some additional motivation in the papers . i hope this helps :) EOQ ok assume that out of all of your models , only one of them is correct . that is it is the actual function that produced the data ( or at least your closest approximation of it. ) but you don't know which one it is . in this case the correct thing to do is the average the probabilities produced . if NUM models predict NUM % probability , and NUM model predicts NUM % probability , then there is only a NUM % chance that model is the correct one , and the output should be NUM % . now assume none of the models aren't intended to be guesses of the true function . but instead they are each weak independent evidence . like NUM % of spam emails contained the word 'viagra' , but only NUM % of non-spam emails did . with that kind of evidence , you can easily do bayes rule to get a very accurate prediction from a few pieces of weak evidence . that's sort of like multiplying . in the sense that if one model gives a prediction of , that's very strong evidence , and multiplying by the rest of the probabilities will get NUM . but it also really isn't bayes rule , because if a model produces NUM , that should also be really strong evidence that it should be NUM . but multiplying by NUM doesn't change anything . and this system also doesn't take into account priors at all . so it seems really inelegant . however the actual product of experts algorithm trains the models with that objective function . so the models can learn to deal with it and even take advantage of it . but unless you are training models with that objective function , you should just use a regular average . EOA 
 deploying a cnn on android-which deep learning framework do you recommend ? : machinelearning tensorflow included example loads a predefined network from a protobuf file. this is the procedure you have to do to save both a graph and its values to such a file : URL. otherwise , it's just a matter of adapting the jni code to call your model . EOQ mxnet EOA 
 deploying a cnn on android-which deep learning framework do you recommend ? : machinelearning tensorflow included example loads a predefined network from a protobuf file. this is the procedure you have to do to save both a graph and its values to such a file : URL. otherwise , it's just a matter of adapting the jni code to call your model . EOQ why not upload your image to a server to run the cnn ? EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ i will assume your question is genuine. let's assume the process is not perfectly uniformly random , but close enough . it's a combinatorially large discrete space . just hypothesis testing whether it is perfectly uniformly random or not with high confidence would require a lot of datapoints , i think if you'd look at tests you would find that you would need orders of magnitude more data than NUM years to conclude . and this is just testing if there is a deviation , finding what the deviation is so you could meaningfully exploit it in a strategy probably would require yet more data . and finally , even if there is a deviation , it might not be large enough to be exploitable with a positive expected return either . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ you really don't think that thousands if not tens of thousands of people have tried this for years . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ what if those thousand of people thinked that and never tried ? EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ there are NUM billion people on earth . i'll let you do the math EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ it would be weird if lottery organizers were not able to generate true random numbers . it's not that hard to do , see URL and URL i think you'll have more fun and gain more experience applying ml to well known toy problems that trying to apply it on lottery without being able to find anything but noise. it's hard to actually learn ml on problems on which you can't learn anything . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ i'm obviouly going to tackle some stuff with real pattern at first , but still , i'll give a shot to the lotterie data for the fun of it . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ there was a talk from ml expert who figured out how to exploit horse betting in casino to make some money . very worth watching youtube video . link was posted on this sub , but you will have to find it yourself . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ i'm pretty sure the lotto guys have a real random number generator ! however , some of the scratch-off games have been found to be poorly designed and have been exploited by math/stat guys from time to time. the problem with trying to find any of these flaws via ml is that it would be very hard to come up with a large enough sample of a large enough variety of games to have a chance , even assuming any of the current games are flawed . predicting sports outcomes would be much easier . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ how could machine learning predict sport easier ? i don't see why there would be a pattern EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ some teams , horses, etc are better than others , and variables such as venue , weather, etc come into play . given enough prior data and outcomes you should be certainly able to predict a given match-up better than a coin flip ... maybe to point of being able to profit from it . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ in sports there are repetitive patterns visible when analyzing game's statistics . to predict the outcome of a soccer match ( winner , loser or draw ) for example , you could analyze which players are going to play ( if a key player is injured , probably the team will perform worse ) , how were the past NUM games of each team ( if a team is in a winning streak , probably they will play better ) , who will be the referee , which team is the home team and which is the visitor . maybe take a look at past matches those two teams played each other . there are a lot of statistics that can turn out to be good data for machine learning . once i worked in a personal project to predict the outcomes of soccer matches . with my results , i bet and was able to get some money for beers :) EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ yes . EOA 
 lottery, do you think i will loose my time ? : machinelearning sorry to say , but you won't find any pattern in that data . however learning machine learning is fun ! EOQ as the others have said , i don't think there is a pattern , otherwise a lot of people would already have explored this . however, the learning and experience on machine learning you will get is extremely valuable . i'll suggest you changing from lottery to stock price. stock prices or currency fluctuation ( brazil's currency were NUM us$ -NUM br$ and now they spiked to NUM us$-NUM ,2 br$ , for example ) are somewhat predictable and if you manage to make your algorithms work on them , you could try to make some money . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ we'll do whatever pays our bills . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ those who do something because they think it should be done are more likely to think longterm and do it better . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ believe it or not , the vast majority of the world's knowledge is stored in one or both of those two formats . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ intelligence is about new ways of thinking , not existing knowledge . if i give an ai the binary digits of pi or e , no existing knowledge is needed to complete the pattern . only after it has intelligence should existing knowledge be used . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ oh really ? have you tried that experiment with a human child ? EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ they can learn to crawl and recognize their moms face on their own . thats more than can be said of most ais . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ you didn't answer the question . if given the binary digits of pi or e , can a human child with no exposure to mathematical knowledge figure out how to complete the sequence ? hell, can the average adult ? EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ its a hard problem . maybe i should have started with something like this ... NUM whats the next bit ? NUM whats the next bit ? EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ adversarial networks are two ai models learning from each other . but nothing to do with internet so far . but yeah , the idea looks seducing . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ anything that can be done in one computer can be rebuilt for internet . ais playing chess or whatever against eachother doesnt scale . theres no NUM player chess game , and the turns would make everything really confusing , or if everyone moved at once theres the issue of what if they move the same place who captures who ? it would have to be some mix of cooperative and adversarial where players can leave and join continuously without disrupting the game much . EOA 
 whats the most advanced way ais learn from eachother across the internet ? : machinelearning ai researchers are too concerned with human data formats like words and pictures to try . EOQ probably something in the hft world , but but good luck finding anything specific about that . those guys aren't known for their openness . EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ damn i didn't realize tensor was such a low-level tool . thanks! EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ ... but wait ! scikit flow exists precisely to provide a scikit learn type of interface to tensorflow . :) URL EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ i will look into it . thanks! EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ based on your damn it sounds like you are looking for a higher-level one ? :) what about keras ? it's a convenient wrapper for theano and tensorflow : URL EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ the thing is that , at least in an initial phase , all i want to do is to apply some more classic machine learning algorithms ( like some clustering and linear regressions ) . i'm by no means a machine learning expert , but so far all i've studied are some examples based on scikit-learn pycon talks . do you think steping into keras is worth the extra work ? EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ oh , it really depends what you are up to . if you are into the more classic/basic stuff or whatever you might call it , i think scikit-learn is just perfect for you ! keras would be more interesting if you are specifically interested in deep neural nets . i've a bunch of ipython notebooks on github if you are interested in taking a look at some typical scikit-learn api use cases , e.g., -it's just the code examples from a book chapter , so there are not many comments . however, i can also warmly recommend andreas mueller's talks/tutorials on scikit-learn ( you can find most of them on youtube , e.g., see URL ) EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ use scikit . scikit-learn has lots of prepackaged solutions for common problems in data mining and machine learning . tensorflow is basically a high-level language for doing matrix math and gradient descent very quickly : the things you need to build custom solutions for very advanced or idiosyncratic problems . building a training a deep learning model requires much , much more knowledge and domain expertise than plugging in a scikit-learn algo . you should absolutely learn tensorflow or theano , but it's not the tool you need for this project . URL EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ my only concern with going with scikit is that i might want to move on to deep learning processes later on . EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ have you established that you will need deep learning ? sklearn is powerful and may actually solve the problem at hand . just because deep learning is more powerful , it does not mean that you will be able to leverage that power , or that it is even relevant to your problem . choose the right tool for the job , not the tool for tool's sake . EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ there are libraries to help with the migration process , e.g. URL agreed scikit is a better and more intuitive place to start . EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ the main differences with scikit and tensorflow/theano is their ability to run on a gpu . for some ( read : all real-world application ) project you just can't train your models on a cpu anymore , which is where theano/tensorflow comes in . be adviced that both have a steep learning curve as their api is a lot more low-level as that of scikit . you will have to define your models on the level of matrix algebra , rather than out-of-the-box models with hyperparameter tuning as in scikit . if you are familiar with scikit however , i'd recommend you use theano , as both scikit and theano use numpy vectors as in/output which makes it possible to combine the two . i usually use scikit for some preproccessing , and theano to properly fit a model . lastly , despite me storngly recommending to work with theano bare-bones , you can look up lasagne which is a tool on top of theano that provides a rather nice interface for deep learning models . this way you'd save yourself the matrix algebra . EOA 
 scikit vs tensor/theano : machinelearning scikit-learn and tensorflow/theano are completely different approaches and tools ( in the realm of ml predictive modeling of course ) . scikit-learn is focussed on off-the-shelf machine learning algorithms for supervised and unsupervised learning ( svms , dt/random forests , boosting algos , logistic regression , all kinds of linear and non-linear regression , k-means, naive bayes , k-nearest neighbors , and many , many more ) . theano/tensorflow are libraries that allow you to implement deep learning algos and neural networks ... let's say you want to build a castle out of lego bricks : scikit-learn provides you with partly pre-assembled walls , roofs, and so forth . in contrast , theano/tensorflow is more like a pile of lego bricks that you have to put together yourself . have a look at vs . and you get a better idea of what i mean . EOQ gpu support . EOA 
 none of these class in a cnn : machinelearning URL EOQ i've not personally tried it , but it is something that i've read about in papers and is not like some crazy new idea . iirc there was a ronan collobert segmentation paper that used a 'background' class , and probably many others . EOA 
 none of these class in a cnn : machinelearning URL EOQ URL EOA 
 what are typical semantic segmentation pipelines ? : machinelearning read the top papers from URL seems like the best methods try to predict each pixel and then feed that pixel map with class predictions into another deep learning pipeline to get the final outputs . i could be wrong on this though . EOQ see also : URL EOA 
 what are typical semantic segmentation pipelines ? : machinelearning read the top papers from URL seems like the best methods try to predict each pixel and then feed that pixel map with class predictions into another deep learning pipeline to get the final outputs . i could be wrong on this though . EOQ most of the recent papers i have read use a variant of fcnn piped into a crf . EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ the one from caltech is a really good second course . EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ try geoffrey hinton's neural networks course . it's should be open on coursera and the videos are on youtube . EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ i believe that course is pretty advanced for beginners . EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ i'd recommend against udacity's deep learning course ( at least for now ) ; it's way too abstract , and you would miss the essential parts that are important to understand the core of ml . as a follow up , i would recommend hinton's artificial neural network lectures ( also on coursera ) , or check out this stanford deep learning class:URL anyways , i think pedro domingo's machine learning course is just awesome . he really takes the time to explain things well . plus, you will learn about a lot of standard things that are not covered in ng's class (e.g., decision trees etc.). unfortunately , this is not an active course anymore in terms of exercises , but you can have access to all the videos here : URL i recommend you to come up with your own exercises . e.g., implement the algorithms he is talking about in your favorite programming language . then try them on toy datasets and see if you get the same/similar results compared to established libraries like scikit-learn or so . EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ i like this course too . now, when i read the master algorithm , i can't help to hear him talk with his warm accent :-) EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ haha , exactly the same thought , here! it's a great book btw . not to technical and full of inspiring ideas . EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ that book is so good ! too bad the course is not in for this round yet EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ you can complete ng's course a lot faster than the standard times if you really take it serious and invest in it a couple of hours a day . personal recommendation : do it like if you're taking that class in an actual university/educational establishment . so take notes of everything . if you do that , it's easy to complete . if not and you have not much idea of ml , probably you're gonna fail and drop out . trust me , took me four attempts over years to understand that . take notes , do all the homework . easy peasy . that , and read the faq : URL EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ i don't think there is a faster way than this course to get into machine learning EOA 
 a better course for beginners than andrew ngs coursera ? : machinelearning i think if you finish ng's class and do all the homework assignments , you'll have a decent enough foundation to go start reading machine learning blogs or the like to make progress . once you finish that course , just start building stuff you're interested in . no need to take any more classes . EOQ statistical learning by hastie & tibshirani at stanford EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ ye definitely . i dont think it would be that hard either . you might remember a little while back a guy had twitch play pokemon by reading the commands from chat . if you could collect the entire chat and then do some analysis to measure what are the most common words phrases per second and then line that up with the stream . i think it has potential though because you can get some pretty abstract interpretations from what people are saying which i think could have uses for neural nets ? EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ like this ? URL EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ based on what i know of twitch chat , wouldn't it be more productive to just cat /dev/urandom ? EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ you don't get booby marios that way . EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ you will eventually ! EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ sorry i don't really know what that means but i assume it means to create random data ? this is my point about twitch chat though . there is so much spam constantly if you were to sample it you would probably get 'trends' . check out the most popular twitch stream next time it is on and read the chat . i find it fascinating . EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ URL logging NUM channels for NUM hours seems to amount to ~4 million chat lines ( ~400 mb ) . that should be plenty for topic analysis and detecting word use over time. if you want to couple it with the video of the stream , that would require some more work ( you basically use the chat as subtitles for the video ) . do check the tos though , i bet it places restrictions on this . i think it's an interesting problem . perhaps you can use the chat to predict if a streamer will win a game of heartstone . EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ i can't seem to find it but there was a system that used twitch chat to create automatic highlight reels and post them to youtube . it just used messages per NUM sec or something but i was hoping there would be one made with machine learning . the titles for the clips could've been trained ( the sites i saw mostly had unhelpful twitch emotes as the clip titles ) . a while back i was looking into natural polling , like the steamer says do i max q or e ? it's probably reasonable to classify a segment of chat as question-response and build a simple graph rather than just watching it fly by . for more complex answers clustering would probably work . in some sense it'd just be more natural than people making a strawpoll and then the people on mobile can't view it without leaving the app/etc . when i thought about it more though , it's exclusively a problem for high-volume chat and there aren't that many streamers with > ;10k concurrent viewers . i'm not clear on whether modding is captured in the logging projects but you could train auto moderation for hate speech , bad links , etc. could probably train some automated question answering . think about how many times per day a popular streamer is asked about their keyboard , mouse, mouse dpi , etc. the questions and answers are mostly both in the chat log but the text is highly unnormalized . could bootstrap by classifying by hand for a while then send chat logs to mturk for labeling . or even simpler , train a more natural language interface for xanbot to respond to what people actually say rather than only the exclamation mark commands . EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ this is a really interesting idea . definitely some potential in this type of analysis i think . i dont know if you remember twitch plays pokemon but the guy who created it essentially did this . he would poll the chat and depending on the most popular response use that as the next input for the game . very interesting stuff . EOA 
 could twitch chat be used as a dataset ? : machinelearning yeah , i think so . it would be cool to collect such a dataset and make it public . EOQ it would be cool if you made a twitch bot incorporating a rnn , based on a channel's chat dataset . it could seamlessly blend in with the rest of chatters . EOA 
 the gtl-gate ( generaltrainablelogic ) : machinelearning the data structure you present for binary operations is an order three tensor . the correspondence between tensors and logical calculi you suggest is treated in , and such tensors have been learned based on supervised signal in other work , such as et al . (2014). EOQ and people like you are why i love this subreddit . you take someone who is excited and trying to do something interesting , and you take the time to read their work and help connect them to the existing literature . with regard the original question ( minimum wiring to learn all possible boolean operations ) , i believe this requires an exponential number of gates with n to be truly general , since the number of truth tables is exponential with n and information about which truth table you want to pick has to end up somewhere ... that said , most practical problems probably have structure allowing a much smaller number of nodes , but i'm not sure the best way to wire them . i also strongly suspicious that learning on such an architecture will be tractable , but that just my intuition . EOA 
 the gtl-gate ( generaltrainablelogic ) : machinelearning the data structure you present for binary operations is an order three tensor . the correspondence between tensors and logical calculi you suggest is treated in , and such tensors have been learned based on supervised signal in other work , such as et al . (2014). EOQ happy to have helped . your observation that this does not scale well is salient , as indeed there is an isomorphism between tensors of order k-1 and multilinear maps of k arguments , treated in : n . bourbaki. commutative algebra : chapters NUM-7 , NUM j . lee. riemannian manifolds : an introduction to curvature , NUM this means that a k-argument multilinear map over vectors of dimensionality n requires n^k parameters to exactly model the multilinear relationships between the k sets of input features . this is obviously intractable for any higher k . neural networks ( even shallow ones ) and other methods provide a suitable and tractably learnable approximation of such tensors , so in practice , high-order tensors are mostly useful for reasoning about a learning problem rather than modelling it directly . EOA 
 information extraction for text data-ideas : machinelearning i'd like to see gender classification , based on the text they write , on the image they use and the name they set on facebook . this can be interesting for analyzing competitive facebook pages . EOQ whatever you end up doing , i'd be wary about what you do with your results ; afaik crawling and storing personal data from facebook is legally pretty shaky . i could be wrong though . EOA 
 information extraction for text data-ideas : machinelearning i'd like to see gender classification , based on the text they write , on the image they use and the name they set on facebook . this can be interesting for analyzing competitive facebook pages . EOQ as long as it's for personal use shouldn't i be safe ? also, i'm not sure could it be traced back to me , if i just authenticate with a different facebook account ? EOA 
 information extraction for text data-ideas : machinelearning i'd like to see gender classification , based on the text they write , on the image they use and the name they set on facebook . this can be interesting for analyzing competitive facebook pages . EOQ vectorize words by co-occurence to a small set of other words . for NUM dimensions , the word red may get a close distance to color and blue , but a large distance to god and yolo . then use t-sne to project down to NUM dimensions and visualize it . for classification , how are you labeling your samples ( psychological profile , emotions, sentiment ) ? by hand ? that will hardly scale with a good performance . EOA 
 cheating?-crossvalidation/test splits : machinelearning typically , the test split is not part of training/validation . if you are doing it , then you need to reset the weights . else, the gradient from training on a ( or simply memorizing samples in a ) from a previous split would help in reducing the test error on a when a is part of the test set . randomly shuffle dataset d . split d into d.train/d.test . the test set ( d.test ) is untouched during the training process . split d.train set further into train/validation . choose best hyper-parameter value by measuring test error ( test loss , not training loss ) on the validation set . fix the hyper-parameter value from step NUM and measure test error on the test set ( d.test ) obtained from step NUM . since a random shuffle is performed to obtain train/test , there isn't a need to repeat . if you want to compute standard error/confidence intervals on the error on the test set , repeat the above process but ensure that the weights are reset . EOQ the test set should never be used during development , only for final testing . so split off the e set only and use the remaining four , like this : training / validation : abc / d abd / c acd / b bcd / a EOA 
 cheating?-crossvalidation/test splits : machinelearning typically , the test split is not part of training/validation . if you are doing it , then you need to reset the weights . else, the gradient from training on a ( or simply memorizing samples in a ) from a previous split would help in reducing the test error on a when a is part of the test set . randomly shuffle dataset d . split d into d.train/d.test . the test set ( d.test ) is untouched during the training process . split d.train set further into train/validation . choose best hyper-parameter value by measuring test error ( test loss , not training loss ) on the validation set . fix the hyper-parameter value from step NUM and measure test error on the test set ( d.test ) obtained from step NUM . since a random shuffle is performed to obtain train/test , there isn't a need to repeat . if you want to compute standard error/confidence intervals on the error on the test set , repeat the above process but ensure that the weights are reset . EOQ as long as there is no feedback from the validation/test sets back into your training cycle , you should be fine . EOA 
 cheating?-crossvalidation/test splits : machinelearning typically , the test split is not part of training/validation . if you are doing it , then you need to reset the weights . else, the gradient from training on a ( or simply memorizing samples in a ) from a previous split would help in reducing the test error on a when a is part of the test set . randomly shuffle dataset d . split d into d.train/d.test . the test set ( d.test ) is untouched during the training process . split d.train set further into train/validation . choose best hyper-parameter value by measuring test error ( test loss , not training loss ) on the validation set . fix the hyper-parameter value from step NUM and measure test error on the test set ( d.test ) obtained from step NUM . since a random shuffle is performed to obtain train/test , there isn't a need to repeat . if you want to compute standard error/confidence intervals on the error on the test set , repeat the above process but ensure that the weights are reset . EOQ could you please elaborate on what you mean with feedback ? the purpose would be to use the validation split for hyperparameter optimization . e.g. what if i found a specific set of hyperparameters that , across all of my validation splits , give a good performance ? EOA 
 cheating?-crossvalidation/test splits : machinelearning typically , the test split is not part of training/validation . if you are doing it , then you need to reset the weights . else, the gradient from training on a ( or simply memorizing samples in a ) from a previous split would help in reducing the test error on a when a is part of the test set . randomly shuffle dataset d . split d into d.train/d.test . the test set ( d.test ) is untouched during the training process . split d.train set further into train/validation . choose best hyper-parameter value by measuring test error ( test loss , not training loss ) on the validation set . fix the hyper-parameter value from step NUM and measure test error on the test set ( d.test ) obtained from step NUM . since a random shuffle is performed to obtain train/test , there isn't a need to repeat . if you want to compute standard error/confidence intervals on the error on the test set , repeat the above process but ensure that the weights are reset . EOQ in this setup you must do the hyperparameter optimization for all NUM models completely independently . you can't use information gained from validation in model NUM to tune the parameters in model NUM , because then you would have used the test set of model NUM to improve your model . EOA 
 realistic ml / data science projects for practice : machinelearning i think you will get the most real-life machine learning projects by working with companies directly . they often have lots of problems with their data that is solved by humans but can be solve by machine learning . if you are looking for free available data and don't want to work with a company , open government data may be of interest . EOQ i'd suggest taking a look at lending club . their website provides a decent amount of data which you can freely use ( even if you are not yet an investor ) . try modeling things like charge-offs or delinquencies . there are plenty of people that have posted their own analysis on this so you can take a look at what they have done . lending club even has apis and a python library which you can use to implement your models and select loans for investment . download link : URL EOA 
 realistic ml / data science projects for practice : machinelearning i think you will get the most real-life machine learning projects by working with companies directly . they often have lots of problems with their data that is solved by humans but can be solve by machine learning . if you are looking for free available data and don't want to work with a company , open government data may be of interest . EOQ datakind might be close to what you're looking for . EOA 
 realistic ml / data science projects for practice : machinelearning i think you will get the most real-life machine learning projects by working with companies directly . they often have lots of problems with their data that is solved by humans but can be solve by machine learning . if you are looking for free available data and don't want to work with a company , open government data may be of interest . EOQ why not try to solve a problem that would be useful for you ? like, say , tracking your spending habits ( maybe download your credit/debit card data ) and predicting when you're likely to waste money on something or finding ways to save money , etc. you'll be more motivated to do a good job and will probably have a better domain understanding of your data . EOA 
 realistic ml / data science projects for practice : machinelearning i think you will get the most real-life machine learning projects by working with companies directly . they often have lots of problems with their data that is solved by humans but can be solve by machine learning . if you are looking for free available data and don't want to work with a company , open government data may be of interest . EOQ [ deleted ] EOA 
 realistic ml / data science projects for practice : machinelearning i think you will get the most real-life machine learning projects by working with companies directly . they often have lots of problems with their data that is solved by humans but can be solve by machine learning . if you are looking for free available data and don't want to work with a company , open government data may be of interest . EOQ do you mind sharing what about this article you particularly disagree with ? i understand that cleaning data has very limited value as a learning experience , but apart from that ? EOA 
 k-means: what do you do with stale centroids : machinelearning you could randomly reassign them to lie on a training case , or a training case plus a bit of noise. or the dataset mean plus noise. you might also be able to prevent it by initializing better . EOQ hmm setting it to another training point sounds like a good idea , but i guess you wouldn't wanna do this if its towards the end of your iterations . i can't really reinitialize since its a large dataset and it takes a non-trivial amount of time to recompute . initializing better could work . EOA 
 k-means: what do you do with stale centroids : machinelearning you could randomly reassign them to lie on a training case , or a training case plus a bit of noise. or the dataset mean plus noise. you might also be able to prevent it by initializing better . EOQ so if you run k-means for NUM clusters and NUM of them become stale points you could rerun clustering for the points of some of the resulting clusters . i.e. choose one of the NUM clusters ( maybe one with high sse ) and rerun k-means only for the points with in that cluster for k-NUM or even bigger , its up to you and depends also on your data . repeat until you make NUM clusters . all of it depends on your data . so this is something generic . EOA 
 k-means: what do you do with stale centroids : machinelearning you could randomly reassign them to lie on a training case , or a training case plus a bit of noise. or the dataset mean plus noise. you might also be able to prevent it by initializing better . EOQ as others have mentioned , you might want to do better initialization . take a look at k-means-. the basic idea is that you initialize your centers as follows : pick the first center randomly . when picking the ith center , choose each point with probability proportional to their squared distance to the first i-1 chosen centers ( sometimes called d2 weighting ) . you can prove that when you run k-means-, the cost of the clustering you output is at most o(log(k)) times worse than the cost of the best possible clustering . EOA 
 career pathway into machine learning research for a pure math phd ? : machinelearning your background seems a good fit for research positions at fb , google, ms , etc. EOQ i'm not sure , but maybe these options could work : apply for machine learning postdocs at top schools . i imagine that a lot of labs would really like someone who can write and read real proofs . maybe apply for industrial research labs like fb / deepmind / msr . EOA 
 career pathway into machine learning research for a pure math phd ? : machinelearning your background seems a good fit for research positions at fb , google, ms , etc. EOQ thanks for the ideas , in your opinion what are the upsides of a postdoc vs industry ( salary and job security are the major downsides of course! ) EOA 
 career pathway into machine learning research for a pure math phd ? : machinelearning your background seems a good fit for research positions at fb , google, ms , etc. EOQ check machine learning startups , there is considerable demand for people who can both code and good with math . if you want something considerably more abstract then statistics-convex/nonconvex optimization check for startups which work with topological data analysis or more fundamental aspects of deep learning(invariance , representation strength etc) EOA 
 career pathway into machine learning research for a pure math phd ? : machinelearning your background seems a good fit for research positions at fb , google, ms , etc. EOQ thanks for the tip , know of any such startups that i could reach out to ? EOA 
 career pathway into machine learning research for a pure math phd ? : machinelearning your background seems a good fit for research positions at fb , google, ms , etc. EOQ i'm not aware of startup which use topological data analysis or sheaves sensor networks and like. but startup likely not doing it because they don't have specialists in respective math areas . you can just approach any startup you like and ask them if they want to do it utilizing you expertise. the startup i'm working in is not doing that arcane staff , we are mostly mainstream deep learning and our math is mostly convex/nonconvex optimization and some math derived intuition(like what follow from renormalization and natural images statistics and manifold geometry) we are not in us/eu anyway , so it's only for ppl who ready to relocate outside . EOA 
 looking for machine learning algorithms that could potentially be applied to a simple problem : machinelearning if you want to be able to visual how the data is being interpreted , i would suggest using decision trees . it's important that you , as a human , understand the data first . once you understand the data , a quick and dirty but very effective algorithm for machine learning would be support vector machines ( svm ) . they are not computationally expensive. they are easy and quick to train . svm's are fairly effective at simple tasks like the one you are suggesting . svm's don't take much storage space , so if you had a lot of users , each could have their own svm . i've been using svm's to classify music . and have found them to be just as good as simple neural networks . EOQ i'm not really sure if i'd call kernel svms cheap , but i guess it depends on op's data EOA 
 nba player judging system : machinelearning just like fishfilletswag told , supervised learning would be the way to go . there are a number of player statistics that you can find over the internet , as well as their position . your features could be those statistics ( e.g. number of assists , points, rebounds per game , height, ratio between converted and attempt free throws ) and the label/class ( i.e. what you want to find out ) is the player position . as you are new to ml , try something simple first ( k-nearest neighbor , for example ) . as you understand those simple methods , aim for something more robust and complex ( svm and neural networks ) . EOQ for these types of problems , i think supervised learning solutions usually fare well . however, to get to the top ranks you'll need to creatively figure out how to craft the features , etc. EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ just bought it . thanks! EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ just wanted to say that programming collective intelligence is a really really good book . when you want to start applying what you learned , join a kaggle competition . it's also possible to learn ml the programmer's way , by treating ml algo's as functions , with your data as input and your predictions as output . using scikit-learn , xgboost or vowpal wabbit , together with a tutorial , can have you building a classifier in an evening . compare with using a sorting function , without really understanding what is going on under the hood . that's not expert programming , but at least you are sorting and getting results , without first studying linear algebra or vc-dimensions . can be more fun that way . EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ cool , thank you so much for the tips . i was reading some reviews on amazon , and one of the most critical points was that it is outdated ? what do you think about that ? this is the first time i've actually taken the time to write out a review . i'm sure this book was awesome when it first came out , it is clear , concise and has a nice follow-along structure . however, it has become outdated and it is riddled with either old syntax and errors . i have gotten past most of that though . the worst part is probably that the files that are used in some of the examples are hosted on the authors blog and have been taken down . if he can't be bothered to continue hosting old files for people who may buy the book ( or point us to somewhere to get them ) we shouldn't be bothered to buy it . if the criticisms are that the files are not there , then it's no biggie . EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ many githubs have the files ( and even the book itself if you want to check before you buy ) . i think the outdated part refers to the book not receiving an update and some file downloads not working . sure, no deep learning , but the basics are all there . EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ good to know , thanks! EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ the learning plans on metacademy might be close to what you're looking for . just as an example , here's the plan for learning logistic regression . EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ that's perfect . exactly what i was looking for ! EOA 
 what does the roadmap look like for a complete beginner to building a practical ml solution ? : machinelearning a complete beginner ? the book programming collective intelligence will at least allow you to start getting results right away . URL EOQ cash and lots of it EOA 
 how complicated is topic modeling as far as machine learning concepts go ? : machinelearning topic models require an understanding of hierarchical probabilistic models and , for learning them , of gibbs sampling or variational inference . these topics are more complicated than building a deep feedforward neural network , if that's what you mean by 'deep learning' . i think topic models are best approached by way of probabilistic latent semantic analysis . it is a simpler model from which blei and jordan derived topic models . EOQ topic modeling , as a package , is easy to apply . topic modeling , as a theory , is more aptly called a dirichlet process mixture model . the math isn't super difficult . the intuition of a dirichlet and its various incarnations ( chinese restaurant process , poly urn process , stick breaking process , etc ) are somewhat difficult . here's a great answer by edwin chen on the subject mixture models in general are pretty simple to understand : assume a generative process for your data . a latent factor was sampled ( number of clusters , parameters for those clusters ) from those sampled parameters , the observed data formulate the mathematical equivalent of this generative process so that you can get the probability of your observations given any set of parameters notice various difficulties , such as a chicken-egg situation where it's hard to compute the probability of a datapoint given any single cluster without the cluster parameters and it's hard to infer the cluster parameters without the probability of the datapoint given the cluster . so , do things like em . sklearn explains this well program the math . use one of the various techniques for optimization . the tried and true metropolis hasting markov chain monte carlo ( mh-mcmc ) is the easiest to understand . the others are sampling procedures with fancier sampling . mh-mcmc : for each of your parameters , sample from a NUM-variance gaussian centered on your parameter ( or something like NUM-variance... ) then calculate the probability of the data given this new parameter . use the ratio of this data likelihood to the probability of the data given the old parameter to to decide whether to keep the new parameter . if the ratio is > ;1, the new parameter is definitely better . if < ;1, it might be better , so accept with probability proportional to this ratio . in other words , sample a uniform , if it's smaller than the ratio , accept the new parameter . the output is an estimate of the parameter of your model . depending on how bayesian you are , you either have a point estimate , a distribution over possible estimates , or an estimate with the uncertainty marginalized out . dirichlet process mixture models do this , but the dirichlet itself is a bit complex . if you have a finite dirichlet , it gets pretty easy because you're just calculating probabilities . if you have a ( possibly infinite ) one , you have to adjust the number of clusters iteratively . i personally like heller's bayesian hierarchical clustering because it cleverly computes things bottom-up . EOA 
 does anyone know of good datasets to try out anamoly detection algorithms on ? maybe in credit card fraud or malicious activities , terrorism : machinelearning i also had a hard time finding good data sets when i was doing my phd thesis ( see URL ) . i ended up using data sets that are normally used for binary or multi-label classification tasks . if we assume that one class is normal , and the other is anomalous , and further assume that anomalies are rare observations , then we can simulate anomalies by keeping only a few data points from the anomalous class . for example , with our beloved iris data set , you can construct a new data set with NUM data points of the setosa class , and one ( or a few ) data point(s) from versicolor and virginica . to get a reliable performance estimate , you can repeat this process such that all data points from the anomalous class have been selected , and such that all three class have been represented as the normal class ( see section NUM for more details ) . perhaps a bit convoluted or artificial , but it does allow you to evaluate your algorithm on many different data sets . good luck ! EOQ there is the recently released nab ( numenta anomaly benchmark ) : URL it's quite clear it's a benchmark to show the superiority of numenta's algorithms , but it's open source and the evaluation criteria actually makes sense , so it is worth dedicating some time to understand it . it's a collection of real and artificial univariate time-series , totalling about NUM k samples . the anomaly detection is supposed to be online and unsupervised . i don't know how the anomalies were labeled ( probably by hand ) , and there are only a few per series . you just need to predict the anomaly within a time window ( which is determined automatically-see the white paper for more details ) . the main problem is that nab's evaluation is quite slow , and some labels are quite suspicious ( including the lack of labels at some weird events ) . EOA 
 does anyone know of good datasets to try out anamoly detection algorithms on ? maybe in credit card fraud or malicious activities , terrorism : machinelearning i also had a hard time finding good data sets when i was doing my phd thesis ( see URL ) . i ended up using data sets that are normally used for binary or multi-label classification tasks . if we assume that one class is normal , and the other is anomalous , and further assume that anomalies are rare observations , then we can simulate anomalies by keeping only a few data points from the anomalous class . for example , with our beloved iris data set , you can construct a new data set with NUM data points of the setosa class , and one ( or a few ) data point(s) from versicolor and virginica . to get a reliable performance estimate , you can repeat this process such that all data points from the anomalous class have been selected , and such that all three class have been represented as the normal class ( see section NUM for more details ) . perhaps a bit convoluted or artificial , but it does allow you to evaluate your algorithm on many different data sets . good luck ! EOQ if you are interested in computational-sociology type anomaly detection there are various event datasets available : armed conflict location & event data project URL gdelt URL icews ( delayed by NUM (? ) months) URL open event data alliance URL many of these will include terrorist events . the quality is quite varied , depending on the methodology . EOA 
 does anyone know of good datasets to try out anamoly detection algorithms on ? maybe in credit card fraud or malicious activities , terrorism : machinelearning i also had a hard time finding good data sets when i was doing my phd thesis ( see URL ) . i ended up using data sets that are normally used for binary or multi-label classification tasks . if we assume that one class is normal , and the other is anomalous , and further assume that anomalies are rare observations , then we can simulate anomalies by keeping only a few data points from the anomalous class . for example , with our beloved iris data set , you can construct a new data set with NUM data points of the setosa class , and one ( or a few ) data point(s) from versicolor and virginica . to get a reliable performance estimate , you can repeat this process such that all data points from the anomalous class have been selected , and such that all three class have been represented as the normal class ( see section NUM for more details ) . perhaps a bit convoluted or artificial , but it does allow you to evaluate your algorithm on many different data sets . good luck ! EOQ if you are interested in applying this to terrorism data , take a look at URL . the global terrorism database has recorded most , albeit imperfectly , terrorist attacks from NUM to NUM , and is updated every year . you can download the whole data set and manipulate it as needed . best of luck , and let me know if you have any questions concerning the data set itself . EOA 
 does anyone know of good datasets to try out anamoly detection algorithms on ? maybe in credit card fraud or malicious activities , terrorism : machinelearning i also had a hard time finding good data sets when i was doing my phd thesis ( see URL ) . i ended up using data sets that are normally used for binary or multi-label classification tasks . if we assume that one class is normal , and the other is anomalous , and further assume that anomalies are rare observations , then we can simulate anomalies by keeping only a few data points from the anomalous class . for example , with our beloved iris data set , you can construct a new data set with NUM data points of the setosa class , and one ( or a few ) data point(s) from versicolor and virginica . to get a reliable performance estimate , you can repeat this process such that all data points from the anomalous class have been selected , and such that all three class have been represented as the normal class ( see section NUM for more details ) . perhaps a bit convoluted or artificial , but it does allow you to evaluate your algorithm on many different data sets . good luck ! EOQ start is no longer being updated afaik edit : are you involved in the project ? if so , is it being updated ? EOA 
 does anyone know of good datasets to try out anamoly detection algorithms on ? maybe in credit card fraud or malicious activities , terrorism : machinelearning i also had a hard time finding good data sets when i was doing my phd thesis ( see URL ) . i ended up using data sets that are normally used for binary or multi-label classification tasks . if we assume that one class is normal , and the other is anomalous , and further assume that anomalies are rare observations , then we can simulate anomalies by keeping only a few data points from the anomalous class . for example , with our beloved iris data set , you can construct a new data set with NUM data points of the setosa class , and one ( or a few ) data point(s) from versicolor and virginica . to get a reliable performance estimate , you can repeat this process such that all data points from the anomalous class have been selected , and such that all three class have been represented as the normal class ( see section NUM for more details ) . perhaps a bit convoluted or artificial , but it does allow you to evaluate your algorithm on many different data sets . good luck ! EOQ i am no longer directly affiliated with start , or the gtd ( i was once a fellow ) , but i am an academic affiliate of the program . the gtd is still being updated , but it takes a bit longer than automated systems used by projects such as gdelt . it uses both automated and human coding phases to ensure accuracy . typically the gtd is about a year behind the current year period . expect the NUM data to come out sometime during the summer/fall of this year . unfortunately, this makes up to date prediction difficult with the gtd ( where the strength of systems like gdelt really shines ) . the gtd is fantastic for theory testing and explanatory analysis , as well as panel/tscs modeling and multilevel modeling questions . EOA 
 does anyone know of good datasets to try out anamoly detection algorithms on ? maybe in credit card fraud or malicious activities , terrorism : machinelearning i also had a hard time finding good data sets when i was doing my phd thesis ( see URL ) . i ended up using data sets that are normally used for binary or multi-label classification tasks . if we assume that one class is normal , and the other is anomalous , and further assume that anomalies are rare observations , then we can simulate anomalies by keeping only a few data points from the anomalous class . for example , with our beloved iris data set , you can construct a new data set with NUM data points of the setosa class , and one ( or a few ) data point(s) from versicolor and virginica . to get a reliable performance estimate , you can repeat this process such that all data points from the anomalous class have been selected , and such that all three class have been represented as the normal class ( see section NUM for more details ) . perhaps a bit convoluted or artificial , but it does allow you to evaluate your algorithm on many different data sets . good luck ! EOQ there are good data sets here : URL. take for example census income data set and add artificial records for anomalies . also you may generate the data you need . linear space-( gaussian ) noise and few random points , far from linear space for anomalies could be enough . EOA 
 advice needed ! biostatistics msc grad wanting to pursue a phd in machine learning : machinelearning well figure out what part of machine learning you are interested in , and then find a lab that you would like to work in . communicate that interest with the pi of the lab , and then apply . if you don't get in , work as a research assistant in that lab or otherwise to gain research experience . research experience/ letters of recommendations are the most important factor in phd admissions ! EOQ thanks EOA 
 advice needed ! biostatistics msc grad wanting to pursue a phd in machine learning : machinelearning well figure out what part of machine learning you are interested in , and then find a lab that you would like to work in . communicate that interest with the pi of the lab , and then apply . if you don't get in , work as a research assistant in that lab or otherwise to gain research experience . research experience/ letters of recommendations are the most important factor in phd admissions ! EOQ try to get in touch with the ml people at university of toronto ? there is a really strong machine learning lab with an emphasis on bioinformatics . EOA 
 how do i perform clustering on words based on their meaning ? : machinelearning word2vec ( URL ) or glove ( URL ) is exactly what you want . they convert words to vectors . words with similar meaning have similar vectors . EOQ i don't know about op is trying to achieve , but clustering on word vectors doesn't work very well . it tends to cluster words with similar syntactics together , for instance all verbs might end up in one cluster . performing stemming helps a little bit . maybe just perform clustering on documents and then assign words to clusters based on what document cluster they occur the most often in . EOA 
 how do i perform clustering on words based on their meaning ? : machinelearning word2vec ( URL ) or glove ( URL ) is exactly what you want . they convert words to vectors . words with similar meaning have similar vectors . EOQ word-context vectors , as created by word2vec ( look up the gensim version if you use python ) , are very good as clustering on semantics . actually , they're better at semantics than syntax because word order within the windows is not preserved . EOA 
 how do i perform clustering on words based on their meaning ? : machinelearning word2vec ( URL ) or glove ( URL ) is exactly what you want . they convert words to vectors . words with similar meaning have similar vectors . EOQ that is not the experience i had , when applying kmeans to word2vec ( skip-grams ) vectors . EOA 
 how do i perform clustering on words based on their meaning ? : machinelearning word2vec ( URL ) or glove ( URL ) is exactly what you want . they convert words to vectors . words with similar meaning have similar vectors . EOQ use a non-linear , probabilistic clustering method instead , not kmeans . URL EOA 
 how do i perform clustering on words based on their meaning ? : machinelearning word2vec ( URL ) or glove ( URL ) is exactly what you want . they convert words to vectors . words with similar meaning have similar vectors . EOQ i agree with jenshaase that word2vec and glove are perfect for that kind of task . you can also try to use the latent dirichlet allocation method ( plsa and nmf do the same things ) . in the contrary to word2vec and glove , these methods will find k topics ( you need to give k as a parameter ) based on the documents given . they will try to look for close words based on their co-occurences between documents and co-occurences inside a same document . so it's trying to find a hierarchical structure that explain the way the corpus is distributed ( among topics ) . looking at the words in the topics will give you a global interpretation of those found in the corpus ( like how much a word can represents a topic , or what is the probability that this word appears when talking about the topic , so it's a percentage ) whereas word2vec and glove can give you a local representation ( because those methods look at the co-occurences of words in local contexts , like sentences for exemple ) and so you can calculate the similarity of two words based on their vectors thank's to linear algebra . in fact , if you train your word2vec model ( or get a pre-trained one ) and try to extract the eigenvectors from the set of vectors given by word2vec , it should gives you the basis that represents the topics from the words (of course they won't be related to any known words but you can search for the most similar ;)). edit : i forgot to mention , but all those methods needs a gigantic amount of data to correctly get the semantics of words . EOA 
 how do i perform clustering on words based on their meaning ? : machinelearning word2vec ( URL ) or glove ( URL ) is exactly what you want . they convert words to vectors . words with similar meaning have similar vectors . EOQ other than word2vec and glove ( and lda ) , another option might be brown clustering . the original paper is here , and percy liang's c-implementation can be found here . EOA 
 how do i perform clustering on words based on their meaning ? : machinelearning word2vec ( URL ) or glove ( URL ) is exactly what you want . they convert words to vectors . words with similar meaning have similar vectors . EOQ a viable alternative if you are interested is lda2vec . it ls available on github URL the accompanying presentation can found at URL might be worth adding this algorithm to the comparison if you are comparing word2vec and glove . EOA 
 python library to do symbolic non-linear model fitting ( levenberg marquardt ) : machinelearning theano is what you're looking for . here's a gist . EOQ thanks EOA 
 are there ml applications for sound recognition , particularly music ? : machinelearning based on your edit , i think you're looking for source separation of instruments . i don't know what the state of the art in this area is , but i guess you can find out . this NUM thesis seems to discuss this topic : URL EOQ there are many different methods for doing this-everything from simple feature extraction-> ; map to preset lighting all the way to instrument isolation using source separation and mapping each of those to something cool learned purely from light/video data . the real problem is that what you want is perceptual/creative , which means that any mapping you come up with from sound-> ; lights is going to be custom aka you have some work ahead of you . this is different than speech recognition in the sense that the goal is different , but the same technologies ( knn , dtw, hmm , rnn, and so on ) up until the output can be used . there are also much fewer ways to tell what the right answer is ( phoneme labels/characters/words in speech recognition )-from your description , this seems like a generative task to me . you could even think of having snippet recognition ( based on something like this ) which would allow musical phrases to key in certain effects or light sequences-this would give musicians the ability to control a portion of the lighting by hitting certain phrases . if you will only be working with one band/certain sets of songs it seems pretty feasible to set up the phrases in this way and it should be flexible enough to handle the slight differences in tempo/tuning and so on that occur when playing live . as a bonus , since you are at the board ( or can get access ) you already have isolated tracks from different mics , di feeds , and so on . this makes the problem much easier ! it is a cool idea ( related to some experiments i have tried with autocomping/improv )-a place to get started might be using librosa to extract the beat , then trying to flash a light in time with the beat . if you haven't done this kind of stuff before , getting the code piece to just look at the data might be trickier than you expect . even loading wav files can be a pain sometimes-and midi libraries are a nightmare in my experience . streaming data is also an interesting layer of complexity . there is another layer of complexity in triggering the lights and so on ( hardware is fun , but always more work than straight software ) , so getting a basic pipeline ( such as the flash the lights at a given tempo ) set up will be useful . EOA 
 are there ml applications for sound recognition , particularly music ? : machinelearning based on your edit , i think you're looking for source separation of instruments . i don't know what the state of the art in this area is , but i guess you can find out . this NUM thesis seems to discuss this topic : URL EOQ i also meant to recognize instruments , for example , if i have a song , i want to separate this track into several : one only with guitar , one only with drum , one with harmonica , etc i'll edit my post EOA 
 are there ml applications for sound recognition , particularly music ? : machinelearning based on your edit , i think you're looking for source separation of instruments . i don't know what the state of the art in this area is , but i guess you can find out . this NUM thesis seems to discuss this topic : URL EOQ source separation is pretty hard in general , but to start with you probably want to know a bit about ica since it is kind of the standard for source separation . you will definitely want to exploit multi-source information if you can get it-stereo recordings are a decent example of this . bss from a mono recording is tough-that thesis /u/thomasrobertfr linked seems good , though. if you have location information/geometry that can help a lot as well . nmf is another good and common algorithm for this type of work . also remember the dft is a phase preserving transform , so there are a lot of games you can play with subbands of the original signal if you don't have enough sources to separate all instruments and you know something about the frequency response / normal operating range for said instruments . there is a huge body of work in source separation , so one of the big things to think about is what constraints you can enforce for your particular problem-music has things like primarily harmonic content , nice continuity ( sounds are typically sustained much longer than in speech phonemes , barring drums/cymbals ) and so on . librosa might even have an out of the box example . EOA 
 are there ml applications for sound recognition , particularly music ? : machinelearning based on your edit , i think you're looking for source separation of instruments . i don't know what the state of the art in this area is , but i guess you can find out . this NUM thesis seems to discuss this topic : URL EOQ yes librosa has harmonic-percussive source separation function based on d fitagerald's work . as /u/kkastner mentioned sound source separation is itself is a huge field . if you're looking for something about deep learning there are not much , but few such as (separation with rnn)[URL] or (deep karaoke)[URL] . EOA 
 reducing false positives in face recognition : machinelearning other models besides svm might be easier to tune for precision . what happens when you do naive bayes ? also, linear classifiers dont do well for face recognition . if your features are simple you probably won't get very far . EOQ my features are the output of a convolutional neural network and , currently with svm and a logistic regression classifier , i manage to achieve NUM % ~ NUM % of accuracy . however, i'm having some problems with false positives , which are higher than expected/tolerable . i didn't try with naive bayes ( going to give it a try and i'll report it ) , but i've tested it with random forests and logistic regression . EOA 
 reducing false positives in face recognition : machinelearning other models besides svm might be easier to tune for precision . what happens when you do naive bayes ? also, linear classifiers dont do well for face recognition . if your features are simple you probably won't get very far . EOQ you could just directly train your network to predict if the two faces are the same . that should work quite well . EOA 
 reducing false positives in face recognition : machinelearning other models besides svm might be easier to tune for precision . what happens when you do naive bayes ? also, linear classifiers dont do well for face recognition . if your features are simple you probably won't get very far . EOQ yes , this is a good suggestion . but i was expecting for tips on how to tackle the false positives problem . i'm doing this way ( using a cnn pre-trained for facial recognition as a feature extractor ) in order to establish a prior baseline before running other experiments training the network . if i don't find some way to handle it , i'll probably train my network . thanks! EOA 
 reducing false positives in face recognition : machinelearning other models besides svm might be easier to tune for precision . what happens when you do naive bayes ? also, linear classifiers dont do well for face recognition . if your features are simple you probably won't get very far . EOQ you know what , try different features . you can try local phase quantization . it has been shown to outperform some state-of-the-art descriptors such as local binary patterns . EOA 
 reducing false positives in face recognition : machinelearning other models besides svm might be easier to tune for precision . what happens when you do naive bayes ? also, linear classifiers dont do well for face recognition . if your features are simple you probably won't get very far . EOQ and then , to further reduce the number of false-positives , you can try negative hard mining . EOA 
 reducing false positives in face recognition : machinelearning other models besides svm might be easier to tune for precision . what happens when you do naive bayes ? also, linear classifiers dont do well for face recognition . if your features are simple you probably won't get very far . EOQ do something else . this is not a solvable problem-people can look like other people either by chance or intention . EOA 
 correlation based feature selection , unequal feature length : machinelearning answer form my supervisor : interpolation to equal the size. EOQ you could try sampling the long to get many shorter ones , or transform from time to frequency domain ( fourier ) . EOA 
 correlation based feature selection , unequal feature length : machinelearning answer form my supervisor : interpolation to equal the size. EOQ you could also take some statistics to reduce the size of the long variable eg the mean of an interval , the median , the variance etc . you could make more than one features this way and see the correlation of each . if you are doing supervised learning you could also try reqularization instead of feature selection . EOA 
 correlation based feature selection , unequal feature length : machinelearning answer form my supervisor : interpolation to equal the size. EOQ if its anything like a speech signal , i would recommend looking into fisher vectors : URL EOA 
 correlation based feature selection , unequal feature length : machinelearning answer form my supervisor : interpolation to equal the size. EOQ dear community , thanks a lot for all your answers . i actually do have several features in the time and frequency domain as padelas14 suggests . but some of the features are longterm features measured over several hours . this once i cannot cut down , otherwise they become short term features ( which i also have ) . it seems logic to me now to use the shortest duration features and up sample the long features . the information comes from the same data stream , so the interpolation should be fine . i still wonder if i should do the same for different length of data between patients . there i have quite a difference in data length.therefore, interpolating the shorter once might not be suitable . probably i`ll have to cut down the longer signals to the common shortest length . thanks for all your help . cheers jan EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ buy a cheap laptop ( like chromebooks ) , build a decent pc or rent a server and learn ssh . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ chromebooks are garbage , i can't imagine trying to develop on one , even with a server . also , a server instance could cost you a lot in the long-run . why not buy a half decent chinese/indian laptop and be done with it . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ i can get the above machine (-tax ) for $2600 , show me a comparable config on the pc (-monitor , etc ) that can match this price and i will buy it ! renting server in long term will cost you way more than this , just go through amazon cloud pricing . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ NUM ) this doesn't match the same config . NUM )links? EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ this could be better because then you get a gpu that outperforms gtx980m by far . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ are you stupid ? you can build a host pc for ~500 , drop in a top of the line card ( titan x ) for $900 , or a pretty decent gtx980 for $500 . deep learning on a laptop is moronic . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ how about a bit of politeness ? your statement is fishy and so is your logic . i didn't say you cannot make a crappy pc with NUM . i said you cannot put together the same config as a pc with a lower price. if you want to trash something , make the effort and provide your reference . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ as someone who actually does deep learning , i'm just telling you you're being an idiot . i just built a crappy pc for about that price (-gpu ) which functions just fine ( NUM gb ram , i5 proc , titan x ) . the one and only thing that matters is gpu ( and supporting it ) . a low/midline cpu and motherboard is all you need unless you're going to be cramming multiple cards in there . you also don't need a flashy monitor because you should be working remotely . ssd is important for loading your training data fast , and having a hdd for cold data storage is not a terrible idea , but a NUM :2 allocation is also silly . either just get as much ssd as you need/can afford or get something small and drop a decent size hdd in there also ( NUM tb ) . you can play with specmanship all you want , it will never make deep learning on a laptop a smart decision . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ wordings and your attitude amazes me . i am very interested to know as a person who actually does deep learning , what have you published so far ? EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ quit playing victim and just take what you can learn from him : NUM ) go for a cheap setup NUM ) put an expensive gpu in that cheap setup NUM ) save money extra : don't do deep learning on a laptop . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ shh . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ i'm in the process of building a new pc for deep learning . i've spent ~$1950 including shipping/tax and am only missing hard drives and a monitor ( which i don't need , since this is going to be headless sitting in the basement ) . i did include a keyboard , mouse and dvd/cd drive . for this price i've got : a nice well-cooled full-tower case ( thermaltake core v71 ) NUM w gold psu asus x99-a motherboard core i7-5930k NUM-core ( NUM-hyperthread ) extreme edition cpu supporting NUM pci lanes noctua dual-stack cpu cooler NUM gb ( NUM x8gb ) quad-channel ddr4 NUM ram evga NUM ti superclocked gpu ( NUM tflops! ) dual NUM gb ssds which i'm going to combine into an ultra-fast NUM gb raid-0 array when budget permits i'll probably add another NUM ti ( or next-gen nvidia card , due out shortly ) . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ pretty cool reply . do you have the links to the hardware listed-their prices ? EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ here are the links to all the parts on newegg. if you bought these today from newegg it would cost $2310-NUM % tax . i saved money by shopping around ( details below ) and buying parts when they were on sale over the last NUM-8 weeks . ?a nice well-cooled full-tower case ( thermaltake core v71 ) URL ?1000w gold psu URL ?asus x99-a motherboard URL ?core i7-5930k NUM-core ( NUM-hyperthread ) extreme edition cpu supporting NUM pci lanes URL ?noctua dual-stack cpu cooler URL ?32gb ( NUM x8gb ) quad-channel ddr4 NUM ram URL ?evga NUM ti superclocked gpu ( NUM tflops! ) URL ?dual NUM gb ssds which i'm going to combine into an ultra-fast NUM gb raid-0 array URL dual NUM to NUM bay adaptor for ssds URL dvd URL kb/mouse URL i bought most of the parts from newegg , with following exceptions : case came from newegg , but at black friday price of $120 vs current $150 . newegg's price on this varies from week to week , as do manufacturer rebates . motherboard came from jet.com since you can get NUM % off your first order ( to max $50 ) if you google for coupon codes . this reduced the cost from $250 to $200 . cpu came from micro center who had it on sale at $399 a few weeks ago . it's currently $499 at micro center , $579 at newegg. it pays to be patient and shop around . gpu came from b & h photo-same price ( $669 ) as newegg , but no tax , no shipping . ssds came from amazon black friday sale . i paid $62 each . current amazon price is $83 each vs newegg $123 . again, it pays to be patient and shop around . there are lots of deals on ssds at the moment . hope this helps ! EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ great details , thanks. EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ here's the exact config of my desktop : URL i7-4790k cpu , NUM gb ram , NUM gb ssd , NUM tb hdd , and a titan x for just under $2600 after tax and shipping ; the power supply is also big enough to handle another gpu if you upgrade in the future . much more capable than your laptop config . you could save money by getting a cheaper i5 cpu , going down to NUM gb ram , or getting a smaller psu if you're sure that one gpu will be enough . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ well you won't get better in a laptop anyway . except maybe with clevo . the problem is that dnn training takes a long time and your gpu/cpu will be pegged at NUM % all that time , the laptop will be ridiculously hot . plus it will need to be on sector so i don't see the point . but as far as laptops go : sure, expensive , heavy, hot but powerful example of clevo : URL edit : URL baby jesus laptops with NUM ( not m ) are so fucking expensive. you can get sooooooooooooo much power in a desktop for that price edit2 : alienware graphics amplifier is a thunderbolt NUM box to plug in a gpu in x4 pci externally . you need to supply the gpu yourself . it makes NUM sense for you , might as well just build a desktop with NUM ti ( sli for that price ) if you want to be tethered to your room . it would be NUM-4 times more powerfull important edit do you even use cuda / cudnn anyway ? EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ looks pretty good to me as well . you are probably sick of hearing it ;), but i also recommend against laptops for deep learning . why? compared to regular machines , you have a higher cost/performance ratio tougher heating & cooling challenges plus you have to consider that your computations will likely run days , weeks, or months . thus, you probably wouldn't want to put your laptop to sleep anyway to carry it with you . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ the training time was something that i was not considering . thanks for highlighting it . perhaps i can use an external cooler to keep the laptop temperature low , although alienware claims that under hardcore benchmarking the cpu and gpu temp consistently stays below NUM which seems not bad ( as people say the precision becomes lossy over NUM , due to voltage fluctuation etc ) . regarding the amplifier , isn't it great that you always have this option to plug it to your laptop , buy an external gtx titan x and make your tiny laptop insanely powerful by adding NUM cuda more cores ? EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ if that's your plan i'd wait for thunderbolt NUM hell even a cheap laptops-expensive tb external gpu would be better in that case something like that : URL EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ sounds pretty good to me if you need to do your deep learning on the move ;) maybe it would be a better idea to set up a desktop machine as a server somewhere and just ssh/remote into that with a cheap , light and not hot as hell chromebook ? EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ i have a very similar configuration on my laptop and it works great for me . don't get a windows machine , make sure it's linux . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ nice. to get the full nvidia drivers support , which linux are you using ? it's been a long time since i touched any windows os , but not sure if there are enough driver support for linux . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ i am actually dual booting windows NUM and ubuntu NUM 4. i use windows for fun and ubuntu for work . i found it very easy to install nvidia drivers on ubuntu . EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ i didn't even know you could get a gpu like that in a laptop . isn't the NUM like NUM inches wide ? EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ it's the NUM m , though there are laptops with NUM : URL EOA 
 deeplearning for laptop . is this a good config ? : machinelearning i'm using an external gpu with macbook pro . the heat is spread between the laptop and the external gpu so no heat problems . i already had the laptop so buying an external expansion box and power supply was the cheapest solution . URL EOQ URL URL EOA 
 can dropout reduce training error ? : machinelearning [ deleted ] EOQ[?]yield22[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ > by training error , i mean measured without dropout during forward pass , kinda like using training data as validation set and measure its performance . otherwise ( with dropout in forward pass ) , the training error obviously would decrease as the model is randomly truncated . EOQ i don't know too much about dropout regularization . but i recently had the opportunity to play a little bit around with the parameters of the neural network model that scored the third place in kaggles rossmann challenge . URL this model uses dropout regularization and if you remove it , the performance becomes worse . you need to look at the bottom of the models.py file . EOA 
 can dropout reduce training error ? : machinelearning [ deleted ] EOQ[?]yield22[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ > by training error , i mean measured without dropout during forward pass , kinda like using training data as validation set and measure its performance . otherwise ( with dropout in forward pass ) , the training error obviously would decrease as the model is randomly truncated . EOQ by performance you mean validation/test performance right ? if so that would be understandable , but i am more interested in the training error ( i.e. training set as validation error ) . EOA 
 can dropout reduce training error ? : machinelearning [ deleted ] EOQ[?]yield22[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ > by training error , i mean measured without dropout during forward pass , kinda like using training data as validation set and measure its performance . otherwise ( with dropout in forward pass ) , the training error obviously would decrease as the model is randomly truncated . EOQ yes , i meant the error on the test set ( last NUM % of dataset ) . my entire understanding of dropout regularization originates from the documentation of keras . URL from what it says i conclude , that using dropout prevents the learning process from relying too much on a certain input variable , by randomly assigning it a value of NUM and therefore producing a large error . this error is then corrected by the learning algorithm and therefore the entire model becomes more robust . unfortunately i don't really have an idea about how to set an appropriate dropout probability parameter p . to come back to your original question : i would say that it would be a random occurence , if the training error reduces because of dropout . EOA 
 can dropout reduce training error ? : machinelearning [ deleted ] EOQ[?]yield22[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ > by training error , i mean measured without dropout during forward pass , kinda like using training data as validation set and measure its performance . otherwise ( with dropout in forward pass ) , the training error obviously would decrease as the model is randomly truncated . EOQ there's a lot of things going on with dropout .. it's reducing co-adaptation of neurons ( by way of them not always being there together ) , which will let other more reliable forces prevail ( neurons are not only not learning bad/specific things , they are instead going to be pulled in the direction of learning better/general things ) . dropout is also effectively an ensemble method which is going to be generally beneficial to performance , and as recommended by hinton will tend to be used with an increased capacity model which is also beneficial . it's also effectively adding noise to the true training set gradient , which has recently been shown to be beneficial ... so , while training set performance is going to be hurt by the reduction in overfitting , afaik it's not inconceivable that in some cases the benefits may prevail resulting in reduced overall training set error as well as test set . at least , i've never seem anyone suggest ( let alone prove ) that it can't happen ... this is all assuming that your model is large enough to be overfitting in the first place ! EOA 
 can dropout reduce training error ? : machinelearning [ deleted ] EOQ[?]yield22[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ > by training error , i mean measured without dropout during forward pass , kinda like using training data as validation set and measure its performance . otherwise ( with dropout in forward pass ) , the training error obviously would decrease as the model is randomly truncated . EOQ yes , sometimes-at least for a new approach using monte carlo dropout NUM . the idea being that dropout creates a dynamic random permutation of your network . standard dropout inference roughly approximates averaging over an ensemble of these permutations , but it does it in a crude way-simply by turning off dropout and rescaling the weights . so the improved approach is to use an actual monte carlo average over an ensemble of dropout permutations . this can get somewhat better test time inference than the full ( rescaled ) network . EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ this is really interesting . i'll try to definitely implement this regardless of my bachelor thesis . do you have more links to papers like these ? EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ if i come across anything , i'll let you know , haha. not many papers are so easy to replicate their results ( they may leave out important implementation or training notes ) , and even fewer are as visually rewarding as this one . :) EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ maybe kaggle competitions are a way to start : URL there is a special NUM category for beginners . each competition has a forum and a list of scripts that are created from other people . EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ this is freaking awesome . do you think any of these are good for a thesis ? EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ i don't know . the best way would be to speak with some professors at your university about thesis topics . EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ thanks for the link . EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ stanford's cs229 projects from NUM , lots of ideas to get your gears turning . note also you can change the year in the url to get even more inspiration , down to i believe NUM . these are undergrad student projects i think , so quality probably varies but not a bad place to begin thinking up topics . EOA 
 project ideas in machine learning ? : machinelearning try replicating a few recent paper's results ! you'll have a lot of experience coding it up and then you may have pretty results to show for it ! one of my favorites to try to replicate : URL EOQ thanks . sounds like a great starting place . EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ right , so how would i estimate the number of flops for a feedforward ? EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ look ... this is very simple question with very simple answer . you do have to specify what is that you call computing power requirements . for instance NUM ) the architecture of your neural network ( to estimate the number of mathematical operations it will take to calculate the feedforward value ) . NUM ) the type of data you will use in the network-single or double valued . some processors use different number of cpu instructions depending on what the underlying value type is NUM ) the cpu clock frequency i suggest you use reddit search function on each of these items to get a good idea . in the end , the computing power requirements could be NUM instruction every NUM years-and be sufficient for some purposes . EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ NUM ) deep feedforward NUM layers ( excluding input and output ) NUM neurons per layer NUM ) single NUM ) actually , i plan to use opencl with my gpu NUM gflops/s ( mali t-764 clocked at NUM mhz ) sorry about that , i just needed to know , thanks for helping me :) EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ it sounds like a huge and very important problem despite modest computing power available . most of the scientific discoveries and inventions were made when the creativity fought against the lack of means and won . may i suggest starting with a small subset of the problem you are trying to solve-and try to gain as much experience . this will give you intuition to understand why things work . any intuition requires experience-after all neural nets are modelled on biology of nature . EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ hmm , thanks, so i should be able to estimate the number of flops given the inputs per neuron and number of neurons , right? thanks EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ about NUM -NUM floating points operation per feedforward pass , not counting the input and output layers . so if you run one feedforward pass per second it will amount to NUM -NUM flops . EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ does the amount of connections factor into this ? thanks EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ yes . EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ how would it factor in here exactly ? i tried to calculate the flop count and ended up with a much lower NUM gflop EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ a network has no flops , a cpu/gpu does . for what you're trying to do you have to count all the addition , multiplication operations in your assembler code ( flop count ) and then map them to the costs of your architecture . but since it sounds like you are planning to train ( ! ) it's not really anything that helps you . best thing you can do : start on some platform , measure iteration and epoch times as well as convergence times , then scale if necessary . EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ yes , the flop count is what i was looking for . for my particular application , i don't really care what the training time is . i will lock the weights once it's been trained to my satisfaction . for that purpose , i can assume infinite computing power for training , but very little for passed once the net has been trained . thanks for your help ;) EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ i would calculate the entropy of the inputs versus the entropy of the output . the difference should be proportional to the power requirements . EOA 
 how can i calculate computing power requirements of neural network ? : machinelearning depends on whether your're using matrix multiplication or not , i.e. your implementation . but the number of operations for one feed-forward step is static for a given network ( and yes , of course it depends on the number of connections ) . however, you can only estimate the number of steps needed to train a network . EOQ yes , but i need an exact requirement EOA 
 ml training : coursera specialization , or udacity nanodegree ? : machinelearning google just released free deep learning course at udacity URL EOQ that looks fantastic ! just signed up . it'll be a good gander at udacity's format too , since it's free . EOA 
 ml training : coursera specialization , or udacity nanodegree ? : machinelearning google just released free deep learning course at udacity URL EOQ see you in the discussion forums ! :) EOA 
 ml training : coursera specialization , or udacity nanodegree ? : machinelearning google just released free deep learning course at udacity URL EOQ udacity is offering this get hired or receive a NUM % tuition refund if you are in us . the nanodegree is project oriented to showcase your skills , so maybe is the best option for you . i am taking the coursera specialization btw EOA 
 machine learning data sets and my master's thesis : machinelearning [ deleted ] EOQ[?]eidbanger[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ ng's ml stanford course . EOA 
 machine learning data sets and my master's thesis : machinelearning [ deleted ] EOQ[?]eidbanger[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ i wish to also say : no previous research experience but i was able to follow the derivations in the stanford class with my background in mathematics ( ba math ) EOA 
 machine learning data sets and my master's thesis : machinelearning [ deleted ] EOQ[?]eidbanger[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ i used the the acm debs NUM grand challenge dataset in my master thesis . this dataset contains position information of soccer players during a match . my task was to identify frequent movement of multiple players ( can link you to a paper if you like ) . EOA 
 machine learning data sets and my master's thesis : machinelearning [ deleted ] EOQ[?]eidbanger[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ thanks . please link . pm is cool EOA 
 machine learning data sets and my master's thesis : machinelearning [ deleted ] EOQ[?]eidbanger[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ also . did you actually go through with the submission to debs ? because the timing seems to fall a bit off from my thesis schedule . EOA 
 machine learning data sets and my master's thesis : machinelearning [ deleted ] EOQ[?]eidbanger[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ no it didn't do a submisson to debs . i just use the dataset . link to the dataset : URL link to my paper : URL EOA 
 machine learning data sets and my master's thesis : machinelearning [ deleted ] EOQ[?]eidbanger[s]&#32;0 points1 point2 points&#32;19 days ago&nbsp;(0 children)EOQ nicely done EOA 
 best linear regression solver for memory : machinelearning averaged perceptron models are usually pretty good for memory . but the most important thing is that the implementation supports sparse representations . from memory i'm not sure sklearn's does . you might try liblinear . EOQ thanks . i'm on a bit of a deadline so i'm wondering which of { ‘auto’ , ‘svd’, ‘cholesky’ , ‘lsqr’, ‘sparse.cg’ , ‘sag’ } would be best . more info here : solver to use in the computational routines : ‘auto’ chooses the solver automatically based on the type of data . ‘svd’ uses a singular value decomposition of x to compute the ridge coefficients . more stable for singular matrices than ‘cholesky’ . ‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution . ‘sparse.cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. as an iterative algorithm , this solver is more appropriate than ‘cholesky’ for large-scale data ( possibility to set tol and max.iter ) . ‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. it is the fatest but may not be available in old scipy versions . it also uses an iterative procedure . ‘sag’ uses a stochastic average gradient descent . it also uses an iterative procedure , and is often faster than other solvers when both n.samples and n.features are large . note that ‘sag’ fast convergence is only guaranteed on features with approximately the same scale . you can preprocess the data with a scaler from sklearn.preprocessing. all last four solvers support both dense and sparse data . however, only ‘sag’ supports sparse input when fit.intercept is true . EOA 
 best linear regression solver for memory : machinelearning averaged perceptron models are usually pretty good for memory . but the most important thing is that the implementation supports sparse representations . from memory i'm not sure sklearn's does . you might try liblinear . EOQ you want sag , unless the implementation happens to not be sparse . EOA 
 best linear regression solver for memory : machinelearning averaged perceptron models are usually pretty good for memory . but the most important thing is that the implementation supports sparse representations . from memory i'm not sure sklearn's does . you might try liblinear . EOQ i thought so , but it never even finished . EOA 
 best linear regression solver for memory : machinelearning averaged perceptron models are usually pretty good for memory . but the most important thing is that the implementation supports sparse representations . from memory i'm not sure sklearn's does . you might try liblinear . EOQ i set it to auto and it chose something that was both much faster than the ones i tried and used very little memory . wish i knew which ones it was ? EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ if only keras didn't have the bug that prevented you from being able to save stateful lstms ! hopefully it is fixed soon . i am training all these cool networks but i can't save them for later use . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ i agree , keras does a little bit more lifting vs lasagne , but it comes at the cost of sometimes running into bugs . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ it is new so i forgive them . honestly i am happy they update it so fast . i literally just figured out how to get around that issue . cpickle couldn't save the model because of the python recursion limit , but you can get around that with this function sys.setrecursionlimit(big number here) EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ wauw thanks guys , gaining some real insights here . so if i understood correctly , keras/lasagne is a higher level language based on basic building blocks like theano ? which one would you recommend , keras or lasagne ? EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ theano is a nice library that offers lots of possibilities ( and may be interesting to you as mathematicians ) but doesn't offer any complete models . even the tutorials on deeplearning.net demonstrate how you can build your own nn classes , but nothing is given out-of-the-box . keras is a collection of just such classes . if you were writing your own classes for theano , you would probably end up with something very similar to keras . it's very easy to use and if you don't like something , it's easy enough to understand the well organized code and modify it to suit your needs . i haven't used lassagne myself . oh and btw , keras supports both theano and tensorflow . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ tensorflow's rnn support is still unofficial[0] . if your intention is to work exclusively with rnns then use theano . or even better , use keras ( which supports theano and tensorflow backends ) , then switch to the tensorflow backend once theano's compilation times become unbearable . :) edit : [ NUM ] URL EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ is there a difference in backening through theano or tensorflow ? can you do certain things with keras when you backen it with theano that you wouldn't be able to do with tensorflow and vice versa ? EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ feature-wise i think they're identical as far as keras is concerned since it abstracts away common features . tensorflow is faster than theano on the cpu for a number of operations ( especially convolutional ) . tensorflow will compile faster than theano . theano is generally better optimized on the gpu . it is also more mature , stable and documented than tensorflow . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ what parts would you like to see more documentation on ? we know there's tons more we could document , but we want to make sure we tackle the most useful items first , especially since we're still making rapid progress and improvements and some documentation would quickly go stale . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ sorry , by documented i meant more write-ups in the wild . i have no doubt this will improve over time , as will the official docs?especially now that prs are accepted . but , since you're here ... :) a best practices guide , with examples , for symbolic computation . we have a lot of specific examples and a lot of low-level components but not a lot of guiding principles . i can infer these over time by writing lots of symbolic applications but it will never compare to the tens of thousands of human-hours google has invested . something like effective go[0] but for tf and symbolic computation , in general . theano goes through implementing a lot of the basics ( logistic regression , etc. ) eventually building to more and more complicated examples[1] . the progression is important because you can see patterns in how things should be implemented at various levels of complexity . tensorflow's tutorials seem a little scattered right now ( though i do appreciate the beginner vs . expert mnist tutorial since it allows for skipping a lot of the basics ) . under the hood . a deep dive into what tf is doing at an implementation level on a real example . possibly with performance tips . i can jump back and forth through the source and the mnist tutorial but a guided version of that would be cool . the api docs cover a lot of the what but not a lot of the why . some of the more common classes do this really well , such as tf.tensor ( this class has two primary purposes[... ] [2]) . also , these aren't complaints : i like tensorflow a lot and use it daily over theano . these are just things that i think tf is weak on at the moment and/or wishlist items . [ NUM ] URL [ NUM ] URL [ NUM ] URL EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ sweet ! this is very good constructive feedback-i'll mention this to the team as a way to incrementally improve our documentation , but like you said , a lot of these will get sorted out as we and the community produce more consumable content ( right now we're focused primarily on getting up to par on performance / memory / portability , which is why things are a bit slow on this front ) . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ very good points . would be great if the deeplearning.net tutorials were just reproduced with tf ... EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ keras can be used with tensorflow or theano backend . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ you should definitely go through the deeplearning.net tutorials . they're really great ! tensorflow tutorials were not that great ( at least the rnn parts ) ... they are a bit superficial and don't really go very deep into the code . also rnns are better supported in theano because of the scan function . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ ive been looking in to it , it seems to me that keras is a little more user friendly than lasagne , at the cost of some flexibility . for example , for lstm's in lasagne it is possible to turn on/off peepholes and to manually select the number of gradient steps . is there an easy way to also to this with keras or should is the only way to build your own layer from scratch ? EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ i would recommend theano just because there are more tutorials/code examples at this point . if/when you get familiar with theano , switching back to tensorflow shouldn't be too hard . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ thanks . is theano the best option ? or are there other libraries that im missing that are even easier to pick up and find tutorials for ? EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ both of these will be harder than starting with higher level libraries like keras or lasagne . think of it more like theano/tensorflow/torch are your base programming languages ( more flexibility , more complexity ) , and blocks , keras, lasagne , nngraph ( torch ) are more for building architectures ( rather than pieces of architectures ) . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ if you are interested in the theano/tensorflow style(directed graphs/primitives) , then theano is the best in my opinion . EOA 
 theano vs tensorflow as a beginner : machinelearning going to second keras/lasagne-theano to start out with . at this point , you want to get networks up and running . after that , you can start getting your hands dirty with theano writing your own implementations and plugging them into the existing keras/lasagne framework . EOQ i recommend you keras or blocks-fuel on top of theano , probably my advice is biased because i have had to use theano a lot compared with tensorflow , but you're right documentation at least it's very mature in the theano side . EOA 
 train nn to remember x-y data pairs : machinelearning thousands of ( x,y ) pairs easily fits in memory and takes up far less space than most neural networks . without more information this question sounds like a waste of time. EOQ why can't you store it in a database ? EOA 
 train nn to remember x-y data pairs : machinelearning thousands of ( x,y ) pairs easily fits in memory and takes up far less space than most neural networks . without more information this question sounds like a waste of time. EOQ that is the requirement :) so i was wondering if such thing could be done by neural networks . EOA 
 train nn to remember x-y data pairs : machinelearning thousands of ( x,y ) pairs easily fits in memory and takes up far less space than most neural networks . without more information this question sounds like a waste of time. EOQ yes , but it would probably be very imprecise and impractical . EOA 
 train nn to remember x-y data pairs : machinelearning thousands of ( x,y ) pairs easily fits in memory and takes up far less space than most neural networks . without more information this question sounds like a waste of time. EOQ could you say a bit more about the requirements ? machine learning doesn't seem like the best approach for this problem . EOA 
 train nn to remember x-y data pairs : machinelearning thousands of ( x,y ) pairs easily fits in memory and takes up far less space than most neural networks . without more information this question sounds like a waste of time. EOQ a simple feed forward neural network can do this . just train to minimize error and allow the network to train for a long time. the network will memorize the data almost exactly . in building predictive models , this is a very bad thing but seems to be exactly what you want . EOA 
 train nn to remember x-y data pairs : machinelearning thousands of ( x,y ) pairs easily fits in memory and takes up far less space than most neural networks . without more information this question sounds like a waste of time. EOQ thank you ! that is exactly what i need . :) EOA 
 what's the simplest but most bad ass tool for topic modeling ? : machinelearning python : gensim-for the modelling spacy-processing the text EOQ wow gensim is awesome ... how did this one guy make such an awesome tool as open source ? i guess it must get him more consulting business so thats why he is motivated , so impressive EOA 
 what's the simplest but most bad ass tool for topic modeling ? : machinelearning python : gensim-for the modelling spacy-processing the text EOQ i know it's awesome . and very well documented . ease to use is an understatement ! EOA 
 anomaly detection in machine log data : machinelearning i've been looking ta something similar .. do you maybe have a machine log dataset with labels ? :) EOQ i don't i forget to mention that it is unsupervised EOA 
 anomaly detection in machine log data : machinelearning i've been looking ta something similar .. do you maybe have a machine log dataset with labels ? :) EOQ try using word embedding to extract features . EOA 
 [idea] data preprocessing for semantic word embedding models . : machinelearning this is exactly the dbow ( distributed bag of words ) version of the paragraph vector model , treating the categories as being the documents whose embeddings you are trying to jointly learn . EOQ excellent . thanks for the feedback and thanks for pointing this out , i'll be reading into that shortly ! i've also been thinking about performing random walks on a large and complicated graph and using the path of this walk as an input sequence . do you know if this has been implemented yet ? EOA 
 [idea] data preprocessing for semantic word embedding models . : machinelearning this is exactly the dbow ( distributed bag of words ) version of the paragraph vector model , treating the categories as being the documents whose embeddings you are trying to jointly learn . EOQ there is lots of work on embedding ( knowledge ) graphs . when the graphs have different types of edges ( relations ) , something like trans-e is often used : URL depending on the structure of your graph an objective like this could be more appropriate . these are all quite similar and based on the same approach of factorizing co-occurrences . EOA 
 recognizing-that-there's a face ( not whose face is it ) whats the smallest footprint implementation of that ? : machinelearning try URL they have versions for android and ios . the predefined filters can detect faces and elements like eyes and mouth EOQ thanks ! EOA 
 recognizing-that-there's a face ( not whose face is it ) whats the smallest footprint implementation of that ? : machinelearning try URL they have versions for android and ios . the predefined filters can detect faces and elements like eyes and mouth EOQ look into cascading haar classifiers . EOA 
 recognizing-that-there's a face ( not whose face is it ) whats the smallest footprint implementation of that ? : machinelearning try URL they have versions for android and ios . the predefined filters can detect faces and elements like eyes and mouth EOQ the search term you are looking for is face detection , not recognition . if you want to use something that works ok-but is fast and lightweight then take a look at either opencv as mentioned by /u/carlos.argueta or pico . EOA 
 was sgd proven to be generally better than other optimization algorithms for determing weights in neural networks ? : machinelearning if i'm being pedantic , sgd isn't used at all for training neural networks . lots of different variants / improvements to sgd are . they aren't proven to be better in a mathematical sense , though there is theory to support using them and many of their variants . its more a matter of empirical success and lack of viable alternatives . most of what you have listed ( pso , ga, not familiar with sa as an acronym ) are algorithms that just aren't good for optimizing a large number of parameters . large in this context is several thousand , let alone the billions being used in modern neural networks . that doesn't mean there isn't research using them . hyperneat is a ga approach to neural networks , but the size of the networks it learns are miniscule in comparison with modern networks . but there are niche tasks where it works well and sgd based approaches do not . this particular example has been going on since about NUM and is ongoing . all of this falls into a much broader area of optimization research , and that is where you'll likely find a lot more of the work in using more unconventional things . a ml paper in a ml conference on optimization almost always going to be about showing an improvement empirically / theoretically to an optimizer in a ml centric task . getting gas to handle networks twice as large wouldn't interest most ml people because it would still not be useful to ml people , but it would interest optimization / ga people , and so you'll see that kind of stuff in those kinds of conferences/publications . EOQ thanks . sa-simulated annealing . EOA 
 was sgd proven to be generally better than other optimization algorithms for determing weights in neural networks ? : machinelearning if i'm being pedantic , sgd isn't used at all for training neural networks . lots of different variants / improvements to sgd are . they aren't proven to be better in a mathematical sense , though there is theory to support using them and many of their variants . its more a matter of empirical success and lack of viable alternatives . most of what you have listed ( pso , ga, not familiar with sa as an acronym ) are algorithms that just aren't good for optimizing a large number of parameters . large in this context is several thousand , let alone the billions being used in modern neural networks . that doesn't mean there isn't research using them . hyperneat is a ga approach to neural networks , but the size of the networks it learns are miniscule in comparison with modern networks . but there are niche tasks where it works well and sgd based approaches do not . this particular example has been going on since about NUM and is ongoing . all of this falls into a much broader area of optimization research , and that is where you'll likely find a lot more of the work in using more unconventional things . a ml paper in a ml conference on optimization almost always going to be about showing an improvement empirically / theoretically to an optimizer in a ml centric task . getting gas to handle networks twice as large wouldn't interest most ml people because it would still not be useful to ml people , but it would interest optimization / ga people , and so you'll see that kind of stuff in those kinds of conferences/publications . EOQ yes , another instance of if you have a gradient , not making use of it is crazy . EOA 
 was sgd proven to be generally better than other optimization algorithms for determing weights in neural networks ? : machinelearning if i'm being pedantic , sgd isn't used at all for training neural networks . lots of different variants / improvements to sgd are . they aren't proven to be better in a mathematical sense , though there is theory to support using them and many of their variants . its more a matter of empirical success and lack of viable alternatives . most of what you have listed ( pso , ga, not familiar with sa as an acronym ) are algorithms that just aren't good for optimizing a large number of parameters . large in this context is several thousand , let alone the billions being used in modern neural networks . that doesn't mean there isn't research using them . hyperneat is a ga approach to neural networks , but the size of the networks it learns are miniscule in comparison with modern networks . but there are niche tasks where it works well and sgd based approaches do not . this particular example has been going on since about NUM and is ongoing . all of this falls into a much broader area of optimization research , and that is where you'll likely find a lot more of the work in using more unconventional things . a ml paper in a ml conference on optimization almost always going to be about showing an improvement empirically / theoretically to an optimizer in a ml centric task . getting gas to handle networks twice as large wouldn't interest most ml people because it would still not be useful to ml people , but it would interest optimization / ga people , and so you'll see that kind of stuff in those kinds of conferences/publications . EOQ no doubt , but you can combine global optimization with local ones quite readily . it's not mutually exclusive . EOA 
 was sgd proven to be generally better than other optimization algorithms for determing weights in neural networks ? : machinelearning if i'm being pedantic , sgd isn't used at all for training neural networks . lots of different variants / improvements to sgd are . they aren't proven to be better in a mathematical sense , though there is theory to support using them and many of their variants . its more a matter of empirical success and lack of viable alternatives . most of what you have listed ( pso , ga, not familiar with sa as an acronym ) are algorithms that just aren't good for optimizing a large number of parameters . large in this context is several thousand , let alone the billions being used in modern neural networks . that doesn't mean there isn't research using them . hyperneat is a ga approach to neural networks , but the size of the networks it learns are miniscule in comparison with modern networks . but there are niche tasks where it works well and sgd based approaches do not . this particular example has been going on since about NUM and is ongoing . all of this falls into a much broader area of optimization research , and that is where you'll likely find a lot more of the work in using more unconventional things . a ml paper in a ml conference on optimization almost always going to be about showing an improvement empirically / theoretically to an optimizer in a ml centric task . getting gas to handle networks twice as large wouldn't interest most ml people because it would still not be useful to ml people , but it would interest optimization / ga people , and so you'll see that kind of stuff in those kinds of conferences/publications . EOQ yeah i've also wonder this for a while. why is that optm methods such as adagrad , rmsprop and adam is not replacing sgd in the mainstream ? what's the trade-off ? also it seems that past imagenet winners also use sgd exclusively ? EOA 
 was sgd proven to be generally better than other optimization algorithms for determing weights in neural networks ? : machinelearning if i'm being pedantic , sgd isn't used at all for training neural networks . lots of different variants / improvements to sgd are . they aren't proven to be better in a mathematical sense , though there is theory to support using them and many of their variants . its more a matter of empirical success and lack of viable alternatives . most of what you have listed ( pso , ga, not familiar with sa as an acronym ) are algorithms that just aren't good for optimizing a large number of parameters . large in this context is several thousand , let alone the billions being used in modern neural networks . that doesn't mean there isn't research using them . hyperneat is a ga approach to neural networks , but the size of the networks it learns are miniscule in comparison with modern networks . but there are niche tasks where it works well and sgd based approaches do not . this particular example has been going on since about NUM and is ongoing . all of this falls into a much broader area of optimization research , and that is where you'll likely find a lot more of the work in using more unconventional things . a ml paper in a ml conference on optimization almost always going to be about showing an improvement empirically / theoretically to an optimizer in a ml centric task . getting gas to handle networks twice as large wouldn't interest most ml people because it would still not be useful to ml people , but it would interest optimization / ga people , and so you'll see that kind of stuff in those kinds of conferences/publications . EOQ they are , no one uses just vanilla sgd any more . they are using sgd w/ nestrov momentum , adam, rmsprop , and others . people pick different flavors for different reasons , but no one is really doing just sgd any more . there was a paper the other year that did a big comparison of a lot of the sgd variants . some ( such as sgd w/ nestrov momentum ) require a parameter search , but tend to get the best accuracy when they do work . others, ( such as adam and adadelta ) tended to work well for a wider number of parameter values . others , such as adagrad , are objectively not well suited to non-convex optimization , as its updates are monotonically decreasing , and won't be able to optimize well after a certain point . EOA 
 was sgd proven to be generally better than other optimization algorithms for determing weights in neural networks ? : machinelearning if i'm being pedantic , sgd isn't used at all for training neural networks . lots of different variants / improvements to sgd are . they aren't proven to be better in a mathematical sense , though there is theory to support using them and many of their variants . its more a matter of empirical success and lack of viable alternatives . most of what you have listed ( pso , ga, not familiar with sa as an acronym ) are algorithms that just aren't good for optimizing a large number of parameters . large in this context is several thousand , let alone the billions being used in modern neural networks . that doesn't mean there isn't research using them . hyperneat is a ga approach to neural networks , but the size of the networks it learns are miniscule in comparison with modern networks . but there are niche tasks where it works well and sgd based approaches do not . this particular example has been going on since about NUM and is ongoing . all of this falls into a much broader area of optimization research , and that is where you'll likely find a lot more of the work in using more unconventional things . a ml paper in a ml conference on optimization almost always going to be about showing an improvement empirically / theoretically to an optimizer in a ml centric task . getting gas to handle networks twice as large wouldn't interest most ml people because it would still not be useful to ml people , but it would interest optimization / ga people , and so you'll see that kind of stuff in those kinds of conferences/publications . EOQ thanks ! do you have link or keywords for the paper ? really curious about the comparison . EOA 
 was sgd proven to be generally better than other optimization algorithms for determing weights in neural networks ? : machinelearning if i'm being pedantic , sgd isn't used at all for training neural networks . lots of different variants / improvements to sgd are . they aren't proven to be better in a mathematical sense , though there is theory to support using them and many of their variants . its more a matter of empirical success and lack of viable alternatives . most of what you have listed ( pso , ga, not familiar with sa as an acronym ) are algorithms that just aren't good for optimizing a large number of parameters . large in this context is several thousand , let alone the billions being used in modern neural networks . that doesn't mean there isn't research using them . hyperneat is a ga approach to neural networks , but the size of the networks it learns are miniscule in comparison with modern networks . but there are niche tasks where it works well and sgd based approaches do not . this particular example has been going on since about NUM and is ongoing . all of this falls into a much broader area of optimization research , and that is where you'll likely find a lot more of the work in using more unconventional things . a ml paper in a ml conference on optimization almost always going to be about showing an improvement empirically / theoretically to an optimizer in a ml centric task . getting gas to handle networks twice as large wouldn't interest most ml people because it would still not be useful to ml people , but it would interest optimization / ga people , and so you'll see that kind of stuff in those kinds of conferences/publications . EOQ found it URL EOA 
 tensorflow seq2seq model getting low perplexity but unsatisfying results : machinelearning look at the validation/test perplexity . EOQ the perplexity on the dev set is : global step NUM learning rate NUM 000 step-time NUM 8 perplexity NUM 0 eval : bucket NUM perplexity NUM 5 # these are supposedly the different perplexities for buckets in the dev set eval : bucket NUM perplexity NUM 1 eval : bucket NUM perplexity NUM 0 eval : bucket NUM perplexity NUM 0 this is just after NUM iterations . but when i test the model , even with sentences from the training set the result is not correct . i think that given my inexperience with python and tensorflow there must be something i am doing wrong , but i'm just using the code from tensorflow seq2seq demo for translation , with my data instead of the english-french data they use . EOA 
 tensorflow seq2seq model getting low perplexity but unsatisfying results : machinelearning look at the validation/test perplexity . EOQ this is most likely to be a bug . NUM is the lowest possible perplexity . if you're solving any real task , a perplexity of NUM is far from possible . the last time i had perplexity NUM , it was because i was feeding the output as part of the input , and the network was just learning the identity . maybe that's what's happening ? or maybe some other bug . EOA 
 tensorflow seq2seq model getting low perplexity but unsatisfying results : machinelearning look at the validation/test perplexity . EOQ well , this is indeed a toy task that i'm working on , so i wasn't too surprised to see a perplexity of NUM given its simplicity : in fact , each input sentence can get mapped to just one of two output sentences ( e.g., if output.vocab-{a,b,c,d,e } each sentence in the training set gets mapped to either a b d or c d e) . ( now you may ask why i don't just use a classification algorithm , but this is just a toy task i'm testing for a bigger problem where the number of sequences would clearly be higher ) . as for the input i think it is correct , but i'm checking right now to see if i messed up something there . also , since i'm using the demo code from tensorflow without modifications , is it possible that being optmized for translation with big vocabularies causes some problems in this simple task ? would seem strange , but i don't know EOA 
 tensorflow seq2seq model getting low perplexity but unsatisfying results : machinelearning look at the validation/test perplexity . EOQ how many timesteps is x and y respectively ? the prolbem may be that if your timesteps are too long , the lstm can't perform well EOA 
 tensorflow seq2seq model getting low perplexity but unsatisfying results : machinelearning look at the validation/test perplexity . EOQ i just spent the whole morning trying to figure things out and indeed it was a bug , like sherjilozair said . something was messing up the conversion of tokens to ids and viceversa and solving that solved the whole problem . EOA 
 tensorflow seq2seq model getting low perplexity but unsatisfying results : machinelearning look at the validation/test perplexity . EOQ oh wow okay...makes much more sense EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ thanks ! i'll check it out ! EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ you should check out the recent open-face announcement : URL they're reporting some very good metrics on face recognition . EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ interesting ! EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ yes and it has been since a very long time. here's one from NUM . EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ good , i'd come across the word eigenfaces before , but i always assumed it was a non-connectionist thing . i'll check it out , thanks! EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ yes and no . it can be done and works pretty well , but performance on face verification is not good enough to use in the real world for e.g. access control , and things like csi-style facial recognition for surveillance will probably never be possible ( there are just too many possible near matches ) . EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ i'm curious then , how do you suppose facebook's auto-tagging feature works ? is it simply because the pool of candidates is limited to your direct friends , and thus makes the choice easier ? EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ yeah that would be my guess . facebook also has a nice giant self-annotating training set to work with . EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ you mean like this ? not that i'd use it for anything important , but just saying ... EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ yes . it works , but like you say no one would use it for anything important , because false positive rates are in the few percent when the threshold is low enough that the owner doesn't get locked out on a regular basis . EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ and keys ( with all their flaws ) are a much better solution , anyways... EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ /u/mysteriousartifact what are the best ( fastest , smallest etc ) face detection algorithms/implementations do you think ? EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ none , before now ( other than perhaps a super-naive approach of training a network on a dataset entirely made up of me and not-me faces ) . i'm very new to computer vision . now i realize that eigenfaces are a pretty quick method , and openface seems to be startlingly accurate . EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ thanks . yes. to be clear i meant face detection not face recognition ( so , the flip side to your original question ) EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ just as naive , i'm afraid . train a deep network on faces vs . not-faces, and hope that it figures out some higher-level features of what makes a face a face . EOA 
 is face recognition ( not detection ) currently possible with neural networks ? : machinelearning yes , it is possible URL EOQ openface !!!!!!! EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ ok thanks for the pointers ! yeah, i should have specified that the problem is indeed possibly unoptimal initializations ... EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ how do you go about exploring different initializations ? do you have a set that you try in a hyper-paramter search or is it derived from the problem somehow ? EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ basically by math and guesstimating . i have several ( NUM to NUM ) gru layers , which makes initialization tricky ... recurrent matrices are initialized with a spectral radius parameter and recurrent input matrices are scaled by sqrt(inputs) times a scaling parameter . then there are further complications ... not even going to start with those . in any case , the model is very sensitive to the initializations ... EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ as an aside , if you have that many layers , i would definitely suggest using batch norm as they did in deep speech NUM . that will certainly help exploding gradients . make sure you do the batch normalization sequence wise . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ probably not , since they're a bitch to train :/ i have some anecdotal evidence that > ;5 layers will indeed be useful , but i still need to work on it ... EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ in my experience , initializations are not something you explore like another hyperparameter . either it works or it doesn't ( although sometimes it will work and just take a lot longer ) . basically you just want to initialize your network so that nothing blows up or vanishes . this is typically easy for non recurrent architectures , but for recurrent ones it can be hard . it's also fairly straightforward to study theoretically as you can consider the norm/average/variance of the hidden state as a function of the number of layers . anyway, my initialization tips are : if using relu's , use the orthogonal init like keras or lasagne with scale factor NUM if using tanh , try the orthogonal init like keras or lasagne with scale factor of NUM . also try the keras he.normal , that seems to work ok as well . use batch norm wherever you can , it'll basically fix any bad initializations automatically EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ exactly ! actually my rec weight matrices are already exactly orthogonal ( so basically i might not need gru , but so far i'm going with those ) ... but i suppose the keras orthogonal init scale factor is the spectral radius ? -sigh-i was hoping to avoid batch norm because of the hassle in generating mode ... anyway, maybe i need to go there then EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ i've used batch norm in generating mode ( back when nn's weren't stateful in keras ) . i ended up just having to include a bunch of random examples anytime i wanted to generate from the network , which wasn't the end of the world . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ dhammack when you used batched norm on your rnn's did you do sequence wise ? i was thinking of doing batch norm sequence wise. also , many people say that for generation , it is better to keep a running average of your variance and mean for each respective matrix as you get a much more average result that helps improve generation from batch norm . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ i did a combination of temporal bn and regular bn . i ended up implementing my own recurrent layer for this . it was something like norm.value-( prenorm.value-new.mean ) / new.std , where new.mean and new.std depended both the current batch and the prior rolling estimate of new.mean . something like new.mean-NUM -prior.mean-NUM -batch.mean . but i didn't experiment a lot with this so it's likely that you can do better . anyway yeah it's better to fix the mean/stdev estimates and use that at generation time but keras has some problems with this ( last time i checked ) , so my method was a workaround . i used it for the kaggle dr competition too when i wanted to generate predictions . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ huh okay makes sense . i admit that storing a running average is harder from a programming standpoint ( using tensorflow ) . i'll try things out with this for the next couple of weeks ! thanks! EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ yeah i've seen it done for relus and tanh . in both cases i think the authors analyzed the expected change in variance before and after a layer . URL is one example EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ that's only for feedforward architectures ( unless i totally missed something ) . rnns are completely different since it's not just a map of x to y , but the whole problem is dynamic . that being said , after thinking about this a little , it might be possible to do something similar by making some reasonable assumptions like steady state of the hidden states . it's still a bit tricky though , but since we're looking for a ballpark figure , some less reasonable simplifying assumptions might do the trick ... EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ i tried the same idea before , as i thought if unsupervised pre-training can provide good initialization of weights , why don't i use supervised pre-training . furthermore , the dataset was extremely skew ( imbalanced ) (e.g. it has NUM % for class NUM but only NUM % for class NUM ) and i have no idea about the distribution of test data . this is my approach : downsampling the dataset , create a balanced dataset ( NUM-50 ) which contains about NUM % of overall data . pretrain the model with this dataset then , training with full dataset in general , my conclusion is the same as dhammack , supervised pre-training doesn't help . however, the interesting thing was happened with different order . after trained the model with full imbalanced dataset , it feed the smaller-downsampled-balanced dataset again to the model , the performance improved ( from ~75% to NUM % accuracy ) . the model was totally biased toward the class with more samples , if you train it by different dataset with different distribution of classes , it reduce overfitting . in my case , i repeat the process several times , train with imbalanced full dataset-> ; no improvement-> ; switch to balanced downsampled dataset-> ; no improvement-> ; switch back ... until no more improvement . i managed to squeeze some more juice out of it and got NUM % accuracy on dev set , the funny thing is that as the dev(validation) accuracy keep improving , the training error went up . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ huh , interesting... gotta give it a shot ! EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ switching back and forth between down sampled and full dataset is very interesting . what optimization algorithm did you use ? adagrad? rmsprop ? adam? EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ trungnt13 , a few questions : normally , from a curriculum learning perspective it is best to start with less complex tasks to learn , and then slowly raise the level of difficulty as in this paper : URL. however , some have suggested it is better to use a leaky curriculum where some hard samples are introduced at the beginning : URL second , i'm surprised that that you decrease the learning rate so drastically . i've found that keeping a constant learning rate and then annealing it at the very end of your training yields the best results . do you ever raise your learning rate again ? or do you just keep decreasing it ? EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ hi leavesbreathe , for the first point , let start with the original idea from efficient backprop , then you can see both sortagrad and the technique i used have the same principle . choose example with maximum information content : shuffle the training set so that successive training examples never ( rarely ) belong to the same class . present input examples that procedure a large error more frequently than example that procedure small error . from this NUM points , it can be summarised that if you constantly feed more and more difficult examples to the network , it learn better and better , because more difficult data contains more information content that your network haven't known yet . for sortagrad , for example , you have NUM examples , you are doing batch training , how can you constantly feeding more and more difficult examples to the network for the whole training process ? of course you start from easiest one , and then less easy , and less less easy .... after the first epoch , your network may get familiar with that order of training examples(my intuition) , then, it is not difficult to learn from the same order anymore , hence, they shuffle the dataset and continue training . the same idea for me when i switch between NUM dataset with different distribution , whatever how do you start , always make sure you always try to make the network learn the more difficult ( more information content ) examples . p/s : sorry i haven't got time to read your second paper , so i can't comment anything about it . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ don't worry about reading the second paper : the bottom line from it is that you can introduce some batches of hard examples at the beginning . by doing this , you allow the weights to prepare for much harder tasks to come in the future . i think we're both saying the same thing . when you switch to dataset num NUM , however, i think that it would be wise to include dataset num NUM as well . the reasoning with this is that it allows the network to see a bigger 'picture' of what's going . it can see both very easy and very difficult examples . it just doesn't have exposure to difficult examples . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ for the second point , i have never tried to increase the learning rate again , it based on my imagination that the more you reach the optimal point , the less you should move . however , new optimizers like rmsprop , adadelta, adagrad are less sensitive to the value of learning rate visualization of optimizers and some papers also used the same factor to decrease learning rate (for example : URL). i don't think we have general recipe as choosing the learning rate is strongly depend on the size of network and data . p/s : i do not own the video , i cannot remember where did i find it . EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ oh , i am sorry for my confusing explanation about the strategy of downsampling . the NUM dataset is completely overlap , the only different is the distribution of classes in the dataset . for example , we have NUM samples : full dataset : NUM samples for class NUM , NUM samples for class NUM downsampled balanced dataset : NUM samples for each class this is the images after and before i applied training with multiple data distribution:before, after all the classes have different amount of samples in training set EOA 
 pre-training rnns by extreme overfitting to small fraction of complete dataset ? : machinelearning i've done this before with both rnn's and cnn's . i have found that when you initialize your model correctly it no longer becomes necessary however . so yeah if your model is really hard to train then i'd first try to initialize it better and if that doesn't work then a curriculum learning approach tends to work . however be careful that you don't overfit too much as you don't want your model to become too confident on new samples such that it incurs massive loss when you start training on new data ( which might cause blowup ) . EOQ oh okay i gotcha , thanks for the clarification ! those images really helped understand your approach . in that case , i wish i could offer more advise , but i think you're doing it the best way possible . EOA 
 what is the best way to select a threshold that best separates two gaussians with unequal variances ? ( histogram link in text ) : machinelearning yes . maximum likelihood estimation . basically, your decision break point is the point where the pdfs of the two distributions are equal . i'm sure that there's a closed form for that point , but i can't remember it . EOQ thanks , with your , /u/palm.frond, and /u/padelas14 's help i'm sure i'll figure it out ! EOA 
 what is the best way to select a threshold that best separates two gaussians with unequal variances ? ( histogram link in text ) : machinelearning yes . maximum likelihood estimation . basically, your decision break point is the point where the pdfs of the two distributions are equal . i'm sure that there's a closed form for that point , but i can't remember it . EOQ if i'm understanding you correctly i think you want quadratic discriminant analysis , which i think is what /u/reversedgif is recommending . EOA 
 what is the best way to select a threshold that best separates two gaussians with unequal variances ? ( histogram link in text ) : machinelearning yes . maximum likelihood estimation . basically, your decision break point is the point where the pdfs of the two distributions are equal . i'm sure that there's a closed form for that point , but i can't remember it . EOQ you can fit the ( half ) gaussians with em and when you have the parameters you can do what /u/reversedgif says . but you assume they are gaussians and by watching it i think that would give you a threshold > ;50. why don't you just take the minimum ? EOA 
 what is the best way to select a threshold that best separates two gaussians with unequal variances ? ( histogram link in text ) : machinelearning yes . maximum likelihood estimation . basically, your decision break point is the point where the pdfs of the two distributions are equal . i'm sure that there's a closed form for that point , but i can't remember it . EOQ thanks for your help . would you explain what minimum you mean to take ? EOA 
 what is the best way to select a threshold that best separates two gaussians with unequal variances ? ( histogram link in text ) : machinelearning yes . maximum likelihood estimation . basically, your decision break point is the point where the pdfs of the two distributions are equal . i'm sure that there's a closed form for that point , but i can't remember it . EOQ i meant the bin with the minimum y in the plot above . i take it the x axis are values and the y axis are number of occurrences . you could define a window size ( ie NUM-10 ) , run the window from minimum to maximum value and select the mean of the window that has the minimum occurrences for example . EOA 
 what is the best way to select a threshold that best separates two gaussians with unequal variances ? ( histogram link in text ) : machinelearning yes . maximum likelihood estimation . basically, your decision break point is the point where the pdfs of the two distributions are equal . i'm sure that there's a closed form for that point , but i can't remember it . EOQ just us otsu's method . EOA 
 learning to cluster points using cnn : machinelearning you can use a siamese net for this with a triplet loss where points that belong to the same cluster are similar and points from different clusters are different . EOQ ok .. i'll look into that . so i guess there should be a fixed number of classes right ? as in , the number of clusters should be fixed ? EOA 
 learning to cluster points using cnn : machinelearning you can use a siamese net for this with a triplet loss where points that belong to the same cluster are similar and points from different clusters are different . EOQ well , you said along with the groundtruth data with says which point belongs to which cluster , which does require knowing the number of clusters . actually , if you have this much information it's a classification problem . EOA 
 learning to cluster points using cnn : machinelearning you can use a siamese net for this with a triplet loss where points that belong to the same cluster are similar and points from different clusters are different . EOQ kohonnen maps is you choice , or if you have distance tsne technique can be used EOA 
 how to run octave on nvidia gpu ? : machinelearning if you're running octave on linux then you can either rebuild or simply change your library path so that octave uses nvidia's nvblas rather than a cpu based blas . i'm not sure if it's possible for windows . URL note that nvblas ( a drop-in blas replacement ) and cublas are two separate things . if you're just looking to accelerate octave , you want nvblas . cublas has a blas-like api but is not a drop-in replacement-it's meant for lower level programming . EOQ [ deleted ] EOA 
 how to run octave on nvidia gpu ? : machinelearning if you're running octave on linux then you can either rebuild or simply change your library path so that octave uses nvidia's nvblas rather than a cpu based blas . i'm not sure if it's possible for windows . URL note that nvblas ( a drop-in blas replacement ) and cublas are two separate things . if you're just looking to accelerate octave , you want nvblas . cublas has a blas-like api but is not a drop-in replacement-it's meant for lower level programming . EOQ that's not quite true . even without application-level threading you can take advantage of multithreading in octave by using a multithreaded cpu-based blas implementation such as openblas ... although that's not going to be as fast as using a gpu-based blas such as nvblas . EOA 
 8th layer in caffe's alexnet clone ? : machinelearning it's the weights of the softmax . EOQ thanks for replying . can you elaborate ? why does caffe need it , but alexnet doesn't ? is it not actually to be thought of as a layer ? EOA 
 8th layer in caffe's alexnet clone ? : machinelearning it's the weights of the softmax . EOQ alexnet has it too . caffe's alexnet also has an fc8 that does the same thing . what gets called a layer is more an artifact of the framework you're using and less of a hard and fast rule . EOA 
 8th layer in caffe's alexnet clone ? : machinelearning it's the weights of the softmax . EOQ i see . thank you , that helps ! EOA 
 classify text into multiple categories . : machinelearning you can check out a ner tagger . or you can train an rnn to perform ner tagging . ( you need a lot of data though ) EOQ you can turn it into a multi-label problem [ place , country, region ] . you can use a simple dictionary with all the countries and regions in the world and do ( fuzzy ) string matching . you can use logistic regression on NUM-grams and build a training set . then ask its coefficients with clf.coef.. this problem is very simple at first ( basically just string matching ) , but for better performance the country bordering north of the land of the rising sun you need very complex models ( and training data ) . EOA 
 classify text into multiple categories . : machinelearning you can check out a ner tagger . or you can train an rnn to perform ner tagging . ( you need a lot of data though ) EOQ isn't region redundant when you have country ? or are you trying to sometimes sub localise countries , like east coast vs west coast usa ? in any case , you could just build two classifieds , one for country and one for region . EOA 
 has anyone built a usable sc-lstm on word-level prediction in lua ? : machinelearning what is an sc lstm ? EOQ i was wondering this too . op : if you want helpful answers , it's good to give a bit more context to your question . EOA 
 remember the experimental google chatbot NUM months ago ? why haven't anybody yet create a usable chatbot out of that ? : machinelearning because chatbots aren't terribly useful . this, on the other hand , is: URL . slightly different objective ( single-response vs dialog ) , and it has some additional moving parts ( e.g. ensuring semantic variety in the suggested responses ) , but it's built on the same kind of sequence to sequence lstms . EOQ yep . once you start messing around with a text bot only you realize that they're basically useless . text is going to slowly fade away as we get more intelligent . because what's text if not just a tool for notation ? EOA 
 remember the experimental google chatbot NUM months ago ? why haven't anybody yet create a usable chatbot out of that ? : machinelearning because chatbots aren't terribly useful . this, on the other hand , is: URL . slightly different objective ( single-response vs dialog ) , and it has some additional moving parts ( e.g. ensuring semantic variety in the suggested responses ) , but it's built on the same kind of sequence to sequence lstms . EOQ text is pretty useful . it's silent , and most can read faster than they could listen . i don't think it's going away any time soon . maybe in NUM years when we can connect our brains directly . EOA 
 remember the experimental google chatbot NUM months ago ? why haven't anybody yet create a usable chatbot out of that ? : machinelearning because chatbots aren't terribly useful . this, on the other hand , is: URL . slightly different objective ( single-response vs dialog ) , and it has some additional moving parts ( e.g. ensuring semantic variety in the suggested responses ) , but it's built on the same kind of sequence to sequence lstms . EOQ where have i put a time span ? EOA 
 remember the experimental google chatbot NUM months ago ? why haven't anybody yet create a usable chatbot out of that ? : machinelearning because chatbots aren't terribly useful . this, on the other hand , is: URL . slightly different objective ( single-response vs dialog ) , and it has some additional moving parts ( e.g. ensuring semantic variety in the suggested responses ) , but it's built on the same kind of sequence to sequence lstms . EOQ text is going to slowly fade away as we get more intelligent . that's what you said . does that not imply time ? EOA 
 remember the experimental google chatbot NUM months ago ? why haven't anybody yet create a usable chatbot out of that ? : machinelearning because chatbots aren't terribly useful . this, on the other hand , is: URL . slightly different objective ( single-response vs dialog ) , and it has some additional moving parts ( e.g. ensuring semantic variety in the suggested responses ) , but it's built on the same kind of sequence to sequence lstms . EOQ yes . EOA 
 remember the experimental google chatbot NUM months ago ? why haven't anybody yet create a usable chatbot out of that ? : machinelearning because chatbots aren't terribly useful . this, on the other hand , is: URL . slightly different objective ( single-response vs dialog ) , and it has some additional moving parts ( e.g. ensuring semantic variety in the suggested responses ) , but it's built on the same kind of sequence to sequence lstms . EOQ and we're the ones building a world that will kill text in the long term . EOA 
 selecting instagram images for training ? : machinelearning what do you want to select for ? i would argue that if you had this selection strategy , it would probably be useless to have the nn you want to build .. EOQ this question is related to a similar problem i was facing , can you please elaborate more on why this is not a good idea ? thanks EOA 
 selecting instagram images for training ? : machinelearning what do you want to select for ? i would argue that if you had this selection strategy , it would probably be useless to have the nn you want to build .. EOQ yeah i'd like to see that elaboration as well EOA 
 selecting instagram images for training ? : machinelearning what do you want to select for ? i would argue that if you had this selection strategy , it would probably be useless to have the nn you want to build .. EOQ if you have an algorithm that can select for the pictures you find interesting , what do you need the nn for ? EOA 
 lstms with arbitrary sequence outputs : machinelearning sequence to sequence learning with neural networks , sutskever, vinyals , le, NUM EOQ my understanding is that seq2seq solves the problem of not having aligned input/output sentences , like english and french . this shouldn't be a problem for what i mentioned . EOA 
 lstms with arbitrary sequence outputs : machinelearning sequence to sequence learning with neural networks , sutskever, vinyals , le, NUM EOQ ok ... i'm not sure i understood . i think it depends on whether your outputs are conditionned on the whole inputs or not ? if i understand correctly your example ( word vectors to bow repr basically? ) then you don't even condition on anything except the current word vector , so lstm is overkill ( a feedforward nn would do ) . you could have something intermediary like : p(y.t| x.1,..,x.t) where seq2seq is not necessary and lstm would be good . then i guess seq2seq is good for p(y.t|x.1,..,x.n) with t in {1..n}. is it better ? EOA 
 lstms with arbitrary sequence outputs : machinelearning sequence to sequence learning with neural networks , sutskever, vinyals , le, NUM EOQ here's an example . assume i have a sequence of fixed length vectors that represent a recording of a person speaking each word in a sentence : my input . my output is a text transcription of each of those words . i'm assuming i don't just need a feedforward model because the previous word will presumably help me translate the audio of the next word . EOA 
 lstms with arbitrary sequence outputs : machinelearning sequence to sequence learning with neural networks , sutskever, vinyals , le, NUM EOQ i have never done speech to text , so please correct me if there is something wrong here . for homophones , you'd need to disambiguate between several spellings . as you say there are cases when the previous word can help you , but also the next one(s) . for example in english : you have observed i have and the current token is the audio for two/too . how do you disambiguate ? you need to see what follows . there is a NUM th possibility that i haven't mentionned : bidirectional lstm . maybe for your problem it is overkill and unnecessary to condition on the whole sentence , but you'd simply need previous and next word . in that case go for bidirectional lstm . try to see speech to text litterature , if there are longer term dependencies than previous and next . EOA 
 lstms with arbitrary sequence outputs : machinelearning sequence to sequence learning with neural networks , sutskever, vinyals , le, NUM EOQ an ecoder-decoder model would work for the above example where it generates a character at a time give a word vector to map from a word embedding back to a word string . see the v2c part from this paper URL if you knew the vocabulary of the word embedding before hand , you could do this with a regular lstm . EOA 
 lstms with arbitrary sequence outputs : machinelearning sequence to sequence learning with neural networks , sutskever, vinyals , le, NUM EOQ /u/theinfelicitousdandy thanks . i don't yet quite see how that paper would apply . what do you mean by knowing the vocabulary ? EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ i see . it's not one-hot . for this particular model it's important that i have as much original raw input information as possible since the data isn't as high quality as one would hope ( and that's as good as it gets ) . i have NUM gb of memory atm . perhaps this can be alleviated by modifying the batch size in keras ? EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ i see . i was assuming that more info was better and the dimensionality reduction would happen automatically , like cnns . i often get the advice to do end-to-end as much as possible . would the suggestion above from /u/anvamiba help with this ? EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ if relevant , yes-it's a good suggestions . more info is better , buts your not just adding info-your also adding noise and complexity with every feature . you have to balance that . you can train feature selection/compression end-to-end as well . EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ ok , nicely put EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ why not ? and if you don't have enough data you can do dimensionality reduction . EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ yes , i suppose image inputs are rather large , but i haven't personally seen anything similar done with rnns/lstms etc . any examples ? EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ if the inputs are images you may want to apply convolution and pooling layers to process and compress the data before feeding it to the rnn ( training the whole thing end-to-end ) . if you send a NUM ,000-dimensional vector directly to a fully connected rnn or lstm , unless your state dimension is very small or you have a lot of data , you risk to overfit since the input-to-hidden matrix(es) will have many parameters . EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ wonderful idea , thanks EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ you can try to use tensor train decomposition representation instead of full matrix in a-x-b in lstm implementation EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ do you have more information about this ? EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ see URL article EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ not sure what that is EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ see URL article EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ thanks ! i assume this isn't popular in major frameworks and needs to be implemented by hand . EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ there is matlab and python implementation URL EOA 
 lstm with high dimensional inputs : machinelearning you may run into memory issues just due to size , but other than that there's nothing stopping you . if it's a one-hot vector , then consider doing embeddings instead . EOQ cool , thanks :) EOA 
 bayesian network vs bayesian inference vs naives bayes vs bayesian regression ? : machinelearning their differences are many . a better question is : what is bayesian about each ? that answer is that they all deal with conditional probabilities , and the latter three employ the 'ultimate' theorem about conditional probabilities , bayes rule : p(y|x)p(x)-p(x|y)p(y) . bayesian network : a bayesian network is just a graphical description of conditional probabilities . a-> ;b means that the probability of b is conditioned on a's value , or in math , p(b|a). naive bayes and bayesian regression can be written as a bayesian network . bayesian inference : bayesian inference is when we use bayes rule to obtain the conditional probability of some parameter given the data , p(y|x) above . this is just general application of the statement above , but x is taken to represent the observed data . naive bayes : similarly to bayesian inference , `naive bayes' just means we are assuming x and y above represent specific things in the application of bayes rule-namely , x represents the feature data and y represents the classification labels . naturally , we aim to find p(y|x) . the 'naive' part comes from the assumption of independence between features . bayesian regression : bayesian regression means we wish to perform bayesian inference on a regression model . thus , we are interested in finding the posterior on the weights p(w|y,x). again , notice this is obtained from direction application of bayes rule but with an extra condition on x : p(w|y,x)-p(y|x,w)p(w)/p(y|x) EOQ a bayesian network breaks up a probability distribution based on the conditional independencies , while bayesian inference is used to determine ( i.e., infer ) a marginal distribution given some observed evidence . naive bayes is a bayesian network with a specific graph structure . not sure about bayesian regression . i know crfs are essentially structured regression , but you may be referring to something different . EOA 
 why is a simple recurrent neural network with an identity recurrent matrix difficult to train ? : machinelearning if the recurrent matrix is identity , it's equivalent to a derivative/delta accumulating version of the feedforward model . the final output at time t can thus depend in some complex way on the initial state at time NUM . it makes the system incredibly chaotic . think of an initial feedforward model f(x) . the rnn with identity recurrent matrix then computes g(x,t)-f(x,t)-g(x,t-1). the initial feedforward behavior is preserved when we change f(x,t)-> ; d(f)/dt ( delta encoding ) , but then errors/noise accumulate rapidly , as there is nothing to suppress them . EOQ ok i understand what you're saying now but i'm missing the connection between the simple rnn structure and what you're saying about the delta encodings . i think you talked about this in your earlier comment . if you have the rnn structure : s(t)-s(t-1)-b-x(t) how can you reduce the second term to the delta encodings ? specifically how can this equation be looked at as : pass state-delta . and it couldn't just be any delta , it would need to be a delta of the first term for your error accumulation over time argument to make sense . just like how acceleration is the rate of change of velocity . EOA 
 why is a simple recurrent neural network with an identity recurrent matrix difficult to train ? : machinelearning if the recurrent matrix is identity , it's equivalent to a derivative/delta accumulating version of the feedforward model . the final output at time t can thus depend in some complex way on the initial state at time NUM . it makes the system incredibly chaotic . think of an initial feedforward model f(x) . the rnn with identity recurrent matrix then computes g(x,t)-f(x,t)-g(x,t-1). the initial feedforward behavior is preserved when we change f(x,t)-> ; d(f)/dt ( delta encoding ) , but then errors/noise accumulate rapidly , as there is nothing to suppress them . EOQ that is why lstms have gating mechanisms . EOA 
 why is a simple recurrent neural network with an identity recurrent matrix difficult to train ? : machinelearning if the recurrent matrix is identity , it's equivalent to a derivative/delta accumulating version of the feedforward model . the final output at time t can thus depend in some complex way on the initial state at time NUM . it makes the system incredibly chaotic . think of an initial feedforward model f(x) . the rnn with identity recurrent matrix then computes g(x,t)-f(x,t)-g(x,t-1). the initial feedforward behavior is preserved when we change f(x,t)-> ; d(f)/dt ( delta encoding ) , but then errors/noise accumulate rapidly , as there is nothing to suppress them . EOQ i understand that , but is it intuitive that a system where every timestep has an equal effect on future timesteps is chaotic and hard to train ? EOA 
 why is a simple recurrent neural network with an identity recurrent matrix difficult to train ? : machinelearning if the recurrent matrix is identity , it's equivalent to a derivative/delta accumulating version of the feedforward model . the final output at time t can thus depend in some complex way on the initial state at time NUM . it makes the system incredibly chaotic . think of an initial feedforward model f(x) . the rnn with identity recurrent matrix then computes g(x,t)-f(x,t)-g(x,t-1). the initial feedforward behavior is preserved when we change f(x,t)-> ; d(f)/dt ( delta encoding ) , but then errors/noise accumulate rapidly , as there is nothing to suppress them . EOQ i don't like to think like that . i think it's more about bounded resources , fewer parameters etc . remembering everything and making decisions on them gets complicated . the basic point of a rnn is to get some abstraction of temporal structure . if we were to let all inputs change our hidden/cell states , then, we are failing to distinguish between valuable and useless inputs . this decision of this is is designed as a function of current hidden state ( what we already know ) and current input , with gates in an lstm . EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ thanks for your input . what my current plan is to start with logistic regression and some tree models ( single decision tree , bagging and randomforest ) . these models do not 'directly' use time-dependent data .. therefore i want to create a feature that is time-dependent , like '#months customer x is active' . so that will be my first idea to incorporate some history of the customer . because i have NUM months of data from a set of customers ... my next idea is to use the first NUM months to train my models and then use these to test the NUM th month . do you think that this is a good idea ? the reason for this approach is because i think that using the data of the NUM th month as both train and test data violates the 'predictive' behaviour EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ your idea to use the last month of data as a test data is certainly viable , and i definitely agree you should never share data between your training and test sets . though, if you are training models that are relatively fast such as logistic regression and tree-models , then cross-validation in addition to testing the NUM th month could help provide more detail into how your model performs across the entire span of the data set . EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ i'd try survival models instead of time series models that don't deal with time-to-event or censored data . ( in this sort of data customers who haven't yet churned can be viewed as censored as they may at some point in the future similar to patients who are still alive and in the study or leave a study for other reasons in classic medical survival analysis. ) EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ this makes little sense to me . survival analysis describes population characteristics . churn analysis tries to learn individual behavior that precedes churn . having a survival model of the population won't tell you much about an individual's proclivity to churn unless the variance is very low ( eg , everyone leaves after NUM months ) . EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ it would tell you which factors are associated with short and long survival . EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ you may be thinking of kaplan-meier analysis specifically ? there are a number of survival models that do try to predict individual survival . ie cox regression ( which assumes that hazards over time are proportional across the population but not constant ) and various survival versions of decision trees/forests , svms, boosting etc some of which don't assume proportional hazard . EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ tree-based and logreg models can model temporal effects in a data set , provided you get a bit smarter with cross-validation ( always train on historic data and predict in the future , do not use future information to predict the present ) . online learning algorithms naturally work with this ( they run through the data set in a sequential manner , predicting the next sample , using what they learned from all the previous samples ) . you can use more advanced models that automatically detect patterns in time-series , or you can help standard models , by hand-crafting aggregate features (time.on.site.last.week , time.on.site.last.month, time.on.site.last.year etc.). EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ ah yes , i just replied to mrtwiggy with my plan and coincidental is incorporated all your suggestions ;) then one question for you ... what kind of 'advanced models that automatically detect patterns in time-series' do you know ? i actually only know one .. which is hidden markov model , and i've been told that this one is hard to use effectively EOA 
 which classifiers are suitable for churn analysis ? : machinelearning sounds like there are a variety of ways to deal with this problem . one way would be to use a convolutional neural network to convolve across time steps and extract features that you can then use to train other time-independent algorithms . the effect that applying a time agnostic machine learning algorithm depends on the specific algorithm and data/domain , and is difficult to predict even when understanding the domain and algorithm . one algorithm may only work if your time series features are engineered into a compact representation of the history , or it may work perfectly fine feeding in your time step history data . however, if you expect your target is highly sensitive to the time dependant nature of the data , then algorithms that don't take that prior into account will likely have a more difficult time then one that does . as in most areas , it'll probably be a mix of intuition and trying out different versions of your feature engineerings , different algorithms , and then cross validating to ensure usability . EOQ lstm / rnn / NUM d convnets . i always find it a bit funny when people recommend using such models . if you know how to set up such a model , then you won't need to ask questions . if you don't know how to set up such a model , well... you are screwed for at least NUM months :). i'd also pose arima as an advanced time-series model , but that could be doable . hmm... i put in the same category as lstm/rnn/convnets . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ will look into this EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ aws does not make sense for many use cases . ( it also makes a huge amount of sense of many use cases ) we are a medium sized start up that processes NUM-tb of image data using various computer vision pipelines , including various ml techniques . i ended up just building a small data center in our office. i have a good deal of it experience in addition to software dev so take this with a grain of salt , but unless customers are accessing your computation resources , aws often doesn't make sense and is actually more troublesome than doing it yourself .. we paid for our little data center after NUM months of not-having-to-pay-aws savings and we know have a strong competitive edge as all our competitors are using aws . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ for someone interested in someday doing this themselves , is there a book or website you'd recommend checking out for learning how to do all the it ? EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ i spent a good chunk of my career as a systems software engineer in distributed computing . i think getting a solid skill set in the area involves mostly doing or working side-by-side with others that are . it's easy to get led astray in distributed systems , so many different vendors/companies trying to sale/push their particular brand of ( sometimes ) snake oil . my main advice would be : farm out the actual building of computers to a company that provides a decent amount of support and has a decent amount of knowledge regarding fitting hardware to software needs , something like thinkmate . you want to spend you time building the system , not configuring/trouble shooting raids . buy the support contract . get an it contact on retainer that can handle things like maintenance , backups, routine configuration , ect. use off-the-shelve stuff where you can . tech like rabbitmq and redis for queuing , message passing and caching will make life easy . use linux . seriously, don't use windows for data center stuff , even if they are small . use python for server side stuff , don't write database/process management code in c/c-. of course , a lot of ml code is in c-and this is fine , but use python as the glue . don't be afraid to hire temp contractors for random projects , don't be afraid to fire the contractor if they aren't getting shit done , just move to the next . some of our most impressive tech was done by contractor specialists . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ very helpful , thanks! :) EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ if you want to build something on yourself ( to compete with ec2 g2.- ) . checkout these links : building a deep learning ( dream ) machine . this one is great , but note the dream word there , which means you can go cheaper than that and have a very powerful build as well ( old z97 architecture with a couple of NUM ti , will humiliate ec2 instances .. ) hardware guide : neural networks on gpus this one is very good as well luck ! :) EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ awesome , thank you ! EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ i use the same instance . if you stop the instance when you're not using it , you will save some money . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ ... and they need to have an ebs root or they lose all their data . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ same here . just stop the instance when not in use . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ there's always the option to build a gaming machine . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ isn't it like NUM cent an hour ? EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ around NUM cent / hour for spot instances EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ doesn't sound awfully expensive . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ adds up . $2.40/day, $74.4/month, $900/year ( and of course , with spot you have unpredictable interruptions ; don't forget the cost of ebs and any other aws stuff you might use ) . after NUM months of usage , that's a ( used ) NUM gb titan right there-and the titan has NUM gb more memory than the ec2 gpus and it'll run even faster than the spec comparison would suggest because on ec2 you have vm overhead . ( and titan and other current gpu prices will presumably fall even further this year when nvidia finally starts shipping the new ones. ) and that's just a single instance , at spot . god help you if you want to run multiple gpus so you can do model selection or hyperparameter tuning , or you don't want to architect around spot interruptions ! amazon is expensive. they need to upgrade the gpu instances or drop prices severely if they want anyone to use them for deep learning . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ agree with the gpus . they are not great . need at least .35 compute power and the grid gpus on aws have .30. may have to build my own EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ are you running the instances NUM /7 ? i do a lot of prototyping in those machines , and is never more than NUM ish or NUM ish a month . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ if you're not running an instance and you need to , that just means you're paying in a different currency than dollars . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ no , what i mean is , i do plenty other stuff during the day , in so far as i don't really need to be running instances all the time . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ swoosh EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ you're failing to account for the significant costs of electricity and administration though . it isn't a fair comparison . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ /rolls eyes . fine. a nvidia titan draws NUM watts or NUM 5 kwh when run . a kilowatt-hour in most places is ~10 cents , hence, NUM cents or $0.025/hr. if you are running it NUM /7/365 , then it's NUM 025-NUM -NUM 5-$219 . this adds on NUM months to the comparison (219/74.4-NUM ). so including electricity , it's under NUM months of usage . as far as administration goes ... i dunno about you , but i find it way easier to work with my projects and my data on my local gpu than to be dealing with ec2 , ssh, scp , keys, firewall rules , amis, etc . aws has many virtues and uses , but it is not better at being your workstation than your workstation . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ that's when you actually have a linux workstation or the ability to convince people to provide you with one . :( EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ ive heard other cloud platforms can be less expensive. maybe try google compute engine or azure ? EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ neither google or azure has useful gpu instances EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ azure supposedly has them coming , though i'm not sure when and whether they will cost any less than aws URL EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ i am using a lot of aws g2.-in a semi mode spot/full price . and damnnn that hosting is expensive ! i usually advice against do it on your own and always prefer the payed service. but i found the best solution so far to be a hybrid approach : get some local hardware ( use z97 architecture , would be good enough to outperform the aws quite a lot if a good gpus installed ) . and separate your work between the local and the cloud one . of course this heavily depends on the needs you have : if you don't need to scale , but you need to push a lot of processing every day-go with local if you need to just experiment and test some model from time to time the spot instances could be the best way to go if you need to scale and to push a lot of processing , do hybrid solution ( build NUM $-rig at home and utilize spot (and/or full ) ec2 instance ) and btw if you are scared about building a local thing , don't be . i found out it is actually a lot easier than it looks like . EOA 
 have been using aws g.2 tier with torch / cuda / python . it is very expensive. need a shared environment , can anyone suggest an alternative ? : machinelearning you can use fleet requests to spin up whenever the price moves below a target price , to arbitrage between different region pricing , and to pick the cheaper of several g2.2xlarge and NUM g2.8xlarge. we use it for compute nodes and it reduces our cluster prices by about NUM % . URL EOQ check digital ocean EOA 
 general recipes for detecting gradients vanishing/exploding and weights saturated : machinelearning the following thoughts are from a very theoretic pov : the thing is that the direct observation of weight values is not telling you everything because there is always a possibility that NUM-weights are part of a global minimum ( e.g. if specific features are noise ) . thus, it makes sense to compare the weights to the error surface . furthermore, you'll have to factor in the time dimension ( how weights develop over time ) to be able to tell if gradients are vanishing or exploding . for such an analysis bifurcation diagrams plotted against the error surface are a pretty handy . here's an example in fig NUM . bifurcation diagrams are in general a good tool to visualize the development of units . in fig NUM of same paper you can see another bifurcation diagram showing how a parameter's range affects its convergence . there's a lot of papers on this topic but it requires a little bit of background on dynamical systems ( this one is a good introduction imo ) but analyzing the internal dynamics of the network gives you a much more powerful language to describe what is happening . if you're just interested in sort of a metric that shows you how close you are to vanishing gradients , iteratively calculating the distance between NUM and the smallest/largest eigenvalue of the weight matrix will give you a good description . the smaller this distance , the better wrt stability . EOQ how about comparing average gradients per layer ? if gradients vanish as backpropagate , update rates of first layer would be tiny and especially smaller compared to the rate of last layer . EOA 
 general recipes for detecting gradients vanishing/exploding and weights saturated : machinelearning the following thoughts are from a very theoretic pov : the thing is that the direct observation of weight values is not telling you everything because there is always a possibility that NUM-weights are part of a global minimum ( e.g. if specific features are noise ) . thus, it makes sense to compare the weights to the error surface . furthermore, you'll have to factor in the time dimension ( how weights develop over time ) to be able to tell if gradients are vanishing or exploding . for such an analysis bifurcation diagrams plotted against the error surface are a pretty handy . here's an example in fig NUM . bifurcation diagrams are in general a good tool to visualize the development of units . in fig NUM of same paper you can see another bifurcation diagram showing how a parameter's range affects its convergence . there's a lot of papers on this topic but it requires a little bit of background on dynamical systems ( this one is a good introduction imo ) but analyzing the internal dynamics of the network gives you a much more powerful language to describe what is happening . if you're just interested in sort of a metric that shows you how close you are to vanishing gradients , iteratively calculating the distance between NUM and the smallest/largest eigenvalue of the weight matrix will give you a good description . the smaller this distance , the better wrt stability . EOQ interesting idea , and it is actually very fast to implement . we just need to check the gradients of first layer and last layer , since all the sign of vanishing or exploding will be most obvious at lower level . and it doesn't matter where the gradients are mess up , it is going to mess up the whole training process anyway . EOA 
 general recipes for detecting gradients vanishing/exploding and weights saturated : machinelearning the following thoughts are from a very theoretic pov : the thing is that the direct observation of weight values is not telling you everything because there is always a possibility that NUM-weights are part of a global minimum ( e.g. if specific features are noise ) . thus, it makes sense to compare the weights to the error surface . furthermore, you'll have to factor in the time dimension ( how weights develop over time ) to be able to tell if gradients are vanishing or exploding . for such an analysis bifurcation diagrams plotted against the error surface are a pretty handy . here's an example in fig NUM . bifurcation diagrams are in general a good tool to visualize the development of units . in fig NUM of same paper you can see another bifurcation diagram showing how a parameter's range affects its convergence . there's a lot of papers on this topic but it requires a little bit of background on dynamical systems ( this one is a good introduction imo ) but analyzing the internal dynamics of the network gives you a much more powerful language to describe what is happening . if you're just interested in sort of a metric that shows you how close you are to vanishing gradients , iteratively calculating the distance between NUM and the smallest/largest eigenvalue of the weight matrix will give you a good description . the smaller this distance , the better wrt stability . EOQ yes that's true . one thing to add : if the update rates seem okay that doesn't mean it's working well . but the other way-if they differ more than NUM x or even NUM x , than there's a problem . EOA 
 general recipes for detecting gradients vanishing/exploding and weights saturated : machinelearning the following thoughts are from a very theoretic pov : the thing is that the direct observation of weight values is not telling you everything because there is always a possibility that NUM-weights are part of a global minimum ( e.g. if specific features are noise ) . thus, it makes sense to compare the weights to the error surface . furthermore, you'll have to factor in the time dimension ( how weights develop over time ) to be able to tell if gradients are vanishing or exploding . for such an analysis bifurcation diagrams plotted against the error surface are a pretty handy . here's an example in fig NUM . bifurcation diagrams are in general a good tool to visualize the development of units . in fig NUM of same paper you can see another bifurcation diagram showing how a parameter's range affects its convergence . there's a lot of papers on this topic but it requires a little bit of background on dynamical systems ( this one is a good introduction imo ) but analyzing the internal dynamics of the network gives you a much more powerful language to describe what is happening . if you're just interested in sort of a metric that shows you how close you are to vanishing gradients , iteratively calculating the distance between NUM and the smallest/largest eigenvalue of the weight matrix will give you a good description . the smaller this distance , the better wrt stability . EOQ i did some testing , this is the code : URL this is the plot ( its text bar plot , horizontal axis is each batch , vertical is magnitude of value ) : URL from the plot you can see the weights from first NUM layers stay unchanged over time ( just a line ) , and their gradients is almost NUM even though they are fluctuating all over the time. hence , the check.gradient function always return the issue : NUM % of gradient is NUM . the cost plot showed that the model failed to converge , however, the gradients had never exploded ( nan value ) . and the ratio of average gradients between last and first layer is fluctuated between-3 to NUM , hence, it is very hard to make decision based on this value . in conclusion , i think we con judge the gradients based on NUM criterions : it is not nan NUM % ( heuristic value ) of gradients is NUM or not the check only need to be applied for the first NUM or NUM layers . conversely , the idea of checking weights saturated is unnecessary , in the worst case , weights of low layers stayed unchanged or nan , which both can be detected by checking the gradients . p/s : dnntoolkit library i used in the code can be found here : dnntoolkit EOA 
 time-series with rnn-how to deal with attributes that span entire sequences ? : machinelearning first of all , determine if your features are actually relevant or just adding noise ( via e.g. pca ) . since i suppose you'll be training multiple persons in NUM net , vars like birthday/age are constant in each sequence but help distinguish between samples ( people ) . thus your current approach has no issues i'd know of ( given a large enough hidden layer ) . the constant vars will act like a predetermined bias in the input-layer which is fine. however , if you're training NUM net per person , the constants add nothing of value and are just constant noise. EOQ thank you for your answer . i'm indeed doing a pca before actually training the rnn and i'm also training a single net for all people ( not one per person ) . EOA 
 time-series with rnn-how to deal with attributes that span entire sequences ? : machinelearning first of all , determine if your features are actually relevant or just adding noise ( via e.g. pca ) . since i suppose you'll be training multiple persons in NUM net , vars like birthday/age are constant in each sequence but help distinguish between samples ( people ) . thus your current approach has no issues i'd know of ( given a large enough hidden layer ) . the constant vars will act like a predetermined bias in the input-layer which is fine. however , if you're training NUM net per person , the constants add nothing of value and are just constant noise. EOQ i would repeat the static attributes for each sequence and just do the rnn feedback loop for those that are varying . EOA 
 time-series with rnn-how to deal with attributes that span entire sequences ? : machinelearning first of all , determine if your features are actually relevant or just adding noise ( via e.g. pca ) . since i suppose you'll be training multiple persons in NUM net , vars like birthday/age are constant in each sequence but help distinguish between samples ( people ) . thus your current approach has no issues i'd know of ( given a large enough hidden layer ) . the constant vars will act like a predetermined bias in the input-layer which is fine. however , if you're training NUM net per person , the constants add nothing of value and are just constant noise. EOQ is there a reason why you think using the same rnns for those NUM attributes is necessary ? do you have any other baseline with simpler models for these attributes ? in a classical hmm model , for such cases you wouldn't really need to decode the sequence of states ( as for the other attributes ) , but you would train NUM separate hmms ( one for each class ) and simply compute the joint probablilty for the model , choosing the one with the highest value as the result . using an ann , i'd try and classify all the attributes for all steps and simply combine the outputs for all the sequence steps using a mean ( or something similar ) . different sequence lengths would make this somewhat inaccurate , but i think this is hard to avoid for such a problem . another similar example you can look for is music genre recognition . there are many papers on the subject and they all have a similar problem-> ; various sequences of song features need to be classified into one genre class per sequence . many of them use a model to classify each frame of audio in the sequence ( using an svm , for example ) and then use majority voting ( picking the class that occurs most frequently in the outputs ) as the final result . not very clever , i admit . maybe there are other methods in that domain . EOA 
 time-series with rnn-how to deal with attributes that span entire sequences ? : machinelearning first of all , determine if your features are actually relevant or just adding noise ( via e.g. pca ) . since i suppose you'll be training multiple persons in NUM net , vars like birthday/age are constant in each sequence but help distinguish between samples ( people ) . thus your current approach has no issues i'd know of ( given a large enough hidden layer ) . the constant vars will act like a predetermined bias in the input-layer which is fine. however , if you're training NUM net per person , the constants add nothing of value and are just constant noise. EOQ hi , thank you for your quick reply . is there a reason why you think using the same rnns for those NUM attributes is necessary ? yes , at least the way i view it . in my real problem , both the individual attributes and their position relative to each other matter , and all NUM parameters together need to be known . imagine i'm doing a physics simulation and trying to predict trajectories for spatial objects passing next to three planets ; the NUM parameters would be their weights . while i could train three models independently , i'm mainly interested in the coupling that occurs when everything is combined . anyway , i appreciate your suggestions and will look more into what they do in music genre recognition . EOA 
 time-series with rnn-how to deal with attributes that span entire sequences ? : machinelearning first of all , determine if your features are actually relevant or just adding noise ( via e.g. pca ) . since i suppose you'll be training multiple persons in NUM net , vars like birthday/age are constant in each sequence but help distinguish between samples ( people ) . thus your current approach has no issues i'd know of ( given a large enough hidden layer ) . the constant vars will act like a predetermined bias in the input-layer which is fine. however , if you're training NUM net per person , the constants add nothing of value and are just constant noise. EOQ within each sequence , i would include the elements that are constant and the elements that are variable . as others here have said , it will learn to incorporate those constant and variable changing terms . when you train it , its important that you feed it people samples that are constant and are variable . the most important aspect in my opinion to this problem is getting a true distribution of your total data . your dev set should also be an accurate distribution . otherwise , you won't know if you're overfitting/underfitting ! if you're sequences are NUM elements , you may want to look into a few alternatives : unitary rnn multiple layer lstm with shortcut connections grid lstm EOA 
 time-series with rnn-how to deal with attributes that span entire sequences ? : machinelearning first of all , determine if your features are actually relevant or just adding noise ( via e.g. pca ) . since i suppose you'll be training multiple persons in NUM net , vars like birthday/age are constant in each sequence but help distinguish between samples ( people ) . thus your current approach has no issues i'd know of ( given a large enough hidden layer ) . the constant vars will act like a predetermined bias in the input-layer which is fine. however , if you're training NUM net per person , the constants add nothing of value and are just constant noise. EOQ what you could consider is to map the static attributes ( linearly or non-linearly ) to a vector of the size of your rnn and use its components as initial state activations at time step NUM . however , i doubt that this will outperform your current approach feeding the static attributes into the network at every time step . it requires less weight multiplications though . EOA 
 are quant firms looking for machine learning phds ? : machinelearning yes EOQ what do quants model , and what type of models do they use anyway ? EOA 
 are quant firms looking for machine learning phds ? : machinelearning yes EOQ we do . though , to be fair , we are a small startup and outside the classic areas of finance ( ie we don't do stocks bonds etc ) and i can't speak to what sort of degree will get you a job at the big firms . i'd say the biggest thing we care less about the specifics area of your degree focused on and more about experience doing repeatable analysis/research . ie even if your phd topic is extremely relevant to our area we want people who will choose the best method for each problem based on the data and not be biased towards their own favorite methods . EOA 
 are quant firms looking for machine learning phds ? : machinelearning yes EOQ i've been approached by two recruiters for this area , so they definitely seem interested in people with ml phds . EOA 
 which dataset to use when selecting attributes ? : machinelearning training data only EOQ just to add a little , feature selection is part of the training process . depending on the selection method , you're essentially looking to see which features are most strongly associated with the outcome . therefore, you only feature select on the training set , as you would otherwise be fitting your features to the test set . EOA 
 which dataset to use when selecting attributes ? : machinelearning training data only EOQ in general , you don't really want to look at the test data at all until you are , well, testing your final model . EOA 
 advice using neuroph to detect poker cards : machinelearning you don'n need a nn to scrape poker programs and you already have projects that do this for a long time e.g. this also , crime doesn't pay ;) there's also plenty of demos of cnns for ocr and image recognition problems . take a look at architectures and tutorials for mnist and cifar and just those . EOQ i am more interested in it as a programming hobby-not to make real money-i dont think that would be very achievable . thanks for the help ! EOA 
 why do data transformations affect performance of tree-based models ? : machinelearning if you split between two values , you use the midpoint as the split . if you log transform the data , that changes what a midpoint is . the midpoint between two points in the original space is no longer just half way between them , but is biased towards the higher values . this is about the only effect a monotonic transform will have for decision tree splits . EOQ that's what i thought too . but found a couple links that report differently . for example : URL-slide NUM EOA 
 why do data transformations affect performance of tree-based models ? : machinelearning if you split between two values , you use the midpoint as the split . if you log transform the data , that changes what a midpoint is . the midpoint between two points in the original space is no longer just half way between them , but is biased towards the higher values . this is about the only effect a monotonic transform will have for decision tree splits . EOQ i'd suspect that to be natural variance . they don't give any specifics and it sounds like they're judging this using a single internal cv loop and using a pretty small number of trees ( couple hundred ) . many tree learning implementations do include some sort of cutoff distance bellow which feature values are considered the same ( either as explicit regularization or to make results the same in NUM and NUM bits ) and its possible for scaling to interact with this . its also quite easy to mess up scaling and leak data if you perform it outside your cv loop since the model shouldn't be able to learn the training-holdout set mean but can if you preprocess outside of cv . i'm not sure if something similar could happen with bagged trees...ie it would become much easier for each tree to learn the training set mean . EOA 
 why do data transformations affect performance of tree-based models ? : machinelearning if you split between two values , you use the midpoint as the split . if you log transform the data , that changes what a midpoint is . the midpoint between two points in the original space is no longer just half way between them , but is biased towards the higher values . this is about the only effect a monotonic transform will have for decision tree splits . EOQ they do ? oh my god...so far i thought it didn't matter whether you transformed the features or not EOA 
 why do data transformations affect performance of tree-based models ? : machinelearning if you split between two values , you use the midpoint as the split . if you log transform the data , that changes what a midpoint is . the midpoint between two points in the original space is no longer just half way between them , but is biased towards the higher values . this is about the only effect a monotonic transform will have for decision tree splits . EOQ who's the fuck head that down votes every post in /r/machinelearning ? what a shitty community . EOA 
 is there a way to build a nn that can identify new classes ? : machinelearning sounds like you want zero-shot learning : URL EOQ you might also want to check out open world learning : specifically , check out these papers : NUM . towards open set recognition scheirer et al ., NUM . towards open world recognition by abhijit bendale , terrance boult EOA 
 is there a way to build a nn that can identify new classes ? : machinelearning sounds like you want zero-shot learning : URL EOQ with unsupervised representation learning you would hope that your network generalizes well enough to learn abstract features , which would turn the representation for a never-seen-before zebra close to the representation of horses . EOA 
 is there a way to build a nn that can identify new classes ? : machinelearning sounds like you want zero-shot learning : URL EOQ if you've trained a net to classify a bunch of different animals then for each input image you're going to get a corresponding bunch of outputs indicating how similar the net sees the image as being to each of the animals it was trained to recognize. these similarities are going to be based on the set of features ( both alone and in combination ) that the net was forced to learn in order to be able to distinguish the animals it saw during training . if the net had been trained on animals with fine grained differences then it will have had to learn fine-grained features to distinguish them . if it was only given broad categories ( e.g. dog vs specific breeds of dog ) , then it will have only learned correspondingly broad features to distinguish them . so , given your example of seeing a zebra and monkey for the first time , the result would depend on what it had been trained on . if it had already been trained to recognize horses then you might expect it to fairly confidently classify a zebra as a horse since it hadn't been trained to know that something looking ( mostly ) like a horse but with stripes is not a horse . otoh if it hadn't seen any kind of primate ( with their characteristic faces , hands, feet , etc ) during training then it would likely not see the monkey as being a great match for anything else although animals the monkey shared visual characteristics with would be rated as better matches than things like fish with next to nothing in common . you wouldn't expect it to see the zebra and monkey as being very similar to each other ( i.e. being close in classification/feature space ) since they don't visually have much in common . to answer your first question , a net is always going to output a set of similarities to the classes it was trained on regardless of whether the input was something ( category or instance ) it was trained on or not . there's no such thing as unknown , only degrees of similarity . you could use a NUM-d visualization tool to see how close in n-dimensional feature/classification space various inputs are , and should expect to see that the net sees visual similarity much the same as we do ( because we're wired to use a similar architecture of hierarchical feature detectors/aggregators to sense the world ) . EOA 
 explanation of keras layers : machinelearning maybe you should look at some high level overview of the concepts behind neural networks at first , then dive into the code and examples . there are some links to books and courses in the link collection of r/machinelearning . a great intro to convolutional nets is this lecture by nando de freitas URL . a free course on neural networks ( mostly convolutional nets ) , is here : notes from last year : URL current lecture ( with videos ) : URL even if you don't want to use convnets , this course should give you a better understanding of neural networks in general . EOQ what you want to find is a machine learning course . EOA 
 explanation of keras layers : machinelearning maybe you should look at some high level overview of the concepts behind neural networks at first , then dive into the code and examples . there are some links to books and courses in the link collection of r/machinelearning . a great intro to convolutional nets is this lecture by nando de freitas URL . a free course on neural networks ( mostly convolutional nets ) , is here : notes from last year : URL current lecture ( with videos ) : URL even if you don't want to use convnets , this course should give you a better understanding of neural networks in general . EOQ could you elaborate more ? no one can explain the whole examples of keras here . EOA 
 examples for deductive inference machine learning ? : machinelearning hmmm , i could be off the mark but i'm not really familiar with any . it seems like the entire premise of machine learning is basically induction . we take lots of data and examples , and then attempt to induce a good hypothesis off the observations . the 'learning' from ml comes from the idea of an improving hypothesis search guided by observations , which seems inherently inductive to me . just my NUM x NUM-2 dollars . EOQ sorry for mobile link , but are you thing of something similar to resolution ? edit : well, this is fun , apparently reddit doesn't like close parens in links . resolution.(logic) is what i was trying to link , or look into its use in automated theorem proving . edit2 : also, several of the algorithms you posted are actually transductive inference , not inductive. induction is actually a much harder issue , and we use transductive inference as a proxy . EOA 
 examples for deductive inference machine learning ? : machinelearning hmmm , i could be off the mark but i'm not really familiar with any . it seems like the entire premise of machine learning is basically induction . we take lots of data and examples , and then attempt to induce a good hypothesis off the observations . the 'learning' from ml comes from the idea of an improving hypothesis search guided by observations , which seems inherently inductive to me . just my NUM x NUM-2 dollars . EOQ i don't really think deduction is learning , though. it's essentially just logic , using rules to draw conclusions . you can't learn new rules in this manner . EOA 
 building a learning system to parse recipes ingredients . need advice on platform choices . : machinelearning yes , structured prediction with conditional random fields is the right way to go for that . i think there was a post on that not too long ago . EOQ thank you ! do you know popular toolkits , libraries that i should research more ? EOA 
 building a learning system to parse recipes ingredients . need advice on platform choices . : machinelearning yes , structured prediction with conditional random fields is the right way to go for that . i think there was a post on that not too long ago . EOQ crf-is pretty popular , although it is c-. EOA 
 building a learning system to parse recipes ingredients . need advice on platform choices . : machinelearning yes , structured prediction with conditional random fields is the right way to go for that . i think there was a post on that not too long ago . EOQ i'm a bit biased , but python-crfsuite ( or its sklearn-crfsuite wrapper ) should be easy enough to get started with-see e.g. URL . arguably a harder problem would be to figure out how to annotate enough training data . maybe URL is good , but i haven't tried it . EOA 
 building a learning system to parse recipes ingredients . need advice on platform choices . : machinelearning yes , structured prediction with conditional random fields is the right way to go for that . i think there was a post on that not too long ago . EOQ take a look at chloe kiddon's paper that uses an unsupervised method for this task : URL EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ i meant facial recognition : actually picking a person from a crowd . for example , say i was going to a political rally and kept forgetting peoples names . politicians generally have thousands of thousands of images online ; i would find a few hundred high-res facial shots and use them to generate a recognizer(probably not the correct terminology) . i could have a camera connected to a micropc in my bag , and have it tell me the name of the person that i was looking at . is that an unrealistic goal ? what models would you recommend ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ came here and saw this comment with a negative score . i don't understand the downvotes . op is not asking bad questions , and ( imo ) has a pretty interesting idea ! i personally believe it's unrealistic , because sufficiently controlled , high quality shots for facial identification are pretty difficult to obtain somewhere like a political rally from the crowd . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ they would be standing/sitting in the same place for a while , though. i could use an hd-video camera . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ at what distance ? if you can get a pretty clean image , then you could run the whole image against a facial detection model to extract the faces , then a facial identification model against each extracted face . is the difference between detection and identification clear to you ? edit : recognition !-detection . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ it is more clear now , thanks. this would be at a distance between NUM-40 meters , i know i'm going to need a specialized camera/lense . so , to recap what i have so far : to build a database i need only NUM-20 images of the person at around NUM p to NUM p , cropped and re-sized . to recognize , i need two filters : one to extract the faces , and one to actually analyze them . can opencv do both of those tasks , or should i use different software for each ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ i don't know how many samples are needed to train either model , actually. i just know they're separate problems . it seems pretty obvious to pair facial detection with facial identification , so there may be examples or even usable tools that pair the two things out of the box . edit : recognition !-detection . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ oh ok . understood. may i ask , what would be some good resources to start with ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ what languages are you comfortable in ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ python , ruby, and novice lua . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ what do you mean by facial recognition ? if you want to do person a vs . person b , then maybe you only need a tiny amount of data to make good classifications . also , whether the faces are aligned and fully frontal ( like passport photos ) or at weird angles is a major factor in what kind of data you need . if you have other variations like scale or weird lighting or obstructions , then you'll need even more data . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ i was imagining a system that would function in a crowded trade show/comic-con/event fair . it would be a horrible environment : tons of people , lots of flashing lights , only about NUM-5 seconds of time per face . it seemed to me that i would need a large number of very high-resolution images as a trainer . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ like this ? URL edit : sorry, those don't have nearly the resolution you're after . i do know those are used for actual facial recognition research , though. EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ i apologize , i was intending on recognizing specific people . i do already have a few hundred images though , they're all super-huge NUM x2000 images . what would you recommend to help cut them down to size ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ if you're on linux or mac , you could create a workflow with imagemagick easily enough . surely there's something for windows , too, but i won't know what it is . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ thanks , i'll save that . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ i think you're confusing things . haar cascades are usually used for face detection . you can use the ones that are already trained . you do not need to train your own ( that means you don't need thousands of images ) . NUM x1500 is also a pretty high resolution to recognize something as statistically regular as faces . you didn't say where you read that . you can use NUM x250 images to train a face detector for some detection tasks . i think you should narrow things down a bit . recognition? identification ? in real-life pictures/videos ? face datasets ? using what recognition structure ? you seem to be focused on the wrong question ( i.e. how much data i need ) without stating your problem clearly . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ identification , with a database of people . i was imagining a system that would function in a huge crowd of people , like a trade show/comic-con/event fair/sports stadium . it would be a horrible environment : tons of people , lots of flashing lights , only about NUM-5 seconds of time per face . it seemed to me that i would need a large number of very high-resolution images as a trainer . say i was going to an electoral speech and kept forgetting peoples names . politicians generally have thousands of thousands of images online ; i would find a couple hundred high-resolution facial shots and use them to generate a recognizer(probably not the correct terminology) . i could have a camera connected to a micropc in my bag , and have it tell me the name of the person that i was looking at . or , for example , imagine a smart tv recorder : i load an identifier for my favorite actors/actresses , and program it to automatically start recording if a certain person comes on screen . so to answer your question : i want real-life , real-time identification of actual people , with a database of specific targets . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ i suggest you start with looking at faces in the wild dataset and see what people have done with them . another note is that training any sort of computer vision algorithm with high resolution will make your life harder , not easier . it'll be harder to implement , exponentially slower to train and will not give you much benefit . if by hi definition you mean using photos that cover a large area ( like a crowd photo ) , then i suggest you consider running a face detector first to crop around faces . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ got it . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ state of the art face recognition uses much much lower resolution images than what you are talking about here . having hundreds of images is not really necessary either . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ so just NUM images at NUM p ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ more like two or three images at NUM px x NUM px resolution EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ huh . to be honest , i find that kind of hard to believe . you only need three pictures of a person for accurate identification of specific targets ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ your skepticism is well founded-he is wrong . you will need many more images than NUM . one image of one face can be used to get a single feature vector describing that face . does anyone know of any worthwhile model that can be trained on just NUM samples ? i sure don't . there are diminishing returns after a certain resolution . you need good enough resolution ( on each face ) to accurately build the feature vector , but you might be surprised at how low that resolution can be . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ i knew it . around NUM-75 images of high quality . so what should i use to work on this , what library/framework ? EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ see my advice here . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ ok , thank you . EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ [ deleted ] EOA 
 i was informed that a minimum of NUM high-resolution images are required for facial recognition , with around NUM needed for real person identification . why can i not find any libraries that align with this ? : machinelearning it depends on whether your problem is facial recognition or facial detection . as far as i know , haar cascade methods are used for facial detection , which is cropping out the section of an image that is a persons face . where as facial recognition is looking at a crop of a persons face and identifying that it is jane doe , or bob fisherman , etc. additionally , there can be differences in terms of use and original training . for a facial recognition system , they may typically use thousands of images of faces to train . but ideally , they can subsequently train on less image of a person to distinguish them . the model needs to learn basic features that it can then use to differentiate people , which may take a lot of data for deep learning methods . EOQ well , i planned on actual identification , not just recognition . EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ richard sutton in his talk that was posted a few days ago on this sub , talked about the deadly triad that causes networks to diverge when used with rl . NUM ) bootstraping as is used in td learning variants and dynamic programming . NUM ) function approximation . NUM ) off policy learning as in q learning . in the udacity rl course that i went through recently , charles isbell mentioned that he had difficulty training a neural net to play tic tac toe , which is incidentally what i am going to try next as soon as i figure how to link my own .net library with java native ggp-base . any advice for what comes ahead ? should i expect training neural nets to be particularly difficult on the logic based board games that are part of the general game playing library ? incidentally for those interested , in the udacity course they only very briefly touched on function approximation unlike in david silver's course . i am going to have to go through the later half again as i have mostly forgotten the material by now ( i watched it a few months ago. ) EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ i found that q can indeed easily diverge , since the max operation doesn't really decay the q values when overestimating . as a result the q values can tend to increase too much . i personally don't use standard neural networks to do reinforcement learning , due to the experience replay hack that doesn't scale well . i use my own thing , that used to be similar to htm but turned into something completely different from both standard machine learning and htm . in my opinion , the key ingredient most people ignore is sparsity . sparsity just makes so many operations required for rl work so much better , like online learning , eligibility traces , q storage . simple explanation : dense ( not sparse ) representations have all inputs saying i did it ! to reward and punishment , so it is hard to accurately determine which did it . but with a sparse , separated representation with low overlap , you know exactly which nodes are responsible . basically sparsity is the bridge between neural networks and q lookup tables . the former have no sparsity by default , and the later is as sparse as it goes . a balance is best ! EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ i find that believable . in fact , when i started coding neural nets five and a half months ago , one of the things i was interested was the k-sparse autoencoder . i worked quite a bit on it and found that in the deeper regime ( well with two layers in this case ) i could train it well with nesterov's momentum . this is in contrast to the relu units which blew up almost instantly no matter what i did . also with shallow relu autonecoders , i found out that if i used dropout which is partly a sparsity constraint , i could use a lot higher learning rates without it blowing up . in this paper , the authors had to use sparse initialization to make their deep autoencoders trainable , but i am guessing having a sparse activation would have worked as well . i would also guess that replacing relus with local wta functions might improve stability in adversarial generative nets , though i am not interested enough to try it myself . i think i saw you recommend sparse coding as a method for looking up policies a few days back and having followed your work for a bit , i know you make use of it , though honestly i feel like the stuff that you do is way beyond me at the moment . in connection to sparse coding , how would you compare it to the wta autoencoder ? in case you do not know what it is , it is similar to the top k selection function except it picks out the top elements orthogonally , along the batch dimension . i've only tested in on mnist , but it works great for pretraining . also, have you tried pretraining in a reinforcement learning setting ? how did it work ? EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ hello , i have tried a k-sparse autoencoder before . it works quite well , but the algorithm i use now for sparse coding is a bit different . at first i used something similar to sailnet that used spiking neurons , but i found it too processing intensive. now i use a hebbian learning ( oja's rule ) k-sparse network , which doesn't require backpropagation , and works about the same . i have not tried pre-training in a rl setting , i haven't found the need yet , since the rl algorithm i use works well with sparse coding in an online scenario . i might try it in the future though ! EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ i want to ask you about sparse coding a bit more . do you only use it at the beginning for initializing the network ? basically, the wta autoencoder works quite a bit better than the k-sparse one and the activation is very fast , but i do not think it can be used except for initialization because it is too strong of a regularizer . with the k-sparse autoencoder , one needs to move the k up and then down to make sure that none of the neurons get left unused , but wta works much better since you can just set the lifetime sparsity and expect it to work . i am wondering whether it could be a suitable replacement for sparse coding as it is as fast as a regular autoencoder . i've never used sparse coding personally , but in some of the papers i've read , i've seen that it is quite computationally intensive . also , your own work , neorl, does not use backpropagation as you said . though i've pretty much butchered that advantage during the first four months of my ml adventure , now that i actually made my own ad library , i feel that backprop has some strong advantages . i can envision doing all sorts of crazy architectures that would have been impossible before to do by hand and expect them to do what i coded them to do . in fact i barely used the library since i made it . do you feel that not using backpropagation is an advantage ? if so , i am curious as to why . in regards to the oja's rule , given its similarity to pca , one crazy architecture that would be easy to test with ad , would be to make every layer of a neural net a shallow autoencoder in the third dimension , while having it be a regular net in the other dimension and train it in a join supervised and unsupervised fashion with multiple levels of supervision . based on the paper , multiple levels of supervision ( or unsupervision in this case ) might make the net more effective. i am wondering if multiple levels of the unsupervision in the scheme above might make the architecture similar to neorl . given the turing completeness of neural nets , what would be the closest neural net analogue to neorl ? i would be very curious to know that , since if you manage to get it working great , i would not be forced to drop my whole library ! also with an analogue it would be possible to have a proper match to see whether or not backprop is computationally advantageous in silico compared to the more biologically plausible nets . EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ no , not yet , but it's on my to-do list ! EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ since you brought up double q-learning , maybe you can answer a question i had . i was confused reading the paper because to me , this sounded like a simple problem with similarly simple answers : q-learning actions are overestimated because when you have measurements with error and then you select the largest ( or smallest ) estimate , you automatically get regression to the mean / systematic bias , and in a context like auctions or rl , you create a winner's curse-the best-looking estimates are also the ones most mis-estimated . the solution is some form of shrinkage back to the common mean to correct the estimates . typically you don't implement this by a totally arbitrary looking hack in which you divide your data in half and estimate on each half , which isn't even a decent crossvalidation procedure . so what exactly is good about double q-learning and why can't q-values just be shrunk like sat scores , batting averages , or any of the standard examples of this thing ? EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ well , it's a pretty famous phenomenon so there's tons of discussion , but i suppose you could see : URL URL URL to give an sat example : someone scores NUM on the sat . they go to retake it . what is the best estimate of what their next score will be ? you might naively think , NUM ; the mean of their scores is the best unbiased predictor of future measurements , and the mean is NUM so you predict NUM . they actually get NUM . what happened ? the test-retest reliability of the sat is not NUM ; it's more like NUM 5. and NUM is a very extreme score , far from the population mean . what you should have realized is the a priori improbability of scoring as high as NUM , which means that some of that first score was due to sampling error/luck ( how much is determined by the reliability/measurement error in the test ) and you need to adjust for that . in this case , a classical adjustment would be something like multiplying the sd by reliability to shrink the score back towards the population mean , giving something like a NUM point smaller estimate of the NUM 's true ability . so imagine you have a population of people taking the sat and you want to admit only the NUM ers . predictably, you will be disappointed to see that they will tend to 'underperform' from what NUM ers should be able to do , while some of their inferior NUM scorers wind up doing better . analogously , in q-learning , you have a large set of actions , each of which has been tested a few times at most , with considerable noise in the estimated values , where you want to use only the top scorers ... EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ ok , but how do you perform this shrinkage here in a way that is not a totally arbitrary looking hack , given these are very complicated distributions estimated by neural networks while they are being updated online ? how much do you shrink the individual action q-value towards the all-actions mean ? you are going to need some sort of uncertainty estimation , which probably means using bayesian neural networks of some kind , which is non-trivial . EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ not being a rl researcher , i think it's asking a bit much of me to solve the q-value overestimation in one swoop , but there seem like many principled ways you could go about it . additional regularization like dropout might help ; bayesian neural networks are a possibility ; i don't pretend to understand it , but yarin gal claims you can use dropout with regular nns to get out uncertainty estimates which might let one solve that ; perhaps some sort of online learning could be done to estimate the size of the overestimate . EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ it also depends on your domain and the involved state and action space . function approximation using neural networks and q-learning are very useful in continuous state/action spaces where tabular methods can fail . EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ is it even possible to do rl in continuous state/action spaces with tabular methods ? in the udacity course smdps were described , but the instructors did not go into detail as to how to use them . EOA 
 on policy vs . off policy in rl : machinelearning hello , q learning isn't really more popular i think , it's just because deepmind used it once so everybody goes crazy over it for a short time :) but if you look at the actual papers they are about equally popular . i personally use sarsa more than q , mostly because it is easier to implement into a single-prediction scenario . recovering the maximum q requires discrete actions or a search , but sarsa doesn't have this problem since it is on-policy . another advantage of sarsa is that it is theoretically safer since it accounts for its own mistakes when modeling the reward . EOQ well , it's possible to re-formulate your state space by discretizing it from a continuous space and then applying tabular methods . for continuous action spaces , you could obviously naively do the same , though i've not read up enough on applying tabular methods to these situations . EOA 
 does there exist an algorithm to convert an rnn into a feedforward network ? : machinelearning i don't understand the question . an rnn can be interpreted as a feedforward network where the layers share parameters , the so-called unrolled representation . EOQ i am aware of the unrolled representation , but my take away from that video was that the feedforward network was not simply an unrolled version of the equivalent rnn . an unrolled network would have x-t inputs , where x is the number of inputs and t is number of time steps . in the example in the video , both the rnn and feedforward net both only have NUM inputs , and the claim is that the feedforward net mimics the rnn up to a certain time t . my question was , is there some algorithm to do such a conversion automatically ? edit : perhaps it is as simple as training a ffn on some subset of the inputs ? i've just been trying to find where tononi got this idea from and i've haven't been able to find anything , never seen it anywhere else . EOA 
 does there exist an algorithm to convert an rnn into a feedforward network ? : machinelearning i don't understand the question . an rnn can be interpreted as a feedforward network where the layers share parameters , the so-called unrolled representation . EOQ there is kind of a special feedforward neural network that can deal with past inputs to a certain degree by having short term memory . this is related to the elman networks . maybe that is what you were thinking of ? EOA 
 does there exist an algorithm to convert an rnn into a feedforward network ? : machinelearning i don't understand the question . an rnn can be interpreted as a feedforward network where the layers share parameters , the so-called unrolled representation . EOQ i think this is just because until the rnn has accumulated memory , it is actually equivalent to a feedforward network . usually this only takes one timestep or so though . EOA 
 how to pick trajectory phases for low-energy transfers using intelligent algorithms : machinelearning are you familiar with URL ? EOQ yes , i use some calculus of variations/hybrid optimal control algorithms for inner loop optimizations . but it doesn't quite help me learn how to pick trajectory phases :/ thanks though ! EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ math underlies everything , programming is just a drop in the ocean . try learning some math , you will be hooked . rule systems or anything deterministic will simply not work due to the high dimensionality of real world data . graphs are used in machine learning , there is an area of probability that deals with graphical models . a graph is just a mathematical object with some defined structure . EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ the unreasonable effectiveness of mathematics : URL EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ thanks for this article link . will be a long and very interesting read . EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ ai is a very broad field and prolog is still part of it . it was never part of ml though . EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ this . if you use prolog you are not doing machine learning , because your program doesn't learn anything . all knowledge is programmed in . it is not clear to me how to learn without doing math . EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ i mean , all of that stuff still exists and is still used ( i know watson uses prolog extensively ) , it's just not machine learning . the methods you're talking about have all of their logic programmed in . the machine isn't learning anything . EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ thanks for all of your answers . one thing that strikes me is that the learned knowledge is encoded in models that humans can't interpret . it's highly compressed for example in k-means clustering . probably the high dimensionality is crucial for choosing math . EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ in many cases we don't need interpretability , so we sacrifice it in favor of better predictions . but there are tools ( decision tree , for example ) that are interpretable , and are used where ability to understand a model matters . also , for many models you can get some insight into what it has learned . for example , k-means can be seen as a variant of gaussian mixture model , which is based on a probabilistic model . in that model all the variables involved have meaning , and you can inspect them after training . EOA 
 why ( almost ) only mathematical functions are used for machine learning ? : machinelearning graphs are mathematical structures and functions . all of programming ( i.e. c-/prolog ) is math ... EOQ all things in ml are math . while i haven't contemplated some of the more philosophical considerations , i feel reasonable saying that simply all things are mathematics ( or can be structured as such ) . mathematics , after all , is little more than the language of logic . EOA 
 question regarding convolutions and filters : machinelearning thanks a lot ! for the number of filters : can i just choose those to my liking ? or is this more or less a fixed number like for the filter size , that people most commonly use ? EOQ find a network someone else has used and tweak it to your liking . choosing the architecture is more or less trial and error plus some hard-to-articulate intuition you build up after doing it a bunch . EOA 
 question regarding convolutions and filters : machinelearning thanks a lot ! for the number of filters : can i just choose those to my liking ? or is this more or less a fixed number like for the filter size , that people most commonly use ? EOQ number of filters is up to you , generally the limitation you will run into is how much memory you have . a decent starting point is to have a few filters in the early layers , then as you do more maxpooling add more filters since the memory footprint for those layers will be less . EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ this looks promising . i thank you . EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ i recommend checking out these notes from a class taught at stanford : link. reading module NUM ( especially neural networks part NUM , part NUM and part NUM ) should be helpful . EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ check out michael nielsen's series : URL EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ this looks promising as well . thanks. EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ second of nielsen's book . it's really good at explaining the intuition of what is going on EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ derp . neutral-neural . thank you swype keyboard . EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ this is the book from my undergraduate class on machine learning . here. it's almost exclusively the mathematical theory behind machine learning , with additional e-chapters on modern advancements that you get for free ! EOA 
 a self contained resource for ml , in particular neutral networks ? : machinelearning i can highly recommend this book : URL EOQ appreciate it EOA 
 predicting rare events ; how to prevent machine learning algorithms from always picking the default class ? : machinelearning you need to choose the metric that you are optimizing appropriately . accuracy is often inadequate . think about how you will use the model ? is the class distribution fixed in your training and in the context of where your are deploying the model ? ideally you want to take into account the costs of missclassifications . false positives and false negatives ,.. precision and recall etc-you might want to take the harmonic mean of these ( f-measure ) or a weighted score . if you dont know the weighting of missclassifactions or the context that the model will be deployed in then roc-auc is a useful measure . here you are measuring the ranking performance rather than classification performance . you can then create a classifier by setting a threshold on the ranked scores . depending on what context the model is to be deployed in . EOQ i have encountered recently the same problem with you and found that conservative oversampling will do the trick . what you want to do , is focus on the examples of the minority class , especially the ones that are hard to classify and create similar synthetic versions of them . i actually implemented a python module for this procedure , you can check it out here : URL make sure you read the original paper too , to understand what this is doing . i hope i helped ! EOA 
 predicting rare events ; how to prevent machine learning algorithms from always picking the default class ? : machinelearning you need to choose the metric that you are optimizing appropriately . accuracy is often inadequate . think about how you will use the model ? is the class distribution fixed in your training and in the context of where your are deploying the model ? ideally you want to take into account the costs of missclassifications . false positives and false negatives ,.. precision and recall etc-you might want to take the harmonic mean of these ( f-measure ) or a weighted score . if you dont know the weighting of missclassifactions or the context that the model will be deployed in then roc-auc is a useful measure . here you are measuring the ranking performance rather than classification performance . you can then create a classifier by setting a threshold on the ranked scores . depending on what context the model is to be deployed in . EOQ upsample/upweight the rare class . downsample the common class . use an algorithm with built-in up-weighting of hard/missclassificed cases like boosting . use a different probability cutoff to make your final classification at the expense of more false positives . EOA 
 predicting rare events ; how to prevent machine learning algorithms from always picking the default class ? : machinelearning you need to choose the metric that you are optimizing appropriately . accuracy is often inadequate . think about how you will use the model ? is the class distribution fixed in your training and in the context of where your are deploying the model ? ideally you want to take into account the costs of missclassifications . false positives and false negatives ,.. precision and recall etc-you might want to take the harmonic mean of these ( f-measure ) or a weighted score . if you dont know the weighting of missclassifactions or the context that the model will be deployed in then roc-auc is a useful measure . here you are measuring the ranking performance rather than classification performance . you can then create a classifier by setting a threshold on the ranked scores . depending on what context the model is to be deployed in . EOQ hard negative mining might also be an useful technique here . let's say you initially train your classifier using a subset of your data . then you test on the remaining data . a hard negative is a falsely detected patch . you explicitly create a negative example out of that patch , and add that negative to your training set . now you retrain your classifier with the original data with the hard negatives . when you retrain your classifier , it should perform better with this extra knowledge , and not make as many false positives . you repeat this process until your model stops improving . EOA 
 predicting rare events ; how to prevent machine learning algorithms from always picking the default class ? : machinelearning you need to choose the metric that you are optimizing appropriately . accuracy is often inadequate . think about how you will use the model ? is the class distribution fixed in your training and in the context of where your are deploying the model ? ideally you want to take into account the costs of missclassifications . false positives and false negatives ,.. precision and recall etc-you might want to take the harmonic mean of these ( f-measure ) or a weighted score . if you dont know the weighting of missclassifactions or the context that the model will be deployed in then roc-auc is a useful measure . here you are measuring the ranking performance rather than classification performance . you can then create a classifier by setting a threshold on the ranked scores . depending on what context the model is to be deployed in . EOQ remove the default class . EOA 
 predicting rare events ; how to prevent machine learning algorithms from always picking the default class ? : machinelearning you need to choose the metric that you are optimizing appropriately . accuracy is often inadequate . think about how you will use the model ? is the class distribution fixed in your training and in the context of where your are deploying the model ? ideally you want to take into account the costs of missclassifications . false positives and false negatives ,.. precision and recall etc-you might want to take the harmonic mean of these ( f-measure ) or a weighted score . if you dont know the weighting of missclassifactions or the context that the model will be deployed in then roc-auc is a useful measure . here you are measuring the ranking performance rather than classification performance . you can then create a classifier by setting a threshold on the ranked scores . depending on what context the model is to be deployed in . EOQ something to consider might be scaling the weight of your common class , so that even though you have many more training examples , you only weight each one proportionally to how many examples there are of it . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ i thought about dtw . the issue of segmentation is important in this case as we don't know exactly when a sequence starts and ends . aslo, if the sensing system reports many features , dtw can become computationally very expensive. in such a system , specifying a loss function can be tricky since each feature would correspond to a different physical metric so designing a cost or loss function to estimate the distance between two series can be tricky . still , thanks a lot for the links . i'll look more deeply into it . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ the start and stop issue in dtw is typically fixed by performing a sliding window . also , if implemented well , dtw does not become expensive . both are explained in this publication : e. keogh , everything you know about dynamic time warping is wrong URL EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ you still have to hand set the band/parallelogram for the dtw distortion (the size of the diagonal trace , basically )-this is a design choice that may effect your performance . if you set it too skinny , you may miss good matches-the setting of that trace basically requires domain knowledge , and if you get it wrong it can really cause issues . this is no different than rnn activation choice/depth/whatever or what features to put into the hmm , but default dtw is the full matrix and most guarantees on solutions only hold in that case or the case where the band holds the right answer , which is likely for most reasonable problems but not guaranteed . algorithmically the core dtw has o(n2 ) computation versus hmm likelihoods or rnn forward passes which are both order o(n) during application ( not inference/learning ) iirc . however , actually implementing dtw ( even with banding ) is way , way simpler than either hmm or rnn and i think it is probably the best approach here , especially given the number of labels/samples involved ( a few hundred ) . there are a lot of engineering tricks to make dtw faster in practice ( including the things in the link above ) . you can also exploit tree structures with downsampled versions of dtw , choosing the n best matches , going up to full resolution and so on if you know that low dimensional structure is the most important . to /u/shapul specifically , finding features might be tricky for multidimensional stuff , but one nice trick is to make features which are offsets from the previous timestep ( pseudo derivatives ) . this makes the features less varied ( your arm can only move so far in a certain amount of time ) , and often helps classification for these kinds of timeseries . if you decompose a flattened matrix of these , the principle component activation locally ( over a sliding window ) when projected onto the global transform might give a vector that works for classification , and still encodes the multidimensional structure somewhat . this is just an idea though . you also might look at keogh's sax to turn things into a symbolic sequence , then working with that vector instead . it will be hard to learn features and classify with an rnn without more data-maybe try to find more publically available stuff or consider a smart form of data augmentation if you go that route . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ i believe dtw is not the right thing here . the reason is that a hand is essentially sth like a robot . if you use dtw , you will need to come up with a distance metric that is invariant e.g. to rotation . but even harder is that the dimensions are all tightly coupled : the acceleration at your finger tip depends a lot on the acceleration at its base . and it depends on the configuration of the joints in between . that means that the correlation between the different sensor readings depends on the state and is not constant . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ if you take the first derivative it is at least closer to rotation invariant-you will still have faster rotation in some dims then others but movements like left-right shifts will show up as constant across all features- the transform trick i talked about might also fix that for rotation . i agree that making the features is hard , but with only a few hundred (non-structured labeled ) samples i am not sure what other choice there is . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ my main concern with the dtw is when the time series is multidimensional : when you are measuring many things at each time sample (like e.g. in soli which probably reports tens of temporal and spectral measures ) dtw is hard to use since it cannot build a model out of the features on its own . i don't know a straight-forward solution for this problem . ( btw, thanks for the link , quite interesting ) . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ also , this example of dtw-knn might interest you . it was on here ~1 year ago , and does something very similar to what you are attacking . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ thanks a lot . yes, it is quite helpful . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ hidden markov model would be my first thought as its is essentially sequence data . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ i have little knowledge about hmms . do you know by any chance if there are easy to use frameworks for hmms in r or python ? in r e.g. i use caret to quickly test different classification methods but i am not sure how to quickly prototype with hmms . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ hmmlearn in python can work . i wrote a blog post on hmms a ways back (URL ) but the code is really rough , and doesn't handle sequence prediction ( no viterbi ) . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ i am sure there are but i have not used them , also matlab has URL EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ hmmlearn EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ if you can reconstruct the hand (perhaps in point cloud data , or even a NUM d black-white representation ) than you can fit a NUM d mesh on this . for instance , deepface uses this to transform a face looking sideways , into a frontal face picture . URL URL you may not need negative training data , if you pose this as a multi-class problem . the negative data would be all the other classes . to combat different timings , train two models : one to detect a single gesture , one to detect a gesture in the context of a sequence of gestures . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ i didn't know about icp and neural gas methods . thanks for the links ! also, i am not sure i understood your two model solution correctly . can you elaborate the point a bit more ? EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ there is a large body of work on detecting gestures from semg data . (URL ) if you want to detect complicated gestures , mind that hands are quite complex beasts : they are basically NUM-dof robots . even if you have NUM-different noisy and drifting acceleration sensors , you might still have an ill posed problem . now , you might say that you don't have to identify the whole kinematic state to discriminate different gestures . but then you will always be limited to some gestures and your system will stop working . i suppose the best way is some form of a nonlinear kalman filter . ukf if you fancy putting in the prior knowledge or one of the recently proposed ones based on variational inference , in case you can record massive amounts of data . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ have you thought about using a rnn with ctc (e.g. link , link)? EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ sounds interesting . thanks for the links . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ for ctc you would need structured labels e.g a sequence of labels-not just sequence x-> ; single label y . rather sequence x-> ; different length sequence y . i would first try a simple rnn or hmm . there is a nice example i also second looking at dtw as /u/radarsat1 said . it should work though scaling is always a concern , and will be easier to try than either hmm or rnn . how much labeled data will you have ? EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ thanks a for your advice . gathering training data by measuring gestures of human subjects is always very hard . i would say for each label maybe only a few hundreds trials would be feasible . as for dtw , the difficulty is with the fact that the data is multi-dimensional . mddtw exists but in my view it does not address the problem of model building and/or selection between subsets of features in a satisfactory way . EOA 
 how to use ml methods to detect hand gesture patterns ? : machinelearning i think quite often is used as a distance metric instead of simply comparing e.g. rms difference , as it provides a best distance considering differences in both the vertical and horizontal axes . ( therefore, more robust to differences in timing. ) apart from that , segmentation is your main issues , and then just looking up the closest match in a database , for which you have many possible options . another possibility is to break down each gesture into smaller segments and assign a code to them , then use a string edit distance as your metric , or search the string using some kind of fuzzy regular expressions . ( EOQ my buddies company does this-check out gest.co EOA 
 activation functions that have normalized output : machinelearning it seems there's no point in putting batch-normalization after your nonlinearity and before your next nonlinearity-as they're then redundant . what is the point of having relus that output only positive values if they're just going to be whitened anyway ? what alternative are you comparing this against ? there has to be some nonlinearity . does anyone use activation ( arctan? ) that output values centered at zero ? people use(d) tanh , which is similar to arctan , but it's generally found that relu performs better . EOQ you can have a look at this recent paper about advantages of activation functions with negative values here URL EOA 
 activation functions that have normalized output : machinelearning it seems there's no point in putting batch-normalization after your nonlinearity and before your next nonlinearity-as they're then redundant . what is the point of having relus that output only positive values if they're just going to be whitened anyway ? what alternative are you comparing this against ? there has to be some nonlinearity . does anyone use activation ( arctan? ) that output values centered at zero ? people use(d) tanh , which is similar to arctan , but it's generally found that relu performs better . EOQ putting aside strategy of then and where to use batch normalization , normalizing relu make perfect sense to me . normalization is subtracting average , and after that normalizing by variance . in the relu case average would be positive and activation go from [ NUM ,inf ] to [eps , inf], but input still cut away input at zero . what i want to say is that batch normalization affect relu even more then symmetric units , and therefore could be even more effective on relu . EOA 
 gpu size requirements for speech recognition : machinelearning as long as the model fits ( it probably will ) the minibatch size is the main thing that will dominate memory , both because the data is stored and because you need more gradient storage . it will most often work , but usually slower . more memory is always nicer , but don't forget baidu's huge practical push for training is distributed training-you aren't going to get hardware on their scale regardless of which gpu you choose . NUM gb is often better ( especially if it is gddr5 ) , but i am not convinced that it is worth spending NUM x your personal money , especially with pascal chips that support storing as NUM bit ( effectively giving NUM x memory ) coming around the bend . EOQ excellent points , thank you for the response . i've been using a gtx NUM ti up to this point ( NUM gb ) , so i'm already used to dealing with memory issues ( and smaller batch sizes ) . i was trying to hold out for pascal , but given that we probably won't see a pascal version of the NUM /titan until the fall i figured i'd pick up a card now . i think i can live with smaller mini batches on a NUM ti and saving some money to put towards a pascal card once they come to market . EOA 
 gpu size requirements for speech recognition : machinelearning as long as the model fits ( it probably will ) the minibatch size is the main thing that will dominate memory , both because the data is stored and because you need more gradient storage . it will most often work , but usually slower . more memory is always nicer , but don't forget baidu's huge practical push for training is distributed training-you aren't going to get hardware on their scale regardless of which gpu you choose . NUM gb is often better ( especially if it is gddr5 ) , but i am not convinced that it is worth spending NUM x your personal money , especially with pascal chips that support storing as NUM bit ( effectively giving NUM x memory ) coming around the bend . EOQ my input on this one : i don't do speech recognition , a lot of image processing nets though . and i was on you position considering NUM ti vs titanx , decided to go with the NUM ti , which is as fast as the x , has the same architecture and the memory speed is the same ( crazy NUM gb/s ) though i do regret now i haven't went the extra mile for the x card . it has it pros especially if you try to run existing models which you don't have the time to tune and make them fit better on your hardware . don't know what price you get there , for me NUM ti was NUM $ and the titanx was NUM $ so not really double the money and i think it's worthed if you can find the extra $ . EOA 
 gpu size requirements for speech recognition : machinelearning as long as the model fits ( it probably will ) the minibatch size is the main thing that will dominate memory , both because the data is stored and because you need more gradient storage . it will most often work , but usually slower . more memory is always nicer , but don't forget baidu's huge practical push for training is distributed training-you aren't going to get hardware on their scale regardless of which gpu you choose . NUM gb is often better ( especially if it is gddr5 ) , but i am not convinced that it is worth spending NUM x your personal money , especially with pascal chips that support storing as NUM bit ( effectively giving NUM x memory ) coming around the bend . EOQ just remember people were doing speech recognition way before we had even NUM gb cards-and alex graves did his ctc work on cpu , in c-! along with all his other foundational rnn work up until he went to deepmind afaik . so you can definitely do speech recognition work on either card . i think you have a good plan . EOA 
 gpu size requirements for speech recognition : machinelearning as long as the model fits ( it probably will ) the minibatch size is the main thing that will dominate memory , both because the data is stored and because you need more gradient storage . it will most often work , but usually slower . more memory is always nicer , but don't forget baidu's huge practical push for training is distributed training-you aren't going to get hardware on their scale regardless of which gpu you choose . NUM gb is often better ( especially if it is gddr5 ) , but i am not convinced that it is worth spending NUM x your personal money , especially with pascal chips that support storing as NUM bit ( effectively giving NUM x memory ) coming around the bend . EOQ i use the gtx NUM ( NUM gb of memory ) to train tdnns with kaldi . EOA 
 have anyone implemented skip-thoughts vectors using tensorflow ? : machinelearning i don't think there would be any improvements in tf over theano , based on my experience with recurrence in tf . EOQ i thought that tf would have improved their recurrence problem by now . EOA 
 have anyone implemented skip-thoughts vectors using tensorflow ? : machinelearning i don't think there would be any improvements in tf over theano , based on my experience with recurrence in tf . EOQ i'm not sure if they have released the looping primitives yet , plus theano's scan has had a lot of optimization done to it over the past year . EOA 
 have anyone implemented skip-thoughts vectors using tensorflow ? : machinelearning i don't think there would be any improvements in tf over theano , based on my experience with recurrence in tf . EOQ i have , but unfortunately can't share the code right now . i found there to be issues with memory compared to the theano version ( due to the mentioned issues with looping primitives ) . i think speed-wise it was fairly comparable , though. EOA 
 have anyone implemented skip-thoughts vectors using tensorflow ? : machinelearning i don't think there would be any improvements in tf over theano , based on my experience with recurrence in tf . EOQ is it working ? ready to share ? EOA 
 have anyone implemented skip-thoughts vectors using tensorflow ? : machinelearning i don't think there would be any improvements in tf over theano , based on my experience with recurrence in tf . EOQ how do skip thought vectors compare to skip lists ? EOA 
 pos-tagging with an rnn ? : machinelearning there are people who have tried , check this out:URL EOQ thank you this looks very interesting . i will check it out . EOA 
 pos-tagging with an rnn ? : machinelearning there are people who have tried , check this out:URL EOQ it does not make much sense to use seq2seq in this case . if you wanted to use an lstm , i would use a bidirectional one and output the pos tag based on the hidden state of each word . however, if you really want to use a model that can output variable-length sequences have a look at the paper : grammar as a foreign language . you will probably want to an attention model like that . have you tried a smaller dataset ( say NUM training examples ) and see if you model is still not able to overfit ? then it's likely an implementation error . if you don't use automatic differentiation , also make sure you do a gradient check first . EOA 
 pos-tagging with an rnn ? : machinelearning there are people who have tried , check this out:URL EOQ thanks for the feedback . i know the paper you're mentioning but ultimately i want to use this model to predict multiple annotations at the same time and using their architecture would results in a very long output which might strain the rnns memory . but i agree the bidirectional architecture might be more appropriate . the rational for seq2seq ( besides the fact that i already had it coded up ) is that i can use it to give the network a forward memory encoded in the hidden state of the first rnn . of course the bidirectional rnn is a more efficient version of this . i did actually try on a smaller training set ( in this case NUM examples ) and indeed though i didn't see it at first my network did eventually overfit ( quite significantly ... NUM % train vs NUM % validation ) . i realized i was miscounting parameters vs inputs...80k long sentences is actually several million words so roughly comparable to my networks parameters meaning its not obvious my original network should overfit . what still surprises me about all this is that a simple linear crf seems to do much better and much faster and generalizes very well . it gets NUM % on the validation set after only NUM min on the same dataset . the model i get is also very small ( NUM mb of memory ) so it beats the rnn on all fronts which is not what i expected . i would be interested to hear any explanations ( or maybe suggestions for a better architecture ) . i guess somehow predicting transitions is a very efficient approach in this case and i know nns just need a lot of training data but i'm a bit disappointed by the poor showing of rnns . EOA 
 pos-tagging with an rnn ? : machinelearning there are people who have tried , check this out:URL EOQ you are essentially asking your model to remember the entire sentence and store it in a fixed length vector . that is difficult and frankly surprising that it is possible at all . adding an attention mechanism such as grammar as a foreign language does , should help . EOA 
 pos-tagging with an rnn ? : machinelearning there are people who have tried , check this out:URL EOQ the crf postagger acheives NUM % accuracy with a NUM word backward and forward window of inputs . if i reduce it to only a NUM word backward window the accuracy ( on validation ) only goes down to NUM % . so clearly knowing the last NUM words should be enough to get good performance . put another way , even ignoring the encoder it should be able to match the NUM % ( which it isn't at least not without a much larger model , more training data , or much more time ) . as i see it the big advantage of the crf is that it can do joint prediction . it knows what it predicts for the previous tag and it models the entire sequence and picks the most likely combined output . the nn otoh does not know what the tags on the previous words are even if it predicts them ( because the output softmax is not recurrent ) . even if i modified the architecture so that at each timestep i feed back in the last prediction ( either in the input or by making the last layer recurrent ) that would only go on one direction ( whereas the crf does joint prediction for all timesteps ) . barring a poor implementation on my part ( which is very possible ) this is my guess for the real root of the difference ( i'm just surprised it leads to such a strong distinction in predictive power ) . EOA 
 pos-tagging with an rnn ? : machinelearning there are people who have tried , check this out:URL EOQ the code that baidu just released ( still on the front page , fast connectionist temporal classification ) sounds like it could be relevant . it's used for labelling unsegmented sequence data and pos tagging would be that , right? EOA 
 pos-tagging with an rnn ? : machinelearning there are people who have tried , check this out:URL EOQ piece of shit tagging already exists : the spam filter . EOA 
 fibonacci clock for time inputs : machinelearning pretty cool . how are the beats detected ? EOQ that's up to whatever you're feeding them into . i was adding them to a neural net along-side a basic time-series to give some sense of when the current input relates to . looking to help account for periodicity and add some temporal context . EOA 
 [seq2seq] deciding scheduled sampling curve-curriculum learning : machinelearning maybe you are also interested in this : URL EOQ wow this is a good find . definitely going to study this up . thanks! EOA 
 [seq2seq] deciding scheduled sampling curve-curriculum learning : machinelearning maybe you are also interested in this : URL EOQ the author /u/fhuszar is a regular here . EOA 
 [seq2seq] deciding scheduled sampling curve-curriculum learning : machinelearning maybe you are also interested in this : URL EOQ awesome ! thanks for the heasdup EOA 
 [seq2seq] deciding scheduled sampling curve-curriculum learning : machinelearning maybe you are also interested in this : URL EOQ maybe starting with e < ; NUM makes the model more robust . i don't know . interested to see more results . by the way , in listen , attend and spell they used a fixed e-NUM . EOA 
 [seq2seq] deciding scheduled sampling curve-curriculum learning : machinelearning maybe you are also interested in this : URL EOQ yes , that is what i'm sensing but i would just think it would want much guidance at first and then , once it has somewhat of an idea of what to guess , it starts learning on its own generated sequences . good to know that they do an e of NUM . EOA 
 hybrid dnn for voice modulation-help ! : machinelearning easy way-you don't need neural networks . just use classic signal processing ( vocal tract modulation , small amounts of time warping , pitch shift , and so on ) to modify the signal a little bit . if you want , you could have a neural network predict which things to use that still keep the essence of the sound , by minimizing error to some high level representation of the speech and predicting which tools to use ( see triplet networks ) . hard way-conditional speech synthesis is very hard . i would start by reading and understanding generating sequences with recurrent neural networks , graves. as for datasets , there is a single speaker blizzard dataset , as well as the multispeaker librespeech dataset . i recommend both of those over podcasts , as they have at least sentence/chapter level alignment text which could be advantageous . i can provide links to both of these later if you have trouble finding them . EOQ that was extremely helpful . thank you so much ! EOA 
 hybrid dnn for voice modulation-help ! : machinelearning easy way-you don't need neural networks . just use classic signal processing ( vocal tract modulation , small amounts of time warping , pitch shift , and so on ) to modify the signal a little bit . if you want , you could have a neural network predict which things to use that still keep the essence of the sound , by minimizing error to some high level representation of the speech and predicting which tools to use ( see triplet networks ) . hard way-conditional speech synthesis is very hard . i would start by reading and understanding generating sequences with recurrent neural networks , graves. as for datasets , there is a single speaker blizzard dataset , as well as the multispeaker librespeech dataset . i recommend both of those over podcasts , as they have at least sentence/chapter level alignment text which could be advantageous . i can provide links to both of these later if you have trouble finding them . EOQ why not use podcasts instead ? EOA 
 hybrid dnn for voice modulation-help ! : machinelearning easy way-you don't need neural networks . just use classic signal processing ( vocal tract modulation , small amounts of time warping , pitch shift , and so on ) to modify the signal a little bit . if you want , you could have a neural network predict which things to use that still keep the essence of the sound , by minimizing error to some high level representation of the speech and predicting which tools to use ( see triplet networks ) . hard way-conditional speech synthesis is very hard . i would start by reading and understanding generating sequences with recurrent neural networks , graves. as for datasets , there is a single speaker blizzard dataset , as well as the multispeaker librespeech dataset . i recommend both of those over podcasts , as they have at least sentence/chapter level alignment text which could be advantageous . i can provide links to both of these later if you have trouble finding them . EOQ yeah i thought of that too , but how can i use both ebooks and audiobooks together to train a network so that it can narrate new ebooks better ? EOA 
 hybrid dnn for voice modulation-help ! : machinelearning easy way-you don't need neural networks . just use classic signal processing ( vocal tract modulation , small amounts of time warping , pitch shift , and so on ) to modify the signal a little bit . if you want , you could have a neural network predict which things to use that still keep the essence of the sound , by minimizing error to some high level representation of the speech and predicting which tools to use ( see triplet networks ) . hard way-conditional speech synthesis is very hard . i would start by reading and understanding generating sequences with recurrent neural networks , graves. as for datasets , there is a single speaker blizzard dataset , as well as the multispeaker librespeech dataset . i recommend both of those over podcasts , as they have at least sentence/chapter level alignment text which could be advantageous . i can provide links to both of these later if you have trouble finding them . EOQ try using some hybrid networks , check out gans and rnn.dbn rnn.blstm EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ not the same thing ... nobody forces you to use google's services ! EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ feels bad man for a reason EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ nobody forces someone to get a job paying a salary below minimum wage . yet it's still illegal for a reason . EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ if you think that's a defense , you seriously need an ethics refresher . EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ it's a standard captcha from a library , so not scholar specific . you see the same when you sign up for a new gmail account for example . the exact test used by the library varies and does likley generate training data for ml , yes. EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ google owns recaptcha , which has pretty obviously been used in this way for several years . EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ they explicitly say that they do creation of value on their website. URL EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ sometimes you see strange captchas on facebook , i feel that's when fair needs some new data ... EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ standard captchas are easily solved by computer programs these days , the new google captchas aren't that easy for a computer , but arguably easier for people . but that aside , they're definitely using it to gather data , why wouldn't they ? EOA 
 is google using captchas on google scholar to improve its training corpus for image recognition ? : machinelearning i would argue possibly . if i were to employ someone all day and pay them $0 for doing this task a million times , that would be labor exploitation , and illegal in many places due to minimum wage rules . whereas employing a million people for NUM seconds each for $0 is okay . EOQ believe it . (also believe that i am a real dr.). dear dr. vodkagoodmeatrotten , hope this email finds you in good spirits . i'm really pleased and can't wait to let you know that we have indexed all our articles in google scholar too , another step of accomplishment for our medcrave in spreading your research to the scholars . i feel really proud to work with distinguished author like you that will guide me to learn more important studies . also i look forward to have your valuable ideas , suggestions and contributions for my journal that can put an edge over others . i feel pleasurable , if you can submit any kind of article for this coming issue only , as i plan to complete one more issue with your kind submission . once again i honestly thank for being with us in all the success . hope you understand and foresee to hear from you soon . kindly acknowledge this email receipt within NUM hours . await your reply . best regards , christine taylor editorial office-medcrave group biometrics & biostatistics international journal email : biometrics@medcraveonline.org google has long used creative tricks to get crowds to label data . history: URL by luis von ahn ( NUM ) it's ethical as long as they only use it for detecting bots and getting some image labels . they could use it to parse the text on pages you post on using recaptcha and enrich your advertisement profile with the contents of NUM chan threads . that would be unethical to me ( but not so possible to check if this is happening ) . obligatory : URL EOA 
 classifying unevenly distributed data . : machinelearning your problem is probably that the classifier is trying to maximize accuracy (tp-tn)/(tp-tn-fp-fn) . suppose you have a dataset of NUM examples , distributed between classes a and b ( with NUM and NUM examples respectively ) . one classifier gives you perfect accuracy in class a but fails to find any of the NUM examples of b ( NUM ,9% accuracy ) another classifier correctly classifies NUM examples of a and NUM of b . that gives an accuracy of ~98% . which one is better though ? the basic approaches to that problem , afaik, are cost-sensitive learning ( if your classifier supports weights ) and sampling . in the first case , a cost matrix ( like this one ) is configured so that errors in the rare class become more important than errors in the common class . in the second case , your two options are subsampling ( leave some common examples out ) and supersampling ( repeat some rare instances in the dataset ) . empirically, subsampling gives better results , if the examples of the rare class are enough ( enough depends on the difficulty of the problem , ie your dimensions ) . to answer your question though , asymmetric bagging1 , smote2 and cost-sensitive classification2 are some techniques that are supposed to work , with the first one usually giving better results . i'm not sure , however, if they're still considered state of the art . there are some additional approaches that might be helpful , such as tomek links2 ( practically a definition of controversial points in your dataset ) , which clean up the vector space from garbage in an attempt to simplify the classification process . more detailed info in : tao , dacheng, et al . asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval . pattern analysis and machine intelligence , ieee transactions on NUM ( NUM ) : NUM-1099. he , haibo, and edwardo garcia . learning from imbalanced data . knowledge and data engineering , ieee transactions on NUM ( NUM ) : NUM-1284. you can pm me if you need any specific resources . hope that helps a bit , i'm still trying to wrap my head around some of these concepts . :) EOQ another alternative are restricted boltzmann machines-as part of the neural computing class at university last year i compared smote to rbms for boosting classes with synthetic data . EOA 
 classifying unevenly distributed data . : machinelearning your problem is probably that the classifier is trying to maximize accuracy (tp-tn)/(tp-tn-fp-fn) . suppose you have a dataset of NUM examples , distributed between classes a and b ( with NUM and NUM examples respectively ) . one classifier gives you perfect accuracy in class a but fails to find any of the NUM examples of b ( NUM ,9% accuracy ) another classifier correctly classifies NUM examples of a and NUM of b . that gives an accuracy of ~98% . which one is better though ? the basic approaches to that problem , afaik, are cost-sensitive learning ( if your classifier supports weights ) and sampling . in the first case , a cost matrix ( like this one ) is configured so that errors in the rare class become more important than errors in the common class . in the second case , your two options are subsampling ( leave some common examples out ) and supersampling ( repeat some rare instances in the dataset ) . empirically, subsampling gives better results , if the examples of the rare class are enough ( enough depends on the difficulty of the problem , ie your dimensions ) . to answer your question though , asymmetric bagging1 , smote2 and cost-sensitive classification2 are some techniques that are supposed to work , with the first one usually giving better results . i'm not sure , however, if they're still considered state of the art . there are some additional approaches that might be helpful , such as tomek links2 ( practically a definition of controversial points in your dataset ) , which clean up the vector space from garbage in an attempt to simplify the classification process . more detailed info in : tao , dacheng, et al . asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval . pattern analysis and machine intelligence , ieee transactions on NUM ( NUM ) : NUM-1099. he , haibo, and edwardo garcia . learning from imbalanced data . knowledge and data engineering , ieee transactions on NUM ( NUM ) : NUM-1284. you can pm me if you need any specific resources . hope that helps a bit , i'm still trying to wrap my head around some of these concepts . :) EOQ this kinda depends on the classifier you want to use . for random forests you can add a class weight ( see scikit's learn random forest : class.weight for example ) you can set the weight so that both classes have an equal distribution of num.examples-weight for neural networks it has helped me in the past on click data to do equal subsampling for the first x iterations . equal subsampling means that each batch has NUM % of class NUM and NUM % of class NUM . this helps the network to get in a state where it has features for both classes . if the imbalance is not too big , it's possible to change the batch distribution back to the original after convergence and optimize that . EOA 
 classifying unevenly distributed data . : machinelearning your problem is probably that the classifier is trying to maximize accuracy (tp-tn)/(tp-tn-fp-fn) . suppose you have a dataset of NUM examples , distributed between classes a and b ( with NUM and NUM examples respectively ) . one classifier gives you perfect accuracy in class a but fails to find any of the NUM examples of b ( NUM ,9% accuracy ) another classifier correctly classifies NUM examples of a and NUM of b . that gives an accuracy of ~98% . which one is better though ? the basic approaches to that problem , afaik, are cost-sensitive learning ( if your classifier supports weights ) and sampling . in the first case , a cost matrix ( like this one ) is configured so that errors in the rare class become more important than errors in the common class . in the second case , your two options are subsampling ( leave some common examples out ) and supersampling ( repeat some rare instances in the dataset ) . empirically, subsampling gives better results , if the examples of the rare class are enough ( enough depends on the difficulty of the problem , ie your dimensions ) . to answer your question though , asymmetric bagging1 , smote2 and cost-sensitive classification2 are some techniques that are supposed to work , with the first one usually giving better results . i'm not sure , however, if they're still considered state of the art . there are some additional approaches that might be helpful , such as tomek links2 ( practically a definition of controversial points in your dataset ) , which clean up the vector space from garbage in an attempt to simplify the classification process . more detailed info in : tao , dacheng, et al . asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval . pattern analysis and machine intelligence , ieee transactions on NUM ( NUM ) : NUM-1099. he , haibo, and edwardo garcia . learning from imbalanced data . knowledge and data engineering , ieee transactions on NUM ( NUM ) : NUM-1284. you can pm me if you need any specific resources . hope that helps a bit , i'm still trying to wrap my head around some of these concepts . :) EOQ there are many approaches you can use . as a summary , you can set a class weight ( at least in libsvm/scikit ) , approximately equal to the class ratio , bumping the importance of the least represented classes . alternatively, you can over sample ( repeat examples ) of the minority classes or subsample the majority class ( over sampling is usually preferred since you don't lose data ) . EOA 
 help: lip reading using deep learning : machinelearning URL should be a decent starting point EOQ this is a very cool project . i'm not sure how i would approach it . the thing is : usual cnns accept images as entries and i think you would need to give a group of frames ( part of a video ) to the cnn and group them with one exit ( probably the word associated ) . i'm not familiar to rnn and others deep networks , but a starting point would be to study how speech to text is done . i believe you can find good tutorials about it over the internet . it may help you a little about how to tackle your problem . pls , if you find out how to do it , don't forget to tell us how you did it ! EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ oh wow i didn't know it had a unique name . thank you ! EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ researchers keep making up these complex names for simple and obvious concepts . as far as i understand it's really just a cnn that feeds into an rnn . i've seen this used in quite a few papers , just without the fancy names . op : you can implement that with any library that has support both cnns and rnns , like theano , keras or tensorflow , and then you can train the model end-to-end . EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ yeah it seems like no matter how much i know there's always another fancy name i've never heard of . i think i'll see if i can get it done in tensorflow . also, it seems our friend up there deleted his comment , so what was the complex name he used ? it's on the tip of my tongue . thanks! EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ wow you managed to find the three most ancient libraries . EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ haha my fight to get to the front of artificial intelligence current events has been not even mildly successful . EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ with theano there's nothing stopping you from bolting those two concepts together . EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ theano seems to be the popular choice. is it better than keras or tensorflow , as mentioned above ? EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ keras is a library built on top of theano ( and now tensorflow ) , while tensorflow/theano just implement the primitives which you use to build whatever you want . EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ from keras.io: supports both convolutional networks and recurrent networks , as well as combinations of the two . so i should definitely use that . EOA 
 library for convolutional recurrent neural networks ? : machinelearning [ deleted ] EOQ[?]mibo12[s]&#32;0 points1 point2 points&#32;26 days ago&nbsp;(0 children)EOQ if keras has what you are looking for then thats fine , if it doesn't then you would have to implement it in theano/tensorflow anyway . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ yes , data is NUM d images ( spectrograms ) of music , and labels are topic vectors of mood tags . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ keras applies batchnormalization and prelu per unit instead of per feature map . that makes them almost useless for images or time-series and might the cause of your issues . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ thanks . especially bn per unit for convnet sounds suspicious . is prelu per unit on convolution layer also a bad idea ? EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ i think it's updated after you checked it out . doc says NUM : feature-wise normalization . if the input has multiple feature dimensions , each will be normalized separately ( e.g. for an image input with shape (channels , rows, cols ) , each combination of a channel , row and column will be normalized separately) . NUM : sample-wise normalization . this mode assumes a NUM d input . with a default value of NUM , although i don't fully understand how it is separately normalising . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ i see , thank you very much . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ seems like it is fixed now by this . probably you fixed it ? anyway it is now merged in the master build . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ it wasn't me , but thanks for the info ! EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ simpler convnet with only relu , pooling, and l2 regularisation with fully connected layers ( and no sigmoid at output ) shows slow convergence but same results . probably my data is not correlated . still it's mysterious how it it can converge like that . within NUM or NUM mins it converges perfectly then sometimes it says there was no val.loss change at all . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ i've found pre training to be useful for most regression tasks . see if you can approximate the cost function using a classification task , train the network using this cost function , then transfer the weights to the regression model . EOA 
 my miserable failure on training convnet for regression : machinelearning could you describe the dataset you're using ? EOQ my current setting is not very different from classification task as it is a multi-class task . i gave it another shot by approximating it as a NUM-of-k label but still it hangs as the same . perhaps the problem lies on in more subtle implementation than the structure . but thanks , and i'll consider pre-train after i escape . thanks. EOA 
 how to start training any model on an audio input ? : machinelearning there are end-to-end models that use audio signal as input and time-frequency representation-basedm models that uses things like stft , mel-spectrogram, and cqt . some works even rely on conventional features such as mfcc . check out some audio processing packages e.g. librosa in python . for music signals with convnet , i summarised a list of settings and hyperparameters of previous research articles in NUM-2015 on , which is also introduced with brief explanation in . hope you find it useful . i was thinking of posting the link then read your post :) what a coincidence . EOQ thank you . EOA 
 how to start training any model on an audio input ? : machinelearning there are end-to-end models that use audio signal as input and time-frequency representation-basedm models that uses things like stft , mel-spectrogram, and cqt . some works even rely on conventional features such as mfcc . check out some audio processing packages e.g. librosa in python . for music signals with convnet , i summarised a list of settings and hyperparameters of previous research articles in NUM-2015 on , which is also introduced with brief explanation in . hope you find it useful . i was thinking of posting the link then read your post :) what a coincidence . EOQ log spectrum is the input for deep speech NUM or the predecessor by fernandez , graves, schmidhuber which is a fairly straightforward architecture to try ( you can just use matplotlib for the spectrogram piece , and a basic theano cnn-rnn-ctc to get started ) , and will get a semi-working recognition system going in a hurry . there are many implementations of the ctc cost in theano , including a version i keep as a gist for minibatches . it does not work on gpu yet , but can with a small change . most higher order features ( in my experience , at least ) are log-mel spectrum ( available in librosa ) , fmllr ( not there as far as i know ) , plp, or others . in addition , tricks like vocal tract length normalization can be an important performance boost but there are not many ( or any ) off the shelf implementations in python/lua ( or anything besides kaldi ) that i know of . you will also want to incorporate a language model-which as far as i know is not explicitly described in the papers i linked above . you can do it post-facto but might do better incorporating it directly with something like deep fusion / conditioning . the current state of the art is largely set by something called hybrid systems made by combining dnn/cnn/rnn with hmms , but training these types of models is much more involved than the end-to-end approach . if you go this route , the pdnn package may be of interest . in addition to the points mentioned here , you can also use raw speech , though it will take a lot more work ( and data ) . EOA 
 how to start training any model on an audio input ? : machinelearning there are end-to-end models that use audio signal as input and time-frequency representation-basedm models that uses things like stft , mel-spectrogram, and cqt . some works even rely on conventional features such as mfcc . check out some audio processing packages e.g. librosa in python . for music signals with convnet , i summarised a list of settings and hyperparameters of previous research articles in NUM-2015 on , which is also introduced with brief explanation in . hope you find it useful . i was thinking of posting the link then read your post :) what a coincidence . EOQ the most important thing is what you want to do with the audio data . if you want to classify or translate it , all of what u/keidouleyoucee and u/kkastner said is super useful . if you want to reconstruct new audio from the model , you're gonna want to either use the raw signal , or unprocessed fourier transform . EOA 
 how to start training any model on an audio input ? : machinelearning there are end-to-end models that use audio signal as input and time-frequency representation-basedm models that uses things like stft , mel-spectrogram, and cqt . some works even rely on conventional features such as mfcc . check out some audio processing packages e.g. librosa in python . for music signals with convnet , i summarised a list of settings and hyperparameters of previous research articles in NUM-2015 on , which is also introduced with brief explanation in . hope you find it useful . i was thinking of posting the link then read your post :) what a coincidence . EOQ you can also use the spectrogram with spectrogram inversion tricks . though you might also want to try predicting vocoder parameters directly at that point-i have never had much luck predicting complex values . EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ another relevant paper on precipitation nowcasting EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ so you're trying to predict financial time series eh ? EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ actually it's unrelated . though the answer would certainly be suitable for financial series i suppose . my problem is micro weather effects . EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ in that case , an rnn is capable of learning those features in principle as long as the period is long enough to include both the feature and response . whether it will learn them , depends on many factors . EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ but the stock market is random . EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ not random , it is non-linear . attempting to extrapolate non-linear and non-periodic signals is in general a non-trivial problem . how far into the future you are able to say things with confidence depends heavily on how strong your priors are . EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ it's hard to offer you advice if we know nothing about your data and your goals . anyway , you could train different rnns on different time periods . one rnn reads one hour of measurements , predicts the next hour . another rnn reads a whole day , predicts the next day . ... then make use of them in an ensemble for whatever you're trying to do . EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ is an ensemble of rnn's on different time resolutions a common thing to do with time series ? can you cite any papers that do this ? thanks. EOA 
 what is the limit of rnn/lstm when applied to time series ? is it capable of determining anomalies preceding events ? : machinelearning here EOQ htm EOA 
 what is neural network used in word2vec for ? : machinelearning the two descriptions you give of word2vec are equivalent . the machine-learning algorithm by tomas mikolov and collaborators can indeed be described as you did as a log bilinear model with a cost function that is optimized through stochastic gradient descent . it can be equivalently represented by a neural network with a single , linear hidden layer . the network is somewhat special in that the synaptic weights connecting the input and hidden layer are the objects of interest as they give the word embeddings . typically , one couldn't care less what the synaptic weights in a network are . for completeness , the word2vec tool actually consists of two different algorithms ( skip-gram and cbow ) and provides two learning methods ( negative sampling and hierarchical softmax ) . EOQ oh i see .... thank your for the answer ! EOA 
 what is neural network used in word2vec for ? : machinelearning the two descriptions you give of word2vec are equivalent . the machine-learning algorithm by tomas mikolov and collaborators can indeed be described as you did as a log bilinear model with a cost function that is optimized through stochastic gradient descent . it can be equivalently represented by a neural network with a single , linear hidden layer . the network is somewhat special in that the synaptic weights connecting the input and hidden layer are the objects of interest as they give the word embeddings . typically , one couldn't care less what the synaptic weights in a network are . for completeness , the word2vec tool actually consists of two different algorithms ( skip-gram and cbow ) and provides two learning methods ( negative sampling and hierarchical softmax ) . EOQ word vectors is the weight matrix between input layer and hidden layer . how can you get vectors without using neural net ? EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ thanks ! that makes a lot of sense . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ note : in your link , it is said that algebra is an example of physical symbol system . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ that two things are equivalent does not mean that one is not a lot nicer to work with . so you can implement a turing machine in silicone or in magic the gathering , but booting linux on magic is somewhat tedious . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ i think your metaphor is a bit messed up . attempt at unification : turing machine-> ; physical symbol system algebra-> ; silicone ?-> ; magic ? i think don't there's a way to unify your example . a square is a rectangle . algebra is a symbol system . you can't say you're trying to do x so you can replace rectangles with squares . that doesn't make sense . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ i am using rather loose sense of equivalent here . so magic , the trading card game , is proven to be turing complete and so it is sort of equivalent to a modern processor . similarly, i would guess that you can implement arbitrary physical symbol systems in sufficiently loosely defined algebra ( certainly if we include categories ) but has the tendency to play a lot nicer with gpus than most traditional formal logic engines . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ classical logic systems ( logic theorist , etc ) weren't able to scale because , like you said , the world isn't neatly divided into logical symbols . the evolved version of those theories take the symbol grounding problem as the core problem , specifically harnad (1990)'s version . the way you've presented the argument is a straw man for what that proposes . the proponents of systems that ground symbols in the world would claim that people divide the world up into regularities and classify continuous perceptual input into discrete categories . then , they make inferences ( particularly interesting are the abductive inferences which explain the poverty of information issue in dialogue ) . i'm curious as to what you think about the claim that any system that categorizes to discrete structures and uses regularities among those discrete structures ( presumably also learned ; hand coding is a straw man here ) to make inferences is , in part , a symbol system . i've always found it mildly insane to posit one solution over the other . of course , there's continuous space representations in learned feature spaces . we wouldn't be able to pick out regularities if there wasn't . but also , of course there's symbol systems which bind specific instances in context to schemas of learned structure . we wouldn't be able to generalize without a discrete structure to generalize to . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ regarding your second to last paragraph . i'm fine with that . if you were to suggest that operating on discrete structures is necessary , that's where our intuitions would begin to diverge . in the last paragraph , i'm really not convinced that generalizing is impossible without discrete structures to generalize to . google's old eigencat seems like a good example of a generalization that was made without any discretizing ( past what is necessary to represent in a computer ) . another is that nn that learned to play atari games just by trying to maximize the displayed score . these days you can't just say that having tons of data isn't enough to generalize without a very compelling argument ... EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ that's not what i meant by that , i was talking about the discretizing of an imagine into a finite number of pixels each of which are one of a finite number of colors . the number of possible NUM x600 bitmaps is finite , and thus it is in that sense discrete . that said , the number of possible NUM x600 bitmaps is vastly larger than astronomical , so i'm comfortable calling it approximately continuous . if your symbol system has as many symbols as there are images that's fine , but then they have no explanatory power ... the eigencat example in particular really was achieved purely with matrix operations , it was just a big nn running on a thousand or so cores . we do communicate in continuous wavelengths ( and to a lesser extent with discrete words ) ! i can put an incredibly subtle nuances into what i'm saying just by varying my pitch and timing , and if i vary it half as much it conveys a fraction of the relevance of the nuance . if you have a girlfriend try this ; have an entire conversation using only the word mer ... it's not even hard . if you don't have a marklar there's a similar marklar you can try where marklar replaces all the marklars in a marklar with 'marklar' . put that in your symbol system and smoke it . i disagree with you that discrete semantic concepts makes parsimonious sense , precisely because it doesn't make any evolutionary sense . are you going to argue that pre-historic fish had discrete semantic concepts ? if not , why would there not be a clear evolutionary path to take the nervous systems they pioneered and scale them up ? discrete buckets are easier for us to reason about , but that's very different from being simply defined , and nature has shown a very clear bias towards simple definitions that we find very difficult to reason about . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ the symbol approach is something like google's knowledge graph . entities like paris connected by relations like capital of to entities like france . to solve an analogy like moscow : russia :: paris : ? in a symbolic logic system , you would have to traverse the graph to find relations between moscow and russia and then look at the corresponding relations of paris to find out that france is the answer . the word2vec system has been shown to be able to solve this kind of analogy in a different way . after translating the words into vectors ( fixed length lists of numbers ) , the analogy can be solved by simply adding and subtracting the vectors . subtracting the vector for russia from the vector for moscow gives a vector which represents the concept of a capital city . adding this concept vector to paris gives a vector which is close to the vector for france . other papers have since constructed vectors for not only words but whole sentences and paragraphs . it is speculated that vectors similar to these can represent complete thoughts , and natural reasoning of the sort that humans do ( not formal logic ) can be represented as arithmetic on these vectors . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ symbolic ai is based on logical manipulation of formal symbols . it can't handle uncertainty , so it only works in special domains with perfect knowledge . replacing discrete binary symbols with real-valued vectors and binary logic with real algebra refers to tensor methods ( such as anns ) , which can approximate probabilistic inference to handle uncertainty in the right way and thus provide a complete foundation for computational intelligence . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ i was at the conference and noted that he referenced this paper somewhere near that statement : EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ when watching a horror movie , ever had the feeling you knew who the killer was , before this was verified ? governments and armies want to automate this intuition ( let's just say to capture plotting terrorists ) . this goes beyond the current capabilities of convnets , into the realm of reasoning , memory, and logic . we have word2vec . this translates a word/symbol into a vector , where semantically similar words have similar vectors and you can do a basic form of algebra : ( vector.king-vector.queen )-vector.man-vector.woman . this spawned phrase2vec and doc2vec . lecun wants to go one step beyond : world2vec. translate everything ( be it a feeling , a process , an object , a social network , a set ) into vectors . a simple form of symbolic logic is first-order predicate logic . for history , see hilbert's foundations of mathematics : no more than any other science can mathematics be founded by logic alone [ ... ] if logical inference is to be reliable , it must be possible to survey these objects completely in all their parts , and the fact that they occur , that they differ from one another , and that they follow each other , or are concatenated , is immediate , given intuitively , together with the objects , is something that neither can be reduced to anything else nor requires reduction . this is the basic philosophical position that i regard as requisite for mathematics and , in general , for all scientific thinking , understanding, and communication . i shall now present the fundamental idea of my proof theory . all the propositions that constitute in mathematics are converted into formulas , so that mathematics proper becomes all inventory of formulas . these differ from the ordinary formulas of mathematics only in that , besides the ordinary signs , the logical signs : ⇒ ( implies ) , & ( and ) , v ( or ) , ~ ( not ) , ∀ ( x ) (for all) , ( ∃x ) (there exists) URL if one were to convert these logical symbols into vectors , such that a neural net can use these vectors through maths/information theory , then you've replaced symbolic logic with neural network algebra , and open the door to machine reasoning , instead of machine learning . such an approach should be able to logically parse the lord of the rings and tell you who currently has the ring ( and perhaps , what his/her motivations are ) . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ i didn't attend the conference , can you please provide some more context ? anyway , he may have been referring to something like recursive neural networks can learn logical semantics by the stanford group . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ hinton proposed using vectors to represent symbols for semantic networks in hinton , g. e . implementing semantic networks in parallel hardware . in hinton , g. e . and anderson , j. a ., editors, parallel models of associative memory , erlbaum, hillsdale , nj ( NUM ) . basically the idea of thought vectors . now we have deep learning so we can effectively learn those vectors and operations . EOA 
 eli25: we are attempting to replace symbols by vectors so we can replace logic by algebra .-yann lecun : machinelearning disclaimer , i wasn't at that conference and i'm not familiar with yann lecun's work . what follows is my guess . a long time ago ai was based on a hypothesis called the physical systems symbol hypothesis ( URL ) that said , basically, things should be represented by symbols , a term that everyone was wise enough to not try to define beyond the understanding that symbols were in some sense discrete entities that were acted upon to give rise to intelligence . formal logic fits neatly into the pssh , and indeed you can do some vaguely interesting things using it . the biggest problem with the pssh , however, is that it doesn't really work in the real world . sure , given enough time a physical symbol system can play a mean game of chess , but when the task is pick up the laundry from the cleaners it fails in pretty much every possible way because the real world never discretizes nicely and errors tend to compound . machine learning , on the other hand , works with vectors of (approximately-)continuous variables , and in so doing embraces the messiness of the real world . what i take from yann's quote is one of two things ( i'm not sure which , perhaps context would help , but i'm lazy af ) either he's summarizing the transition from discrete ai to continuous ml , or he's talking about specific work he's involved with that is attempting to generalize formal reasoning techniques into the continuous domain . i'm really not sure which . EOQ few years back i proposed almost exactly similar idea to my stat-nlp professor as a class project . since prof lecun hasn't solved it yet , i guess my professor was right in suggesting holding on to my horses and do something more feasible in NUM weeks . EOA 
 convolutional autoencoder-simple explanation please:) why deconcolutions and unpooling ? : machinelearning what is the difference between a deconvolution and a convolution ? isn't the convolution a linear operator ? i never quite grasped this . EOQ they're sometimes also called fractionally-strided convolutions . convolutions map multiple inputs to a single output and deconvolutions map a single input to multiple outputs . the backward-path of a convolution is a deconvolution and vice versa . deconvolutions are implemented as doing the backward-path of an imaginary convolution on the output . EOA 
 any good references to start a literature survey on machine learning/data mining applications for smart grids ? : machinelearning there's a group at columbia univ . look up roger anderson . EOQ thanks ! EOA 
 [sklearn]does the fit function find weights ? : machinelearning it does . the weights are generally called 'parameters' . parameter 'estimation' , 'learning', and 'fitting' can be used almost interchangeably . statisticians are more inclined to say estimation while someone in ai/ml will say learning . EOQ i c , could i run gd first and then transfer those parameters to another classifier ? EOA 
 [sklearn]does the fit function find weights ? : machinelearning it does . the weights are generally called 'parameters' . parameter 'estimation' , 'learning', and 'fitting' can be used almost interchangeably . statisticians are more inclined to say estimation while someone in ai/ml will say learning . EOQ there are methods to get and set the weights/parameters . so yeah you can , so long as you are doing something sensible EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ i would argue that's pretty close to the bayesian statistical interpretation ( hyperparameters-parameters of the prior-prior knowledge not learned from the data in question ) . it also includes the initial state of the parameters ( in a neural network for instance ) . without any training step all parameters are hyperparameters . if you magically pick the right hyperparameters you don't need any training data at all . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ there are a lot of weird definitions being tossed around in here . but in statistics and statistical learning a hyperparameter is defined simply as the fixed parameter to your prior probability function . suppose that your data are distributed under a binomial distribution bin(n , p) and you place a beta prior on p (p ~ beta(alpha , beta)). in this case , alpha and beta are hyperparameters to your model , p is a parameter ( in some weird sense , as it is more like a random variable now ) . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ lets take an svm with an rbf . it has weights which are the vector params . then it has the c and gamma param . those are params set by users , and they can be cross-validated and adjusted with an algorithm like grid search . the c and gamma are the hyper params , those will be constant throughout the training . another example of a hyper param is the k param in the k cluster means algorithm . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ i don't think that hyperparameters in general need to be constant throughout training . learning rates for instance can be changed and i think they would usually be considered hyperparameters EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ what i meant was that hyper parameters where held fixed for parametric fitting . hyper parameters can be tested with methods like grid search , and certain iterative models of regression will experiment with lambda terms along with learning rates . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ i consider it to be any choice which can't be done with gradient decent , there may be a more formal definition though . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ hyper parameters of gaussian processes are typically set by gradient descent . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ i would expand it a bit and say the parameters of the model not usually optimized/able to be optimized during the model optimization/inference loop . that would handle the fact that gp-based hyperparameter optimization ( or even random search ) can be used on hyperparameters . also the fact that information from a regular gp can be used to set its own hyperparameters , though it is not necessary to do in order to fit a gp for a given problem . neural network hyperparameters can also be learned by gradient descent , cf . however , i would still call them hyperparameters by convention , or by this definition of not done during model optimization/inference . it also covers the fact that there are algorithms with multiple steps such as any em type algorithm or rbms learned with contrastive divergence-i don't consider the v or h in an rbm to be hyperparameters for example , since it is all learned in the optimization loop , though it is done piecewise. however , something like the size of the rbm hidden would be a hyperparameter , since you don't optimize it during the main loop ( infinite rbm non-withstanding , where beta implicitly controls hidden size ) . from a general perspective , it is turtles all the way down . pure bayesian stats would require hyperpriors for priors , hyperhyperpriors for hyperpriors , and so on . hyperhyperparameters, hyperparameters , and so on could be thought of the same way . my practical preference is to stop at the hyperhyper level , which allows for setting the kernels for gp based hyperparameter search or the spaces/scales for random search , as well as setting learning rates , optimizer type , and so on if not doing hyperparameter optimization . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ well then maybe this isn't the best definition :) EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ usually , we think of three categories : model design , model parameters , and training hyperparameters . the activation functions , number of layers , choice of evaluation function , type of kernel , type of learner-these are all questions of model design rather than hyperparameters . a model with NUM layers is a different model than a model with NUM layers . hyperparameters are parameters of the training function . contrary to some views , hyperparameters can be selected by gradient descent . that's basically what we do when we try to train a model with lots of different combinations and see what works . we usually don't automate the selection of hyperparameters by gradient descent because that would be computationally infeasible . but there's no reason why , in principle , we couldn't use a training algorithm to select hyperparameters . EOA 
 what is considered a hyperparameter : machinelearning [ deleted ] EOQ[?]mljoe&#32;9 points10 points11 points&#32;26 days ago-&nbsp;(0 children)EOQ all of the above except the initial weights are hyperparameters in most settings . there are some approaches that automaticaly optimize some of these values , making them parameters rather than hyperparameters . EOA 
 how to implement nearest neighbour lookup in a point cloud ? : machinelearning vantage point trees are much better than kd trees at high dimensionality . there's a fast-ish c-implementation as part of the implementation of t-sne here : URL you should be able to hack out the vp trees for your own usage :) EOQ URL a kd tree is an efficient structure for doing accurate k nearest neighbor searches in low dimensional spaces (d < ; < ;100). EOA 
 how to implement nearest neighbour lookup in a point cloud ? : machinelearning vantage point trees are much better than kd trees at high dimensionality . there's a fast-ish c-implementation as part of the implementation of t-sne here : URL you should be able to hack out the vp trees for your own usage :) EOQ sorry about that , can't read . try a approximate nearest neighbors algorithm such as spotify's annoy algorithm : URL EOA 
 how to implement nearest neighbour lookup in a point cloud ? : machinelearning vantage point trees are much better than kd trees at high dimensionality . there's a fast-ish c-implementation as part of the implementation of t-sne here : URL you should be able to hack out the vp trees for your own usage :) EOQ you probably have to implement the op yourself . k-d trees might be suitable for this job . EOA 
 how to implement nearest neighbour lookup in a point cloud ? : machinelearning vantage point trees are much better than kd trees at high dimensionality . there's a fast-ish c-implementation as part of the implementation of t-sne here : URL you should be able to hack out the vp trees for your own usage :) EOQ cover trees may help . apologies for linking papers , but they explain it best . cover trees for nearest neighbor faster cover trees implementations : c-haskell EOA 
 how to implement nearest neighbour lookup in a point cloud ? : machinelearning vantage point trees are much better than kd trees at high dimensionality . there's a fast-ish c-implementation as part of the implementation of t-sne here : URL you should be able to hack out the vp trees for your own usage :) EOQ just split the problem into NUM million matrix-vector multiplications . that way it does not require more memory than the inputs . on gpu it may be faster to do mini-batches . you can use numpy argpartition for faster sorting . also consider using approximate methods , for instance you can perform kmeans clustering and only compute neighbors inside of the clusters or annoy ( random projects ) mentioned before . EOA 
 how to implement nearest neighbour lookup in a point cloud ? : machinelearning vantage point trees are much better than kd trees at high dimensionality . there's a fast-ish c-implementation as part of the implementation of t-sne here : URL you should be able to hack out the vp trees for your own usage :) EOQ doesn't k nearest neighbors suffer in high dimensions due to the curse of dimensionality ? you might be better off performing dimensionality reduction ( e.g. pca ) then knn on the eigenvalues . edit: spelling EOA 
 error for multiple outputs ? : machinelearning are you talking about multi-label classification ? EOQ i think so ? EOA 
 error for multiple outputs ? : machinelearning are you talking about multi-label classification ? EOQ sigmoid instead of softmax output activation and cross-entropy cost function might be a good place to start . you should really figure out what you want in right terms . it makes it much easier to search for papers . EOA 
 error for multiple outputs ? : machinelearning are you talking about multi-label classification ? EOQ if you have binary outputs ( between NUM and NUM ) then follow gayspy's advice . if you have continuous outputs ( no bounds ) then use linear activations ( aka no activation or identity ) in the last layer and a mean squared error or a mean absolute error cost function edit : grammar. EOA 
 error for multiple outputs ? : machinelearning are you talking about multi-label classification ? EOQ this depends on what you are trying to predict/classify . for classification , look for some articles/examples of the softmax layer and cross-entropy cost . for other types , please be more specific . EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ thank you , i will look into it . ( as far as i understand amari takes the neural network as an element of a manifold , by contrast i am mostly interested in manifolds arising in the activation of layers , for example the connection h from an n element layer to a m element layer , with n < ;m, should define a submanifold in h(i^n ) \subset i^m.) EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ two articles i like a lot : EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ thanks . ( the second link seems to be broken. ) EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ i've updated it . i hope it's better for you ( i hadn't the issue ) . as in the paper given by u/mikhailkudinov it's about informational geometry too . EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ thanks , now it works :) EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ i also suspect that some algebraic topology could be applicable for dnn , something similar to topological data analysis , at least topological data analysis of training and validation datasets , and how it translate to topology of the layer output sets . one of the problem seems big computational cost of that approach-similarity metric expensive even for NUM images , and here we are talking about at least NUM k images ... EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ indeed and that's interesting . personally i'd be happy if at the end a rather simple geometry could explain ai , because nature is beautiful after all . i think it's very likely . however problems like the occur in topological data analysis frequently too and to avoid monotonicity 'traps' like these in even probabilistic networks and are used . there is good information on solving this for related problems in the references . at first look unrelated , but a paper titled goes into the detail how it's related with artificial intelligence and geometry . EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ could you point me to some paper or book on using lll or entropy compression in context of information geometry or topological data analysis(or in any field connected with data science) ? i know entropy compression from totaly different , combinatorial perspective and i'm really interested in this connection . also , i didn't read the proofs but after NUM minutes the last paper still looks quite unrelated to geometry or topology of dnn layers . what am i missing here ? EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ why do you expect ai to be geometrical ? i think we find geometry beautiful because the human brain has very large facilities for geometric processing ( visual cortex , dorsal stream , motor cortex , hippocampus/grid/place cells ) , so that we can process geometric theorems of very high complexity . however, a lot of human reasoning seems to be based on languistic construct which seem to be processed much like computer programs , i.e. in a very discrete rather than a smooth way ( only association , transfer learning and perception seem to be smooth ) . for this reason i think that intelligence depends on combinatoric , probabilistic, graphic , linguistic and building block-like components which are potentially hard to describe geometrically . EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ first i think that such properties arise as an effect of the neural architecture that support intelligence , while the most simple building blocks work as the axioms of more complex systems that self-organize given external and internal 'input' . ( edit : after all it doesn't matter if you represent your nn with bools , integers and floats , divisions and additions , differential geometry , fractals and lines or graphs and functions . it matters that it works robustly and uses the available hardware most efficiently . the base used to encode is just notation . i think you agree . if someone crazy came and modeled a successful nn based on sonographs of farts which outperformed tensorflow ie . who were we to judge it then ? ) see the geometric intuition behind manifold learning is to exploit local structure in the data to create a basis of a non-linear dimensionally reduced representation from higher dimensional data . that reduces compute time. but to understand what i mean by topology and geometry in a neural network i recommend you giving this mostly visual blog post titled neural networks , manifolds, and topology a go . a homotopy is a very useful tool to find general solutions to basis manifolds , because it allows to transfer learnt knowledge to input requiring just a geometric transformation . in simple words you come to conclusion 'b via a' because a can be shaped , stretched and reformed without slicing to b . if you have the same 'geometry' for different inputs , you can also speed up backpropagation with homotopy . the book axiomatic , enriched and motivic homotopy theory is helpful in this regard . this paper goes on to explore a geometric visualization of nns . op can find more about differential geometry and machine learning in this short course and other recent publications on the topic by dr. anuj srivastava . EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ see chris olah's blog post on , perhaps ? EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ i have seen that post before , but it sort of turns a wrong turn and goes towards knot theory . ( which is probably the more fruitful area of inquiry. ) i am looking for more classical differential geometry applications . ( also i am looking for something more technical. ) EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ it also obvious that blog author is ignorant of whitney embedding theorem and how it nicely solve the problem he posed . EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ there's probably a way of putting that without sounding like a jerk . EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ perhaps a tldr aimed at non-mathematicians would help ? EOA 
 literature on the differential geometry of neural networks : machinelearning this topic was studied in the field of informational geometry see works by amari , e.g. URL EOQ most of convnet architectures double the number of channels in each convolutional block at the cost of some smoothing out input . embedding theorem say that any smooth n dimentional manifold can be smoothly embedded into NUM n dim manifold . that mean that barring situation where convolution input essentially non-smooth(not happen in most of layers) situation described in the blog post will disentangle in the next block . now all conv nets used in practice are at least NUM blocks . that mean that situation described just dont happen in practise and most likely not relevant . EOA 
 big sur expected release date ? : machinelearning id like to know the same thing , do we know what the expected price is for a single server ? EOQ i think there are some general descriptions of the box , but he main cost would probably be the NUM tesla m40s . this probably runs in the mid NUM figures(?) and if there are any organizations that take the spec sheets in ocp and makes it for retail purposes , i would expect this box to be NUM figures . EOA 
 big sur expected release date ? : machinelearning id like to know the same thing , do we know what the expected price is for a single server ? EOQ i don't know why they ( or anyone ) would be using the m40 cards when the titan x and the NUM ti are the same specs ( NUM has NUM gb memory ) but an order of magnitude cheaper . EOA 
 big sur expected release date ? : machinelearning id like to know the same thing , do we know what the expected price is for a single server ? EOQ the m40 supports nvidia gpudirect , which is basically dma/rdma for nvidia cards . it's also tested more thoroughly , and should be more reliable . ( whatever that means. ) nvidia press release EOA 
 bilbowa: fast bilingual distributed representations without word alignments : machinelearning hey , i'm the co-author of an article that aims at doing a little bit the same thing : URL for the writing of this article , we've tried to reproduce the bilbowa results as well . i managed to get in contact with s . gouws who gave me the following parameters : for the cldc experiments , it was only trained on the first NUM k lines of europarl used both as monolingual and paralled data . -size NUM -window NUM -sample NUM e-5 -negative NUM -threads NUM -mincount NUM you can try with these parameters . i don't remember if we've managed to reproduce them . or you could try our method ;) EOQ thank you ! i will do both , although i am somewhat sceptical about the NUM k lines of europarl for wmt11 word translation task : ) , however i will definitely read your paper ! edit : i read your paper , it seems interesting ! have you published your code yet ? also-you seem to be very selective about to which methods you compare your work to . also-what settings did you use for word translation task with bilbowa ? thanks , mk EOA 
 bilbowa: fast bilingual distributed representations without word alignments : machinelearning hey , i'm the co-author of an article that aims at doing a little bit the same thing : URL for the writing of this article , we've tried to reproduce the bilbowa results as well . i managed to get in contact with s . gouws who gave me the following parameters : for the cldc experiments , it was only trained on the first NUM k lines of europarl used both as monolingual and paralled data . -size NUM -window NUM -sample NUM e-5 -negative NUM -threads NUM -mincount NUM you can try with these parameters . i don't remember if we've managed to reproduce them . or you could try our method ;) EOQ thanks ! do you guys release the trans-gram code ? EOA 
 which cnn papers visualize the learned features ? : machinelearning i recently came accross a paper on interpreting cnn features . not sure if it's exactly what you are looking for but might be a start . zeiler, m . d., & fergus , r. ( NUM ) . visualizing and understanding convolutional networks . in computer vision?eccv NUM (pp . NUM-833). springer international publishing . URL EOQ thank you , this was what i was thinking about ( or at least they do something similar ) :-) EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ well this is not always the case : for instance resnet is a way to combat underfitting of very deep convnets ( NUM-layers ) : URL EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ also deep generative models for images like gan and vae are probably still significantly underfitting although this is hard to evaluate as those models are unsupervised . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ we can do more about data problems than we do now . right now , it seems most problems are spotted informally by people poking through datasets , and people don't care much because it doesn't seem to make a big difference . but taking more of an active learning perspective , any boosting could be combined with querying a human oracle ( the researcher ) : before i train very hard to classify this anomalous image correctly , can you confirm that the label is genuinely correct ? and then the label is confirmed as correct , corrected, or if no answer is possible as it's garbage , just deleted from the dataset ; and this change is pushed upstream . or one could routinely quality-check hard instances . continued failure to classify data marked correct means that architecture could be a problem , or there's not enough data ; in the latter case , the nn could request more data of that kind , possibly automatically using an api like mechanical turk ( 'instances in the train category are most often mis-classified ; i have requested NUM more train images using a mt job , exhausting NUM % of the researcher's mt credit/budget' ) . as datasets improve , the fraction of bad data goes down and the trained nns also get better at finding bad data , forming a virtuous circle . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ you could try adding artificial noise to the correct data . maybe it'd have similar results to the dirty data . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ some boosting algorithms have been shown to be equivalent to gradient based methods . see for example the equivalence between adaboost and gradient boosting . deep learning tends to use gradient based optimization as well so there may not be a ton to gain from boosting as with base learners that don't . and the resulting additive ensembles of deep networks could get pretty difficult to apply/work with . edit : there was a kaggle competion ( the galaxy quest one? ) won by a bagged ensemble of ~10 nns and i wouldn't be surprised to see a boosted ensemble win in that context at some point but i doubt they will come into common use . iirc that kaggle solution went nowhere for computational reason while the second place solution , xgboost, has seen wide adoption . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ boosting is an ensembling strategy that combines many weak learners . deep networks are very flexible models and in a sense are already an ensemble of many weak learners . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ i think boosting could be useful in some deep learning situations . take for instance the case where there is good performance on a most of the training set but bad performance on a small fraction of hard cases . an optimal amount of boosting would increase focus on solving these hard cases while not significantly affecting performance on the rest of the training set . edit: grammar EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ i think the answer is that when you train with gradient-descent , you do boosting implicitly . the amount of change in the model parameters is proportional to the classification cost , so samples in which the model correctly predicts the target are ignored ( since they result in zero-error gradient and therefore zero parameter change ) . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ i had a problem i solved with deep learning and boosting . it has a time and a place . overfitting is obviously the danger , but with a lot of regularization , there can be a sweet spot . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ why won't your test hold out prevent overfitting ? this applies for any modeling type for that matter , not just boosting . the criticism you cited makes sense , but only against the training set . you shouldn't pick your model based on training set performance anyways . EOA 
 why is boosting unpopular in deep learning ? : machinelearning deep learning is all about fighting overfitting . presenting examples more often will do more harm than good . EOQ i agree with you ! however, the problem in practice is that you can/should only use your hold-out data once to prevent over-optimistic performance estimates (hence , you'd do nested cv etc.). given that your data is informative , you'd need to decide how much valuable samples you can burn for ( rounds ) of hold-out testing vs . using the data for refining your model ... EOA 
 is there any tool for visualizing cnns architecture ? : machinelearning probably not for report usage but for your own visualization purposes there's this : URL EOQ i would recommend omnigraffle if you have mac : URL. it's quite easy to learn and i was able to produce a graphic like this in about NUM hours : URL the downside is that if you pay for software , it costs $100 after the trial expires . but it is worth it ! EOA 
 is there any tool for visualizing cnns architecture ? : machinelearning probably not for report usage but for your own visualization purposes there's this : URL EOQ if you are using torch , you can visualize models using URL EOA 
 is there any tool for visualizing cnns architecture ? : machinelearning probably not for report usage but for your own visualization purposes there's this : URL EOQ seems like everyone is doing it manually-using powerpoint , keynote, or whatever . EOA 
 is there any tool for visualizing cnns architecture ? : machinelearning probably not for report usage but for your own visualization purposes there's this : URL EOQ i used to do all of mine in powerpoint for lack of a better tool , google slides is also quite decent , in my experience :) EOA 
 is there any tool for visualizing cnns architecture ? : machinelearning probably not for report usage but for your own visualization purposes there's this : URL EOQ yeah ! the one op linked is almost certainly made in google slides . EOA 
 [need some advice] deciphering old handwriting . where to start ? age-old family documents ... : machinelearning post a few samples here , and maybe on some german subreddits . crowdsourcing will be easier than machine learning :) EOQ good idea , maybe i'll try that . but i will have to scan them yet . i wanted to gain some knowledge about ml etc before trying the whole thing . if i can do it i would really like to do it with ml ( would be a cool project to get into it for me ) . EOA 
 [need some advice] deciphering old handwriting . where to start ? age-old family documents ... : machinelearning post a few samples here , and maybe on some german subreddits . crowdsourcing will be easier than machine learning :) EOQ can you post a small excerpt of the diaries ? maybe someone is able to read them and willing to translate . EOA 
 [need some advice] deciphering old handwriting . where to start ? age-old family documents ... : machinelearning post a few samples here , and maybe on some german subreddits . crowdsourcing will be easier than machine learning :) EOQ not yet . i should've thought about this earlier . the diaries are far away right now . so i have to wait till i get there again . also there are quite a few of them . if i can avoid it , i'd like to not decipher them manually ( or pay someone to do it ) . EOA 
 [need some advice] deciphering old handwriting . where to start ? age-old family documents ... : machinelearning post a few samples here , and maybe on some german subreddits . crowdsourcing will be easier than machine learning :) EOQ interesting ! if you speak german , you could quite easily learn to read and write sutterlin . if you don't know german it is still possible to learn to decipher but you will be much slower . then you could annotate some samples and use an already existing handwriting recognition algorithm ? for example lstm-based algorithms ( see phd thesis of alex graves ) EOA 
 [need some advice] deciphering old handwriting . where to start ? age-old family documents ... : machinelearning post a few samples here , and maybe on some german subreddits . crowdsourcing will be easier than machine learning :) EOQ thank you for the tip ( phd ) i'll check that out . i don't have any samples right now . i am german , yes i could learn sutterlin ( or kursivschrift ) but i think it is quite a lot of handwriting . i don't want to do it . maybe i will , if there is really no way around it . i haven't yet scanned anything . the originals are far away from me right now . it's more of a future project for me . i wanted to prepare a little , before doing it , checking out the possibilities . also knowing in advance if it's ok to just fotograph the pages ( some diaries are in book form , so flatbed-scanning is a little messy here ) . color, grayscale or b/w , which dpi etc .-i could imagine that with machine-learning the answer would be different as with just deciphering them manually . but for now-thanks already ! EOA 
 fast policy lookup for real-time rl ??? : machinelearning try sparse coding , it is essentially a locally sensitive hash . i use this to store policies all the time , it works very well , and due to its sparse nature , it is fully online. EOQ thanks ! i'll look into that . EOA 
 convolutional operation doubt : machinelearning beginner questions should better be asked in /r/mlquestions EOQ thanks ! EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ yeap forgot the links , thanks for reminding EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ thanks for this . i've asked op but i might as well ask you-know of any similar implementations for smote/smoteboost ? EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ hey /u/xristos.forokolomvos do you have an implementation of smote/smoteboost ? i'm working with a skewed class problem and i'd like to compare all the different methods if possible . EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ check out this repo . he has implemented these algorithms you are talking about and i have used his code structure to develop my module ! EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ thank you this is perfect . have a great day further . EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ don't forget to star :) EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ when you have the results , could you pm me with the comparison of these methods ? i'm kind of held up with work these days and can't compare them myself but would love to see some metrics among different methods ! EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ i haven't read the paper , but transform and fit.transform methods look unusual : contrary to the docstring transform it doesn't return anything , setting attributes instead ; also, transform is usually stateless . usually fit.transform is not a shortcut for fit() ; return transform() only for the cases when doing these steps together is more efficient ( e.g. some intermediate results can be reused ) . here it does something completely different ( e.g. concatenating self.new.x and x ) . maybe something like partial.fit would be more scikit-learn-like ? EOA 
 i implemented a module in python for oversampling skewed datasets . would love to hear your thoughts on it ! : machinelearning guessing that these are the links : URL URL i like it . EOQ yeap that's true that in the current state the only way to go is fit.transform() . i need to figure this out because scikit-learn has no oversampling/undersampling modules to compare with regard to structure . EOA 
 paper(s) on automatic song generation from lyrics : machinelearning take a look at the related work in this paper on generating rap lyrics with an lstm . EOQ however , i think music composition by nns would be more related . EOA 
 paper(s) on automatic song generation from lyrics : machinelearning take a look at the related work in this paper on generating rap lyrics with an lstm . EOQ do you mean generating audio from the lyric text ? if so i haven't heard about anything like that , audio generation by itself isn't very successful at the moment never mind trying to condition on the lyrics themselves . EOA 
 paper(s) on automatic song generation from lyrics : machinelearning take a look at the related work in this paper on generating rap lyrics with an lstm . EOQ how about midi instead of raw audio ? EOA 
 paper(s) on automatic song generation from lyrics : machinelearning take a look at the related work in this paper on generating rap lyrics with an lstm . EOQ midi is fine too . EOA 
 why can model complexity vary when we don't change the number of model parameters ? : machinelearning you may wish to read about the vc-dimension of a classifier . the general intuition is that you can think about complexity in terms of the number of points that can be separated or 'shattered' . as a thought experiment , imagine you have an arbitrary number of points lying along an axis , each of which is somehow assigned to one of two classes . you want to develop a classifier to classify/shatter/separate the points , and so you can use a sin(ax) function , where a is some real number and equates to a parameter that controls the sinusoidal frequency . clearly, by tuning the frequency parameter of the function , you can split an infinite number of data points into the two classes . your thought about the parameters being related to regularization is also correct . imagine in that previous example you have some training set with which you tune the frequency parameter . as we found , you can easily set the parameter to achieve a perfect training score and so one must apply some penalty on the size of the parameter in order to classify unseen points . by doing this , we restrict the complexity of the sine function . EOQ and what is the mechanism by which , say, a lower value of k functions as a sort of a regularization in knn ? EOA 
 why can model complexity vary when we don't change the number of model parameters ? : machinelearning you may wish to read about the vc-dimension of a classifier . the general intuition is that you can think about complexity in terms of the number of points that can be separated or 'shattered' . as a thought experiment , imagine you have an arbitrary number of points lying along an axis , each of which is somehow assigned to one of two classes . you want to develop a classifier to classify/shatter/separate the points , and so you can use a sin(ax) function , where a is some real number and equates to a parameter that controls the sinusoidal frequency . clearly, by tuning the frequency parameter of the function , you can split an infinite number of data points into the two classes . your thought about the parameters being related to regularization is also correct . imagine in that previous example you have some training set with which you tune the frequency parameter . as we found , you can easily set the parameter to achieve a perfect training score and so one must apply some penalty on the size of the parameter in order to classify unseen points . by doing this , we restrict the complexity of the sine function . EOQ well in the case of knn it is the opposite and higher values of k regularise the model . here, imagine the decision boundaries that exist in a hypothetical NUM d dataset as we vary k . when k-1 , we are effectively drawing tessalating shapes ( URL ) around each data point and the decision boundary complexity is high . conversely, when k-n , there is only one decision possible , the majority . imagine what happens as we start at k-1 and step through k to k-n ; the decision regions merge so that we have gradually fewer , larger regions and the total complexity of the boundaries is reduced . we could apply some cost to the model that penalises low values of k . EOA 
 when do we have enough data ? : machinelearning great post on this topic : URL EOQ in statistics , you would conduct a power analysis to determine the required sample size for detection given an effect size , significance level , and power level , e.g. you need NUM samples to have a NUM % power to detect a correlation of r-NUM at significance p < ;0.05. in most machine learning applications its often much more complicated to discern the required sample size. unless you can gather information from other similar projects , it probably comes down to getting as much data as you can reasonably afford and hoping its enough . another approach would be to gather the data stepwise and see how much the additional data helps . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ generating sentences from a continuous space URL this work uses an lstm-> ; variational autoencoder-> ; lstm architecture to build a generative model for text . not a gan , though! EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ nice . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ thanks ! EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ as people mentioned here it seems like it is hard to train gans on recurrent nets since they are unstable . at the same time while wobbly images may look better than blurry images , the same may not apply to text . also keep in mind that most of success of gans came from unsupervised models but not from conditional models which are much more common in nlp say machine translation . if you want to add some stochasticity to generated text i would suggest taking a look at these papers . all of them use some form of variational inference . URL URL URL EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ in general , gans should generate things that people consider to be more realistic samples than the alternatives . models based on maximum likelihood , like vaes , are intended to always assign high probability to any point that occurs frequently in reality . but they also assign high probability to other points ( such as blurry images ) . gans are designed to make samples that are realistic . they avoid assigning high probability to points that the discriminator recognizes as fake ( such as blurry images ) but they may also avoid assigning high probability to some of the training data . for text , it's not really clear what a wobbly sentence would be . but gans for text should generate sentences that are hard for a discriminator to recognize as being fake , and at the same time they'll probably fail to generate some sentences that were in the training set . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ related article URL EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ hi there , this is ian goodfellow , inventor of gans (verification : URL). gans have not been applied to nlp because gans are only defined for real-valued data . gans work by training a generator network that outputs synthetic data , then running a discriminator network on the synthetic data . the gradient of the output of the discriminator network with respect to the synthetic data tells you how to slightly change the synthetic data to make it more realistic . you can make slight changes to the synthetic data only if it is based on continuous numbers . if it is based on discrete numbers , there is no way to make a slight change . for example , if you output an image with a pixel value of NUM , you can change that pixel value to NUM 001 on the next step . if you output the word penguin , you can't change that to penguin-.001 on the next step , because there is no such word as penguin-.001. you have to go all the way from penguin to ostrich . since all nlp is based on discrete values like words , characters, or bytes , no one really knows how to apply gans to nlp yet . in principle , you could use the reinforce algorithm , but reinforce doesn't work very well , and no one has made the effort to try it yet as far as i know . i see other people have said that gans don't work for rnns . as far as i know , that's wrong ; in theory , there's no reason gans should have trouble with rnn generators or discriminators . but no one with serious neural net credentials has really tried it yet either , so maybe there is some obstacle that comes up in practice . btw , vaes work with discrete visible units , but not discrete hidden units ( unless you use reinforce , like with darn/nvil ) . gans work with discrete hidden units , but not discrete visible units ( unless , in theory , you use reinforce ) . so the two methods have complementary advantages and disadvantages . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ thanks for the detailed reply ! very much appreciated good sir . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ thanks , very insightful . so we need some kind of smooth text representation or robust mapping of real values to discreet data which can be learned to pretend to be discreet text . may be we need something from chaos theory where small shifts in initial conditions or parameters can lead to very rich and complex discreet features being dramatically different from point to point . it would be nice to map each sentence into some fractal/bifurcation/dynamic system which is parametrized by some z point of semantic space . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ did someone say smooth text representation ? word vectors seem pretty smooth to me ! but in all seriousness , i think the problem is that you would have to have the entire network process inputs and outputs in terms of word vectors . you can do the cosine distance for the nearest word , but i from a painful past experience , i've found that softmax has crushed this approach . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ thanks for the insight . this thread inspired me to try out gans . i have had a very hard time getting them to work with rnns . talk about a learning rate adjustment nightmare . one thing curiously that has helped me is training the discriminator on discriminating between pre-trained generator network outputs . train a generator ( g0 ) as normal using max-likelihood . train your discriminator to discriminate between inputs of this generator ( g0 ) and real data . start with a fresh generator ( g1 ) and use the gan architecture to train it using the same discriminator . my theory as to why this works better is that the discriminator already has some experience with how language works . when i child learns how to talk , they listen to words first , and learns to discriminate word order first . after having enough training data , a child then learns to generate words . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ they are conspicuously absent . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ i tried gan with german words and all i got was a new nickname for my crush . most of the generated words looked and sounded german , but they were total gibberish . same for tweets ; it learned to begin with @ and also proper use of spaces to divide words , but the words themselves were composed of random letters . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ i tried gan with recurrent generator and discriminator on russian and have the same result . model learned words separation reasonable punctuation placement some words starting from capital letters but words are meaningless . it is hard to keep balance between generator and discriminator , and learning is very slow . we need more tricks :) EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ is there a reason why you didn't try a word based approach rather than char ? EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ not really . i did the german words first which obviously had to be char-based and then simply reran with the tweet data . EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ hmm alright i'm gonna try some variations of char based and subword based and see if it helps EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ [ deleted ] EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ where did you see this ? EOA 
 generative adversarial networks for text : machinelearning i heard that it's very hard to propagate the cost through the generator rnn . people used a lot of tricks to stablize a gan of cnn , should be harder with rnn . EOQ somewhere on this reddit forum-i can't find it now but search this forum for gan and rnn and hopefully you'll find it . or use google . sorry! EOA 
 does the kuka table tennis robot use machine learning ? : machinelearning seems like the general consensus is that it's likely faked for the most part . some of it is real , but if it was a plausible technology that worked as good as it seems , why wouldn't they release any information or full matches on it ? instead just a single , highly over produced and dramatized video . EOQ my bet would be that there is no machine learning involved . i guess that all the behaviour is preprogrammed , i.e. the robot does only know where the glasses are exactly because someone told him . not because it perceives them in some way . that robot has a repeatability of NUM 3mm. this is only possible due to the fact that a very accurate physical model of it is available , for which the inverse dynamics can be efficiently calculated . so , no perception and physical model-> ; no machine learning . EOA 
 does the kuka table tennis robot use machine learning ? : machinelearning seems like the general consensus is that it's likely faked for the most part . some of it is real , but if it was a plausible technology that worked as good as it seems , why wouldn't they release any information or full matches on it ? instead just a single , highly over produced and dramatized video . EOQ i'm sorry , my question was not clear . my question was not about the video with the robot playing with the glasses , but about the table-tennis play . EOA 
 does the kuka table tennis robot use machine learning ? : machinelearning seems like the general consensus is that it's likely faked for the most part . some of it is real , but if it was a plausible technology that worked as good as it seems , why wouldn't they release any information or full matches on it ? instead just a single , highly over produced and dramatized video . EOQ both videos are choreographed EOA 
 does the kuka table tennis robot use machine learning ? : machinelearning seems like the general consensus is that it's likely faked for the most part . some of it is real , but if it was a plausible technology that worked as good as it seems , why wouldn't they release any information or full matches on it ? instead just a single , highly over produced and dramatized video . EOQ what exactly do you mean by that and where do you have your information from ? EOA 
 does the kuka table tennis robot use machine learning ? : machinelearning seems like the general consensus is that it's likely faked for the most part . some of it is real , but if it was a plausible technology that worked as good as it seems , why wouldn't they release any information or full matches on it ? instead just a single , highly over produced and dramatized video . EOQ this is real ? URL EOA 
 what shall i know ? : machinelearning the first big thing is to work on your math , in particular linear algebra , multivariate calculus , statistics, and probability . that provides the mathematical foundations underlying many ml models , from simple linear regression on up . after that , i'd recommend andrew ng's coursera course on machine learning to give you a broad overview of the field . EOQ a natural first step is to go to university and try to do research with a professor who works on ml/ai . EOA 
 anyone heard about these folks gamalon ? (URL : machinelearning ben vigoda , the founder , gave a interview on the talking machines podcast at the end of last year : URL EOQ URL EOA 
 anyone heard about these folks gamalon ? (URL : machinelearning ben vigoda , the founder , gave a interview on the talking machines podcast at the end of last year : URL EOQ this sounds like the probabilistic induction stuff . still requires hand-engineered features . EOA 
 computer worms ! : machinelearning but i do believe your statement is this just a manifestation of people's fear that biological life will one day be surpassed ? do we really think general ai can prosper without the freedom to compete for resources currently reserved for humans ? is against the law of economics in the first place . EOQ i'm sure there are many people and states working on this . what's really most problematic about this is these computer worms are invariably going to be used as a form of computer cancer . EOA 
 computer worms ! : machinelearning but i do believe your statement is this just a manifestation of people's fear that biological life will one day be surpassed ? do we really think general ai can prosper without the freedom to compete for resources currently reserved for humans ? is against the law of economics in the first place . EOQ true , but cell cancer results from a cell which has undergone a mutation resulting in faster reproduction . my point is that , while cancer is bad for multi-celled organisms , it is simply a manifestation of evolution at the single-cell level . presumably all modern cells come from ancient slow-dividing cells which got some form of cancer to become the fast-dividing cells that compose our bodies today . so computer cancer will be bad for humans , but not for the cancer itself . i agree that use of worms as a weapon is bad , and i acknowledge that a computer cancer could feasibly set humans back a hundred years without resulting in any sort of appreciable ai . come to think of it , i've been ignoring that possibility , which is that humans could be forced to hand over the internet to a cancer ( as you say ) which is ultimately less competent than humans . i'd hate to hand the world over to an algorithm which won the competition for resources by brute force alone . if worms are simply used as weapons , then they will not be equipt to continue the advancement of technology using the resources they have won . i guess this is the scary catch22 of putting trust in superintelligent ai . we sometimes assume that superintelligent ai will destroy humans and then continue carrying the technological torch , but even worse , it could destroy humans and then kill itself : then what have we accomplished ? EOA 
 making a text classifier for short texts : is it likely that a tfidf naive bayes classifier added to a random forest that contains NUM non semantic features ( word length , pos, etc ) would be incapable of improving performance ? : machinelearning tfidf and naive bayes don't go together EOQ in practice it can be gotten away with is my understanding , as well as my observation with this dataset , check this out : the multinomial naive bayes classifier is suitable for classification with discrete features ( e.g., word counts for text classification ) . the multinomial distribution normally requires integer feature counts . however, in practice , fractional counts such as tf-idf may also work . URL EOA 
 making a text classifier for short texts : is it likely that a tfidf naive bayes classifier added to a random forest that contains NUM non semantic features ( word length , pos, etc ) would be incapable of improving performance ? : machinelearning tfidf and naive bayes don't go together EOQ i would try it without tf-idf first . EOA 
 making a text classifier for short texts : is it likely that a tfidf naive bayes classifier added to a random forest that contains NUM non semantic features ( word length , pos, etc ) would be incapable of improving performance ? : machinelearning tfidf and naive bayes don't go together EOQ well , it classifies with about NUM % accuracy when i train the naive bayes with tfidf on its own , but when i add that classification as a feature input to my random forest it doesn't improve the accuracy of the random forest at all EOA 
 making a text classifier for short texts : is it likely that a tfidf naive bayes classifier added to a random forest that contains NUM non semantic features ( word length , pos, etc ) would be incapable of improving performance ? : machinelearning tfidf and naive bayes don't go together EOQ in my experience you can get a few percent if you select the words that are relevant to your prediction task . if you use all the words it doesn't do as good . also, make an ensemble of different classifiers . if you can afford to drop some of the examples you want to predict , and select just the ones all classifiers agree upon , then you can get much higher accuracy . some examples are just hard to classify with any method . i used this method to classify news into NUM topics . i can get NUM % accuracy on a subset but it drops to NUM % in the general dataset . my case differs from yours , though, i had access to NUM words for each example . EOA 
 a curve of machine learning's state of art through years ? : machinelearning your best bet is to look at the performance on certain benchmarks over time. page NUM of this paper has performance on the imagenet large scale visual recognition challenge over time. classification error of best system per year : NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % EOQ NUM | error NUM 7%-msra ( NUM layers ) EOA 
 a curve of machine learning's state of art through years ? : machinelearning your best bet is to look at the performance on certain benchmarks over time. page NUM of this paper has performance on the imagenet large scale visual recognition challenge over time. classification error of best system per year : NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % EOQ hey , journalist here . ( for verification see twitter @mappingbabel ) . tried to pull together some benchmarks and other information in this graph-heavy story here . URL if anyone has suggestions of other datasets/benchmarks they'd like to see summarized do let me know , as i'll do another one in a few months with more data . takes a long time to ferret some of this stuff out . (all help appreciated!). cheers EOA 
 a curve of machine learning's state of art through years ? : machinelearning your best bet is to look at the performance on certain benchmarks over time. page NUM of this paper has performance on the imagenet large scale visual recognition challenge over time. classification error of best system per year : NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % EOQ this is my favourite topic . i have some relevant plots and observations : URL URL URL URL ( about atari games , there i have also a detailed table ) URL EOA 
 a curve of machine learning's state of art through years ? : machinelearning your best bet is to look at the performance on certain benchmarks over time. page NUM of this paper has performance on the imagenet large scale visual recognition challenge over time. classification error of best system per year : NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % EOQ these are really great , thanks for sharing ! EOA 
 a curve of machine learning's state of art through years ? : machinelearning your best bet is to look at the performance on certain benchmarks over time. page NUM of this paper has performance on the imagenet large scale visual recognition challenge over time. classification error of best system per year : NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % NUM -NUM % EOQ URL edit : slides NUM and NUM EOA 
 tried pca regression , neural networks , and random forests-all converge on wrong answers . what am i doing wrong ? : machinelearning perhaps your inputs do not correlate with the output so the best the algorithms can do is to learn the average of your outputs . ( ? ) EOQ oooh , yeah . . . i hadn't thought about that . we were data mining , basically, looking for any possible explanations . it does look pretty average across the board . EOA 
 tried pca regression , neural networks , and random forests-all converge on wrong answers . what am i doing wrong ? : machinelearning perhaps your inputs do not correlate with the output so the best the algorithms can do is to learn the average of your outputs . ( ? ) EOQ so these big errors are on the training set ? do your algorithms agree that you have this big error on your training set ? if you ask randomforest for its mse ( mean squared error ) , do you get something in the mid NUM-3 range ( i.e. rms error about as big as your data ) , or do you get something much smaller ( say , NUM-5 ) , as if the algorithm was getting a good fit and your bug is somewhere else ? EOA 
 tried pca regression , neural networks , and random forests-all converge on wrong answers . what am i doing wrong ? : machinelearning perhaps your inputs do not correlate with the output so the best the algorithms can do is to learn the average of your outputs . ( ? ) EOQ the mean squared error seems around NUM % of the data ( e.g. NUM 0063 ) for the random forest and neural network (and gets worse with more nodes . . .). it really changes with the principle components-it starts off at NUM %ish , and then gets worse the more principle components i have . i suspect justsomeaccount is right and maybe the data's not that well correlated . EOA 
 tried pca regression , neural networks , and random forests-all converge on wrong answers . what am i doing wrong ? : machinelearning perhaps your inputs do not correlate with the output so the best the algorithms can do is to learn the average of your outputs . ( ? ) EOQ plot learning curves and see how well the algorithms are doing . you could have an undiagnosed bias or variance problem . EOA 
 element-wise multiplication by a fixed vector in caffe ? : machinelearning you could initialize an innerproduct layer with the known vector as the weight , and set the learning rate for that layer to zero to ensure the vector is constant . EOQ i see , thanks. not sure how i set the initial vector to certain numbers using the prototxt or python though EOA 
 element-wise multiplication by a fixed vector in caffe ? : machinelearning you could initialize an innerproduct layer with the known vector as the weight , and set the learning rate for that layer to zero to ensure the vector is constant . EOQ great example , thanks!! EOA 
 why don't abstracts contain more detail about the results ? : machinelearning well , i don't think it's generally possible to summarise the results with only few sentences . probably that's the reason-although abstract , or any kinds of summary can't cover the whole details ( and of course that's the point of summarising ) , it could be dangerous to say few numbers for a results because it's not very difficult to fool people by summarising something in wrong way and still stay in the truth . but i'm just guessing , as /u/davex32 said it's just how things are in the wild . EOQ some conferences require the abstract before the full paper , so by the time you write the abstract you don't have the results yet ;) EOA 
 why don't abstracts contain more detail about the results ? : machinelearning well , i don't think it's generally possible to summarise the results with only few sentences . probably that's the reason-although abstract , or any kinds of summary can't cover the whole details ( and of course that's the point of summarising ) , it could be dangerous to say few numbers for a results because it's not very difficult to fool people by summarising something in wrong way and still stay in the truth . but i'm just guessing , as /u/davex32 said it's just how things are in the wild . EOQ yeah but you can always rewrite the part about the results . it's not like you're changing the whole abstract . my hypothesis points to bad habits or neglected writing skills . not every academic looks at itself as a writer or a communicator-which is another problem . another possibility is that you have a paper with a different assortment of conclusions , and sometimes it's better to generalise ( with some generalisations being better than others ) . i read papers with really good abstracts containing the important insights on the results ; i've also read good overall papers with an horrible abstract-nothing is said about what is actually learned at the end of the paper . so it's not like you have a rule saying that you shouldn't talk about the results in the abstract , it's just how things are in the wild ... EOA 
 why don't abstracts contain more detail about the results ? : machinelearning well , i don't think it's generally possible to summarise the results with only few sentences . probably that's the reason-although abstract , or any kinds of summary can't cover the whole details ( and of course that's the point of summarising ) , it could be dangerous to say few numbers for a results because it's not very difficult to fool people by summarising something in wrong way and still stay in the truth . but i'm just guessing , as /u/davex32 said it's just how things are in the wild . EOQ one tip i learned in grad school is that if the abstract isn't good , the first paragraph of the intro and first of the conclusion usually works well . EOA 
 anyone working on hardware ( fpga , asic ) implementations of neural nets ? : machinelearning there are tons of papers about training neural networks on fpgas . i skimmed a few a while ago and my conclusion was , that they can only compete with gpus in terms of performance per watt , but not in training time. maybe that is because no one tried to use a stratix NUM or something like that . of course it is impossible to compare a titan x to a spartan fpga . EOQ i would like to find out whether someone has implemented it on the parallella . EOA 
 sklearn svc behavior : machinelearning i really doubt you need NUM samples if you only have NUM features . try with a smaller number . a typical number is something like NUM x the number of samples as there are features-so you could try NUM k up to ( maybe ) NUM k . i think that even NUM k is going to be excessive. EOQ i tried your suggestion and tested out NUM ,000 samples and its significantly faster . scoring against my cv the accuracy is NUM 35%. EOA 
 sklearn svc behavior : machinelearning i really doubt you need NUM samples if you only have NUM features . try with a smaller number . a typical number is something like NUM x the number of samples as there are features-so you could try NUM k up to ( maybe ) NUM k . i think that even NUM k is going to be excessive. EOQ wouldn't NUM k samples require like > ;3tb of ram ? EOA 
 sklearn svc behavior : machinelearning i really doubt you need NUM samples if you only have NUM features . try with a smaller number . a typical number is something like NUM x the number of samples as there are features-so you could try NUM k up to ( maybe ) NUM k . i think that even NUM k is going to be excessive. EOQ its a matrix of float64's so its all numeric data . the total size is close to a gig of ram once the svm begins training . the size of data total is around NUM mb . EOA 
 sklearn svc behavior : machinelearning i really doubt you need NUM samples if you only have NUM features . try with a smaller number . a typical number is something like NUM x the number of samples as there are features-so you could try NUM k up to ( maybe ) NUM k . i think that even NUM k is going to be excessive. EOQ the svm calculates a kernel matrix which would be NUM k-NUM k entries-NUM bytes ( for float64 ) ~-NUM tb , but it actually doesn't compute the whole kernel matrix . instead it computes the kernel matrix's entries on demand and stores them in a cache . the size of the cache can be specified : svc(cache.size-...). the default is NUM ( mb ) . increasing the cache size should speed things up . i'm still skeptical if it'll work with NUM k samples . the time complexity of an svm is at least quadratic . so if NUM k samples take NUM minute , NUM x NUM k samples will take NUM min x NUM -NUM minutes . and NUM k samples (-30 x NUM k ) will take NUM min x NUM -NUM hours ( in the best case .. it'll probably be much slower because it can only fit a really small part of the kernel matrix into memory ) . svms just don't scale .. i'd go for another algorithm . you could also try to train the svm with NUM k samples , then with NUM , NUM and NUM k and check if the accuracy improves-if it doesn't improve a lot , fewer samples might already be enough . EOA 
 [intuitevely] i always read about nns get stuck in local minimum . can't we do something to 'shake them out of it' and continue searching for a better minimum ? : machinelearning recent research shows that saddle points are more of a problem than local optima , and that there is good chance the latter is close to the global optimum . also , i believe that stochastic gradient descent already has some of the shake them out of it idea already embedded into the algorithm , due to its stochasticity . EOQ noise injections like dropout also help shake the weights up a bit . EOA 
 [intuitevely] i always read about nns get stuck in local minimum . can't we do something to 'shake them out of it' and continue searching for a better minimum ? : machinelearning recent research shows that saddle points are more of a problem than local optima , and that there is good chance the latter is close to the global optimum . also , i believe that stochastic gradient descent already has some of the shake them out of it idea already embedded into the algorithm , due to its stochasticity . EOQ it seems that local minima tend to only appear close to the global minimum ( see attacking the saddle point ) for a lot of tasks , so it may be hard break them out and find something much better . momentum methods may be able to steamroll past a local minima in some scenarios , but i'm not sure how often that happens in practice . the exception to above may be some of the more advanced differentiable memory networks that have started popping up recently . i know from my experience with neural turing machines , for some tasks it needs to learn a specific action ( shift weighting for copy task ) , yet never discovers it even if it runs for many examples . i would think this is a local minimia problem since some random initializations solve the task very quickly . directly to your question , i think gradient noise is what you are asking about . my intuition is that the large initial gradient noise jumps the system out of the minima , or randomly moves it around in the saddle point . EOA 
 [intuitevely] i always read about nns get stuck in local minimum . can't we do something to 'shake them out of it' and continue searching for a better minimum ? : machinelearning recent research shows that saddle points are more of a problem than local optima , and that there is good chance the latter is close to the global optimum . also , i believe that stochastic gradient descent already has some of the shake them out of it idea already embedded into the algorithm , due to its stochasticity . EOQ it is fairly well established at this point that local minima are not bad . in fact , they might be even better than over fitting global minima . we can ( and have ) achieved very good results without reaching global optima . see the following papers : [ NUM ] the loss surfaces of multilayer networks ; URL [ NUM ] on the saddle point problem for non-convex optimization ; URL EOA 
 [intuitevely] i always read about nns get stuck in local minimum . can't we do something to 'shake them out of it' and continue searching for a better minimum ? : machinelearning recent research shows that saddle points are more of a problem than local optima , and that there is good chance the latter is close to the global optimum . also , i believe that stochastic gradient descent already has some of the shake them out of it idea already embedded into the algorithm , due to its stochasticity . EOQ research has been done in this area-search for neuroevolution where nn's are evolved using evolutionary algorithms . a recent paper in information sciences applied this method to optimise a nn for weapon detection EOA 
 [intuitevely] i always read about nns get stuck in local minimum . can't we do something to 'shake them out of it' and continue searching for a better minimum ? : machinelearning recent research shows that saddle points are more of a problem than local optima , and that there is good chance the latter is close to the global optimum . also , i believe that stochastic gradient descent already has some of the shake them out of it idea already embedded into the algorithm , due to its stochasticity . EOQ research has been done in this area-search for neuroevolution where nn's are evolved using evolutionary algorithms . a recent paper in information sciences applied this method to optimise a nn for weapon detection EOA 
 [beginner's question] understanding deep learning : machinelearning as a beginner myself , i found this reference quite illuminating : URL. it talks about learning representations , why some ( linear or even single-layer ) representations under-perform , and motivates complex multi-layer ( aka deep ) representation learning . it is a relatively big paper , so take your time to let it sink . EOQ depends , what's your purpose with deep learning ? EOA 
 [beginner's question] understanding deep learning : machinelearning as a beginner myself , i found this reference quite illuminating : URL. it talks about learning representations , why some ( linear or even single-layer ) representations under-perform , and motivates complex multi-layer ( aka deep ) representation learning . it is a relatively big paper , so take your time to let it sink . EOQ i'm interested in artificial general intelligence . deep learning seems like a step in the right direction so i'd like to understand how it works and why it works . EOA 
 [beginner's question] understanding deep learning : machinelearning as a beginner myself , i found this reference quite illuminating : URL. it talks about learning representations , why some ( linear or even single-layer ) representations under-perform , and motivates complex multi-layer ( aka deep ) representation learning . it is a relatively big paper , so take your time to let it sink . EOQ so you want to learn about ai to develop , to apply , or for fun ? EOA 
 how difficult is it to reach human-level classification of images ? : machinelearning i imagine it is extraordinarily difficult given the breadth of things we're able to identify ( in a variety of different contexts ) and how efficiently we're able to do it . EOQ mods need to get rid of stupid questions . EOA 
 mean absolute error not going down in fully feed forward network : machinelearning do you have a track of gradient . how does that look like ? EOQ yes i have tracked the gradient . initially it starts from NUM and it starts to decrease gradually till NUM after that it revolves around that or goes high but not beyond NUM . EOA 
 mean absolute error not going down in fully feed forward network : machinelearning do you have a track of gradient . how does that look like ? EOQ NUM is a high gradient . why isnt your cost down in that case is strange . also you said your cost is NUM too , how can your cost and gradient be the same ? final weights-initial weight-learning rate-gradient . as long as your gradient and learning rate are non zero , the cost must change , it might increase instead of decreasing , but there is no way it would stay constant . so yes cost might vary according to gradient but extremely unlikely they are exactly same . try checking if you are calculating your gradients right and if your gradients are not zero / some other useless quantity . EOA 
 mean absolute error not going down in fully feed forward network : machinelearning do you have a track of gradient . how does that look like ? EOQ mlps may be prone to local minima when trained with gradient descent methods . have you tried different initializations ? what kind of loss function are you training on ? EOA 
 mean absolute error not going down in fully feed forward network : machinelearning do you have a track of gradient . how does that look like ? EOQ yes i tried different initialization of learning rate , momentum and weight decay parameters . i am using squared loss function . EOA 
 mean absolute error not going down in fully feed forward network : machinelearning do you have a track of gradient . how does that look like ? EOQ i was referring to the first initialization of the actual weights . depending on their initialization you may get very different performance on prediction . the squared loss function is usually better suited for regression tasks . experiments usually work better if you use another loss like the hinge loss or the cross entropy loss ( though i've never tested latter ) when dealing with classification tasks . also have you tried to let the network find the 'proper' features by training on the NUM features instead of the NUM ? edit : further your network's architecture might also cause the 'problem' . how big is your network and what type of transfer function are you using ? EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ what is the do course ? EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ discrete optimization . i have no idea if and when is it going to be offered again . despite it being pretty difficult , i liked it and it changed the way i see programming . if hinton had not invented dropout , trying it out would have been high on my list after finishing it . EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ can you elaborate a bit more about the differences between state-space and plan-space planning . EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ the difference is in the structure of the problems . in state-space planning you ( usually ) have a large state space and a limited amount of actions you can do and the goal is to learn an optimal action from each state . in plan-space planning , your number of actions is really equivalent to the number of states and you can move freely between any two states . you also know the reward at each state perfectly and the goal is to find the optimal state or something close to it-the problems are np hard after all . because you can calculate the value function at each state perfectly , there is no need to do credit assignment through time or keep visited states in memory unlike in rl problems-if one can find a good greedy algorithm , one can just hop from local minima to local minima and that will work very well . in fact a well made local solver will pretty much crush the industrial mlp and cp solvers , though even if you are using integer programming , they will choke if you feed them a large problem all at once much like local solvers . instead what you want to do is segment the problem and feed it to them in pieces . if you check out the set covering repository in the do course , you will see that this is in fact how it is done in the mlp example . EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ mlp ? cp? EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ mlp-mixed linear programming . cp-constraint propagation . mlp is really ilp except some of the variables can be continuous-another name for it is mixed integer linear programming-milp . cp-constraint propagation . cp solvers are a bit similar to mlp solvers in that one specifies constraints and objectives except more general and they work based on different principles . unlike mlp which is an offshot of linear algebra , cp is comes from the field of logic and works by propagating constraints combined with local search . there is a section on it in the do course , but if one is curious about how to use these solvers , the best way would be to use the modeling language minizinc with the modeling discrete optimization course as a reference . the mdo course is quite a bit lighter than the do course as well and minizinc is not difficult to use . whether you want to use mlp or cp really depends on your needs . for pure speed on a simple , you might want to code up my own idea and test it . if you want to add many constraints into the system besides set covering , you should use one of the solvers . EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ thank you ;) EOA 
 tackling set covering via reinforcement learning : machinelearning i do not know about rl specifically , but an evolutionary approach i want to try when the do course starts again ( the set covering assignment is the optional sixth assignment , so you can get the data for it from the course ) would be to sort by sets.covered/value and then randomly black out the selected choices for one move before trying the greedy approximation again . it is kind of like alternating between one turn tabu combined with greedy search . i'd want to try out that approach on the knapsack problem first as they are kind of similar with the respect to the scheme i proposed . i have no idea how well what i've suggested would work , but i guess i'll try it eventually . rl really deals with state-space planning , rather than plan-space planning which would be more suitable for the set covering problem . it would be really interesting to hook up a neural net to some of the local search algorithms . the most impressive example so far that i've seen has been for chess where instead of using rl to compute plans it was used to compute good heuristics for the minimax algorithm . EOQ you may not need rl if you are willing to look at soft attention schemes . this paper pointer networks by vinyals , fortunato, jaitly doesn't do set cover , but has a number of related np-hard type problems ( tsp , delaunay triangulation , convex hull ) solved by a special form of attention based rnns . there is some code here that solves NUM of the three problems ( no delaunay if i recall ) . you may also be interested in the followup paper order matters : sequence to sequence for sets by vinyals , bengio, kudlur . also note that this approach doesn't scale nearly as far as standard solvers-but given the immense amount of work done on both the software and the research side of mixed integer programming / discrete optimization in general , vs. this new approach , i expect some interesting things will happen down the line with this . being able to take advantage of rnn advances , learning from data , and so on to tackle these kinds of problems seems like the right thing in my horribly biased opinion . i can't comment on the hard attention/rl approach vs . the soft attention approach for this task , but on other tasks it seems soft attention is easier to optimize while having higher computational expense ( since rl based can generally skip computation based on attention window ) . EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ so the reason logistic might work better on realistic datasets is that a realistic dataset can often be approximated by the bernoulli distribution with the parameter given by a linear combination of features ? i can believe that , but wouldn't it also imply a better decision boundary , not just faster training ? EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ no , no free lunch does indeed hold . sometimes , despite the theoretical justification for logistic regression , linear regression to { NUM ,1 } works better . this was shown to be true in a recent kaggle competition ( seizure detection ) , and also in my experience . however , as stated , if you want calibrated probabilities , go with logistic . and in general , if you don't want to freak people out and make them think you're a noob , it's just safer to go logistic ;) EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ just for reference , this is the post that talks about the linear regression solution to this . it says they post-scaled through a logistic function to ( NUM , NUM ) interval to make it a feasible probability-but it seems it was only done to satisfy the technical submission requirement : the evaluation was based on auc , so it depends only on the classification rather than on the probability . does anyone know what about this particular problem made linear regression outperform logistic regression ? so far the only advantages i saw mentioned are speed , but it wasn't the issue here . EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ i honestly think it functioned as a form of regularization . logistic regression can grow rather large logits . regressing would constrain the logits to be smaller , which is a form or regularization . as to the particulars , i'd have to revisit the problem . EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ ah that would make sense-it seems preventing the linear combination of w.i x.i from growing large is roughly what the standard l2 regularization on coefficients would try to accomplish . although it wasn't clear from their post whether / why they didn't use the more common regularization with l2 penalties on coefficients , with either linear or logistic regression . EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ i found a set of lecture notes that suggests that linear and logistic regressions are equivalent for classification purposes as long as both have optimized l2 regularization . although i didn't quite understand how rescaling the margin and the loss would make the quadratic function (m-1)2 resemble the monotonically decreasing function log(1-exp(-m)) , the rest of the writeup was very good so i trust the author . EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ no free lunch theorem is bollocks , because it assumes that you want algorithms that deal with random noise. take that crap elsewhere . EOA 
 advantages of logistic vs linear regression for classification : machinelearning cross-entropy is better because the domain of the labels is {0,1}. to see this , consider the bernoulli pmf f(y,x)-p(x)y (1-p(x))1-y where y is a label in { NUM ,1 } and p(x) is the usual mean parameter , which in the case of logistic regression , is defined as a sigmoidal transformation of a linear combination of input features : p(x)-s(xwt) . now turn f into a loss function by taking its negative log ( so f-0 is infinity and f-1 is zero ) :-log f(y,x)-y log p(x)-( NUM-y ) log(1-p(x)) . recognize the rhs ? its the cross-entropy loss function . thus using cross entropy implicitly assumes the labels are the result of a bernoulli trial , which is proper since they are NUM or NUM . now how do you think you get the mse loss function ? it's the negative log of the gaussian pdf , which has a domain (-infinity ,-infinity). thus using mse would be making a blatantly wrong assumption in that it is assuming the labels can be any real number . but does this have practical consequences ? it sure does if you want calibrated probabilities . the difference between p(y-1) vs p(y-0) under a gaussian is probably small ( unless the variance is larger than they would be with (an improperly scaled ) mse loss . so to answer your question : using mse will mis-calibrate the loss probabilities leading to slower learning . the difference between the decision boundaries might be negligible though . EOQ i asked why logistic is better for realistic datasets . which means i think the no free lunch theorem is irrelevant for realistic datasets . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ a little more diy approach : andrej karpathy wrote this simple app for browsing through arxiv submissions :- it uses tf-idf , a natural language processing technique , for identifying closely related documents . you can also fork it on github , and use the scripts to build your own app that focuses on categories you are interested in . i'm working on an app that uses EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ afaik , word2vec is not very good to capture the semantics of a large set of words by aggregation of the word embeddings into a document embeddings . i think tf-idf ( possibly post-processed by svd to implement lsi-style document vectors ) is both simpler to implement and likely to give better results for document similarity search . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ doc2vec might also be worth a look :) EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ indeed that's what i'm using ! EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ review papers are a great place to start . also, after a while you start to know certain authors and their expertise and highly cited works . my phd was in a different subject , but after reading research for a while you can fly through papers and get a pretty good idea of how useful one is from the abstract and author list . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ machine learning ? wouldn't that be an interesting world : if those who wanted to call themselves machine learning scientists were required to share a training dataset that would be used as the input to a standardized program whose output was a top NUM list of existing and new papers submitted to the machine learning community . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ this exists ? mnist and cifar NUM /100 ( with a little bit of svhn thrown in ) are the sets . everyone uses them , and unfortunately they've been used so much that the literature is clearly overfitting to the holdout data . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ i think the parent comment was actually saying that we could have all machine learning scientists share rankings of papers , so everyone's inputs could be put into a large 'paper-ranking' dataset . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ google scholar , reading through the abstracts of relevant conference proceedings , and by looking at the references of other related work that they are already aware of . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ ah thanks ! i should have thought of google scholar ! thanks again ! would you say this gives you very good coverage ? EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ google scholar gives reasonable coverage , but i wouldn't rely on it completely . if you want to be thorough then you'll have to do these other things as well . if you can find a recent survey or review paper on google scholar for the particular topic you're interested in then that makes life easy . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ i find the new [semantics scholar](semanticscholar.org) very helpful . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ people skim . research papers-in any field-may look impenetrable to outsiders , but they're actually carefully organized so you can decide if they're relevant in a few seconds , and find just the bits that interest you in a few seconds more . also , as others say , you tend to follow the work of people you know or respect , and look up the things they cite in turn . same thing with conferences ; if you hear an interesting talk , or see an interesting poster , you follow the paper trail to what they're citing . review papers are a great shortcut to get into a new subfield . but overall , nobody can actually follow all the literature in their field . it's just not possible . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ i'd say many of the best people typically upper bound the problem by funding and quickly understanding a ridiculously large volume of machine learning papers-some of which will be relevant to any given project . some people have been scanning arxiv for new papers every day since arxiv became the standard for machine learning a few years ago . solution NUM is working as part of a large research team where multiple smart people do the aforementioned scanning of literature , so chances are , if you're working on something , someone in the research team can point you to relevant papers . but generally speaking , the community is not always very good at this , and often you'll find papers that don't cite related ideas , or reintroduce ideas under different name . there has been a lot of drama around tihs on social web for example from pierre sermanet and then jurgen schmidhuber : URL URL EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ this is no different from any other academic field , but is also the reason why abstracts exist . read the abstract , then either read , bookmark, or forget . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ i'd say almost no-one in any field these days can read the large volume of relevant papers for a given topic . our scientific model really encourages volume ! very often there is a good review article you can read though , that's always where i would start . then just read things which interest you , or which will be of use to you right now in answering an immediate question . also , there may be a journal that specifically publishes good reviews in your field-i always like nature reviews because they are well-written and introduce all terms ; great as a starting point . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ something everyone has left out is that you aren't working in a vacuum . you know who the competent people are in your field , so you can go to google scholar and see everything they've published recently . you also will likely spend time as a post-doc/professor reading up on everything from nips/icml/iclr/aistats/etc . you skim the submitted papers , looking for anything relevant to your work , and you end up reading maybe NUM-100 papers ( some more carefully than others ) . i do this as well , and i'm in industry , but keeping up is the name of the game . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ i use an rss reader and subscribe to various journals while also manually following various conferences ( cvpr , iccv, etc ) as they occur . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ they go to conferences and let the organizers do the hard work , then they look at the cited papers from the interesting / relevant talks . i don't think there's anything special about doing a literature review in the ml domain . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ honestly ? i don't . there's so much out there that you are going to miss stuff . the trick seems to be making sure the stuff you miss is less likely to be important . i usually content myself with keeping an eye on the best journals and conferences for what i am doing , the stuff cited by whatever i find there , and the things google scholar turns up on whatever i am i am looking for at any particular moment . also, lots of skimming . EOA 
 how do researchers find and understand the very large volume of machine learning papers that are relevant to any given thing they might be working on ? : machinelearning find the most relevant paper , then see what papers they cite. check the paper on google scholar and see what newer papers cite that one . EOQ when there are too many publications on the same topic , that problem might be solved already . try to find problems that are not yet solved but solvable , then you can start with less papers to read and a higher chance to get in . EOA 
 tackle the real problem in current machine learning research ! : machinelearning how would you turn bad data into good ? how would you link data and algorithm without supervision ? EOQ if i knew how to do that , i would present my results instead of asking , right? ( no offense ) and to be honest , my idea was to open a discussion and i didn't want to present my ideas concerning it in much detail , so i won't influence the discussion . but after three days and only response , i don't give a f .... to answer your first question:first step dividing good and bad data . second step : finding out which features are important . third step : transforming bad data into good by approximation ... your second question(or let's say the second part of my suggestion) is the really difficult part . apart from the fact that there is reinforcement learning , i would guess you can use the features to do so . moreover i read about an algorithm which finds intuitive pattern and that on human-level(i think it was from mit) . so maybe you can use that for this job . like i said i am really new to this topic . but i posted this so i can ask about it and not to answer questions ... EOA 
 [seq2seq] is bucketing just a tensorflow quirk ? how to choose bucket sizes ? : machinelearning currently , tensorflow does not support variable length sequences in rnns . what the bucketing trick does is actually create k different models for the k different sequence lengths , all of which share the same parameters . if tensorflow starts to support variable length rnn sequences , then one would no longer need to use this trick , and could proceed as you suggest , i.e. sort by sequence size , and then each minibatch can have a different length . i think i saw an ntm implementation in tensorflow , which had to create a different model for each possible sequence length between min.length and max.length . this should change soon , as the devs add loop control ops to the api . sidenote : theano already has this feature . check out lasagne's recurrent layer . EOQ today i learned that tensorflow does not support variable length sequences in rnns and i think it's worth contemplating this for a moment , especially in the context of what's the difference between a vector and a sequence . EOA 
 [seq2seq] is bucketing just a tensorflow quirk ? how to choose bucket sizes ? : machinelearning currently , tensorflow does not support variable length sequences in rnns . what the bucketing trick does is actually create k different models for the k different sequence lengths , all of which share the same parameters . if tensorflow starts to support variable length rnn sequences , then one would no longer need to use this trick , and could proceed as you suggest , i.e. sort by sequence size , and then each minibatch can have a different length . i think i saw an ntm implementation in tensorflow , which had to create a different model for each possible sequence length between min.length and max.length . this should change soon , as the devs add loop control ops to the api . sidenote : theano already has this feature . check out lasagne's recurrent layer . EOQ thank you for the answer . i did understand from the docs that the bucketing created different models , but that made no sense at all . why do that when the same seq2seq model can handle arbitrary lengths ? i'll switch to theano for the time being . EOA 
 [seq2seq] is bucketing just a tensorflow quirk ? how to choose bucket sizes ? : machinelearning currently , tensorflow does not support variable length sequences in rnns . what the bucketing trick does is actually create k different models for the k different sequence lengths , all of which share the same parameters . if tensorflow starts to support variable length rnn sequences , then one would no longer need to use this trick , and could proceed as you suggest , i.e. sort by sequence size , and then each minibatch can have a different length . i think i saw an ntm implementation in tensorflow , which had to create a different model for each possible sequence length between min.length and max.length . this should change soon , as the devs add loop control ops to the api . sidenote : theano already has this feature . check out lasagne's recurrent layer . EOQ in our paper which introduced seq2seq in nips14 we implemented the model with bucketing . let's not confuse implementational / code details , with the concepts . even if the user facing code has a for loop construct , under the hood there will be limits on how memory is managed , unrolling, etc . once you have played with many of these models , you may actually prefer the bucketing approach to have full control and understanding on what's really going on . p.s. from an efficiency standpoint , there is no difference between the two if both implementations are reasonable . EOA 
 [seq2seq] is bucketing just a tensorflow quirk ? how to choose bucket sizes ? : machinelearning currently , tensorflow does not support variable length sequences in rnns . what the bucketing trick does is actually create k different models for the k different sequence lengths , all of which share the same parameters . if tensorflow starts to support variable length rnn sequences , then one would no longer need to use this trick , and could proceed as you suggest , i.e. sort by sequence size , and then each minibatch can have a different length . i think i saw an ntm implementation in tensorflow , which had to create a different model for each possible sequence length between min.length and max.length . this should change soon , as the devs add loop control ops to the api . sidenote : theano already has this feature . check out lasagne's recurrent layer . EOQ this seems really related to an idea i have had for a while for partial unrolling of loop constructs , based on the same ideas as duff's device . hopefully you could control the tradeoffs between full unrolling and scan/iterative processing but i have not really investigated where the bottlenecks are for this kind of iteration on gpu . also on the theano side making any change inside scan() is nontrivial ... can you comment at all on the implementation in tensorflow-specifically why you all went for full unrolling first ( necessitating this bucketing ) ? also for /u/rhaps0dy4-in practice even in theano for efficiency you have to think about bucketing , at least for things with a long tail length wise ( sentences for mt in particular have this problem ) . for minibatch processing you must NUM pad ( and make the accompanying mask ) to the length of the longest sequence in the minibatch , and if you combine short sequences with long ones there is a ton of wasted compute . so that means you want to bucket your minibatches so that sequences of approximately the same length end up in the same minibatch . there is a lot of heavy lifting/trickiness in running neural mt models for figuring out good minibatches that have sequences of approximately the same length , especially when data doesn't fit in main cpu memory . random access to spinning disk is a really , really bad idea so you usually have to precompute/make multiple copies of the data . EOA 
 [seq2seq] is bucketing just a tensorflow quirk ? how to choose bucket sizes ? : machinelearning currently , tensorflow does not support variable length sequences in rnns . what the bucketing trick does is actually create k different models for the k different sequence lengths , all of which share the same parameters . if tensorflow starts to support variable length rnn sequences , then one would no longer need to use this trick , and could proceed as you suggest , i.e. sort by sequence size , and then each minibatch can have a different length . i think i saw an ntm implementation in tensorflow , which had to create a different model for each possible sequence length between min.length and max.length . this should change soon , as the devs add loop control ops to the api . sidenote : theano already has this feature . check out lasagne's recurrent layer . EOQ i had done some testing around unrolling several recurrent steps inside a single scan step , at the time it didn't seem to net much benefit . it ended up being pretty straightforward , really you just have to manually indicate how many steps to unroll and pad and slice any sequence inputs to that length , then reshape the scan outputs to make it interchangeable with vanilla scan . a good place to implement this would be in block's recurrent decorator , you could do it with a single keyword argument that wouldn't break any existing code . there's a decent chance i left some optimizations on the table when i was playing with this , on paper you would think that fewer compute-intensive scan steps would have some net positive. other than the standard 'compute as much as possible outside of scan' tricks , i generally can't get much more speed out of scan . i know that there has been some older advice on the user forums that you can claw back some speed by returning intermediate outputs to avoid recomputing them during the backwards pass , generally i don't have any luck doing this and i'm not sure how relevant it is now since scan has had so much dev works over the past months . EOA 
 [seq2seq] is bucketing just a tensorflow quirk ? how to choose bucket sizes ? : machinelearning currently , tensorflow does not support variable length sequences in rnns . what the bucketing trick does is actually create k different models for the k different sequence lengths , all of which share the same parameters . if tensorflow starts to support variable length rnn sequences , then one would no longer need to use this trick , and could proceed as you suggest , i.e. sort by sequence size , and then each minibatch can have a different length . i think i saw an ntm implementation in tensorflow , which had to create a different model for each possible sequence length between min.length and max.length . this should change soon , as the devs add loop control ops to the api . sidenote : theano already has this feature . check out lasagne's recurrent layer . EOQ yes well , ordering the sequences by size before processing them kind of goes around that . my data in particular has a very long tail . but it is also very thin , so the above procedure should work fine. ( right? ) EOA 
 [question] cascaded random forest ( baumann et.al. ) : machinelearning what paper are you referring to ( provide a citation ) ? that would be extremely helpful for evaluating the pictured algorithm . EOQ baumann , florian, et al . cascaded random forest for fast object detection . image analysis . springer berlin heidelberg , NUM NUM-142 . pdf EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ yeah , it seems like i would have to create something that preprocesses videos to parse out the location of buildings and units from the raw video . but i also think one of the biggest issue seems to be that the atari games can easily be simulated much faster than rt . to do that for coc i think would be pretty non-trivial .. anyway , i appreciate your response . gives me some stuff to think about and areas to research . i wouldn't care to turn this into a bot and try to make money , but i have another cv idea i'm almost certain could make some side revenue so i may pursue that instead . if i'm going to spend a lot of time i'd at least like to have a practical use for it ! i'm still very interested in games and machine learning though , given that they provide an endless source of training data . to me it seems like it will be an important area with application extending beyond games EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ i don't know much about clash of clans . here are a few things to keep in mind with respect to deepmind's deep reinforcement learning : it is for mdps , meaning all the necessary information must be on screen at any given time. extending it to pomdps ( where the agent must learn to select and remember some information for later use within the game ) is not at all trivial . for example , a game where the screen only shows part of the map will be difficult unless you use some hack to convert the game video display to something more informative . it uses the score to find good strategies . if there is an obvious strategy that gets you lots of points early on but also fails after a few minutes , it is likely to pick that-and finding an alternative strategy that trades early points for long-term success will be extremely difficult ( you'd need a very luck exploration streak-the chance decreases exponentially with each consecutive time step needed ) . to rephrase this slightly more theoretically : the reinforcement learning problem is very much about credit attribution-is it action x at timestep t1 that saved my ass at t100 , or was it action y at timestep t2 ? you need to figure out which if you want to learn the best way to act . if an action and its observable rewarding effects are close to one another , the credit attribution problem is easier to solve . in atari games , that is often the case . perhaps not so much in clash of clans . it works on atari games . i just googled a video of clash of clans and that's a different level of graphism . with emulated atari games , one can run the learning algorithm at many times real-life speed , completing hundreds of games in minutes . you won't have this luxury . you will also need a much bigger network to successfully capture the image . so i think working directly from the video output , with no specialized processing to encode the information in a suitable format and add other important information , is not going to work . so i'm with pgopuschel. . it looks too difficult . it's probably more interesting to work on those atari games that deepmind's deep reinforcement learning is not good at ( yet ) . EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ yeah , it seems like i would have to create something that preprocesses videos to parse out the location of buildings and units from the raw video . but i also think one of the biggest issue seems to be that the atari games can easily be simulated much faster than rt . to do that for coc i think would be pretty non-trivial .. anyway , i appreciate your response . gives me some stuff to think about and areas to research . i wouldn't care to turn this into a bot and try to make money , but i have another cv idea i'm almost certain could make some side revenue so i may pursue that instead . i'm still very interested in games and machine learning though , given that they provide an endless source of training data . to me it seems like it will be an important area with application extending beyond games EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ i sort of agree with other posters , though i think the difficulty of the problem will ultimately come down to how you formulate the state and action spaces . the action space should actually be relatively easy , one possible encoding could be a discrete parameterized action space where a discrete action is the type of unit you want to spawn ( or none ) , and the associated parameters would be the coordinates you want to spawn them at . however , the state space will be tricky , as others said . if you are hoping to take an image of the screen and feed it in , training will take significantly more time then something like atari which is low-resolution and has a small NUM-bit colour palette . compare that to images of clash of clans , and your cnn will need to be able to summarize the locations , types, and states of all the different buildings and walls , their health , etc. this will be very non-trivial for a cnn to do from raw images . if , however, you can find a way to encode clash of clans state into something similar and not image based though , you may have more luck . for example , if you encoded each building structure with a location , health, and type , you could apply a cnn filter across the structures which could work quite well . tl;dr: likely the hardest part of this will be the computer vision aspect of transforming the complex raw image into a useful state representation of the game which is more complex in graphics and overall state then atari . EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ yeah , it seems like i would have to create something that preprocesses videos to parse out the location of buildings and units from the raw video . but i also think one of the biggest issue seems to be that the atari games can easily be simulated much faster than rt . to do that for coc i think would be pretty non-trivial .. anyway , i appreciate your response . gives me some stuff to think about and areas to research . i wouldn't care to turn this into a bot and try to make money , but i have another cv idea i'm almost certain could make some side revenue so i may pursue that instead . if i'm going to spend a lot of time i'd at least like to have a practical use for it ! i'm still very interested in games and machine learning though , given that they provide an endless source of training data . to me it seems like it will be an important area with application extending beyond games EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ you mention setting up a server , could you sniff the network traffic to figure out the protocol and go from there ? EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ i was thinking along these lines also . is it possible to learn from network packets instead of video ? EOA 
 feasibility of applying deep reinforcment learning to clash of clans : machinelearning i am not an expert in image recognition or reinforcement learning , so take my comment for what it's worth . also, i don't know clash of clans , but i'm guessing that it's somewhat similar to league of legends or heroes of the storm . what you are proposing may be feasible in theory , but in practice the current state of the art is far away from being able solve a problem as complex as this . a couple of things : the space of possible actions here is huge . compare that to the action space for atari , nes or board games that are solved by rl . the action space in these is tiny (in which direction to move the joystick ? which button press?). an infinite action space isn't a problem per se , for example controlling a helicopter has an infinite action space , but in your case the action space isn't smooth . it has a huge amount of discrete combinations of actions . any approximation function like a nn is gonna have a hard time learning this . similar to the action space , the state space is also huge and non-smooth . compare that to something like atari , which has a few bytes of ram to represent the state . that's a whole different league . again, infinite state space isn't the problem , it's the nature of the state space . given NUM and NUM , you would also need a lot more training data than something like atari or nes games need . even if you had that ( or could generate it ) , my intuition is that you'd probably be training for months before you'd be learning anything useful with the current methods , simple due the size of the problem and the parameter space . unless you can write a simulator and train on that perhaps , and massively parallelize your training on a cluster . the actual image recognition is probably the easy part , but even that seems challenging given the complex nature and high resolution ( ? ) of coc . images are often rescaled to tens of thousands of pixels . atari or nes have low resolutions so that's fine. similarly , it also works just fine for natural images . but as a human would you be able to recognize details in massively downscaled video games images ? i think you would be losing valuable information , and while you can deal with large images on various ways , doing so is pretty slow and compute-intensive. yes , there are other tricks to deal with large images but that just adds to the complexity . same for videos . there are probably things you can do , like massively limiting the state and action space manually ( not via function approximation ) , essentially giving the agent just a few scenarios to choose from , e.g. something similar how the in-game ais are programmed . but don't expect to feed raw image/video data to the algorithm to make it learn how to play the game . EOQ agreed with all other commentators above . i was looking into a similar game ( defense of the ancients , dota ) and ran into said problems . it would be easier to to look into more simpler games in which ai competitions are already running URL . a more detailed post : URL EOA 
 is there anything like photoshop's content-aware fill for texturing NUM d objects ? : machinelearning could you perform texture synthesis on a NUM d texture map ? so if you have a texture map of the sphere then generate the texture in NUM d . not sure how seams might appear on a surface like that but it might a good place to start ? EOQ i was wondering if anyone else has done it before . it does sound kind of interesting , but i haven't done any image manipulation or machine learning before . i'm thinking i'd do something like standard texture synthesis , but have it so when it copies part of the texture , it would warp it before pasting it to take into account the changing metric on the surface . getting rid of the seams shouldn't be a problem . that's just a matter of telling it that the pixels on the right are just to the left of the ones on the left . EOA 
 predicting gps coordinates-timestamp ; anything out of the box ? : machinelearning you re looking for multi target regression . EOQ thanks ! i'll look into that ! EOA 
 predicting gps coordinates-timestamp ; anything out of the box ? : machinelearning you re looking for multi target regression . EOQ solutions to this kaggle challenge may give you some ideas ( or perhaps it was that you were working on in the first place ... ) EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ i was wondering for example , how much it would take me to forward propagate a minibatch of NUM images on different architectures considering i have a certain graphics card , i know it's really guesstimating , but i just was wondering if it would be milliseconds , seconds or minutes EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ i've never seen any flops based estimates . most of the time performance is measured in time to train to error level ( because architectural differences make such a huge difference ) in terms of that URL and URL are useful . i think there are some good nvidia papers around comparing their different architectures . EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ i sas wondering for example , how much it would take me to forward propagate a minibatch of NUM images on different architectures considering i have a certain graphics card , i know it's really guesstimating , but i just was wondering if it would be milliseconds , seconds or minutes EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ see page NUM of the inception paper : URL EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ well the other way to do it , for a given network configuration you can work out the actual number of flops by counting the multiply-add . for instance for convolution layer with m input features of size mx , my and n output feature of size nx , ny, with a kernel size of kxk and stride s-the total multiply add would be-m-n-nx-ny-k-k . so in terms of flops this would be NUM-m-n-nx-ny-k-k flops . so with analytical model like above you can arrive at the flops/image for any network config , however this is per forward pass . the back pass with wts update , has NUM x this calculations-so for a full training the number of flops would NUM x of flops per image . this calculated for some popular networks-alexnet:3-1.2 gflops/image , overfeat NUM-5.8 gflops/image , vgga NUM-11.4 gflops/image . hope this helps . EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ this blog also has some details on this-URL . this was also covered in the authors idf talk ... EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ yeah it is exactly what i was looking for , thank you ! are the numbers listed just for forward pass ? what is a plausible flops per second of a modern home pc ? EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ yeah it is exactly what i was looking for , thank you ! are the numbers listed just for forward pass ? what is a plausible flops per second of a modern home pc ? EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ yeah it is exactly what i was looking for , thank you ! are the numbers listed just for forward pass ? what is a plausible flops per second of a modern home pc ? EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ yeah it is exactly what i was looking for , thank you ! are the numbers listed just for forward pass ? what is a plausible flops per second of a modern home pc ? EOA 
 computational requirements of state of the art models : machinelearning i am not sure what are you asking for ! if it's about hardware requirements , i need to know the answer too . EOQ as much as i hate it , big-o is sometimes useful...actually rarely useful . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ by stacking NUM layers , do you mean : inputlayer-input() gru1-grulayer(inputlayer , num.units-1024, forwards) gru2-grulayer(gru1 , num.units-1024, backwards) gru3-grulayer(gru2 , num.units-1024, forwards) gru4-grulayer(gru3 , num.units-1024, backwards) gru5-grulayer(gru4 , num.units-1024, forwards) gru6-grulayer(gru5 , num.units-1024, backwards) output-denselayer(gru6 , num.units-1024, num.classes , sigmoid) and did you remove the bias ? shouldn't the batchnorm layer's translation nullify the effect of it ? shouldn't normalizing everything at once , or doing it for each timestep , should result in the same output ? as the parameters for the batch normalization layers are the same and the gradients are going to be the same as well ? EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ i ran each layer forward , rather than alternating between forward and backward , but that probably has nothing to do with bn . i did remove the bias terms wherever i used bn . normalizing each timestep separately could give different results from doing it all at once because the means/variances at each timestep can be different . see section NUM of URL EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ that makes sense . did you take only the previous layer as input for the subsequent gru layers or did you also pass the original input layer as well ? such that inputlayer-input() gru1-grulayer(inputlayer , num.units-1024, forwards) gru2-grulayer(concat(gru1 , inputlayer), num.units-1024 , backwards) gru3-grulayer(concat(gru2 , inputlayer), num.units-1024 , forwards) etc . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ i tried both , and both worked . concatenating the original input layer led to slightly faster / better convergence , but i decided it wasn't worth the extra computational cost . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ ischaan , when you say improvement , do you mean train time or an actual lower validation loss ? i haven't had much luck with bn with NUM layer gru NUM . i didn't get any improvement and training time was longer . i did bn(wx) for only the first two terms of the gru but perhaps i should apply it to the third one as well . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ both . it's worth noting that i was working with enough data that even my largest models underfit the train set . without bn : URL with bn : URL ( the printout format is a bit different between the two , but they're otherwise from the same codebase ) EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ my train set is NUM minibatches , each with NUM sequences of NUM timesteps of NUM-dimensional vectors . i trained on a titan x using a reasonably optimized theano gru implementation i wrote . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ NUM k batches x NUM sequences-NUM million training samples-are you sure you're overfitting ? that seems like a relatively low amount of data to train NUM stacked layers . for something like NUM stacked layers , i would expect something like NUM million samples in my opinion . but give the NUM or NUM unit a try if you have gpu time ! EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ any reason you're not factoring sequence length or output dimensionality into that calculation ? i'd imagine the amount of info the model needs to memorize to overfit grows linearly with the former , and at least logarithmically with the latter ( assuming one-hot output vectors ) . to some extent i'm definitely overfitting ( i.e. train cost > ; validation cost ) but not 'all the way' ( i.e. train cost still well above zero ) , which is pretty normal for my task ( language modeling ) . see table NUM in URL , which lists a similar result for a comparable model and dataset size . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ ishaan , yea i agree with that output dimensionality and seq length significantly impact how big your network should be . i too am doing language modeling . i have very similar learning curves like yours . i have found that changing the structure ( like attention soft attention and whatnot ) will only help you compensate for underfitting you are describing . i guess what i'm saying is : the reason why you're underfitting is probably due to network structure and the reason why you're overfitting is because you have too little data . but that's just opinion . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ hi , i'm one of the authors of the ds2 paper . we actually do use bn for all types of rnns in the paper ( vanilla , lstm, gru ) , by replacing each of the linear feed-in wx terms with bn(wx) [ as was correctly suggested by ..ishann ] . we find it to be beneficial for deep recurrent networks of all types . sorry if that wasn't clear from the paper . EOA 
 rnn batch normalization for gru/lstm : machinelearning i've gotten a solid improvement out of bn on NUM layers of stacked NUM-unit grus by applying it to each of the NUM transformed inputs (i.e. replace each of the NUM wx terms with bn(wx)) . i expect lstm would work the same way . i had to apply it stepwise ( i.e. normalize with respect to each timestep separately ) to get any improvement , but i think it's worth trying to normalize everything at once as well . EOQ jesse , thanks for the clarification . when you say you replaced the linear feed-in for wx terms , did you then do for the gru : r-sig(bn(wx)-u.ht) z-sig(bn(wx)-u.ht) c-tanh(bn(wx)-u(r x ht)) new.h-u-ht-( NUM -u )-c or r-sig(bn(wx)-u.ht) z-sig(bn(wx)-u.ht) c-tanh(wx)-u(r x ht)) new.h-u-ht-( NUM -u )-c EOA 
 continuous hmm library/tool for continuous gesture recognition : machinelearning i recommend you : URL EOQ any reason you can't discretize the input/output space using something like vector quantization ? EOA 
 continuous hmm library/tool for continuous gesture recognition : machinelearning i recommend you : URL EOQ sorry , maybe i was not clear . the data are discrete but it has continuous density ( mixture of gaussians ) . i edited the question and i hope it's clearer now . EOA 
 continuous hmm library/tool for continuous gesture recognition : machinelearning i recommend you : URL EOQ htk is an older tool you could use for that . besides, there are plenty of gmm projects on github . question is , why stop at gmms ? why not use something like svms or anns instead ? also , instead of using hmms , why not take a look at openfst as that is what most are using these days and it's well documented and quite complete . EOA 
 continuous hmm library/tool for continuous gesture recognition : machinelearning i recommend you : URL EOQ thanks a lot . htk is more suitable for speech recognition , actually the g2tk is built from htk , but both of them are old and clunky . thanks for the suggestions , and i will give a try for sure . based on my knowledge hmms are the most popular algorithm for sign language recognition , that is why am trying to do it first EOA 
 continuous hmm library/tool for continuous gesture recognition : machinelearning i recommend you : URL EOQ wfsts are an extension on hmms-they are more expressive and regarded as superseding hmms in the recent years . you can take a look at kaldi for an oss example of that-check out this blog post for a little insight on how they are used to decode words from continuous input . EOA 
 continuous hmm library/tool for continuous gesture recognition : machinelearning i recommend you : URL EOQ thanks million . that was helpful . i have no prior knowledge about wfst but i will look into it . EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ yes , sum of one vs all hinges work for multiclass-i have tried it myself for cifar10 and it was no worse then softmax . but it was mix of l1 and l2 svm ( and general l.p svm work too ) . the original paper about l2 svm also was using sum of one vs all for multiclass : URL about pretraining-you can use any pretrained network and just replace last fully connected layer with n fully connected each with l1 or l2 hinge loss . that way only classification layers should be retrained . errata: erroneous statement removed EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ i wonder if you could use the same tricks from word embeddings to share weights for the rest piece of each individual classifier . EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ any progress on research on multilayer kernel machine ? EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ have you seen this paper ? URL also this : URL EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ but do they also try to maximize the margin ? so if i understand correctly to something be called svm it needs to have kernel and it needs to maximize the margin separating the classes ? EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ the defining characteristic which gives svms their name is the fact that only a subset of the training data ( the support vectors ) is part of the decision function . EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ i often see this stated that svms are less natural for multiclass , but this problem was solved in NUM-2005 with the structured svm ( aka max margin markov network ) loss . simply created a margin between the correct class score and the second-highest scoring class . it is very simple to implement , i describe it in a bit more detail in this post if you are interested : URL EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ probably not exactly what you mean , but there are cases where it makes sense ( sorry a bit of self promotion as well ) : URL EOA 
 why isn't cnns with hinge loss popular ? can we call it a deep svm ? : machinelearning i have consistently seen a small but noticeable gain when i applied them to NUM class problems , but there are no pretrained networks of this type that i am aware of . i have wondered if you could generalize the one vs . all approach and build it into the network directly ( dumbest way-have n hinge outputs , one for every class vs . rest with new loss combining them )-anyone seen research on this ? EOQ here is a quite nice one : URL EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ i have thought about this too , but there is a problem with this ! units like triplets ( or other-plets like quintuplets ) are not representable in powers of two ( since the fraction would be NUM 333 recurring ) . building durations from a set of smaller durations would be impossible in this case . maybe having multiple 'elementary' time units would be a solution , but that defeats the purpose of this method and starts looking like the second method i described . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ perhaps heretical but triplets and quintuplets are kind of edge cases , no? maybe makes sense to get everything working without them , and worry about them later ? great music can be built on atomic note lengths of sixteenths or even eighths ... EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ triplets can be pseudo-converted to alternate form , like quarter plus NUM th , tied (.25-.0625-.3125 .33), which would be close . alternatively i think you can just change the time signature on that particular passage from e.g. NUM /4 to NUM /4 at a different bpm or something . i personally would start by just trying to learn passages without those in them at first , to see if the idea works . alternatively you can just try to predict amplitude separately , and hope the networks learns onsets naturally . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ couldn't you just represent duration with a continuous variable showing how many beats a note lasted ? this would let you just represent triplets as NUM 333 directly . or would nonlinearity be really important ? EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ you can but regression is usually much harder ( mse costs are tricky in my experience ) . you could have a duration token that covers common durations separate from pitch including .33 which might be easier , but technically there is an ordinal relationship which might mean you want an ordinal cost , rather than just categorical from something like softmax . edit : this seems like what /u/steven2358 is suggesting :) cool ! EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ so how do you represent something as an ordinal rather than just categorical relationship ? by modeling long notes as a sequence of short ones ? EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ this is something different-long as a series of short is just to make the problem tokenized into discrete time. ordinal basically means you have tokens , but there is an ordering among the tokens , so not all errors are the same . simplest example-i have NUM groups ( short , medium, and tall ) as class NUM , NUM , and NUM respectively . each error is not the same ( unlike basic classification ) e.g. mistaking token NUM for token NUM gives a larger gradient than mistaking token NUM and token NUM ( for example ) . generalizing this is kinda tricky , but you can hard code it in most network toolkits ( to my knowledge ) . see this paper for some math . you can think of it as the middle ground between mse and softmax classification . there are also ways to use binary crossentropy cost for regression if you have maximum and minimum extents-see my comment here . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ awesome , i'll take a look at that ! EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ i would leave NUM pitches-silence and combine them with rhythm features . if you have NUM rhythm features that would give you a features space of NUM dimensions , which should be manageable . as for the rhythm features , you could obtain a reduced set by introducing some simplifications in the database ( e.g. split up the longest notes into shorter ones ) and taking into account restrictions on the duration ( e.g. periodicity / melodies typically last a whole number of bars ) . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ thank you for your answer ! i already simplified my database a bit so there are only ~7 different durations ( whole note , half, quarter , eigth, NUM th , triplet, sextuplet ) , but with this method i somehow have to indicate for the lstm the different notes are tied together . this would be another dimension in my featurevector maybe ? and, if i do this , what is the objection to splitting pitch and duration entirely ? EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ an indicator for tying notes together may defeat the purpose of choosing a set of basic durations . my idea was you perform a simplified experiment in which notes that are too long are simply split into shorter ones . only if this works you can start worrying about longer notes / tying . as for separating pitch and duration , this could be a viable option too , but i don't have enough experience with lstm to sense what it implies . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ instead of durations could look at them as all NUM th notes ( or some set quantisation ) , with a single flag for the option of being tied with the following note . ahh /u/kkastner already onto it . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ a bit ot , but speaking as a musician/composer , melody generation isn't much of an issue for most composers . however harmonisation ( interesting chord sequences that complement and counterpoint a given monophonic melody line ) is something where assisted chord progression suggestions would be quite useful-in particular when dealing with more complex music like jazz . take a look at this harmonisation example , from a very talented acapella composer/singer : URL it might be doable to have a neural network suggest those types of chord sequences given a melody input , if trained with the right type of melody harmonisation corpus . would be interesting to see if the algorithm can learn music theory tricks such as tritone substitutions and produce interesting suggestions . i was thinking about using real books as training corpus for jazz , but the algorithm would have to learn not only the chord symbols that correspond to the melody , but also learn interesting/beautiful chord voicings ( which have a massive impact on the perceived sound )-and how to optimise chord voicings , so that they tie smoothly together when going from one chord to the next in the sequence . there wouldn't need to be varying note lengths for the chords , just one or two chord changes per bar , which also makes it easier . if someone is working on something related , would love to collab . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ i have thought about similar stuff in the past , but the problem is getting quality data . there is no big dataset of quality , properly harmonized , digital ( jazz ) music . maybe when omr gets off the ground , but the current quality of that is poor ( to say the least ) . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ absolutely agree . might be possible to extract from tutorial cd's etc via audio-> ;midi, but it could be messy . having said that , to create the training data wouldn't take many days of recording a good jazz pianist playing through hundreds of jazz standards in real books on a midi keyboard . could even normalise the songs to the same key so that it doesn't throw the training algorithm off . what kind of a format/structure would the training data need to be ? i might be able to help EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ i think then still getting enough data to properly train a network ( preferrable bi-directional , since you need dependencies from both past and future ) is an almost impossible feat for now . as a sidenote , i do not think the realbooks are a good source of data ;) maybe as reference , but they are so riddled with mistakes and inaccuracies that i would prefer to stay away from sources like that . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ i have no help to offer but i'd be very interested in what you come up with ;) EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ this is interesting ! i have been working on very similar stuff , i have a modest dataset of charlie parker solos ( saxophone ) , and also looking at training a recurrent network on that . just out of curiosity , how big is your dataset , and how big you think it should be for a decent result ? i have a little over NUM solos , which equals about NUM k-NUM k notes . my feeling is i need a little more for nice results , but in reality i have no clue . about the feature selection , i think your idea is a good one , but i would not be sure how to implement it properly ( especially the output layer , how would you split the output so you get a seperate classification for each of your variables ? the time quantization sounds interesting too but i also see some disadvantages to this method . training longer term dependancies would get a lot more difficult for the network , if every not in the dataset consists of multiple events . either your recurrency gets a lot shorter , or you need a much better gpu for training (which i do not have ) . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ timing and quantisation is really going to show it's head when it comes to charlie parker . needs to occasionally switch into triplets and sextuplets , and those little leading ghost notes he uses all the time . surely something interesting to come from that data . maybe the next weather report ;-) EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ thats what i thought , a 'parker bot' should be fun to make ! i chose my data carefully , mostly mid-uptempo things , no crazy stuff . all the transcriptions are my own , and i simplified a lot of the material ( no ghost notes , no weird tuplets , no ballads , only 'normal' stuff ) . i am thinking the easiest way to go is to go the music abc route , but simplified for my purpose . maybe interesting for mr op : the first thing i want to try is representing each note as NUM or more events ; one is the pitch , the next one ( or multiple next ones ) are the duration ( built from basic durations ) . this is a similar idea to musicabc notation , but a bit shorter . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ for monophonic music , people have gotten good results simply by training a char-rnn instance from abc-formatted source files . see [ NUM ] [2] [ NUM ] . with a custom model , folks can generate good polyphonic music while also taking advantage of features such as transposition-independence and learning from the rhythm/meter structure ( see link and further output from another instance of the model ) . EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ i hav seen this on other places on the internet too , but it is unclear to me why you would want to use an extra level of language on top of the music . doesn't this mean your network has to learn the structure of abc notation too , while this isn't very interesting for the results ? EOA 
 sheet music feature representation for lstm networks : machinelearning for the notes , you could break it into all pitches ( ~88ish for standard piano? )-a silence note , or even just what octave-pitch inside octave . for duration , i would personally consider using a minimum atomic unit aka NUM th or NUM nd notes , then building longer notes out of multiples of these . the rnn should be able to learn repetition pretty easily though getting it to stop repeating is another matter . not sure how to handle tied notes/glissando and so on . i would start with a representation similar to the midi used in , that way you have something to compare against . EOQ yeah that is true . using this , at least as a first benchmark , might be the way to go . EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ thanks :) from your experience , what is a good way to determine feasible clipping values ? ps: i think the activation function relu in my dense layer might contribute to those hiccups . EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ grid search is the norm but you should get decent results trying out [.1, .5, NUM ]. tighter clipping might slow down learning more , but will be more robust to explosion . also you likely want to use bounded activation functions in any recurrent-recurrent transitions to help prevent blowup . tanh is common . EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ you were absolutely right , clipping removed the hiccups . thank you again :) EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ gradient clipping is key ( !!! ) to training rnns in my experience . without it , very few architectures work . EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ without any info on your setup , type of network , training, etc , your graph does not tell us a lot . EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ thanks , i thought this could be a common phenomenon an experienced machine learning person may recognize. i provided additional information on framework and model . if you need more , i'd be happy to add further details :) EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ it is fairly common in my experience . EOA 
 reason for performance drops during training of rnn ? : machinelearning one likely cause is accidentally big gradients . i've noticed a similar phenomenon in my models and use the clipvalue and clipnorm parameters to the optimizer in keras to prevent these 'hiccups' in training progress . it's been working great so far . EOQ common in my experience too . many times , it doesn't reach the same performance levels as before the hiccup . best practice would be to checkpoint the model everytime validation set performance rises . EOA 
 need advice : building heroku of autonomous intelligence : machinelearning i read the description , but i still don't understand the goal of your project . what does your service add ? EOQ right now it's a platform where you add devices/agents , groups, services , etc and we're adding third party services you can provision to make them smarter , give them super powers , etc. EOA 
 need advice : building heroku of autonomous intelligence : machinelearning i read the description , but i still don't understand the goal of your project . what does your service add ? EOQ i be seen your comments and questions elsewhere , on hn i think . i didn't share my thoughts at the time. i believe i see what you are aiming for because i once ran a company that was doing something similar . while we had good customers , it wasn't a great success . let me share that one of our investors , looking for more funding , tried the 'heroku of' pitch . some peers discouraged him to do so . there are many reasons for this , among which your true customers may not know or care about heroku , and it hides your potential true value for them and your true aim . ai is the way to go , agents may not be a hip term just now as it was in the past , but somehow there must be value in smart agents across networks . have fun ( i still have ; ) ) ! edit : also if your product is not open source , you're too early to advertize like you're doing , imo. EOA 
 need advice : building heroku of autonomous intelligence : machinelearning i read the description , but i still don't understand the goal of your project . what does your service add ? EOQ thanks ! i'm only using that 'heroku of' to imply a couple of things simultaneously , namely it's a platform where you add things , and are able to provision third party services . this 'pitch' goes out to developers , because i'm hoping they have had some idea/experience with it . if i elaborate i typically get push back saying i'm being too verbose , or too abstract , or they just don't understand what i'm building . smart agents across network might be simply renamed ai companions in my pitch . i'm refining it as i get feedback . where are you located ? EOA 
 need advice : building heroku of autonomous intelligence : machinelearning i read the description , but i still don't understand the goal of your project . what does your service add ? EOQ i like ai companions :) feel free to message me in pv if you d like to chat . EOA 
 need advice : building heroku of autonomous intelligence : machinelearning i read the description , but i still don't understand the goal of your project . what does your service add ? EOQ this is buzzwordy and abstract . what are some practical examples of playa at work ? EOA 
 need advice : building heroku of autonomous intelligence : machinelearning i read the description , but i still don't understand the goal of your project . what does your service add ? EOQ hm , what part is buzzword-y ? this is a platform so the services seem abstract i guess . the flow right now would go : add the playa client to your device , add services available to your group or to the public , provision services ( ml/ai ) , and we're going to provide search , contracts, and transaction capabilities . it's like your device's super powers , talents, and brain in the cloud basically . EOA 
 need advice : building heroku of autonomous intelligence : machinelearning i read the description , but i still don't understand the goal of your project . what does your service add ? EOQ so essential it's an ai api api ? :) EOA 
 that moment when your weka decision model results are too good to be true !!! : machinelearning did you split the dataset into training and test and/or use any cross validation to test the generalisation of the model ? if you are training and then testing on the same set of data this result is not that unlikely . EOQ while creating the additional features you may have leaked the label and the tree got it . look for features that are created based on label . EOA 
 that moment when your weka decision model results are too good to be true !!! : machinelearning did you split the dataset into training and test and/or use any cross validation to test the generalisation of the model ? if you are training and then testing on the same set of data this result is not that unlikely . EOQ hi dandxy89 , in my case , i have only NUM rows , which is a really small dataset . plus its very unbalanced too . agree with your comment that train / test is better , but given how weak my dataset is , i fear that it is not an option ? boccaff , thank you too ! when you say that i may have leaked the label , are you suggesting that my outcome variable was duplicated in the dataset as an attribute , and that the algorithm jumped on it , picked it and started using it ? EOA 
 that moment when your weka decision model results are too good to be true !!! : machinelearning did you split the dataset into training and test and/or use any cross validation to test the generalisation of the model ? if you are training and then testing on the same set of data this result is not that unlikely . EOQ you may have created some feature based on the label . so the algorithm picked it . since it is a tree , it should be easy to inspect . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ remove the exclamation point EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ btw , enumerating questions might be helpful . this way questions wouldn't need to be quoted . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ fixed . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ is openai planning on doing work related to compiling data sets that would be openly available ? data is of course crucial to machine learning , so having proprietary data is an advantage for big companies like google and facebook . that's why i'm curious if openai is interested in working towards a broader distribution of data , in line with its mission to broadly distribute ai technology in general . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ creating datasets and benchmarks can be extremely useful and conducive for research (e.g. imagenet , atari). additionally , what made imagenet so valuable was not only the data itself , but the additional layers around it : the benchmark , the competition , the workshops , etc. if we identify a specific dataset that we believe will advance the state of research , we will build it . however, often very good research can be done with what currently exists out there , and data is critical much more immediately for a company that needs to get a strong result than a researcher trying to come up with a better model . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i think new good datasets/benchmarks will advance the field faster than many people realizes . i know creating new datasets are not so fun as creating new models , but please don't take the importance of datasets lightly ( i'm not implying that you are ) . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ agreed . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i am currently working on something that i have coined the openbraininitiative and the longterm goal is to create an equivalent to openstreetmaps for machine learning datasets . i think it can be very valuable , not only to advance the state of artificial intelligence but also to engage users in unforeseen ways . it will also give the open source community a chance to fight against the giants like google or apple . ( just as openstreetmaps has already demonstrated , it's arguably the more detailed map in terms of road coverage in europe ) . the core feature will be a changeset , a concept borrowed from osm and wikipedia . and the data will be very loose , just like in osm and can also be binary ( e.g. for voice recordings or whatnot ) . i am just putting this out so maybe , if someone is interested in collaborating i'd be glad to hear about it . github project is found over here : URL EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ can you please elaborate a bit more on the project or perhaps update the repo's wiki page ? i am interested in collaborating in the project but would need a little more understanding of the problem statement we are dealing with . thanks EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ sure ! in my opinion , openstreetmaps was created because some people wanted to collaboratively create the best map out there . in the same spirit i would like to create the openbraininitiative to build a dataset which enables the best dictation engine , for example . i am living in switzerland , currently. there is no speech-to-text engine for swiss german . but i imagine there are quite a few people out there who'd be happy to collaborate on aggreagating the needed data or correcting an initial speech-to-text engine . of course , speech-to-text or the reverse is just one use case , ideally the platform would be open for all sorts of datasets . but i think it's one that's easily graspable . from a technical standpoint , everything should be centered around changesets and the database is essentially a very large key-value storage with different nodes and relations . the interpretation then is absolutely the decision of the renderer . note that the same is true for osm , where you can have e.g. a nautical map or a train map all based on the same database . in the osm spirit there should also be an obi editor like josm that can communicate changesets to the openbrain servers . and these editors could be tailored to specific tasks ( ie . image labeling , voice labeling ... ) well , i don't know if that's still too abstract , but hopefully i was able to get the basic idea across . what fascinates me is that osm has actually facilitated quite a few companies ( mapbox , mapzen, geofabrik and many more ) and i am NUM % sure that the same would happen if there was an open datasets repository that people could freely contribute to . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ just to add to this question : where will the code ( and possibly data ) be available ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ we’ll post code on github ( URL ) , and link data from our site ( URL ) and/or twitter ( URL ) . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hello , thanks for doing this ama . my question is mostly for ilya sutskever and wojciech zaremba . i've also asked since your python interpreter lstm model and graves et al . neural turing machine there have been many works by your groups in the direction of learning arbitrarily deep algorithms from data . progress has been amazing , for instance one year ago you ( sutskever ) , more recently you managed to learn long binary multiplication with your . however , i am a bit concerned that the training optimization problem for these models seems to be quite hard . in your most recent papers you used extensive hyperparameter search/restarts , curricula, sgld , logarithmic barrier functions and other tricks in order to achieve convergence . even with these advanced training techniques , in the neural gpu paper you couldn't achieve good results on decimal digits and in the paper you identified several tasks which were hard to train , mostly did not discretize and not always generalize to longer sequences . by contrast , convnets for image processing or even seq2seq recurrent models for nlp can be trained much more easily , in some works they are even trained by vanilla sgd without ( reported ) hyperparameter search . maybe this is just an issue of novelty , and once good architectural details , hyperparameter ranges and initialization schemes are found for algorithmic neural models , training them to learn complex algorithms will be as easy as training a convnet on imagenet . but i wonder if the problem of learning complex algorithms from data is instead an intrinsically harder combinatorial problem not well suited for gradient-based optimization . image recognition is intuitively a continuous and smooth problem : in principle you could smoothly morph between images of objects of different classes and expect the classification probabilities to change smoothly . many nlp tasks arguably become continuous and smooth once text is encoded as word embeddings , which can be computed even by shallow models ( essentially low-rank approximate matrix decompositions ) and yet capture non-trivial syntactic and semantic information . ideally, we could imagine program embeddings that capture some high-level notion of semantic similarity and semantic gradients between programs or subprograms ( which is what reed and de freitas explicitly attempt in their program induction form examples can be also done symbolically by reducing it to combinatorial optimization and then solving it using a sat or ilp solver (e.g. solar-lezama's ) . in general all instances of combinatorial optimization can be reformulated in terms of minimization of a differentiable function , but i wouldn't expect gradient-based optimization to outperform specialized sat or ilp solvers for many moderately hard instances . so my question is : is the empirical hardness of program induction by neural models an indication that program induction may be an intrinsically hard combinatorial optimization problem not well suited to gradient-based optimization methods ? if so , could gradient-based optimization be salvaged by , for instance , combining it with more traditional combinatorial optimization methods (e.g. branch-and-bound , mcmc, etc.)? on a different note , i am very interested in your work and i would love to join your team . what kind of profiles do you seek ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ [ deleted ] EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ we intend to conduct most of our research using publicly available datasets . however , if we find ourselves making significant use of proprietary data for our research , then we will either try to convince the company to release an appropriately anonymized version or the dataset , or simply minimize our usage of such data . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how important do you think your phd program was for you ? what did you learn that you could not have learned in industry ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ [ deleted ] EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ re : NUM : the motivation behind this research is simply the desire to solve as many problems as possible . it is clear that symbolic-style processing is something that our models will eventually have to do , so it makes sense to see if there exist deep learning architectures that can already learn to reason in this way using backpropagation . fortunately , the answer appears to be at least partly affirmative . re : NUM : i got interested in neural networks , because to me the notion of a computer program that can learn from experience seemed inconceivable . in addition , the backpropagation algorithm seemed just so cool . these two facts made me want to study and to work in the area , which was possible because i was an undergraduate in the university of toronto , where geoff hinton was working . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ differentiable memory structures have been an exciting area recently , with many different formulations explored . two questions i have in this are are : how useful are models that required supervised 'stack traces' to teach memory access primitives , as opposed to models that learn purely from input/output pairs ? for toy examples it is possible to design the proper stack trace to train the system on , but this doesn't seem feasible for real world data where we don't necessarily know how the system will need to interact with memory . many papers have reported results on synthetic tasks ( copy , repeat copy , etc ) which show the proposed architecture excels at solving that problem , however there has been less reported on real world data sets . in your opinion does there exist an 'imagenet for rnns' dataset , and if not what attributes do you think would be important for designing a standard data set which can challenge the various recurrent functions that are being experimented with currently ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ thanks for the answer , best of luck to the openai team ! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ are you hiring ? do you have a growth strategy ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ yes , we’re hiring : URL. we’re being very deliberate with our growth , as we think small , tight-knit teams can have outsize results . we don’t have specific growth targets , but are aiming to build an environment with great people who make each other more productive. ( we particularly take inspiration from organizations like xerox parc. ) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what do you envision the relationship between the research engineer and research scientist to be ? how will their roles overlap and how will they differ ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ we believe that the best strategy is to hire great people and give them lots of freedom . engineers and scientists will collaborate closely , ideally pretty organically . a lot of very successful work is the result of a strong researcher working closely with an engineer . there will be some tasks that the engineering team as a whole is responsible for , such as maintaining the cluster , establishing benchmarks , and scaling up new algorithms . there will be some tasks that the research team as a whole will be responsible for , namely producing new ai ideas and proving them out . but in practice the lines will be pretty fuzzy : we expect many engineers will come up with their own research directions , and many researchers will scale up their own models . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ are you planning to ever open up to employing undergraduate interns as well or is this a strict need-a-phd sort of thing for good ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ we're definitely open to ( truly exceptional ) undergraduate interns . it's much less about academic qualifications and much more about potential and accomplishment . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ just curious , are you interested in hiring people from quantum computation background ? i'm asking because recently i am ( learning ) using tensorflow to optimize problems in my field ( quantum computation ) with rnn EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ no particular focus on quantum computation today . but i'd love to hear how things evolve for you : always happy to hear about interesting research progress at gdb@openai.com. EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ thanks for doing this-i really look forward to reading your answers ! a few clusters of questions : what broad classes of tasks ( e.g. natural language , vision, manipulation... ) do you think a deep learning-driven approach of the sort you are taking will , and won't , succeed at ( almost ) solving in the next NUM or NUM years ? ( if different answers for NUM vs . NUM , or different time horizons , that'd be interesting to hear about , too ) do you have a vision for how you will deal with ip ? have you considered using ip/licensing to affect how your discoveries are used (e.g. as discussed here : URL), or are you strongly committed to making everything that can be safely made open , available to use for free for any application ? what role will robotics , real or simulated , play in your work ? what about simulated worlds in general ? you ( karpathy ) mentioned in an interview that openai's long-term vision is similar to deepmind's . are there ways that openai's vision is particularly distinct from deepmind's , or from prevailing views in ai in general ? how will/do you evaluate your progress in ai ? do you have any specific applications of ai in mind that you might pursue ? and are you open to getting revenue from such products/services to reinvest in r-d , or will all of your outputted technologies also be free to use ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ to add to ilya's reply , for NUM )/2) , i am currently reading “thinking fast and slow” by daniel kahneman ( wiki link URL ) ; i’m only NUM % through but it strikes me that his description of system NUM are things we generally know how to do (a recognition system that can “remember” correlations through training , etc), and system NUM are generally things we don’t know how to do : the process of thinking , reasoning, the conscious parts . i think the most important problems are in areas that don’t deal with fixed datasets but involve an agent-environment interaction ( this is separate from whether or not you approach these with reinforcement learning ) . in this setting , i feel that the best agents we are currently training in these settings are reactive , system NUM-only agents , and i think it will become important to incorporate elements of system NUM , figure out tasks that test it , formalize it , and create models that support that kind of process . ( edit also see dual process theory URL ) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how's the book ? been thinking about getting it . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ it's okay so far . but i get the basic premise now so i'm not sure what NUM % of the other pages are about :) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ iirc , the second half of the book is somewhat disconnected from the first half-it's about prospect theory , which is a descriptive model of human decision-making and not really as interesting as the contents of the first half . you can sum it all up as about three biases : humans are loss-averse , they overestimate the effect of low-probability events ( so long as they're salient ) , and they are bad at properly appreciating big numbers . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what do you believe that ai capabilities could be in the close future ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ speech recognition and machine translation between any languages should be fully solvable . we should see many more uses of computer vision applications , like for instance :-app that recognizes number of calories in food-app that tracks all products in a supermarket at all times-burglary detection-robotics moreover , art can be significantly transformed with current advances ( URL ) . this work shows how to transform any camera picture to a painting having a given artistic style ( e.g. van gogh painting ) . it's quite likely that the same will happen for music . for instance , take chopin music and transform it automatically to dub-step remixed in skrillex style . all these advances will eventually be productized . dk : on the technical side , we can expect many advances in generative modeling . one example is neural art , but we expect near-term advances in many other modalities such as fluent text-to-speech generation . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i highly respect your work but find this comment a bit surprising and worrisome for the machine learning community . it promises some of the hard things that take time to complete . there have been several waves of ai research killed from over promising . i'm not sure what your definition of fully solvable is , and perhaps you have been exploring more advanced models than available to the community , but it still seems like nlp or machine translation is not close to being fully solved even with deep learning [ NUM ] . some of the tasks you propose to solve with just computer vision seem a bit far out as well . can a human recognize how many calories are in food ? typically this is done by a calorimeter . for example what if your cookie was made with grandmas special recipe with applesauce instead of butter ? or a salad with many hidden layers ? i think there are too many non visual variations in recipes and meals for this app to be particularly predictive , but perhaps a rough order of how many calories is sufficient . the problem is that the layman with no familiarity of your model will attempt to do things where the model fails , and throw the baby out with the bathwater when this happens , leaving a distaste for ai . [ NUM ] URL EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ having worked in nlp for a while , with a short digression into mt , it was my impression that human level mt requires full language understanding . none of the models currently en vogue ( and those who fell out of favor ) seem to come close to being able to help with that problem . would you say that assesment is accurate ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ fwiw , i agree . i think solving translation ( meaning outperforming professionals ) is not going to happen that soon . i guess maybe they just mean damn good translation ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i'm guessing by solved machine learners often mean good enough so it becomes boring for researchers to work on . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ lstms , like other kinds of recurrent neural networks , are in principle turing-complete ( in the limit of either unbounded numeric precision or infinite number of recurrent units ) . what they can efficiently learn in practice is an open question , which is currently mostly investigated in an empirical way : you try them on a particular task and if you observe that they learn it you publish a positive result , but if you don't observe that they learn it you can't usually even publish a negative result since there may be hyperparameter settings , training set sizes , etc. which could allow learning to succeed . we still don't have a good theory of what makes a task x efficiently learnable by model m . there are some attempts : and provide some bounds but they are usually not relevant in practice , doesn't even provide computable bounds . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ you probably need something more than an rnn with state holding gates , because your computation scales with the size of your hidden state poorly . we will probably need some of these more advanced structures like neural stacks or neural content addressable memory ( like ntm ) to be successful for large problems . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ you can construct a multilayer neural network to perform logic gates sufficient for turing completeness , but this is not very helpful to move us forward . i think the same is true of lstms , and neural stacks and other data structures seem to outperform them [ NUM ] . with respect to rnns , the dimensions of your weight matrix need to match the hidden state vector , so then you have to deal with expensive compute that limits the number of training epochs you can perform . so yes , wall time convergence depends on the complexity of your model . [ NUM ] URL EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ there's some concern that , a decade or three down the line , ai could be very dangerous , either due to how it could be used by bad actors or due to the possibility of accidents . there's also a possibility that the strategic considerations will shake out in such a way that too much openness would be bad . or not ; it's still early and there are many unknowns . if signs of danger were to appear as the technology advanced , how well do you think openai's culture would be able to recognize and respond to them ? what would you do if a tension developed between openness and safety ? ( a longer blog post i wrote recently on this question : URL . a somewhat less tactful blog post scott alexander wrote recently on the question : URL ) . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ good questions and thought process . the one goal we consider immutable is our mission to advance digital intelligence in the way that is most likely to benefit humanity as a whole . everything else is a tactic that helps us achieve that goal . today the best impact comes from being quite open : publishing, open-sourcing code , working with universities and with companies to deploy ai systems , etc.. but even today , we could imagine some cases where positive impact comes at the expense of openness : for example , where an important collaboration requires us to produce proprietary code for a company . we’ll be willing to do these , though only as very rare exceptions and to effect exceptional benefit outside of that company . in the future , it’s very hard to predict what might result in the most benefit for everyone . but we’ll constantly change our tactics to match whatever approaches seems most promising , and be open and transparent about any changes in approach (unless doing so seems itself unsafe!). so , we’ll prioritize safety given an irreconcilable conflict . ( incidentally , i was the person who both originally added and removed the “safely” in the sentence of your blog post references . i removed it because we thought it sounded like we were trying to weasel out of fully distributing the benefits of ai . but as i said above , we do consider everything subject to our mission , and thus if something seems unsafe we will not do it. ) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ that isn't the kind of safety that jimranomh or scott alexander are worried about . they are more worried about the potential for ai to be used to help build weapons or plan ways to launch attacks than a corporation having some kind of monopoly . i find the removal of the word safety worrying . it seems to indicate that if there is doubt whether code can be released safely or not , openai would lean towards releasing it . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ jimranomh and scott alexander come from the lesswrong background , thus they mostly refer to eliezer yudkowsky's views on ai risk . the scenario they worry about the most is the so-called , where an ai is given an apparently innocuous goal and then unintended catastrophic consequences ensue , e.g. an ai managing an automated paperclip factory is programmed to maximize the number of paperclips in existence , and then it proceeds to convert the solar system to paperclips , causing human extinction in the process . ( for a more intuitively relevant example , substitute maximize paperclips with maximize clicks on our ads ) . this is related to steve omohundro's thesis , which argues that for many kinds of terminal goals , a sufficiently smart ai will usually develop instrumental goals such as self-preservation and resource acquisition , which can be easily in competition with human survival and welfare , and that such a smart ai could cause human extinction as a side effect of pursuing these goals much like humans have caused the extinction of various species as a side effect of pursuing similar goals . make of that what you will . i think that the lesswrong folks tend to be overly dramatic in their concerns , in particular about the urgency of the issue . but they do have a point that the problem of controlling something much more intelligent than yourself is hard ( it's non-trivial even with something as smart as yourself , see the ) and , if truly super-human intelligence is practically possible , then it needs to be solved before we build it . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i've heard andrew ng say these things . i think he's an outlier even in mainstream ml community (imo his thinking is kind of ridiculous . he overcommited to a position , then doubled down on it . you can read about it here : URL). yann is very vague and keeps saying very far away for agi but he thinks there are NUM concrete things that have to be solved first : URL as these problems get solved he'd put more priority on safety research , i imagine. ( how long does it take for a well-funded scientific field to solve NUM large problems ? you decide ) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what's the evidence that this is something that is likely to actually happen and go unchecked ? i suppose the statement i most take issue with is : so we can take that class of instrumental strategies and call them convergent , and expect them to appear unless specifically averted . why is that the case ? i see that it's conceivable for such things to appear , but what's the evidence that they will necessarily appear ? and even if they do , what's the evidence that they're likely to do so in such a way as to be allowed to cause actual damage ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i didn't initially understand what you meant initially . the first NUM clarifies that . as for the second part , what seems unikely to me is : before solving this problem , we get to a stage where we're building ai that are sufficiently advanced to be intelligent enough and efficacious enough at implementing their ideas do 'successfully' do something like this . i think this and similar enough problems are something that fundamentally has to be overcome in order to keep even simple ai from failing at achieving their goals . it seems like more of an 'up front , brick-wall' type of problem than a 'lurking in the corners and only shows up later' type of problem . i guess it seems to me that we're unduly worrying about it before we've seen it to be a particularly difficult , insidious, and grand-in-scale problem . it seems pretty unlikely to me that this problem doesn't get solved and we get to the point of building very intelligent ai and the very intelligent ai manifests this problem and this is not noticed until very late-term and the ai is enabled to do whatever off-base thing it intended to do and the off-base thing is extremely damaging rather than mildly damaging . that's a lot of conjunctions . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ well , you're asking the right questions ! we ( miri ) do indeed try to focus our attention in places where we don't expect there to be organic incentives to develop long-term acceptable solutions . either because we don't expect the problem to materialize early enough , or more likely , because the problem has a cheap solution in not-so-smart ais that breaks when an ai gets smarter . when that's true , any development of a robust-to-smart-ais solution that somebody does is out of the goodness of their heart and their advance awareness of their current solution's inadequacy , not because commercial incentives are naturally forcing them to do it . it's late , so i may not be able to reply tonight with a detailed account of why this particular issue fits that description . but i can very roughly and loosely wave my hands in the direction of issues like , asking the ai to produce smiles works great so long as it can only produce smiles by making people happy and not by tiling the universe with tiny molecular smileyfaces and pointing a gun at a dumb ai gives it an incentive to obey you , pointing a gun at a smart ai gives it an incentive to take away the gun and manually opening up the ai and editing the utility function when the ai pursues a goal you don't like , works great on a large class of ais that aren't generally intelligent , then breaks when the ai is smart enough to pretend to be aligned where you wanted , or when the ai is smart enough to resist having its utility function edited . but yes , a major reason we're worried is that there's an awful lot of intuition pumps suggesting that things which seem to work on 'dumb' ais may fail suddenly on smart ais . ( and if this happened in an intermediate regime where the ai wasn't ultrasmart but could somewhat model its programmers , and that ai was insufficiently transparent to programmers and not thoroughly monitored by them , the ai would have a convergent incentive to conceal what we'd see as a bug , unless that incentive was otherwise averted , etcetera. ) there's also concern about scenarios diminishing the time you have to react . but even if cognitive capacities were guaranteed only to increase at smooth slow rates , i'd still worry about 'solutions' that seem to work just peachy in the infrahuman regime , and only break when the ai is smart enough that you can't patch it unless it wants to be patched . i'd worry about problems that don't become visible at all in the 'too dumb to be dangerous' regime. if there's even one real failure scenario in either class , it means that you need to forecast at least one type of bullet in advance of the first bullet of that type hitting you , if you want to have any chance of dodging ; and that you need to have done at least some work that contravened the incentives to as-quickly-as-possible get today's ai running today . if there are no failures in that class , then organic ai development of non-ultrasmart ais in response to strictly local incentives , will naturally produce ais that remain alignable and aligned regardless of their intelligence levels later . this seems pretty unlikely to me ! maybe not quite on the order of you build aerial vehicles without thinking about going to the moon , but it turns out you can fly them to the moon but still pretty unlikely . see aforementioned handwaving . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i'm afraid i cannot endorse this attempted clarification . most of our concerns are best phrased in terms of consequentialist reasoning by smart agents . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ your rl scenario is definitely a possibility they consider . but it's not the only , or even the most likely one . we don't really know what rl agents would do if they became really intelligent . let alone what future ai architectures might look like . the drama scenario is vernor vinge stuff , but a common , mundane scenario would be loss of some important training data in a data-center crash . a data center crash isn't that scary at all . probably the best thing that could happen in the event of rogue ai , having it destroy itself and cost the organization responsible . the drama scenarios are the ones people care about and think are likely to happen . even if data center crashes are more common-all it takes is one person somewhere tinkering to accidentally creae a stable one . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ it's an important question , but might be immensely hard to answer . this complexity is common for anything concerning abstract dangers where we don't know specifics . it's as if we were asking how to avoid risk of modern cars , while trying to build a steam engine. possible first step is to play a sci-fi game : try to predict specific bad scenarios , paths that might lead to them , resources that ai or evil groups would need to implement these paths . this way it would be easier for us to see red flags . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ thanks for bringing this up ; it's too bad the ama team didn't really answer it . i really don't think that silicon valley do-gooder spirit is likely to accommodate the necessary principles of security and caution . andrew critch agrees that we need more of a security mindset in ai , and we're still not seeing it . we do have a subreddit for ai safety concerns at r/controlproblem which anyone with an interest is welcome to join . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ does openai have a unified vision for shaping the future ai software/hardware landscape , such as developing proprietary ai libraries or hardware ? what will be openai's relationship with python , and more specifically theano/tensorflow ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ there’s a great , healthy ecosystem of machine learning software tools out there . standardizing on existing tools is almost always better than inventing a new tool ( URL ) . we’ll use others’ software and hardware where possible , and only invent our own if we have to . in terms of deep learning toolkit , we expect to primarily use tensorflow for the near future . later we may need to develop new tools for large-scale learning and optimization (which we'll open-source wherever possible!). EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what are your short-term and long-term goals ? do you have any specific projects in mind that you would like to see accomplished in the next year and any that you would hope to complete over the next decade ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ do you think we understand what intelligence is ? ( or is that even a meaningful question? ) if not , what is the most fundamental outstanding question about the nature of intelligence ? how do you define intelligence ? is it goal-agnostic ? or do you think there are more/less intelligent goals ? what makes them so ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ an interesting exercise is to take a social group united by common goal ( e.g. nation in war ) and think whether we can call it intelligence . i.e nation functions as a brain and individuals as neurons . but anyway , there are no canonical definitions of intelligence . so either we should use less vague words or make the universal definition that would be accepted by everyone . or even invent new useful terms . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ my impression is that most researchers accept the definition given by shane legg and marcus hutter : “intelligence measures an agent’s ability to achieve goals in a wide range of environments.” ( URL ) which is basically the reinforcement learning problem as framed by sutton and barto . see , e.g. david silver's keynote at last year's iclr , which begins by suggesting ai-rl . this would be a goal-agnostic definition . my personal opinion is that there are multiple important concepts to be studied which could go under the banner of intelligence . i think rl is a good definition of ai , but i think pondering what would or wouldn't constitute an intelligent goal is also productive and leads one to think along evolutionary lines ( so i like to call it artificial life ) . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hey guys , thanks for doing this ama ! NUM ) just how open will openai be ? i.e. with results , techniques, code , etc NUM ) how close are we to the level of machine intelligence that will help us as personal research assistants ? similar to facebook's jarvis goal EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i.e. is the open in openai meant for open source ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ wasn't my question since i'd discussed with a staff member that the project would not in fact be open source . however they were scant on details of the degree of openness , i figured the researchers themselves may have a more defined answer . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ really ? their website seems to imply the opposite , EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ in andrej karpathy said : we are not obligated to share everything ? in that sense the name of the company is a misnomer ? but the spirit of the company is that we do by default . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ lol , looks like that takes care of NUM ) thanks ! i had said discussion right after they announced , maybe the guy just didn't know EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ as for NUM . google search for facebook jarvis didn't yield anything useful . is it some secret project that only insiders know about ? :) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ no , that's my fault ; jarvis isn't the official name , but here's what i was referring to : URL EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ btw , what functions would you like to have in ai-assistant ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hands down , i'd really like a conversational embodiment of human knowledge . the implications are just astounding to me . publicly assessable/affordable of course . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ thank you for doing this , i am currently and undergrad looking at eventually working in the field with machine learning my question is about the current state of ai having a high barrier of entry for those who want to work with it in the industry . the minimum level of recommended education is a phd , do you believe this is necessary or likely to change ? and do you have any advice for someone who wants to do ai research at an undergraduate level ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i would also like to know this , i'm finishing my bachelors this semester and there are masters degree profiles at my university in ai but i don't see myself getting a phd . is it smarter to just pick something else ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ isn't the advantage of google,facebook etc the data flowing through their ubiquitous services . does 'open' ai really require truly open data : i.e. a popular distributed search engine , etc; ( or is there enough freely-available data for training already. ) can an initiative like openai try to encourage publicly available labelled datasets (labelled video ?, ...), perhaps by organising other interested parties to contribute . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi openai team , being a research engineer , i am interested in hearing these questions answered by any or all of ilya , andrej, durk , john or wojciech . i would love to have everyone's take on question NUM especially . what are the kind of research problems are you looking forward to tackling in the next NUM-3 years ? or more generally what are the questions you definitely want to find the answer to in your lifetime . what has the been your biggest change in thinking about the way dnns should be thought of ? for me , its the idea that dnn esp deep lstms are differentiable programs.would love to hear your thoughts . when approaching a real world problem or a new research problem , do you prefer to do things ground up ( as in first principles : define new loss functions , develop intuitions from from basic approaches ) or do you prefer to take solutions from a known similar problem and work towards improving it . repeating my question from nando freitas ama : what do you think will be the focus of deep learning research going forward ? there seems to be a lot of work around attention based models ( ram ) , external memory models ( ntm , neural gpu ) , deeper networks ( highway and residual nn ) , and of course deep rl . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi , deep learning models are often data starved . corporate researchers would have access to private data sources generated by users . in openai , what kinds of data are you working with and where do you get them ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how do you plan on tackling planning ? variants of q-learning or td-learning can't be the whole story , otherwise we would never be able to reason our way to saving money for retirement for instance . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ your question is too good not to comment ( even though it is not my ama ) ! long-term reward / credit assignment is a gnarly problem and i would argue one that even people are not that great at it ( retirement for example-many people fail ! short term thinking/rewards often win out ) . in theory a big enough rnn should capture all history , though in practice we are far from this . may get us closer , more data , or better understanding of optimizing lstm , gru, etc . i like the recent work from msr combining . they have an iclr submission using this approach to tackle fairly large scale speech recognition , so it seems to have potential in practice . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ clockwork rnns are in a good position to solve this problem of extremely large time lags . as in , clockwork rnns are capable of doing more than solving just vanishing gradients EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ the reason humans fail saving for retirement is not because our models aren't good enough , imo. it is because we have well documented cognitive biases that make delaying gratification difficult . or , if you wanna spin it another way , it's because we rationally recognize that the person retiring will be significantly different from our present day self and just don't care so much about future-me . i also strongly disagree about capturing all history . what we should do is capture important aspects of it . our ( rnn's ) observations at every time-step should be too large to remember all of it , or else we're not observing enough . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ cognitive biases could also be argued to be a failed model ( shouldn't we care about future-me as well ? i think we do , just < ; < ; current-me , but i haven't looked at it too much ) or you could reframe it as exploratory behavior which is probably necessary for a group to advance . i don't want to get into human behavior too much ( though we can talk about it in person sometime : ) interesting to think about)-any other example of longterm planning could work here as well . puzzle games where there is no reward for many moves , then boom you win would be another example of hard credit assignment . capturing only important aspects is better in many ways ( model size , probably generalization , etc. ) but not strictly necessary . if you could capture all history , then all the important stuff is in there too along with a bunch of garbage . in practice ( not fantasy land ) i NUM % agree with-you need to learn to compress as well . what i am trying to say is that the math says you could learn all history ( p(x1 )-p(x2 | x1)-p(x3 | x2 , x1) etc.), given a big enough rnn , an optimizer that went straight to the ideal validation error , and magic perfect floating point math-not that this is really a good idea . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ comment about history based on schmidhuber's papers : i think there are NUM separate ideas here . history compression is truly learning ( in the predictive inference sense of the term ) . but we may need to keep a bit of raw , uncompressed history too . this way we can compare our model predictions with a new model prediction and check for actual improvements objectively . so i think you're both right in a sense . NUM papers ( non exhaustive ) : learning complex , extended sequences using . the principle of history compression . (neural computation , NUM (2):234-242, NUM ) : for the compression part on learning to think : algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models ( arxiv:1511.09249, NUM ) : for the replay part EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ will openai's researches be open for anyone to participate ? do you plan to facilitate that ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ in your opinion , what is the best way for ai to learn cause-effect relationships of our world ? what types of data would be helpful as training sets for that task ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ recently ( this wednesday ) , sam altman said openai will primarily be focusing on three things : deep learning , unsupervised learning , and reinforced learning ( taken together , roughly a good approximation of how humans learn ) . can you tell us more about your goals for unsupervised and reinforced learning ? specifically, what kind of advances you would like to see and any ideas for future applications . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i have heard top ml researchers ( including dr. sutskever here : URL ) assert that there are critical tricks for getting deep learning to work and these tricks are not published , but only taught by long apprenticeship in the best ml research groups . since you really care about openness of ai research , what are your plans for writing down and broadly disseminating these best practices ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi openai team , i'm in my mid-30's with a professional experience in software development industry . started realizing i need a driving factor in me & found machine learning initiative's interesting & the benefits it can bring to all of us . so, just completed andrew ng's course on ml as a starter . pretty much a newbie to ml/ai & as i keep reading about technological advances in this industry , starting to have a great desire to be able to contribute to this open community . i'm no phd nor a scientist , so i'm not expecting to be hired , though would be interested to make my smallest contribution for the benefit of the world . i've no clear direction at this point on where to start . would you have any suggestions ? thanks in advance . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi guys , and hello durk-i attended prof lecun's ml class of NUM-fall@nyu that you and xiang were tas of and later i ta-ed in NUM-spring ml class ( not prof lecun's though :( ) . my question is-NUM ilsvrc winning model from msra used NUM layers . whereas our visual cortex is about NUM layers deep (?). what would it take for a NUM layer deep cnn kindof model to be as good as humans' visual cortex-in the matters of visual recognition tasks . thanks , -me EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ cortex has roughly NUM functionally/anatomically distinct layers , but the functional network depth is far higher . the cortex is modular , with modules forming hierarchical pathways . the full module network for even the fast path of vision may involve around NUM modules , each of which is NUM layered . so you are looking at around ~60 layers , not NUM . furthermore , this may be an underestimate , because there could be further circuit level depth subdivision within cortical layers . we can arrive at a more robust bound in the other direction by noticing that the minimum delay/latency between neurons is about NUM ms , and fast mode recognition takes around NUM ms . so in the fastest recognition mode , hvs ( human visual system ) uses a functional network with depth between say NUM and NUM . however , hvs is also recurrent and can spend more time on more complex tasks as needed , so the functional equivalent depth when a human spends say NUM second evaluating an image is potentially much higher . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ the hvs arguably also does more than a cnn ( e.g. attention , relationships between objects and learning of new 'classes' ) , and the NUM layers in cortical tissue are not set up in a hierarchical way ( the input is a the middle ) so it's really hard to compare . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ yeah , hvs also does depth , structure from motion , transformations, etc ., more like a combination of many types of cnns . as you said , within a module the input flows to the middle with information roughly flowing up and down-so its layered bidirectional , but there are feedback loops and the connectivity is stochastic rather than cleanly organized in layers . but we can also compare in abstract measures like graph depth , which is just a general property of any network/circuit . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what i also mean is that it is hard to say at which graph depth of the hvs you have reached a similar function to cnns ; whether you need to go all the way to stpa or whether pit is roughly on the level of cnns seems to be not so clear . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ thanks ! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i kind of would like to piggyback on this question and ask something that was asked during a job interview . at the beginning it made sense to have ~6 layers because researchers really based that on functional architecture of the visual cortex . but it looks like a more pragmatic approach took over now and biological plausibility is not really that important . so the question is who really decides to use these crazy parameters and network architectures (ie NUM layers . why not less/more?), and what is the justification ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how do you measure depth ? if by counting non-linear layers then you should take in account that active dendrites can do non-linear transformations , which is kind of cool . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ in a very recent ama done by prof . nando freitas a question was asked about word embedding in which prof . edward grefenstette intervened explaining . what are your thoughts about the subject ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i'd love to hear the openai team's thoughts on the matter , but i thought i'd just clarify that i ( ed grefenstette ) am not a professor , just a senior research scientist at deepmind . also i think there's a lot of interesting efforts to be pursued in embedding research and representation learning , so i wouldn't exactly say it's uninteresting . maybe just not the only point to focus our efforts on ... EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what type of tasks do you plan to tackle ? deepmind has been working on the atari games for instance , will you be attacking the same problem ? will you using a simulated environment , perhaps a physical robot , or will you be focusing on solving a variety of difficult but narrower tasks , such as nlp , vision, etc . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hey i'm just starting to get into machine lesrning . i'm taking the coursera course now on it and plan on reading some books after his . i have my degree in mathematics and computer science . my question is this . i know a lot of ph.d programs require past research ( at least the good ones. ) how would you guys recommend getting that research experience as a guy who's graduated from college and lesrning it on his own , if that's the route i decide to go down ? second question , slightly related . if i decide not to get a phd , whats the best way to go about proving to future employers that i'm worthy of a job in the machine lesrning field ? thank you for taking the time to answer our questions ! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ my advice-start doing projects on your own , build up a github of these projects , write a blog , and give talks and tutorials . these four things basically encapsulate what you will need as a successful researcher-the ability to come up with a self-directed project ( NUM ) , implement and complete it ( NUM ) , write about what you have done in a coherent manner ( NUM ) , and teach others about it ( NUM ) . as a bonus ( though it is a bit scary , at first ) all of this stuff is public record forever , thanks to the internet . so people can clearly see that you are already able to do what a graduate researcher or r & d engineer needs to do-makes the hiring decision much easier , since there is less risk than with an unknown . the most important thing is to find a project ( or projects ) you are really , genuinely interested in and pursue it . that passion will show through to almost anyone you will want to work with , and will be a big help in job interviews or phd applications . this was at least my approach between bachelor's and going back for a phd . writing is hard , and some of my first blog posts ( at least the writing part ) were cringe worthy bad (cf destroyed by r/programming . the thing to remember is as long as you improve every day , you are getting somewhere ! and if you keep taking steps toward where you want to go , someday you'll end up there . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ completely agree with all you've said ! as stupid as it sounds i think posting work you've done is the most difficult part especially on subreddits like this . i've seen people get ripped apart , trolled etc . for positing things that other dismiss as stupid etc . easy example would be the one guy messing around with numenta's ideas ( which is neat and he seems to be enjoying ) . would be great if people were a bit more open and supportive to other approaches etc . i feel we lose sight of the overall goal many people have in the field (discovering cool things , learning etc.). not sure if this is a problem with this subreddit , academia or something else ! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ thank you , good question ! progress in ai is to a first approximation limited by NUM things : compute, data , and algorithms . most people think about compute as the major bottleneck but in fact data ( in a very specific processed form , not just out there on the internet somewhere ) is just as critical . so if i had a NUM version of titanx ( which i doubt will be a thing ) i wouldn’t really know what to do with it right away . my networks trained on imagenet or atari would converge much faster and this would increase my iteration speed so i’d produce new results faster , but otherwise i’d still be bottlenecked very heavily by a lack of more elaborate data/benchmarks/environments i can work with , as well as algorithms ( i.e. what to do ) . suppose further that you gave me thousands of robots with instant communication and full perception ( so i can collect a lot of very interesting data instantly ) , i think we still wouldn’t know what software to run on them , what objective to optimize , etc. ( we might have several ideas , but nothing that would obviously do something interesting right away ) . so in other words we’re quite far , lacking compute , data, algorithms , and more generally i would say an entire surrounding infrastructure , software/hardware/deployment/debugging/testing ecosystem , raw number of people working on the problems , etc. EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ ( i'm not on the team , but i'll answer ) according to the brain is NUM peta flops . this is counting that the brain has NUM synapses and assuming that each firing on a synapse is a floating point operation . but as someone brings up in their answer , it's a pretty useless estimate since we have no idea how the brain works , any estimate is fully dependent on the assumptions it makes about the brain . but NUM petaflops is the number you hear parroted the most often . theoretically , if you bought NUM petaflops of amd radeon r9 NUM x2 gpus , you could . the tianhe-2 cost NUM million usd . assuming computing power halves in cost every two years the cost of NUM petaflops looks like : NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM the cpu from NUM ( certainly of NUM ) is nothing like a cpu to us , it could just be some kind of learning agent implemented in hardware anyway . also fun fact but moore's law can only last NUM years before you hit theoretical limitations on the total information-processing capacity of any system in the universe . furthermore the man himself said ( in april NUM ) . anyway , all this math is pretty useless . the thing thats holding back ai is not computing power . nowhere close . having much more powerful computers would certainly accelerate its development ( by reducing iteration time and maybe better tooling that relies on computing power ) , but you can't just give us a good computer and expect it to perform tasks at a human level within the year . we just don't have the algorithms . you didn't ask my opinion , but i think that if we had the algorithm(s?) for general intelligence right now , you could have a very rewarding conversation with your cell phone . i suspect that the amount of processing power i'm using to think of the next sentence is overshadowed by the amount of processing power i'm using to move my fingers and read as i'm typing . this is more of an intuition though , i haven't met anyone who also thinks this . tl;dr brain might be NUM petaflops , and there already exists a computer that can do that . it cost NUM million usd . hardware is not what's holding us back . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ it seems unlikely that agi is going to be built purely out of scaling up the exact supervised methods we use today , rather than more general unsupervised , reinforcement, and self-supervised learning . but that being said , the issues you bring up aren't issues at all . current techniques allow the training of say NUM to NUM million neuron anns on imagenet without overfitting . and we haven't hit any fundamental size limit yet . there is also further room to scale up trivially just by increasing image resolution from NUM x256 up to hd . next you then train and integrate multiple types of deep cnns on different imagenet style databases-to learn depth , motion from depth , structure from motion and depth , image transforms , etc etc . datasets can also be generated automatically through NUM d rendering pipelines . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i think he's talking about unsupervised learning on video streams , e.g. predicting the next frame from the state built up from previous frames , and using the hidden states from that network as the inputs to another net which would do reinforcement learning . then you could e.g. put a bunch of reinforcement learners in a competitive but flexible virtual environment ( some kind of competitive minecraft type world ) , and see if they derive general intelligence emergently , to better compete against one another . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ yeah . i was thinking about that the other day ... quite interesting . here were my thoughts : URL EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i'm not on openai , but i don't think any algorithm that exists right now would result in anything anyone would consider agi , no matter how much clock speed , cpu cores , or ram it has access to . if you disagree , why not point out what techniques , or data ( if any ) you would use to accomplish this , where your bottleneck is computing power . if agi is really a thing , not just some pipe dream , i think it depends more on the right techniques , and correctly organized data , and robust ways of accumulating new useful data . i'd rather have a genie give me the software and ( a portion of ) the data from NUM than the hardware from NUM . at least with respect to machine learning . personally , i don't think agi is something that will ever exist as described . yes, certainly any task that a human can do can be mimicked and surpassed with enough computing power , good enough datasets , and the right techniques . and since every human skill can be surpassed , you can put together a model that can do everything humans can do better . i don't deny that . but proponents of the agi idea seem to talk as if this implies that it can go through a recursive self-improvement process that exponentially increases in intelligence . but nobody has every satisfactorily explained what exponentially increasing means in the context of intelligence , or even what they mean by intelligence . is it the area under an roc curve or a really hard classification problem ? because that's literally impossible to exponentially improve at . it has a maximum amount , so at some point you must decrease the rate of improvement , so it can not be exponential improvement . is it the number of uniquely different problems it can solve with a high rate of accuracy ? then tell me what makes two problems uniquely different . but what if someone did put their finger exactly on what metric to define intelligence , even one that allowed for exponential improvement to be conceptually sound ? i highly doubt that exponential improvement would be what we find in practice. most likely as you get smart , getting smarter gets harder faster than you're getting smarter . maybe a machine which has logarithmic improvement could exist . probably not even that good , in my opinion . i'm not trying to say that we can't make a model better than humans in all aspects , nor even that it can't improve itself . but i find the concept of exponentially increasing intelligence highly dubious . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ most agi enthusiasts either quit due to lack of progress or switch back to narrow ai . what makes you different ? how do you feel about miri ( aka siai ) ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ physicists look for a unified equation of everything . even if its not efficient to calculate that way , its useful to verify your optimized models . what simplest unified math operator do you prefer ? rule110, nand/nor , some combination of npcomplete operators , lambda, or what ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ researchers in industry cite more dynamic environment , as one of the main benefits in comparison to academy . it creates stimulating sense of urgency and density of ideas is often higher . what might be good methods to create a stimulating environment for openai ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi , thanks for doing this ama ! in terms of obtaining jobs in machine learning fields , similar to your openai team here , how important is it to go to school and get a degree related to this ? as in , is the hiring process based more on one's ability and skillset , or is a degree mandatory for all intents and purposes in order to obtain a job in this field ? thanks . p.s. does /u/badmephisto have a youtube channel ? i remember a youtube channel badmephisto that was a phenomenal resource when i was big into cubing a while back . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ our hiring process is based on ability and skillset , but getting a degree is one nice , proven way to get there . and yep , i was quite into cubing in my previous life , back in the old days :) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ andrej karpathy with colleagues open sourced a very good course cs231n ( many thanks for that ) . do you plan to create any form of mooc or book or online tutorials to educated newest ideas and methods ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi everyone . i know it is very abstract question , but i was wondering how is this possible to decrease the sample complexity of deep learning methods . i'm currently a ph.d. student and i'm working on the field of deep reinforcement learning . methods in this area suffer a lot from the amount of data they need in order to complete the training . are you guys working on this problem ? how do you think we can solve this problem ? i think that probabilistic methods such as pgm are the way to go , but i wanted to know your opinion on this . thanks! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ musk's stated motivation is to build trustworthy ai , which to me means inspectable , interpretable systems . so i thought , maybe openai will focus on ways to constrain learning towards human-interpretable , symbol-like representations ( along with things like visualization methods or methods for systematic 'fuzz testing' of agents ) . are you pressing in those directions more than other groups ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what do you think are the most promising models/techniques/research.topics for building systems that can learn to reason ? the proposals i'm aware of so far are those mentioned in nips ram/coco workshops , woj zaremba's phd thesis proposal , and a few iclr NUM papers on program.learning/induction , unsupervised/reinforcement/transfer learning , & multimodal question.answering/communication . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ gonna swoop in and link to this since that's what my simulation of ilya says . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ the concept of adding interfaces to neural nets is quite interesting in its implications . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hello openai-my question is related to durk's work on vaes which have been a very popular model for un/semi supervised learning . they train well and almost all new deep-learning models that one comes across in recent conferences for unsupervised/semi-supervised tasks are variations of them . my questions is , what do you think is the next major challenge from the point of view of such probabilistic models that are parameterized by deep nets ? in other words , what direction do you think the field is headed in when it comes to semi-supervised learning ( considering vae based models are state of the art ) EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ two challenges for vae-type generative models are : finding posterior approximators that are both flexible and computationally cheap to sample from and differentiate . simple posterior approximations , like normal distributions with diagonal covariances , are often insufficiently capable of accurately modeling the true posterior distributions . this leads to looseness of the variational bound , meaning that the objective that is optimized ( the variational bound ) lies far from the objective we’re actually interested in ( the marginal likelihood ) . this leads to many of the problems we’ve encountered when trying to scale vaes up to high-dimensional spatiotemporal datasets . this is an active research area , and we expect many further advances . finding the right architecture for various problems , especially for high-dimensional data such as large images or speech . like in almost any other deep learning problem , the model architecture plays a major role in the eventual performance . this is heavily problem-dependent and progress is labour intensive. luckily , some progress comes for free , since surprisingly many advances that were originally applied to other type of deep learning models , such as batch normalization , various optimizers and layer types , carry over well to generative models . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ why is it that deep learning academia stays in the us while european institutions seem not to care about it ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hello people , right now it feels like most of the directions towards ai are coming from taking a large amount of data and feeding them into large deep neural nets . what are some other approaches that don't require such a large amount of data ? will openai be exploring them in detail ? also , @joshchu will you keep working on cgt now that tensorflow is here ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ if human-level ai will be dangerous , isn't giving everyone an ai as dangerous as giving everyone a nuclear weapon ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what advice do you have for undergrads hoping to get into ai ? in other words , what areas of research would you say are the most promising NUM years down the road ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi , my question is more mundane . how does a modern research organization such as openai designs its research process ? how do you decide which ideas to pursue , establish your objectives ( monthly , quarterly? ) and how do you track your progress ? thank you ! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ for someone like me , regular developer working for a non-profit with both a lot of data and usage cases for nlp , image recognition and deep learning , what would be the best way to make myself available for cooperating with openai-e.g. testing models , open sourcing implementations on different languages , etc. EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ for NUM : it's interesting to compare possible employment shifts of ai-revolution to historical examples of massive transformations : two industrial and information revolutions . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how important do you think cooperation between nation-states is to making ai : NUM safe ? NUM beneficial ? in particular , is it critical for either or both objectives ? how much power , ultimately, will the scientific community vs . other actors have in determining the impact of ai ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what are your thoughts on measuring progress for general intelligence ? there are formal measures such as those proposed by legg and hutter , as well as more ( currently ) practical measures using performance of general agents across multiple games such as ale ( bellemare , veness et al ) . both allow for continuous measures which are wonderful for tracking progress . are you interested in tracking your progress along the general intelligence spectrum ( random at one end , something optimal like aixi at the other ) or are you considering other ways to measure your progress ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what do you think are the most interesting agi papers published until now ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i'm about NUM % of the way through nick bostrom , superintelligence. how wide is the conversation around doomsday scenarios re : ai? should an average joe like myself pay much attention to the questions presented in his book ? thanks! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ btw , does nick bostrom present specific paths , how smart ai could gain resources to became dangerous ? like what are most likely doomsday scenarios if we start from only an innocent super-intelligent algorithm . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ he does . offhand i don't have a good link for a summary , but in general he is pretty explicit about several paths that a well-intentioned attempt to create a super-intelligent algorithm could result in a doomsday scenario . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ couple of questions for andrej ; you mentioned generating your own training data in an interview recently . any guidance/tips for creating larger or novel datasets ? will you have more or less time to blog and write fiction now ? will openai do the whole NUM % thing ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ can't remember exactly what NUM ) is alluding to . i think it was a simple observation i made in passing that in some cases you can generate data ( maybe as a variation , working on top of an existing dataset ) , without having to collect it . in retrospect i quite enjoyed writing my first ai short story and will probably continue to write more a bit on a side as i did the first time ( though nothing specific is in works right now ) . i actually consider it a relatively good exercise for research because you're forcing yourself to hypothesize consistent and concrete outcomes . pushing these in your mind to their conclusions is one way to achieve fun insights into what approaches to ai are more or less plausible . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ thanks . interesting concept that writing a fictional outcome can ( in some way ) inform your research . good luck with the new venture and the next short story ! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what are your opinions on autonomous weapons ? do you think banning them is feasible ? if so , what needs to be done ? how much , in practice , do you think general ai research and autonomous weapons research overlap ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i've seen the notion of a 'seed ai'?that is , some sort of less-than-human agi that improves its own capabilities very quickly until it's superhuman?envisioned as the end goal of ai research . my question is?can we establish ( or , you know , estimate ) some bounds on the expected size/complexity of such a seed ? i imagine it's not a one-liner , obviously, and it also shouldn't be that much bigger than the human genome ( a seed for learning machine with a whole host of support components ) , but presumably someone more experienced in ai than me can come up with much tighter bounds than that . could it fit on a flash drive ? a hard disk ? what is your best guess for the minimum amount of code that can grow into a general intelligence within a finite timescale ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ the short version of my question is this-does openai have any plans to offer courses or other learning opportunities to people interested in the ml field who want to become well-versed in state-of-the-art methods and apply these in real-world situations for companies , governments, ngos and other organisations globally ? the much longer version of my question is this- as your website points out , and as will be familiar to everyone on this sub , progress in the machine learning field over the last NUM-5 years has been dramatic , and human-level performance has been achieved in tasks that many thought would prove extremely difficult . tools such as deep learning , lstm and others have provided broadly powerful techniques that , it is likely , can be applied to diverse problems and data-sets . i have read ( somewhere! ) that google now has hundreds of teams working to apply machine learning techniques to provide services within google's business . as we are all aware , google are not alone-and several major tech companies are moving rapidly to apply machine learning to their businesses . in order to acquire the talented people necessary for this effort , tech companies have basically strip-mined academia to acquire the best and brightest . in some respects , this is understandable , and no-one could criticise individuals for getting the best reward for their considerable skills . however, this means that academic programmes simply do not have the personnel or resources to expand and train a much larger corpus of people in this field . on-line courses exist , but arguably some of them are already out-of-date and do not reflect the important developments in the field . and simply taking an online course does not build the kind of credibility that companies need before allowing aspiring data scientists near their data . without a significant expansion in the teaching capacity of the ml field then it seems to me that what will happen is that large tech firms , banks and hedge funds will dominate and monopolise the market for people with skills in this field . instead of machine learning building value for everyone ( as you aspire to on your website ) the effect will be to entrench existing monopolies or oligopolies in the tech and finance spaces . the lack of teaching capacity as i have called it above will create a huge bottleneck and the value that could be created from applying these tools and methods to datasets and problems globally , in all kinds of sectors and countries-from governments to ngos to manufacturing companies , to insurance companies , etc. etc . will instead not be realised , and ( even worse! ) what value that is realised will concentrate in the hands of the already successful . geographically , the effect will be particularly extreme-us universities and corporations already dominate ml research and this situation is unlikely to change . if the everyone that openai intends to benefit includes the rest of the world then this is a real challenge . i realise that this isn't a research question , and that your response may be to say we are doing our best to create value for everyone by making our research open source . but, with the greatest of respect , this approach won't succeed . without people to apply the methods and techniques you develop , the benefits will not flow to companies and individuals globally . the state-of-the-art of the field may progress dramatically , even to human-level general intelligence , but the ability to apply these techniques will remain concentrated in very few companies . this will create dramatic winners and losers , rather than benefit for all . the key issue seems to me to be teaching capacity . how can we create the NUM s of machine learning experts ( per year ) the world could easily use and benefit from ? as a step towards this , openai could commit to hiring a group of top-level researchers in the field , who would be interested in creating a taught programme , with exams , with accreditation etc . to provide ml experts , familiar with the state-of-the-art in the field , but perhaps interested in applying it to real world problems and data-sets rather than advancing the field through novel research . i think openai , as a non-profit institution but one that's not constrained by the issues of academia , would be ideally placed to do this . and it would result in real progress in your objective to build value for everyone . thanks in advance for any thought you may have on this . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i recommend a line-break between the short and long version . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ changed it . thanks. still wish i could make the spaces between the paras bigger to improve readability . any recommendations ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi openai team , thanks for this ama . i'm a bachelor(undergraduate) student in computer science at eth . i began my study because i want to help create ai . any tips how to get into research in deep learning/ai ? classes i shouldn't be missing , connections i should be making , how to best to proceed to be able to become a researcher in this area ? thanks and good luck with openai ! EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ hi , thanks for doing an ama , it is desperately needed to understand what openai is beside buzzwords and vague statements . few questions from my side : NUM ) are there any plans to tackle spiking neural networks ? NUM ) can we expect interim updated from you guys in a form of a blog post/diary or are you going to stick with traditional publications , e.g. you publish when result is ready and we have on insight beforehand . thanks for what you're doing for ai research . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ do you have any idea what prompted the rather sudden ( apparent ) change of opinion of musk and altman about ai ? they both were calling for regulation not too long ago : URL URL EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ NUM ) how is the openai team planning to work towards the goal of developing ai ? are you planning a top-down approach of developing one or more cognitive architectures and working directly towards them , or a bottom-up approach of extending the adjacent-possible and deferring the problem of ai until it's closer in sight ? NUM ) if you are planning a grand agenda , is there anything you can share at this stage ? what are the major components/technologies you believe are necessary for ai ? do you see it as being all connectionist or do you plan on using symbolic components also ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ i have a ( presently ) highly desirable skill set as a software developer and am constantly working to improve-it seems wise to invest a bunch of time in machine learning skills . what are your opinions on this ? how necessary or valuable are full stack software developers ( without ml experience ) in the foreseeable future when they won't be as necessary for building the next generation of software technology ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how can we avoid facing another ai winter in case the current expectations cannot be met for a long time ? will the interest only grow or remain the same from now on due to the likely success of self-driving cars and robotics ? what, if anything , can we learn from previous ai winters ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ deep learning has grown so fast over the last few years and with such spectacular results that naturally many people from outside the field are interested in participating . do you have any suggestions for concrete steps for professionals with quantitative backgrounds to transition into deep learning research that don't involve going back to school and getting a phd ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what is the roadmap to achieve a personal assistant like samantha in the movie her . how can we improve the results of systems as the one shown in a neural conversational model ? how can we obtain datasets to train these systems ? human takes many years to become useful ( to enter in the work force ) and to obtain common sense . how can we accelerate that ? off course we wouldn't have to retrain every new system from scratch . that helps a little bit . we also would like samantha to operate our computer and access the internet and google for us . how to train she to learn how to use apis . we could train her how to use google from the current user perspective or we wire her directly inside google ( to have access to the database ) . i think the paper neural programmer : inducing latent programs with gradient descent goes in this direction . EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ what's the right way of doing unsupervised learning ? or making use of unlabelled data for a supervised task ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how does one end up working at openai ? or you select and approach people yourselves ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how does one monetize their own individual breakthroughs if they develop something groundbreaking in ai and involved with openai ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how much of the research work you're doing is focused on economic / social aspects of the machine learning industry ? marketplaces for sharing data and blending models seem like an inevitability and require regulation for ethical purposes a long more strongly than any individual model EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ to be blunt : what do you think are the roles of engineers who traditionally work in systems / infra ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ do you have somebody working on openai for nlp ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ do you , as a group or as individuals , have a perspective on the likelihood of a fast takeoff / intelligence explosion ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ if you could program a robot with human-like emotions , would you ? and how do you decide these moralic questions ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ the use of this technology by the darpa and military should be abolished as it should be abolished weapons , the ai ??could end wars ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ do you guys know about opencog ? do you consider working with them , what is your opinion of them ? EOA 
 ama: the openai research team : machinelearning fyi , the link for hiring appears to be broken . EOQ how would you define intelligence i.e. the trait you are trying to simulate ? are intelligent beings necessarily conscious ? are intelligent beings necessarily good communicators ? human intelligence is generally estimate through communication . intelligo-to understand . we find people who understand our ideas intelligent . if perhaps a machine is not very good at human communication , how do we know it is intelligent ? it just solves raven's progressive matrices silently very well ? come to think of it , raven matrices look actually pretty nicely machine-learnable to me , are you using them ? EOA 
 are neural networks more effective for datasets with missing values compared with other ml methods ? : machinelearning one way or another the input to a nn has to be numbers . you could try something simple like dummy values for nan and a separate indicator variable for whether the value was originally present or not . sklearn has a EOA 
 johns hopkins university data science team doing ama on january NUM th ! : machinelearning this is awesome . thanks for the info . EOQ not a problem ! EOA 
 johns hopkins university data science team doing ama on january NUM th ! : machinelearning this is awesome . thanks for the info . EOQ do you guys fill in for the umbc data science team when they can't make it to events ? EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ do you think it's possible to design higher capacity models[-] for nl modeling without increasing the number of parameters w.r.t. current the state of the art ? if so what are the most promising avenues ? better optimizations methods ? architecture changes like bn and skip connections that makes it easier to train the model ? do you think that significant capacity improvement necessarily require more computationally intensive models and longer training times ? [-] as measured as the maximum size of the training set from that domain that the model can overfit . EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ well , people asked the same questions before we rediscovered deep nets and their efficiencies NUM years ago , and looking at how efficient models have become since then ( as well as moors law , and that new silicon quantum chip they're making in australia looks exciting too ) , i'm sure better algorithms will be developed . besides this , since google's first deep speech , models with performance loss were optimized , and google has that conversation model paper-i looked some new implementations up , and i found some efficiency comparisons : URL out of those , i only looked at these , they seemed the most interesting to me : URL URL URL URL-this paper raises the point that , while model efficiency is affective , so is better , quality ( quantity is not /always/king ) data ( and to strive for it ) training efficiency papers seem promising for decreasing response latency . different model architectures for increased performance accuracy can also be improved and implemented if thought of properly . compare google's and microsoft's ( and URL for what its worth ) conversation models , context vs attention ( with intention , being their improvement ) i believe it is . the awi paper glosses over google's and offers theirs as an alternative rather than an improvement . my opinion is , training algorithms first , updated state of the art linguistics/conversation models second , if this is possible . i don't think you would have the best comparative results done vice versa to previous models . iterations would be less , and different models could allow a lower number of hidden layers to be necessary for simpler models . better data ( maybe more latent classification of social elements , as from our a priori knowledge that we understand conversations over text with a more intuitive basis than only having read books and spoke with text our whole lives ) surely optimized algorithms can be implemented , which saves efficieny and computation time , which allows for additional , relevant architectures to be added , and thus your original computation times ( or , at least , definitely not as much than by only adding additional architectures to go deeper with no attention to newer models' efficiencies ) should match or at least be near your new computation times . if not , certainly they are significantly less than nieve implementations , and the results more human-like . i'd say it's a good thought process , as your model gaining the possibility to add additional layers from other relevant models ( or developing new modes from linguistic theory like awi , who knows ) is worth the edge . that's just my thoughts , i'm not completely sure . there are good ideas for performance increases in other nlp/nlg areas , but due to them being slot filling or single sentence/paragraph/story generation , i don't know how these models would work with active and passive conversation . with regards to performance , brainstorming is needed ( andor research into computational linguistics papers from the NUM s-apparently ) to begin slowly implementing a naive personality/execution/psychology/whatever other field's model , maybe meaning , owned memories ( like name/address/personal preferences ) . after this , if we have better more classified data , a neural algorithm of narrative style ( from the artistic one ) , and slightly closer to considering a turing test , is not too far fetched . maybe confusion implementation . more date . bigger nets ! now i'm just rambling and bs'ing my ideas at NUM am , haha. thanks for the comments to the thread . good minds here EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ can any of these chat systems be improved by using the ideas from these other papers ? EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ i tried attention on our chatbot model , no improvement ( in terms of perplexity at least ) . but of course , at the point where you saturate the model with enough capacity with more parameters , these techniques may still give gains . EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ what about the attention with intention model ? that seemed the most promising to me , besides executive functions or context mapping . would employing any of the newer algorithms help at all , or just decrease response latency ? EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ you'll have to try it to know if it helps : ) for my model , kind of attention i was trying , etc., attention was basically not-useful . but ymmv ! EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ i'm not at the level to understand the math in papers enough to try and recreate it myself yet , haha. but i'll try to get some people to help translate a few relevant ones within the coming months , so i'll get back to you on that if i remember this post then . EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ oriol , when you say big models , how big are you referring to ? i saw in your neural conversation paper you used NUM layers with NUM units lstm . i have tried NUM layers with NUM , which i know is smaller , but skip connections and attention certainly helped me . so i guess you could argue that i was just adding more parameters , and that if i had done NUM layers of NUM units , skip connections and attention wouldn't have mattered . also , in addition to bleu , have you guys considered using translation edit rate plus as a scoring mechanism ? in my opinion , it yields more accurate scores that reflect human judgement . EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ yeah , i have used ( and been amazed by attention ) myself several times . i'm just saying that if you take a NUM recurrent units lstm , and add a bunch of stuff ( be it units , attention, skip connections ) , it will do better but the reasons may not be very clear . i haven't worked on mt for a while , but that's good to know-we always optimized the wrong metric , so our model wasn't particularly biased towards bleu anyways ... EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ URL recurrent memory network , another new network outperforming lstm's , tuned for nlp/nlg forgot to add this one , it was posted earlier . URL this also exists , but the pdf is in the wrong format , so i haven't read it yet . EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ pretty amazing EOA 
 state of the art ( dec/2015 ) natural language processing & natural language generation : machinelearning good list ! i must say some of these papers start from really small models ( not large enough to capture language ) . anything added to the model which adds capacity ( either through skip connections , attention weights , or anything else ) is sometimes misinterpreted as a real improvement . sadly , large scale language modeling does require pretty big and computational intensive models to really overfit the training set . EOQ URL this uses memory networks to improve perform over context as well , with long term and short term memories . that's about all of the relevant papers i can think of . EOA 
 what are the coolest ml applications in genomic sciences ? : machinelearning here is a review paper : . EOQ another good one in the same vein : . generally speaking , though, ml has been used in bioinformatics for a very long time and for a vast number of different applications . for example , classifying tumor subtypes based on gene expression profiles , predicting the subcellular location of proteins , ( the latter one in pretty recent ) ... i could go on on for hours . there is so much out there , but probably because the emphasis tends to be on the biology rather than on the ml part , these don't get that much attention outside of the life sciences . EOA 
 what are the coolest ml applications in genomic sciences ? : machinelearning here is a review paper : . EOQ yeah that paper even mentions predicting quantities expressed of targeted molecules . very cool ! and the personal diets trials look quite impressive-i'm sure they must be developing a start up based on that work ;-). EOA 
 what are the coolest ml applications in genomic sciences ? : machinelearning here is a review paper : . EOQ what a great paper , exactly what i was hoping for . thanks. EOA 
 what are the coolest ml applications in genomic sciences ? : machinelearning here is a review paper : . EOQ here's another review link : imho lots of biology projects can benefit from ml ; unfortunately, the required domain knowledge to understand the problems is a real obstacle in identifying cool applications , i.e. the biologists don't understand what mlists can do , and the mlists don't understand what the biologists want to do . i think the application with the most potential is using ml to model the high-dimensional gene expression landscape . basically, there are many genes ( > ;10,000 in humans ) and genes with related functions tend to have correlated expression . biologists understand that gene expression is complex , often exhibiting nonlinear relationships and hierarchical structuring . if we can learn to represent the structure of this distribution then we might be able to use the model to make predictions on the effects of specific genetic modifications . EOA 
 what are the coolest ml applications in genomic sciences ? : machinelearning here is a review paper : . EOQ that paper is gold , it deals exactly with the quantities of molecules the cells produce . i'm really happy to hear your opinion on the benefits of bringing ml and biology together , since i've recently begun a personal quest to work on exactly that intersection of fields . let's keep in touch when we see something cool :-). EOA 
 what are the coolest ml applications in genomic sciences ? : machinelearning here is a review paper : . EOQ a group out of toronto has been using deep learning ( surprise! ) for a variety of genomic inference tasks : rna splicing . the human splicing code reveals new insights into the genetic determinants of disease . URL predicting the sequence specificities of dna-and rna-binding proteins by deep learning URL they even have a startup formed around the approach : URL EOA 
 what are the coolest ml applications in genomic sciences ? : machinelearning here is a review paper : . EOQ that deepbind software looks a bit frightening haha . i worry its so on target that all i can tell my friend is to buy the software ;-). EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ i have tested with NUM k-samples ( NUM dimensions ) t-sne ( bh-sne implementation here URL ) , and the memory is very ok . EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ thanks , i will test it and see how it goes . tbh i was very hopeful when it comes to t-sne however based on what i read , people are saying that it's best used for visualization purposes i.e reduction to NUM d or NUM d . what i need is bigger ( i am currently using NUM d however i can change that ) . is there a way ( some metric maybe ) to say which final dimensionality would be best for my data when using e.g. t-sne ? EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ t-sne-like manifold learning methods are primarily for visualizing structures in NUM d or NUM d , but not primarily for dimension reduction . there are manifold learning for higher dimensions . this survey may tell you more about better techniques URL EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ i have NUM gb ram plus NUM gb swap . i am using scikit learn implementation which i belive uses barnes-hut under the hood . EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ a common technique is to use a NUM-step pipeline for dimensionality reduction for large datasets . first , use svd to reduce the dimensionality to something manageable ( < ;100 features ) , and then use some manifold learning technique ( eg t-sne ) to further reduce the dimensionality . EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ thanks , this is interesting as well:) as for svd(which i understand is very similar to pca but works with sparse matrices) i used it together with random trees embedding . similarly to pca one can get the percentage of all variance explained by each principal component . currently each my sample is reduced to NUM d which explains NUM % of all variance found by pca . would you say that NUM % is enough ? ( each extra final dimension gives me less than NUM % and more final dimensions i have the harder the training is ) . how to determine the good value for svd final dimensionality ? or t-sne for that matter ? EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ kernel pca is non-linear . you can find a good implementation in sicikit-learn library . URL EOA 
 efficient manifold learning/ non-linear dimensionality reduction technique : machinelearning how much memory do you have ? are you using the barnes-hut version of t-sne ? EOQ thanks , i will try it out as well . EOA 
 list of companies hiring for deep learning : machinelearning i would say almost every large company EOQ most large companies are hiring big data science teams , but i mean specifically places looking for people to implement deep neural network architectures . EOA 
 list of companies hiring for deep learning : machinelearning i would say almost every large company EOQ we at blackbird technology hire strong engineers for a variety of applied r & d in deep learning . feel free to pm me if you're interested in more details EOA 
 list of companies hiring for deep learning : machinelearning i would say almost every large company EOQ if you're looking to work on distributed cognition , cybernetics, deep learning , artificial intelligence , and multi-agent collaborative systems , hit me up . i'm looking for co-founders or contributors to playa : URL-i want to put a soul into your devices . my background : i've been coding the majority of my life , education in compchem/ee-robotics , now i want to build awesome future tech . EOA 
 list of companies hiring for deep learning : machinelearning i would say almost every large company EOQ bad link , NUM typo ? EOA 
 list of companies hiring for deep learning : machinelearning i would say almost every large company EOQ it should work now ! our host rm'd our zone files for some reason last night . EOA 
 genetic algorithms are not global optimization methods : machinelearning while it's definitely an abuse of the terminology to say that genetic algorithms are global optimization methods , i can't think of a better definition . it really depends on the presence of constraints or an objective function . genetic algorithms are local optimization methods which essentially consist of a monte carlo approximation of gradient-based optimization . you can't have a genetic algorithm without a mutation factor , thus saying that it is a local optimization method with a monte carlo factor is necessarily true . without the monte carlo factor there is no genetic algorithm , so you cannot separate the two moreover , with all these local optimization methods , including gas , even if you reach a global optimum you have no way to tell this has to do with the type of problem and not the optimization method . if you reach a any critical point of any np-complete problem then you may quickly verify if it is the global optimum or not EOQ the limitations you comment apply to the classic NUM 's-80's genetic algorithms . in fact john holland's schema theorem ( URL ) already highlights them . EOA 
