 since then there has been a lot of work about how to overcome them , by finding principled mechanisms to set up parameters to ensure that the algorithm avoids facing these scenarios , by designing smarter exploration mechanisms ( e.g. the bayesian optimisation algorithm ) , among other approaches . a good starting point to read about this topic would be URL EOA 
 genetic algorithms are not global optimization methods : machinelearning while it's definitely an abuse of the terminology to say that genetic algorithms are global optimization methods , i can't think of a better definition . it really depends on the presence of constraints or an objective function . genetic algorithms are local optimization methods which essentially consist of a monte carlo approximation of gradient-based optimization . you can't have a genetic algorithm without a mutation factor , thus saying that it is a local optimization method with a monte carlo factor is necessarily true . without the monte carlo factor there is no genetic algorithm , so you cannot separate the two moreover , with all these local optimization methods , including gas , even if you reach a global optimum you have no way to tell this has to do with the type of problem and not the optimization method . if you reach a any critical point of any np-complete problem then you may quickly verify if it is the global optimum or not EOQ i don't agree , ga are indeed global optimization methods . first ga use crossover and mutation operators . the first one exploits existing solutions ( parents ) to find ( produce ) new solutions ( descendants ) and yes it can be stuck in a local optima but not NUM % sure because they don't follow any gradient descent rule , and if they get stuck it's related to the fitness expression and the selection rule used in the algorithm . the second operator which is the mutation is used specifically to overcome the problem of getting stuck in local optima . this operator process in a random way with a small probability and push the search to look for new candidate solutions in new regions of the search space solutions . also if you want to find out experimentally that ga or any other ea tend to converge to the global optima . there are many problems artificial ones ( multimodal functions ) containing several local optima and one global optimum and when you use ga or ea the global optimum is found with no worries . the real problem facing ea is when we have problems with multi-global optima and we want them all , in this case the ea tends to converge to one of them and not all of them . this is what you should research on if you want to investigate on ga or ea at all . in my opinion ga or any other ea mut be first well tuned specifically for each problem . because the principe is clear and simple , it's natural evolution . the problem is the choice or the design of a suitable reproduction operators , fitness evaluation and selection process . you should refresh your bibliographic references before you judge that genetic algorithms can't be practically used . EOA 
 is it necessary to normalize the data again after extracting the data from autoencoder ? : machinelearning you shouldn't need in theory-the autoencoder should predict normalized data . anyway, just see how the data is-it should take you a few minutes . EOQ if the autoencoded result isn't already pretty centered , something's gone wrong . EOA 
 is it necessary to normalize the data again after extracting the data from autoencoder ? : machinelearning you shouldn't need in theory-the autoencoder should predict normalized data . anyway, just see how the data is-it should take you a few minutes . EOQ the output of autoencoder is not centered . so what i should do ? EOA 
 is it necessary to normalize the data again after extracting the data from autoencoder ? : machinelearning you shouldn't need in theory-the autoencoder should predict normalized data . anyway, just see how the data is-it should take you a few minutes . EOQ by output , you mean the hidden layer of the ae ? ( the 'real' output layer trivially has to be centered if the input is centered , otherwise the ae is just doing a very bad job in reconstructing ) i see no obvious reason why the values in the hidden layer have to be centered . depends on the nonlinearity : using a sigmoid will give you a mean of NUM in the hidden layer . anyways , just normalize it again before feeding it into the supervised net ( just helps training ) . however , it might be tricky once you try to fine tune both the ae and the supervised network jointly , there you have to do the normalization on the fly for each batch i guess . EOA 
 flops/$ : machinelearning their angle is URL , which is very much not targeting an individual trying to maximize flops/dollar . EOQ i couldn't find a price tag for the new drive px NUM . i doubt it will be NUM k $ ( link ? ) but not really sure about this one . in regards to your comparison-drive px NUM is optimized for deep learning purposes . NUM trillion deep learning operations per second means will outperform the amd card quite a lot in terms of deep learning . but this is only one piece of the puzzle . also for deep learning it is very important the size and the bandwidth of the memory and how fast you can connect to another similar modules in order to scale if you have to . EOA 
 flops/$ : machinelearning their angle is URL , which is very much not targeting an individual trying to maximize flops/dollar . EOQ as mentioned below , pascal adds fp16 support . gpus used to have separate vertex & pixel shaders , which in turn used to have higher & lower precision respectively . then they went for unified shaders , which had many advantages , but was fp32 all round . adding fp16 support to unified shaders tries to be the best of both , i guess . i gather fp32 is an overkill for neural-nets , but it's a nice compromise across the board for a nice range of use cases . EOA 
 flops/$ : machinelearning their angle is URL , which is very much not targeting an individual trying to maximize flops/dollar . EOQ what is a deep learning operation ? EOA 
 flops/$ : machinelearning their angle is URL , which is very much not targeting an individual trying to maximize flops/dollar . EOQ it's their marketing word for lower-than-fp32-precision operations . EOA 
 flops/$ : machinelearning their angle is URL , which is very much not targeting an individual trying to maximize flops/dollar . EOQ interesting . is there somewhere i can find benchmarks for < ;32 bit precision on different cards , or even a list of which cards support these ops ? EOA 
 flops/$ : machinelearning their angle is URL , which is very much not targeting an individual trying to maximize flops/dollar . EOQ so far fp16 ( only for storage , for compute they are converted to fp32 ) is implemented in maxwell URL with full fp16 coming in pascal . EOA 
 flops/$ : machinelearning their angle is URL , which is very much not targeting an individual trying to maximize flops/dollar . EOQ nvidia just came up with this term on ces16 presentation of drive px NUM ( see the link in the description ) . of course it is more or less for the show and it is not explained in details just yet . but i believe they try to create hardware which will be like the asic devices used for mining bit coins but narrowed to dl math . from nvidia site : specialized instructions that accelerate the math used in deep learning network inference . i reckon this would be operations specifically needed for deep learning i.e. matrix multiplications , fft convolutions , etc .. EOA 
 in a cnn how do each of the feature extractors ( filters ) train themselves to detect different features , when they get the same input ? : machinelearning because they get initialized randomly , just like the hidden layer of any other nn . EOQ symmetry breaking :) URL EOA 
 in a cnn how do each of the feature extractors ( filters ) train themselves to detect different features , when they get the same input ? : machinelearning because they get initialized randomly , just like the hidden layer of any other nn . EOQ because they are optimized jointly and not trained separately . repeating the same filter wouldn't reduce the error so the gradient for the weights will point towards a different one . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ thanks for the correction and the resources , it's a new area for me so i'm still getting to grips with the terminology . i'll check out the material you linked , much appreciated . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ i had a look through the paper by hu , koren and volinsky . it looks really promising but my main concern is r value described in section NUM as observations . i do not have the ability to get detailed data that would give varying values of r , only NUM and NUM . this doesn't appear to be an issue until the confidence value is introduced in section NUM , as i would only be able to get two possible confidence values . do you believe this approach would work using only the notion of likes and unknowns , a pu data set . edit : i've seen in the spotify presentation they seem to use a similar notion , so i'm assuming it's doable . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ yeah , you can probably get away with just two confidence values :) the confidence for the unobserved data should be NUM ( else the algebra doesn't work out ) and the confidence for observations could be any constant larger than NUM ( you'll have to treat this as a hyperparameter and optimize it to get the best results ) . perhaps you can come up with other heuristics to vary the confidence value , the two approaches they propose ( linear or logarithmic in the count ) are just two possibilities . maybe there is some other signal that could serve as a proxy for confidence in your case . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ awesome will look into it , cheers for your help . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ instead of varying alpha , could i modify the r values to be a function ? i.e instead of like-NUM , unknown-NUM could i possibly used like-NUM-(some metric) , unknown-NUM whilst keeping alpha constant for my model once optimised ? i was thinking of possibly using how popular the item is . i.e if they like a niche item they must really like it , if they like an extremely popular item theres less of a ranking . that way viral hits etc are treated with less importance than say a hidden gem which a user really enjoys . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ whether you vary alpha or r per item essentially amounts to the same thing , you multiply them together to compute the weights , so keeping one constant and varying the other is the same as vice versa . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ i would say that in practice ( i.e., industry ) , recommenders using implicit feedback ( that should be your key search term ) dominate-a relatively small percentage of users are willing to take the time to provide explicit feedback ( how many people actually give star ratings on netflix ? not many ) . the prime algorithm example being matrix factorization using weighted alternating least squares URL which is available in many libraries . EOA 
 recommender system-binary ratings without explicit dislike ? : machinelearning this is referred to as implicit feedback and there is actually quite a lot of literature out there on the subject . when you talk about papers that work in the binary domain , are you sure they are working with likes/dislikes and not with implicit feedback ? the latter seems more likely to me . at any rate , i recommend checking out collaborative filtering for implicit feedback datasets by hu , koren and volinsky : URL it's a fairly old paper by now , but a good starting point . the algorithm they propose is used extensively in practice , at spotify for example : URL EOQ cheers for the heads up , will have a look into that paper . EOA 
 when the central limit theorem don't apply ? : machinelearning well there are several clt's , many assume a finite second moment or other stuff . in economics unit roots will cause estimators to go to a gaussian density , but very fast (n rather than sqrt(n)) . EOQ this is correct . the central limit theorem only applies to the estimated means of a population parameter when sampled randomly and independently . this comes in handy when working with statistics because statistics are sample estimates of population parameters . in machine learning , we are interested in a variety of other techniques for learning from data in which you don't necessarily have a clt-granted guarantee of normality . EOA 
 when the central limit theorem don't apply ? : machinelearning well there are several clt's , many assume a finite second moment or other stuff . in economics unit roots will cause estimators to go to a gaussian density , but very fast (n rather than sqrt(n)) . EOQ well the classical example is when each random variable x.i is cauchy distributed . EOA 
 when the central limit theorem don't apply ? : machinelearning well there are several clt's , many assume a finite second moment or other stuff . in economics unit roots will cause estimators to go to a gaussian density , but very fast (n rather than sqrt(n)) . EOQ another way to think about it is that , under certain assumptions , a sum of random variables will have a distribution that approaches gaussian as the number of variables increases . so , if you are measuring something that could have been generated from a contribution of many variables , you might expect it to be approximately gaussian . the effect , however, is independent of the amount of data you collect . rather, it is an effect visible when the amount of random variables underlying each measurement is large in number . collecting more data will not produce a gaussian distribution . EOA 
 will intermediate steps like pos tagging become less important in nlp as deep learning becomes increasingly used ? : machinelearning i don't really know how to answer that question directly , since i'm far from a deep learning expert , but here's a paper with a perspective from chris manning on the role of deep learning in computational linguistics and nlp that you might find interesting : URL EOQ for tasks with large training sets available ( e.g. machine translation , maybe some kinds of question answering ) , yes. given enough data , lstms, grus , etc. can implicitely learn all the relvant syntactic features . if data is limited though , the additional syntactic information provided by preprocessing can be beneficial . if i recall correctly , there is a recent paper about this by the stanford ( chris manning's ) group , but i can't find it at the moment . EOA 
 will intermediate steps like pos tagging become less important in nlp as deep learning becomes increasingly used ? : machinelearning i don't really know how to answer that question directly , since i'm far from a deep learning expert , but here's a paper with a perspective from chris manning on the role of deep learning in computational linguistics and nlp that you might find interesting : URL EOQ i can imagine models that have inbuilt notions of pos , but that are constructed purely by training on large datasets . nobody will tell the model what words are verbs , what words are nouns , ect ect ect ... EOA 
 using rulefit : machinelearning using rulefit is a pain in the ass ; it crashes at weird times . i used it a few years ago but i don't remember much . you can figure most of the functionality out from here URL EOQ cool , thanks for your comment ! i have a question i was hoping you could help me with . i'm reading friedman's paper on rulefit and i'm not sure what support means ? any help would be very appreciated . see this image for context URL EOA 
 popular science book on machine learning : machinelearning maybe try the master algorithm by pedro domingos : URL EOQ some of the better popular books on ai and its uses : wired for war , pw singer URL machines of loving grace , john markoff the signal and the noise , nate silver final jeopardy , stephen baker behind deep blue , feng-hsiung hsu superintelligence , nick bostrom how to create a mind , ray kurzweil the age of spiritual machines , ray kurzweil on intelligence , jeff hawkins godel , escher, bach : an eternal golden braid , douglas hofstadter ( not strictly ai , but a truly great read about thinking about thinking ) . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ it's not really related to markov models at all , other than the notion that there is probability involved . if it helps to think of a stochastic process in time to simplify things for you , that's fine , but it need not be . you're not modeling probabilities of transition , you're modeling correlation/covariance between each point and every other point . there is no markovian assumption at all , so i'm not entirely sure where you're coming from frankly . i drew this to maybe make the idea more clear . URL EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ wow , that drawing helps a lot . thank you ! EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ couldn't you model transition probabilities of a latent state variable with a covariance matrix and vice-versa ? i somehow imagine that markov models are more general in that it may be impossible to describe correlations induced by a latent state space with a single covariance matrix . has anyone derived the guassian process covariance matrix which best approximates the statistics of an arbitrary markov random field ? which model do you think can describe a more general class of functions ? what types of functional forms are likely to be well described by a markov random field but poorly described by a gp ? EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ ( generally speaking ) . it's not necessary to consider such cases when trying to learn about gpr EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ generally speaking , it's not necessary to explicitly rule them out either . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ sure , but it's not particularly interesting or relevant for explaining what gps are EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ i found ( edit : fixed link to NUM st gp vid in playlist ) quite good . after NUM mins you get the basic theory , and after NUM mins you understand some nice examples in matlab which he is so geekishly enthusiastic about that it becomes entertaining . the regression vids come a bit later though , but they are at the end of that playlist . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ the first NUM mins are a pretty good intuitive explanation of realizations of gaussian processes , but let's not kid ourselves , there is not even any basic theory in the first NUM minutes . i don't want people to think they know the theory when it's not even remotely touched upon . i mean , these videos don't even get into definitions until after NUM minutes (which is absolutely fine and good!), but i prefer to call it what it is EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ i posted the wrong link , updated now ; it should now start with definitions and some theory for ~18 mins in NUM vids , followed by two ~10 min vids of realizations that i assume you were referring to . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ the first two aren't really theory at all either . i think your concept of what theory means is quite a bit different from what i'm talking about . the first two are really all just definition and explanation of what they are , not why they work the way they work or even how . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ you can check this post : to get a better insight . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ this is a good post EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ the other explanations of gps here are decent , but you may be confusing a gaussian process with a kalman filter-i guess this from when you say they're similar to markov models but with a continuous state and gaussian probabilities for evolution and emission . that's a kf . a gp is defined by a covariance function , which tells the model how similar two observations are . a commonly used function is the squared exponential ( exponentiated quadratic ) which says two points that are close are more similar than two points that are far away . it looks just like a normal distribution/bell curve . this covariance function is then used to define a multivariate normal . the use of a normal distribution allows a bunch of mathematical tricks ( look up marginalisation properties of the mvn ) which allow us to really easily predict the mean and variance of this normal distribution at unobserved locations . the maths is quite simple and the end result is a linear basis function model where the functions are kernels centred on each observation . why you would use a gp : less magic-with a bayesian basis function models you select the number and type of functions , and learn distributions over them . with a gp you select a kernel that expresses some belief about how points might covary and then just have to learn hyper parameters . you get uncertainty estimates-i like to know when my model is not sure , and these are useful from bayesian optimisation and quadrant ute . good with small data-often you'll won't have enough data to really justify something like a dnn but still want a rich model-gps ! there are connectios between the kf and the gp but these are something fun to look into later :) EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ btw , gp regression is not great with small data if you have > ; a handful of dimensions . it's very very difficult to estimate functions in high dimensions ( high for nonparametric estimation is much smaller than high for , say, glms ) . it's really hard to estimate a complicated NUM dimensional function with gpr if you have only a few tens of thousands of points EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ i'd put the handful nearer to NUM-30 dim , but sure . standard gpr doesn't work well with > ; a few NUM s of thousands anyway due to scaling . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ it really depends on how complex the function is , but in the applications of my field , NUM is extraordinarily difficult EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ that will be true of any regression model though . if you are trying to model a complex function in more than a few dimensions with small data , you are doomed no matter what model you use . you just don't have the information to train the model . rule of thumb : simple function , low-d, large data : trivial simple function , low-d, small data : easy simple function , high-d, large data : easy simple function , high-d, small data : hard complex function , low-d, large data : easy complex function , low-d, small data : hard complex function , high-d, large data : hard complex function , high-d, small data : totally hopeless , need scotch EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ also probably worth pointing out but not news to either of you guys , 'number of dimensions in the data you get handed' and 'number of dimensions you need to model it' are rarely equivalent . you can usually get away with projecting into a much lower space . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ [ deleted ] EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ i'm sorry . i didn't mean to come off as patronizing . i thought it was worth pointing out that no model will do well with small data and a complex , high-d response . the rule of thumb wasn't posted just to you , but as a general thing for anyone who comes across this thread . i don't know your background or field , but i do know that sometimes people come through these threads without a lot of background and i figured the extra bit might be helpful for someone . sorry again . i certainly didn't mean to be insulting or patronizing . cheers ! EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ i found the post above informative , so you're definitely not patronising at all . that's just his opinion . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ here's an explanation i like. i'd normally draw some stuff on a piece of paper for you , but you'll just have to use your imagination . imagine taking NUM i.i.d. samples from a univariate gaussian distribution . now plot the with the x-axis being the sample # , and the y axis being p(x) from the univariate gaussian . what's it going to look like ? boring white noise. nothing to see here ; move along . next , imagine taking a single sample from a NUM-dimensional gaussian distribution , where each dimension is independent . i.e. the covariance matrix is just some multiple of the identity matrix . what's it going to look like ? the same as above-boring , white noise. nothing to see here ; move along . ok , now replace that boring covariance matrix with something more interesting . anything at all really . what's it going to look like ? well without knowing the covariance matrix you picked , i can't say for sure , but i can say that it's probably going to be something very interesting . it's not going to look like boring white noise anymore . i daresay it's going to look like something you would call a function , having working with functions since about grade NUM or so . it may be a smooth function , or a jagged function , or a periodic function , or a function with a linear trend , etc. it all depends on what covariance matrix you picked . so it me , that's the key idea-for data created by just about any function you can think of , there is a multivariate gaussian distribution that can generate similar data . sub-idea : multivariate gaussian distributions are really cool and flexible tools . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ when sampling from a NUM dimensional gaussian distribution , what are you plotting ? EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ this is an incredibly good explanation , thank you so much . i actually have a much better idea of what a gp actually is now . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ to supplement the explanations given by others , i hope this little animation may be helpful to you . it shows in a toy NUM d example on how the approximation of the function is improving by adding more samples ( red dots ) . the confidence intervals are also shown ( eli5 version : if the blue line does not happen to be the true solution , the technique is NUM % sure that the function should be somewhere in the blue domain ) . the example was made with python-sklearn ( link ) . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ the animation itself illustrates it but doesn't really tell me how it works . however , that tutorial looks very good , i've bookmarked it for reading . thanks! EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ there is already a number of explanations in the other comments . i felt that a k-1th wouldn't contribute as much as this animation to supplement the understanding of the topic . EOA 
 eli5 gaussian processes ? : machinelearning you are modeling an arbitrary ( but somewhat smooth ) function by viewing it as a realization of random process . by viewing it as such , you can estimate the function by modeling the correlation of each point with every other point . to predict the function at any arbitrary new point , based on the correlations you've estimated from your training data , you find the prediction by taking an optimal weighted linear combination of all of the training points . gaussian process regression is remarkably simple in concept once it clicks ( albeit the rkhs theory behind it is fairly heavy stuff ) edit : i assumed you meant gaussian process regression . a gaussian process as a stochastic process is a type of stochastic process which is utilized for gpr EOQ have you tried understanding any textbook chapter on gps ? which book's ? EOA 
 ml libraries for matlab ? : machinelearning matlab has some built-in ml libraries for trees ( fitensemble ) , knn, svm , and log regression . the matlab site is very easily searchable for these . for nn there is caffe . but python will be faster . r probably too , and matlab doesn't have a good xgboost implementation . EOQ pardon my question , but what is a xgboost ? EOA 
 ml libraries for matlab ? : machinelearning matlab has some built-in ml libraries for trees ( fitensemble ) , knn, svm , and log regression . the matlab site is very easily searchable for these . for nn there is caffe . but python will be faster . r probably too , and matlab doesn't have a good xgboost implementation . EOQ extreme gradient boosting . a ridiculously fast implementation of gradient boosted trees . EOA 
 ml libraries for matlab ? : machinelearning matlab has some built-in ml libraries for trees ( fitensemble ) , knn, svm , and log regression . the matlab site is very easily searchable for these . for nn there is caffe . but python will be faster . r probably too , and matlab doesn't have a good xgboost implementation . EOQ some people in my office use matconvnet for deep learning and they seem to like it . EOA 
 ml libraries for matlab ? : machinelearning matlab has some built-in ml libraries for trees ( fitensemble ) , knn, svm , and log regression . the matlab site is very easily searchable for these . for nn there is caffe . but python will be faster . r probably too , and matlab doesn't have a good xgboost implementation . EOQ huh , that looks interesting . but it says it doesn't have normal mlp/fully-connected layers , just a special case of conv layers . is that efficient ? EOA 
 ml libraries for matlab ? : machinelearning matlab has some built-in ml libraries for trees ( fitensemble ) , knn, svm , and log regression . the matlab site is very easily searchable for these . for nn there is caffe . but python will be faster . r probably too , and matlab doesn't have a good xgboost implementation . EOQ matlab is a steaming pile of shit . learn python . EOA 
 ml libraries for matlab ? : machinelearning matlab has some built-in ml libraries for trees ( fitensemble ) , knn, svm , and log regression . the matlab site is very easily searchable for these . for nn there is caffe . but python will be faster . r probably too , and matlab doesn't have a good xgboost implementation . EOQ i would suggest mxnet for nn , it has bindings for python , julia, r , and c-. julia has matlab-style array access and is otherwise comparible to r and python and is a great upcoming language , but if that doesn't work for you , you can use the r bindings . EOA 
 ml libraries for matlab ? : machinelearning matlab has some built-in ml libraries for trees ( fitensemble ) , knn, svm , and log regression . the matlab site is very easily searchable for these . for nn there is caffe . but python will be faster . r probably too , and matlab doesn't have a good xgboost implementation . EOQ matlab-EOA 
 what is the state of the art in text classification ? : machinelearning the conv-net paper looked interesting . if anyone else is looking for the code i found it here : URL EOQ is anyone interested in a discussion about this paper ? i agree very interesting . EOA 
 what is the state of the art in text classification ? : machinelearning the conv-net paper looked interesting . if anyone else is looking for the code i found it here : URL EOQ thx thx ! EOA 
 what is the state of the art in text classification ? : machinelearning the conv-net paper looked interesting . if anyone else is looking for the code i found it here : URL EOQ thanks ! i hadn't seen this paper ! EOA 
 what is the state of the art in text classification ? : machinelearning the conv-net paper looked interesting . if anyone else is looking for the code i found it here : URL EOQ you're best starting off with a bag of words model without convolutions . this paper has a simple set up if you want to use neural networks : URL linear models work very well though . add drop-out on the tokens , and set the feature values to be the log-odds ratio of that feature in the data . this is called nb-svm ( naive bayes svm ) . depending on the task , it's often important to drop some tokens from the text to learn a more domain independent model . this is rarely discussed in the literature because it goes a bit beyond a single academic evaluation . but let's say you're learning a sentiment model , and you're training on imdb movie reviews . if you want the model to be useful on any other sort of review , you should probably manipulate the training data . removing named entities is a good start . my library , spacy, is a useful way to do these things . if you're just using bag of words it isn't so helpful . but as soon as you add any transformation spacy can help a lot . this is an implementation of iyyer's model , using spacy for the basic text processing and numpy to implement the model : URL EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ this is exactly what i am talking about . people seem to use it in practice-why is it not talked about in the ml community ? EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ well , some people use it in the practice of optimization , but very few use it in the practice of building predictive models ( which is essentially a multivariate optimization problem for which a genetic algorithm is well-suited ) . if you google about you'll find several studies over the past NUM-20 years that show how ga's out-perform other basic modeling processes like stepwise regression . this is in part because a ga can identify and optimize suppressor effects that stepwise regression might otherwise ignore . i've also demonstrated , convincingly ( where convincingly is having an ivy league phd statistician sit at his desk , holding his head in his hands , muttering this cannot be , and then said same phd statistician sign-off on the work ) that daisy-chaining a ga as a variable reduction / feature selection tool on the front-end of other algorithms like treenet ( stochastic gradient tree boosting , which has won kdd cup ) can significantly improve model performance relative to allowing treenet to search the broad space of a data set with hundreds of predictors . why this is the case i do not know-theoretically , treenet should out-perform just about anything out there and should not need pre-processing to narrow the search space . but the proof was very conclusive. this is why i am a big fan of using a ga-i've collaborated on a tool that outputs a solid model , that helps you learn what data and what magnitude of parameterization gets your prediction optimized , and at times you can take the output of the ga as the variable selection input to other tools and get further improvement beyond what you would get if the second tool stood alone . another point-the ga i use creates models that are very easy to interpret , with direct control over the complexity of the model . i can set it to a NUM , NUM , NUM feature equation , and include or exclude NUM st order interactions . the results stand in stark contrast to black box algorithms that may out-perform the ga on validation , but where interpretation can be a real pain and actually a barrier to approval in regulated industries such as banking and insurance . again , google on using ga for variable selection , you'll find there's empirical support for this approach . one last thing-the tool i use requires no programming skills beyond basic wintel point-click at data sets and folders , and managing and debugging an excel spreadsheet that controls large batch processing . so it is a modeling process that you can teach a kid to use in about an hour , and that kid's modeling results will rival the work of a traditional phd statistician while eliminating NUM-95% of the modeling effort ( modeling effort , not interpretation effort , i don't mean to imply that a novice can interpret and understand a model like an expert , and i do not discount the importance of interpretation ) . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i did that exact same feature-selection trick ( used differential evolution instead of genetic algorithms , but it's the exact same idea ) at an old job ! horribly slow , but the performance i got out was just incomparable . thanks for letting me know i'm not the only insane one . :-) EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i think the big argument against gas is that they are quite slow compared to typical ml algorithms . however, deep learning has shown that people are willing to invest the time and hardware for slower algorithms-if they prove they can advance the state-of-the-art . i think gas are simply lacking a champion ( or champions ) to prove their worth . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ what big players use gas for ml in practice ? lots of amateurs use gas because they are easy to implement and understand , and because they don't know much about ml or optimization . in fact , that is one of the most common weaknesses i see in practical users of ga , almost a complete lack of knowledge of optimization EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i don't think that's a fair characterization of the ga userbase . gas can be a bit hype-y and attractive to new users , but gas are also applied to many problems that simply can't be touched by standard optimization techniques . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ NUM reasons . firstly, because it's not generally considered to be machine learning , more a branch of optimisation and/or search . secondly, apart from the handful of dissenters here , i don't know anybody who is using them in practice. i wrote my msc thesis on gas and find them generally interesting , but most problems have a more predictable , effective, and mathematically rigorous approach available . for example , you can use gas as a simple way to train neural nets , but backprop is more effective because it makes extensive use of the error vector . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ no , not really . these terms have reasonably specific implications . ml often uses optimisation techniques ( eg. in how to minimise a cost function ) to train a system , but you can't just drop an ml algorithm in to replace an optimisation or search algorithm . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ about the same reason that high-frequency trading algorithms are not discussed . people don't want to part with their secret sauce . ga has been heavily used to create new products and algorithms , with some even rewarded patents . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ what is the commercial ga tool you're using ? EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ www.semcasting.com their modeler product . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ gas tend to become competitive when you have no information about the gradient of your error function , and even then there are other options ( some which have a better theoretical grounding ; although see for some provable statements about gas ) . when we design ml algorithms , however, we usually build them so that we have some useful information about the gradient of the error function and then we can use more powerful optimization techniques . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ another strength of gas is that they work even if the problem solution cannot be formulated as a vector of numbers . gas can essentially produce any sort of structure as a solution , including program code . for purely numeric problems i've found that other meta-heuristics , like particle-swarm optimization and the firefly algorithm perform much better than simple ga . the problem with ga is that it's blind , and has no idea in what directions better solutions might lie . pso & fa are less blind . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ exactly . i've been researching and it's felt fairly natural to pose as a ga problem . i'm unsure how i would optimize such a task with anything other than ec or a hillclimber . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ yepp , like in reinforcement learning where the state and the action could be literally anything ... they could be numeric , even in a continuous space . but they don't have to . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ this is one of the reasons why i used gas and other similar ( better ) alternatives in aircraft design . in many cases , there is a lot of of interplay between computational fluid dynamics , empirical data , and a host of other software packages in conjunction , and getting error gradients is impossible . but gas have basically no direction in design/program space . they work in manners that are quite mysterious , tbh. in the context of neural networks though , it is fairly well established at this point that there are plenty of local minima(especially if the model is deep) and finding global minima is not necessary to achieve good performance . thus, it makes sense to use a direction vector based on gradient to quickly reach the nearest local minima , rather than perform a global search . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ do we have reliable proofs of convergence ? EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ what's the typical approach to using the firefly algorithm for an optimization problem ( e.g. how might it be applied to hyperparameter optimization ) ? the algorithm is nominally about firefly clustering ( attracted to each other ) , so i'm wondering what typical mappings to/usage for optimization might be ? EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ the fireflies are just points in an n-dimensional space , so anything that can be formulated that way will work . the only thing you need is a fitness function from the points in the space . for example , i'm using it to tune a record linkage algorithm . the vector is basically the configuration of the algorithm using naive bayes ( the vector is a set of probabilities ) . there's no way to know what the error gradients are , and so firefly is perfect for this . the next step is to see if active learning works with firefly . i assume it does , but haven't tried yet . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ well , about NUM years ago , they were hyped ( much like ml and big data now ) but failed to deliver on the great promises they made . so, now they are left out of mainstream ml . additionally, most gas are too slow as optimization algorithms in ml . for global optimization they are still used though . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ this scenario sounds familiarly . neural networks were hyped and failed to deliver , then been left out of the mainstream ml . now they are back , straight to the top . i am wondering aren't we missing something by not walking deeper into the gas territory .... EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i believe nature applies ga approach because it is not possible for it to measure gradient ( like : hey i have a bit longer beak than my dad and i'm more succesful so i will make my kids have it even longer , thought a woodpecker. ) i feel like ga ever got so popular mainly because of its hype name EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i agree with you . both neural nets and gas got hype from name and empty appeals to biology . however, i feel like that is all that gas had while neural nets were actually a sexy-name for something with more clear mathematical justification and hope for formal theory . thus, with neural nets , the field could guide its development by both experimental/engineering trial-and-error and mathematical insight , while gas mostly has to rely on the former . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i think we're going to see a lot of neat-like algorithms in the coming decade . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ could someone comment how ga's compare to bayesian optimization techniques . as far as i understand , the latter can also be used to optimize non-convex or discontinuous functions over compact parameter sets . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ in my experience , it's just that genetic algorithms are one special case of markov chain monte carlo ( mcmc ) which has been around for NUM years , has a rigorous statistical foundation , and does find extremely widespread use in a variety of fields , including machine learning [ NUM ] . look at the metropolis-hastings algorithm [ NUM ]-starting from a random sample , you propose a new sample ( mutation ) , then accept or reject the sample according to the acceptance ratio ( fitness ratio ) . [ NUM ] URL [ NUM ] URL EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ genetic algorithms also allow for the crossover operator , which some claim is the key to life , the universe , and everything ( literally and figuratively ) . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ good point-so it appears that the mcmc equivalent in that case [ NUM ] is to run multiple markov chains in parallel and interpret crossover as a specific kind of proposal . [ NUM ] URL EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ crossover is only important for sexual reproduction . that's a modern innovation which mostly , but not always , beats asexual reproduction . for example , certain snails switch between sexual and asexual depending on the disease-risk in their environment . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i was about to say something similar . it does seem that many of potential use-cases for genetic algorithms are crowded out by other methods tailored to the model at hand . in some sense , an evolutionary algorithm is being used when you have a very complex model and you're playing around with the parameters to get it it to converge on a value . the few genetic algorithms i've come across seem to be poor replacements for those sorts of things . perhaps ( and i'm just musing here ) genetic algorithms might be useful in creating ecosystems of models that work well together , or for some other task like that where we don't have better , less random , ways of optimizing a model . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i use pso for svm but thats about it . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ could you explain why ? EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ coarse parameter tuning for non linear kernels , derivative free , non-convex EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i used to do a lot of ga and evolutionary simulations , and i still love them for certain tasks . they can do certain tasks where other algorithms simply don't apply , like variable-parameter-count optimization . if you disagree , show me a non-genetic algorithm ( or similar evolutionary approach ) that can reproduce something like this : URL EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ imo evolution is a very powerful approach , though at this point we cannot make it go as fast as modern ml algorithms . i've tried before and probably will try again in the future to apply some ga models but for base machine learning problems it's simply not there . though a couple of years ago neural networks were problematic too and not very good fit for most of the ml tasks , look them now . :) EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ it's my understanding that a lot of top firms use genetic algorithms in place of 'intuitive guesswork' that goes into setting hyper variables up for neural network training . basically, since at best an expert in neural networks can only have a good few guesses about what the best settings for the variables that set up the neural network , some experimentation with genetic algorithms is often warranted . this is something i've been using and when i recently read ray kurzweils how to create a mind it sounds like something he and his teams have been doing for several decades . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ URL this library uses ga to do hyperparameters search in scikit-learn EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ genetic algorithms are different from machine learning techniques . ga is an optimisation algorithm , a heuristic , attempting to find a good enough ( hopefully optimal ) solution to a problem which has many possible solutions . its context is different than that of ml , since in the latter you are trying to predict data . i would say they are complementary to each other , as you may first predict data to define your optimisation problem , and then try to solve it . however there are numerous other heuristics that can be used as well as exact algorithms , to tell you the truth i practically never consider ga . given the input data ( predicted using ml ) , i first try to solve an optimisation problem exactly ( find the optimal ) , if proven to be too difficult , i move to local search based heuristics , which in my personal view are superior to ga based heuristics . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ ultimately , nearly every machine learning task can be broken down into two components : selection of a cost function , and an attempt to optimize that cost function . ga is a general purpose non-linear optimization algorithm . you don't hear people talk about it for the same reason you don't hear people talking about simulated annealing or the simplex algorithm : these are really better discussed in the context of optimization specifically than machine learning generally . if you take an optimization or numerical methods course , you'll definitely spend some time on this . but probably not in s machine learning class . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i'm just now reading a paper on using dropout to improve the training of neural networks , and the authors say the following for how they got the idea : a motivation for dropout comes from a theory of the role of sex in evolution (livnat et al ., NUM ). sexual reproduction involves taking half the genes of one parent and half of the other , adding a very small amount of random mutation , and combining them to produce an offspring . the asexual alternative is to create an offspring with a slightly mutated copy of the parents genes . it seems plausible that asexual reproduction should be a better way to optimize individual fitness because a good set of genes that have come to work well together can be passed on directly to the offspring . on the other hand , sexual reproduction is likely to break up these co-adapted sets of genes , especially if these sets are large and , intuitively, this should decrease the fitness of organisms that have already evolved complicated co-adaptations . however, sexual reproduction is the way most advanced organisms evolved . dropout : a simple way to prevent neural networks from overfitting by : nitish srivastava , geoffrey hinton , alex krizhevsky , ilya sutskever , ruslan salakhutdinov sounds like it shares a lot in common with genetic algorithms . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ disclaimer : i'm learning this stuff in my free-time. set me straight if something i say doesn't sound right . you can accomplish much of the same thing using neural networks . i'm not sure if you know yet , but genetic algorithms represent a similiar approach to learning except instead of using a mathematically derived gradient descent algorithm(rooted in fancy things like dynamical systems and convergence theory) , you use something that more closely resembles an ai search function . with ga's you have parameters compete using a fitness function and expanding your search in the direction of fit parameters . with neural nets you have a mathematical guarantee that your gradient descent algorithm is going to get your parameters to a locally optimal position that minimizes an error function in vague evolution-y related terms , both approaches get your object towards a set of genetic traits that work better than any configuration you've had before . both run the risk of arriving at a point that gets us to a stable , but not necessarily optimal , gene pool . neural nets work faster . evolutionary algorithms , from the perspective of a developer may be easier to approach ... it is much easier to understand and work with a greedy-optimization algorithm , than it is to pick up tensor calculus , statistical optimization , pde's, and numerical analyis . genetic algorithms can also be effective if your parameters are combinatorially constrained to a small set of parameters , or if your system seams un-supervised . you can hold on to a certain amount of bias in your gene pool . you can add genetic variance to a stagnant population . neural networks are bit more difficult to implement but iteratively find a straight route to some locally optimal set of parameters . its less about guessing to optimize your learning machine and more about experiences adjusting the trajectory of a neural net flying through parameter space . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ as chico.science said , ga is a family of optimisation methods . so you cannot oppose ga to neural networks , rather to backpropagation . neural networks can be trained with genetic algorithms . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ again set me straight if i'm off here : i think you may be right . do you know , off the top of your head,any examples of how that happens ? to my knowledge ga's are only used to modify ann topologies(example would be neat) . that falls into the sort of optimization that deals with a combinatorially constrained search space . that is why i think neural networks offer a more efficient way to deal with most problems , you're not randomly hopping around in parameter space . you need a domain specific function to evaluate fitness to make a genetic algorithm work correct ? EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ there's ga code thats basically not related in any way to neural nets . for example , you could use a ga to implement a traveling salesman problem ( navigate every location exactly once in the shortest distance ) . your input would be the list of locations to visit ( say , NUM points in this example ) . your input is [{45,23},{12,-60},...]. your ga tries a bunch of different mutations of that input ( ie : re-orders the inputs ) , and when the optimization function ( ie : total length for the mutated input ) finds that one input is doing better than the previous known best , it will tend to mutate from that input . ie : nothing at all related with neural nets . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ ga theoreticians tend to be randomly found in the machine learning community , and they are too competitive to well integrate that community EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i'm skeptic wrt to generalization of gas used to fit predictive models . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ why moreso than for any other ml technique ? EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ just anecdotal and personal experience . i guess it's not even borderline mainstream for a reason , although gecco does have an evolutionary machine learning track . i can't remember reading about convincing applications of evolutionary and swarm intelligence in ml . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ i apologize but i'm new to ga's . call me naive , but wouldn't the current backprop through time be far more efficient in terms of training time ? i feel that if you mutated your selected chromosomes randomly , it would take much longer to converge . it seems that backprop would be far more efficient . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ you're right , backprop is much more efficient . ga's and other local search methods can be useful if your objective(loss) function isn't differentiable , or if you don't even have one . they are also better at avoiding local minima . EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ oh okay i gotcha-i wonder if they could be applied to translation for bleu and meteor scoring or even translation edit rate...thanks! EOA 
 genetic algorithms in machine learning ? : machinelearning i've used a commercially available genetic algorithm for over a decade . primarily to build rank-ordering predictive models for use cases that don't require the model to be built in an hour . it can build a simple model in minutes , but can also crunch through a NUM gigabyte modeling dataset with NUM predictor columns , using $2500 of hardware ( intel i7 chip , no over-clocking ) . it works very well . placed in the top third in the NUM kdd cup while keeping my modeling process aligned with actual business workflow-in other words , my modeling was done with the constraints that it to pass regulatory scrutiny and i got the job done in about NUM hours of work , with about NUM submissions as compared to the top kdd models which involved large teams and hundreds of submissions . in my last gig , i spent about NUM months pimping out the system . built in a coarse classing feature that improves model robustness in the face of raw input distributional shifts while also forcing all effects to be monotonic ( important in regulated industries such as credit scoring where adverse action reasons must be interpretable ) . also perfected a batch modeling process that allows me to build the same model over and over again to create a distribution of results for better understanding of the natural variability in modeling outcomes that result from a non-deterministic algorithm and repetitive sampling . that feature also allowed me to empirically test ga settings and data combinations , so i can learn what data actually improve a model rather than dumping in the kitchen sink and not knowing what is actually moving the needle . also a batch scoring process so that the same machine that is building the model can score the model without any scoring code management . EOQ genetic algorithms are just hill climbing with random restarts , but with the added assumption that variables near each other are related . this sounds like a good assumption until you realize that near means near each other in your program's data structures , not necessarily in reality . try simulated annealing instead . EOA 
 resources for feature engineering on sound samples : machinelearning there's literally a ton of stuff out there on this topic . it may depend on what you want to do , but usually mfcc is one of the main feature types used . i recommend reading a few surveys from recent years . URL EOQ thanks i will look into it . EOA 
 resources for feature engineering on sound samples : machinelearning there's literally a ton of stuff out there on this topic . it may depend on what you want to do , but usually mfcc is one of the main feature types used . i recommend reading a few surveys from recent years . URL EOQ relevant : URL EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ i want to try to generate paragraphs of maybe NUM words or more . which direction would you suggest i move ? EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ [ deleted ] EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ thanks ! i'll definitely look into it EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ URL is a cute use of an rnn to generate magic : the gathering cards . it's implemented in torch and there's source somewhere in the mtgsalvation thread . EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ also , i suppose you have come across karpathy's seminal blogpost about rnns and text , right? URL ( with code and output examples ) EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ yes i've seen it . it is my first option so far although i would prefer to find a python implementation for compatibility issues because i will be working on django for the frontend . do you know maybe ways that this model can be used from python ? EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ i ported karpathy's char-rnn code to python/tensorflow . check it out , and let me know if it serves your need . raise an issue if you'd like a certain feature . URL EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ karpathy has a basic numpy-only version ( min-char-rnn ) , which works surprisingly well . but true , it's not a lstm network and you can't run it on a gpu , so it's never gonna be as fast and as performant as the lua/torch implementation he has there . but i'd try there , see how far the basic numpy version gets you , and experiment from then onwards ! EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ just fyi : mxnet has a NUM line lstm example in python for rnn text generation , plus a tiny shakespeare text generation URL EOA 
 how hard is it to code a rnn for text generation ? : machinelearning [ deleted ] EOQ[?]xristos.forokolomvos[s]&#32;0 points1 point2 points&#32;1 month ago&nbsp;(2 children)EOQ as your goal is to generate new sentences and not to generate new words , you could try to use words as inputs with a one-hot encoding ( see sentiment classification with lstm ) , but your dataset has to be large enough . keras could be a framework that fit your needs . don't expect to train anything without a gpu . EOA 
 simple dataset for training recurrent neural network ? : machinelearning if you want a simple toy problem to solve , take a look at the reber grammar . the embedded ( not word embedding ) version of this grammar was used in the original NUM lstm paper by hochreiter and schmidhuber . also , an advantage of using a grammar is that data can be generated on the fly . EOQ you can just use mnist dataset , and use a rnn to classify digits . this will run easily on cpu . see : URL EOA 
 simple dataset for training recurrent neural network ? : machinelearning if you want a simple toy problem to solve , take a look at the reber grammar . the embedded ( not word embedding ) version of this grammar was used in the original NUM lstm paper by hochreiter and schmidhuber . also , an advantage of using a grammar is that data can be generated on the fly . EOQ i think he'd prefer a time series dataset , rather than datasets with images . multi-dimensional rnn perform well on mnist , but the recurrence mechanism is a bit different to the recurrence mechanism in rnn for time-series ( which is what he wants to try i guess from his message ) EOA 
 simple dataset for training recurrent neural network ? : machinelearning if you want a simple toy problem to solve , take a look at the reber grammar . the embedded ( not word embedding ) version of this grammar was used in the original NUM lstm paper by hochreiter and schmidhuber . also , an advantage of using a grammar is that data can be generated on the fly . EOQ people have been doing one-pixel at a time classification of mnist ( starting with irnn from le , hinton as far as i know ) , for reasons i can't fully understand . it works pretty well if you leave everything in order ( it only really needs part of the pixels to figure out class with high accuracy ) , but if you use a fixed permutation of the pixels the long-term dependencies break most existing rnn architectures . personally i think reber grammar or penn treebank make nice examples . alternatives include char-rnn stuff or even just regression of a sine-wave . EOA 
 simple dataset for training recurrent neural network ? : machinelearning if you want a simple toy problem to solve , take a look at the reber grammar . the embedded ( not word embedding ) version of this grammar was used in the original NUM lstm paper by hochreiter and schmidhuber . also , an advantage of using a grammar is that data can be generated on the fly . EOQ that's true ! i could just feed the image one pixel ( or one line ) at a time info the model . but that leaves the advantage of a recurrent model untapped . EOA 
 simple dataset for training recurrent neural network ? : machinelearning if you want a simple toy problem to solve , take a look at the reber grammar . the embedded ( not word embedding ) version of this grammar was used in the original NUM lstm paper by hochreiter and schmidhuber . also , an advantage of using a grammar is that data can be generated on the fly . EOQ you could also just build a language modeling rnn . you don't need any labeled data for that , any text will do , since your goal is to predict the next word given the previous words . URL EOA 
 can anyone explain the sandblaster l-bfgs ? : machinelearning ummm ... its running multiple models in parallel against the same data , the coordinator keeps track of starting and stopping models , and the parameter server keeps track of what parts of parameter space have been explored ? EOQ i would assume it's the same models but different data . EOA 
 is it an accurate statement : natural language understanding has been somewhat neglected : machinelearning that slide deck is from around when deep learning was really catching on , and was still heavily focused on images . text understanding has become a very active area in the past year . EOQ hey thanks ! is there a place i can view all of the papers applying deep learning to text understanding ? wish i had another upvote to give ! this one looks very interesting URL EOA 
 is it an accurate statement : natural language understanding has been somewhat neglected : machinelearning that slide deck is from around when deep learning was really catching on , and was still heavily focused on images . text understanding has become a very active area in the past year . EOQ oh man , there's a ton . i think a few you should at least be aware of are : paragraph vectors seq-2-seq , and attention models which improve on this model . skip-thought vectors text understanding from scratch EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ and the computational complexity associated with the kernals with increasing dimensionality of the data is another problem . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ well with each kernel the svms become a different algorithm so this is not a very well defined question . so one drawback of the svms is you have to choose the kernel . that means you have to provide the true structure of the data as an input , while other algorithms , like neural networks or random-forests , try to automatically find the structure . also you have to tune the parameters for the kernels and the c parameter which can be time consuming and decrease the performance if you do it wrong . here is a so you can either find some non-linear dataset and try to fit it without a kernel to show that it doesn't work or you can set some random values at the c parameter to show that it decreases the accuracy . or you can find a very simple and big dataset and show that it takes very long for the svms to train while a simple logistic regression has the same results in less time . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ i guess ... you earn a prize today for avoiding completely the word deep . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ he must have missed the paper on srpsudfchrue-svms ( super resolution pruned stochastic ultra deep fully connected hierarchical rectified unreasonable effective-svms ) /s EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ non-linear svms have a training complexity between o(n2 ) and o(n3 ) where n is the number of training examples . therefore , for difficult machine learning tasks where you need lots of data to obtain good accuracy , and lots of data is indeed cheaply available , svms will usually perform badly since you wouldn't be able to train on all the data you have . neural networks dominate these tasks since usually they can be efficiently trained by stochastic gradient descent ( which also has the nice property of being an online algorithm , which makes it highly suitable for training sets that are too large to fit in main memory , or even training sets that are streamed during training ) . on smaller training sets you should always compare non-linear svms to random forests , as empirically random forests often , but not always , perform better . linear svm classifiers don't have these training complexity issues , but like all the other linear classifiers they can be accurate only on approximately linearly separable tasks . both linear and non-linear svms can be also used for regression , but i don't think they are widely used for it , thus i suppose they don't perform as good as other machine learning regression methods . anyway, the same complexity considerations apply . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ i use svm to classify and rank music from my collection . svm works fairly well at classifying the music into genres . but it doesn't do well at ranking music i would like . my main complaint with svm's is parameter selection . as i add new music i find that the svm have over-fit previous samples . which forces me to go back and adjust the parameters of the svm . parameter selection with svm's is a tedious process and just as time consuming as training a neural network . just as an example , i hate death metal . i've given the svm NUM 's of examples of death metal . but when i get new death metal music , the svm will inevitably rank some of the death metal with a high ranking ( music i would like ) . svm don't seem to be able to generalize very well . and when faced with new data tend to be at a loss of what to do with that data . most likely because of over-fitting . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ just curious here , what features are you using to classify and rank your music ? EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ i first convert each song to a wav file of the first minute of the song . i start with a vector of NUM floating points to represent each song . NUM of the floats represent the average frequency , NUM represent changes in frequency over blocks of the data ( NUM blocks NUM second each ) , and NUM represent the average beats frequencies during the song ( energy levels ) . i then normalize the data , then use pca ( principle component analysis ) to reduce the NUM floating points down to NUM floating points . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ i actually skip NUM seconds . but if i were to skip NUM seconds , i would have to ignore all the songs that were less than NUM :30 in length . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ select the middle minute of the song ? EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ you need an online learning method . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ for fine-grained text classification , say detecting subjectivity in a phrase ( or sentence ) , or the sentiment ( pos , neg, neutral ) . try it on social text . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ i'll give that a shot ! mind explaining what makes them unsuitable for that kind of text classification ? EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ well it's not just svms that perform poorly , most out-of-the-box classifiers will perform poorly on those tasks . svm-linear kernel with a bag of words representation will probably get you around NUM % accuracy for short text . just a few percentage points higher takes significant effort . an rbf kernel won't make any sense applied to text , and you'll be sitting at your computer for . EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ so what are the appropriate method to use for that kind of task ? EOA 
 where do support vector machines perform badly ? : machinelearning well , linear svms would not perform well on data that is not linearly separable . but of course you could fix that by choosing a better kernel . of course , choosing the right kernel is an additional problem . EOQ svms are rather stiff / inflexible models . the main drawback is that you either have to use a linear decision boundary-which is sub par for many problems , or you have to bake assumptions into the kernel-like that all features should be taken with equal importance . an example of a problem where an svm would be awful is a nonlinear problem with many features , only few of which are informative. a linear kernel would suck in that case for the obvious reason , and something like an rbf would suck because it would give equal weight to noisy and informative features , and would be swamped by noise . a random forest with the right params would destroy the svm . EOA 
 what type of job should a fresh graduate ( b.s. ) get ? : machinelearning a couple thoughts . most people say that if you go to industry , you won't go back to academia . that is generally true . of course there are some exceptions but what typically happens is that you get accustomed to getting a paycheck , you buy a car , a nicer place...etc and you don't want to give that up and go back to being a poor college kid . also , you'll quickly forget a lot of what you've learned because you won't use it , so you'll be intimidated to go back . some people get their m.s. in the evenings going part time , but that takes a lot of extra work and drags out a NUM year program to be much longer . the most determined can do it , but most don't . my advice is that if you want to go to graduate school , go now . it seems like NUM years is a long time , but it's not and the remaining NUM-years of your career will benefit . while in school , bust your ass to get an internship or part time gig at a startup company to get industry experience . there's a lot of benefit you get from working in industry that you don't get from school work , so doing both at the same time is best and will set you up for a great job in the future . and since you're a student without a mortgage and family , you can work with startups that don't pay much ( or at all ) but have the potential to pay off huge if they are successful . that could put you into a leadership position at a company fresh out of college , which is something you'll never be able to do in traditional industry . in a big company , you will be a grunt until your NUM s unless you get lucky . if you're interested in machine learning , i think graduate school is what you should do . it's very math-heavy and it's harder to pick up advanced math skills casually than it is many other skills , like a new programming language . in my experience ( i'm a data scientist in industry ) , people with advanced degrees in math/physics have more upward potential because the people that either stopped at b.s. or went the computer science route will never have the deep understanding of statistics you need . and there's a big push right now to automate all the brainless machine learning stuff . it's easy to transform data , train a bunch of models and choose the one with the best score . the real skill that data scientist will add moving forward is understanding why they are performing poorly and what strategies should be used . a real application of statistics that leads to good questions/hypothesis to be verified in the data is what the machines can't do . if you just train yourself to be able to read the formulas of an algorithm and code them up , you'll be replaced in no time. finally , you need to ignore the job requirements you find in postings . they are useful to find which skills companies are talking about the most , but that's about it . you get a job because NUM ) you know someone and NUM ) you have done cool things that show people you can probably learn to do what they want you to do . that means you should be doing everything you can to meet people and work on projects for a portfolio . i actually started a blog to document my journey of becoming a data scientist and what strategies i found useful . check it out if you want : changefields.com EOQ wow thank you for your insightful reply ! i've talked to some other people who have given me information similar to the first NUM paragraphs , but the last NUM really gave me some new perspective . do you think a phd or a masters has more value given the time invested right now in this field ? also , what type of programs do you think are most useful ? i've looked at the coursework for applied math , bioinformatics, comp sci , etc..., and they all seem to capture different aspects of the field that i'm interested in . i'm just not sure where i should hone in on to get the most useful knowledge for the field . i still think i will take NUM years off ; i really just want a break from structured learning for a bit . but i'm very sure that i'll go back to school . i've always loved learning for its own sake , have constantly loved going to school , and live a very low-maintenance lifestyle despite being supported for the time being by a well-off family . i'm not worried about putting in the effort . but i will take as much advice as i can get before that point ! EOA 
 what type of job should a fresh graduate ( b.s. ) get ? : machinelearning a couple thoughts . most people say that if you go to industry , you won't go back to academia . that is generally true . of course there are some exceptions but what typically happens is that you get accustomed to getting a paycheck , you buy a car , a nicer place...etc and you don't want to give that up and go back to being a poor college kid . also , you'll quickly forget a lot of what you've learned because you won't use it , so you'll be intimidated to go back . some people get their m.s. in the evenings going part time , but that takes a lot of extra work and drags out a NUM year program to be much longer . the most determined can do it , but most don't . my advice is that if you want to go to graduate school , go now . it seems like NUM years is a long time , but it's not and the remaining NUM-years of your career will benefit . while in school , bust your ass to get an internship or part time gig at a startup company to get industry experience . there's a lot of benefit you get from working in industry that you don't get from school work , so doing both at the same time is best and will set you up for a great job in the future . and since you're a student without a mortgage and family , you can work with startups that don't pay much ( or at all ) but have the potential to pay off huge if they are successful . that could put you into a leadership position at a company fresh out of college , which is something you'll never be able to do in traditional industry . in a big company , you will be a grunt until your NUM s unless you get lucky . if you're interested in machine learning , i think graduate school is what you should do . it's very math-heavy and it's harder to pick up advanced math skills casually than it is many other skills , like a new programming language . in my experience ( i'm a data scientist in industry ) , people with advanced degrees in math/physics have more upward potential because the people that either stopped at b.s. or went the computer science route will never have the deep understanding of statistics you need . and there's a big push right now to automate all the brainless machine learning stuff . it's easy to transform data , train a bunch of models and choose the one with the best score . the real skill that data scientist will add moving forward is understanding why they are performing poorly and what strategies should be used . a real application of statistics that leads to good questions/hypothesis to be verified in the data is what the machines can't do . if you just train yourself to be able to read the formulas of an algorithm and code them up , you'll be replaced in no time. finally , you need to ignore the job requirements you find in postings . they are useful to find which skills companies are talking about the most , but that's about it . you get a job because NUM ) you know someone and NUM ) you have done cool things that show people you can probably learn to do what they want you to do . that means you should be doing everything you can to meet people and work on projects for a portfolio . i actually started a blog to document my journey of becoming a data scientist and what strategies i found useful . check it out if you want : changefields.com EOQ if you are confident that you want to end up in industry i think a phd is overkill in most cases . i'm actually in a phd program right now ( part time ) and am quitting because that effort put in other places has a larger payoff potential . on on the other hand , if you can go to a top school and work with a well-respected mentor in the field , that may be the way to go . i am not in a top program however , so the phd isn't going to buy me much . at my company about NUM % of the people have a phd and it doesn't give them any discernible career benefits . the skills you learn by undertaking a phd are valuable , but you can learn those same skills ( and more ) in industry . it's really a matter of : can you do self-directed learning on a large project where the answer isn't in the back of the book ? i originally joined a phd program because , like you , i would look at job postings and see that research positions at , e.g. amazon , listed it as a requirement . but in reality , the top positions at the top companies go to the rock stars or the guys coming out of harvard , so ticking the phd box with a mediocre school isn't going to get you much closer . i would argue ( and reasonable people could disagree ) that it's more valuable to build something that adds real value instead of focusing on something that contributes to the knowledge of humanity in a likely irrelevant way . my reasoning is that to make a contribution worthy of a phd you have to hyper-focus on some esoteric problem because it takes so much time to become an expert in anything to the depth required to make a unique contribution . of course you have to choose something that's solvable in that amount of time and unless you get really lucky , that means something of very little consequence . most phds are not land-mark discoveries , they are dead-ends . on the other hand , you could build some end-to-end data analytics pipeline on aws where you're spinning up hadoop clusters , running parallel processing jobs and building nice web-based , interactive visualizations . you're not going to get a phd for that , but you're going to learn a lot and top companies are paying big bucks for people like that . i think that's more valuable than having spent NUM years writing something that no one will ever read , not even your parents . in terms of a masters program i think you should strive for a couple things . first , a well respected program is a big nice-to-have . the connections you make there will be invaluable-even more important than what you learn . if they have strong ties to local industry , even better . next , i would strive for something that focuses on hands-on . imo , too much schooling is focused on concepts themselves and not their application . something like applied math is probably where i would go . getting experience using math/stats to build real predictive models and then managing them will provide you valuable experience . then in your free time get a pet project or NUM and try to build a product to form/join a small company around . that's how you get rich . when a bigger company acquires you , guess who has the big hot-shot vp position ? best of luck . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ thanks ! do you have a component list already ? perhaps we can compere notes ? i was looking at the end-all config i found here : URL probably with i7 , ssd and perhaps cheaper gpu for now . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ take a look at this : URL there are many builds for lots of different target profiles ( workstation,gaming etc.. ) or you can build your own ! :d EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ cool page ! i would recommend a better cpu since this will be the most important piece . and consider you'll need a lot more ram . you should find a motherboard that can contain that amount of ram . for fast storage , i suggest pcie ssds . the power supply is important as well , but probably you won't need NUM w . check here URL . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ if you're not doing gpu work ( and sometimes even then ) it makes more sense to use aws with spot requests . you can access instances with up to NUM cores and NUM gb of ram , and increase your storage capacity to whatever you need . and you only pay for the capability you're using . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ i put together a list on part picker . just went for specs i want and then chose the top rated ( also trading off cost ) for the other components ( see below ) . comes out a bit over budget though . also seems max ram is NUM g for motherboards available here . anything i can shave price on ? pcpartpicker part list / price breakdown by merchant type item price cpu intel core i7-4790k NUM ghz quad-core processor 245.94 @ aria pc cpu cooler cooler master hyper NUM evo NUM cfm sleeve bearing cpu cooler 24.98 @ novatech motherboard asrock z97 extreme4 atx lga1150 motherboard 112.64 @ more computers memory corsair vengeance pro NUM gb ( NUM x NUM gb ) ddr3-1866 memory 145.32 @ more computers video card gigabyte geforce gtx NUM ti NUM gb video card 93.77 @ ccl computers case bitfenix neos white/blue atx mid tower case 27.64 @ aria pc power supply evga NUM w NUM-gold certified fully-modular atx power supply 109.99 @ novatech operating system microsoft windows NUM professional sp1 oem ( NUM-bit ) 109.97 @ ccl computers monitor asus vg248qe NUM hz NUM monitor 217.49 @ ccl computers prices include shipping , taxes, rebates , and discounts total 1087.74 generated by pcpartpicker NUM-01-06 NUM :35 gmt-0000 EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ are you sure you need : windows ? NUM hz monitor ? NUM w psu ? an off-board vga at all if all you are doing is cpu number crunching ? you can also save a little cash by going with a i5-4690k . you lose NUM mb l2 cache and some clock speed , but i suppose you are planning on overclocking anyway . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ why windows ? that's a hundred pounds wasted right there and it won't run a lot of the popular ml libraries and frameworks . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ i thought a lot of machine learning could blitz the gpus cores better than cpus ? since the type of processing needed is done better by gpus these days . maybe look into an older workstation cpu like the quatros by nvidia , or the new telsa's but they are way expensive . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ he should not have to buy a quadro or tesla , something like titan x or NUM , even NUM should be enough for starting machine learning . it is probably a good choice , to start with a lower end gpu at first ( like gtx NUM , or gtx NUM ) . later on , you could upgrade to a better one , this summer there should be a new generation of gpus available , with NUM-3x speed ups compared to current gpus for machine learning calculations . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ bigger ones better for larger data sets then ? i'm new too and was wondering about gpus for this . i have a pair of NUM s i was thinking of upgrading to a NUM ti since they are NUM years old now . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ if you could just wait a few monthstm , you might get better/cheaper gpus . nvidia should release much improved gpus this summer (q2,q3?). they should offer substantial improvements for machine learning-see these anandtech links : article.1, [ article.2 ] (www.anandtech.com/show/9902/nvidia-discloses-2016-tegra) . the bigger gpus offer bigger memory ( larger datasets ) and better memory-bandwidth ( faster training times ) . this comparison table of the NUM series on wikipedia , might be usefull . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ you should probably spring for a NUM NUM gb graphics card now . only adds NUM and can be used for training neural nets . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ if you have specialized needs then self-build is the way to go . it's fun and easy enough as long as you're ok with assembling things . the downsides of a pre-built pc are likely to include a barely adequate psu for the given configuration , and small amount of ram compared to what you need ( which you may then have to replace to free up ram slots to expand with larger capacity sticks ) . ssd performance is going to be limited by the interface , so consider raid-ing a couple of smaller sata ones together rather than using a single big one . pcie vs sata ssd would have faster transfer speed , but cost more , and for random access ( e.g. dataset shuffle ) the raid array may be faster . you definitely want an nvidea gpu , not radeon ( like one of those pre-builts ) since the software support ( nvblas , cudnn, etc ) is way better . the gtx NUM is not a bad entry-level card , but the NUM gb of ram may be limiting ... for a little more you could get a gtx NUM with slightly more oomph and NUM gb . best bang for the buck ( but a lot more expensive ) is a gtx NUM ti . you could save a bit by buying the graphics card used off ebay . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ have you thought about building with old server parts ( e.g. xeon NUM cpu ) ? NUM s are quad core @ NUM ghz and run about $20 on ebay . the server boards they fit in are typically multi-socket so you can get two cpus for an NUM-core machine. boards normally run < ; $40 . getting greater than NUM gb of ram should also be fairly affordable ( < ; $50 ) as long as you pick a board that supports it . in all , you can build an NUM-core machine with modern ssd/hdd and power supply for < ; $250 . there are a number of drawbacks though . first, there are no warranties , so when something breaks , you are stuck with it ( though it should be cheap to replace ) . second, fans for server cpus are really loud and there are not a lot of after-market solutions . third, most of the server boards i have seen have at most one pcie x16 slot so you will never be able to run more than one gpu . finally, there won't be any way to upgrade in the future . overall , it is a pretty cheap way to get an NUM-core machine as long as you put in the time to ensure part compatibility and are okay with the prospects of not being able to upgrade anything in the future . EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ thanks so much for ll the advice. it's really helpful ! i think i've settled on a config but i have NUM more questions : pci-e is it worth it ? e.g. kingston shpm2280p2h/240g 170 vs samsung NUM evo-series NUM gb NUM solid state drive for 56 ? is the cooling i've gone for sufficient ? is there anything else i could tweak to improve this without too much extra cash ? my updated list (without os or display , thats my next conundrum.): pcpartpicker part list / price breakdown by merchant type item price cpu intel core i7-4790k NUM ghz quad-core processor 248.00 @ amazon uk cpu cooler cooler master hyper NUM evo NUM cfm sleeve bearing cpu cooler 24.98 @ novatech motherboard asrock z97 extreme4 atx lga1150 motherboard 112.64 @ more computers memory corsair vengeance pro NUM gb ( NUM x NUM gb ) ddr3-1866 memory 145.32 @ more computers storage samsung NUM evo-series NUM gb NUM solid state drive 55.71 @ amazon uk case bitfenix neos white/blue atx mid tower case 27.64 @ aria pc power supply xfx ts NUM w NUM-gold certified atx power supply 58.26 @ ccl computers prices include shipping , taxes, rebates , and discounts total 672.55 generated by pcpartpicker NUM-01-06 NUM :22 gmt-0000 EOA 
 pc build for machine learning and data mining : machinelearning i would go for self building : it's fun , less expensive and you can choose the components you want ; moreover, you'll know your hardware better . i was looking to build a pc for machine learning as well . the best cpu for me is the i7-4790k , especially for its single-thread performance . if most of your code is parallelized , you may find something less expensive but as effective. take a look here for cpu benchmarks : URL . EOQ if you really just want to go through the process of picking out and getting a new computer to play with , building one is a good option . but if you actually just want to do machine learning and build your skills , it's much more worthwhile to rent computer performance on aws or azure . not only is it a shit-load cheaper since it's unlikely you'll ever spend $1000-on aws experimenting , but it's actually a really useful skill to work in aws that can lead to you getting a job . not only will you have access to a powerful computer , but you'll also be able to spin up hadoop clusters and work with the latest and greatest tools . with the job that sort of skill set will get you , you could buy a new computer every month . much better long term strategy . EOA 
 object detection/localization on cpu ? : machinelearning it's generally not required ; most libraries have an option you can set somewhere to use the cpu only . the reason why they say it's required is because you will incur a tremendous speed penalty by doing this . you may find the test times bearable but training will be very very slow ! EOQ do you have experience with this particular library , and know where i can find this option ? i can't find it . i understand the reason for using gpu's . EOA 
 object detection/localization on cpu ? : machinelearning it's generally not required ; most libraries have an option you can set somewhere to use the cpu only . the reason why they say it's required is because you will incur a tremendous speed penalty by doing this . you may find the test times bearable but training will be very very slow ! EOQ caffe , torch, theano , and tensorflow all of a cpu option . my favorite is the keras library , which can run on theano or tensorflow . these are all generic neural network libraries . they all have classification examples , and moving to object detection / localization will take a bit more work . EOA 
 object detection/localization on cpu ? : machinelearning it's generally not required ; most libraries have an option you can set somewhere to use the cpu only . the reason why they say it's required is because you will incur a tremendous speed penalty by doing this . you may find the test times bearable but training will be very very slow ! EOQ if you can't figure that out , just don't bother . you'll have much harder problems doing anything useful with the code . EOA 
 object detection/localization on cpu ? : machinelearning it's generally not required ; most libraries have an option you can set somewhere to use the cpu only . the reason why they say it's required is because you will incur a tremendous speed penalty by doing this . you may find the test times bearable but training will be very very slow ! EOQ yolo is pretty easy to set up and supports cpu-only computation for object detection , it takes longer on cpu though , about NUM sec/img . URL EOA 
 object detection/localization on cpu ? : machinelearning it's generally not required ; most libraries have an option you can set somewhere to use the cpu only . the reason why they say it's required is because you will incur a tremendous speed penalty by doing this . you may find the test times bearable but training will be very very slow ! EOQ caffe , torch, probably mxnet , all support both cpu and gpu training . but you'll be better off running your main training on something like amazon aws ec2 , once you've got your code more or less working , which you can do on your own computer , using your cpu . if you have a recent cpu by the way , you might actually have a gpu embedded in the cpu itself , and can use opencl . both caffe and torch support using opencl for training . you can find out the model of your cpu from control panel , or from /proc/cpuinfo , and then google for it , to check . EOA 
 how to correctly train rnns for sequence classification ? : machinelearning question NUM : you reset memory after the pass of one complete series . you want to simplify the lstms job of sequence classification , thus it is a good idea to start and stop the sequence yourself . also, sometimes it is not possible to understand where the sequence starts or ends from the sequence alone (ex : handwriting/text). you can reset the cell states to all zeros or to a learned vector . if you have a single stream of data , you fix a limit on the number of timesteps the lstm takes and then back propagate for that many timesteps , and finally reset the cell states . question NUM : i didn't get what you were conveying question NUM : if you have NUM sets ( examples ) of time series data , call a-[ a1 , a2, ... , an ] , b-[ b1 , b2, ... , bn ] , and c-[ c1 , c2, ..., cn ] , at each time step , you feed in ( ai , bi, ci ) at the 'i'th step i.e ( a1 , b1, c1 ) , ( a2, b2 , c2 ) and so on . if you have a single continuous stream of data , split it into multiple parts , for example NUM parts to get a , b and c . a is first one third , b is the second third and c is the last third . EOQ thanks for the reply . for q3 that definitely makes more sense than what i initially thought . will have to do some more reading to find out exactly what gradients are used when doing the 'unfolding' of the network when using batches EOA 
 in nlp , how can i measure the parlance between NUM corpora ? : machinelearning i am not sure but maybe try running lda on your whole corpus ( model generated result-training ) and from that you can get topic distributions for each text , which could be a measure to try . EOQ if i correctly understood , here's one option : train a language model on the training data and then measure its perplexity on the generated text . the lower , the better . to get an idea of what would be a good number , you can split the training data , train the lm on one part and measure perplexity on the second . compare this number to the one you get for the same lm on the generated text . EOA 
 in nlp , how can i measure the parlance between NUM corpora ? : machinelearning i am not sure but maybe try running lda on your whole corpus ( model generated result-training ) and from that you can get topic distributions for each text , which could be a measure to try . EOQ for what it's worth , perplexity is NUM cross entropy so it's similar to the op's idea but using regular lms rather than just unigrams . EOA 
 in nlp , how can i measure the parlance between NUM corpora ? : machinelearning i am not sure but maybe try running lda on your whole corpus ( model generated result-training ) and from that you can get topic distributions for each text , which could be a measure to try . EOQ thanks for pointing out the ce-ppl relationship that i neglected to mention . i think ngrams vs . unigrams can make a big difference and with lms like srilm or irstlm , you can pretty easily get the numbers . EOA 
 in nlp , how can i measure the parlance between NUM corpora ? : machinelearning i am not sure but maybe try running lda on your whole corpus ( model generated result-training ) and from that you can get topic distributions for each text , which could be a measure to try . EOQ yeah definitely . probably i'd do both so i could get a sense of how much of say the bigram ppl difference is due to vocab differences ( which i'd guesstimate with unigram ppl ) . though the op's test is a little odd-comparing text generated with the rnn to the source corpus . in some ways using a plain lm to evaluate that is like checking the rnn with a reference non-rnn implementation . might be a useful sanity check that the rnn's ppl of the text it generated should be lower than the ppl of a plain backoff model . EOA 
 in nlp , how can i measure the parlance between NUM corpora ? : machinelearning i am not sure but maybe try running lda on your whole corpus ( model generated result-training ) and from that you can get topic distributions for each text , which could be a measure to try . EOQ by parlance you mean the vocabulary ? if so , jaccard or cosine similarity are simple and symmetric . is this for a diagnostic test of sorts ? if so , you might consider taking several corpora of different genres/styles and comparing both to those so that you can say that one distribution is more/less formal , etc. EOA 
 my rbm and backprop implementation in haskell : machinelearning that's cool . just started looking into haskell ! what most interests me is your use of quickcheck and property testing . how did you find using it for machine learning ? did you use it upfront or after the fact ? did it find bugs you think regular testing would not have ? EOQ its just a force of habit . i am a c developer by trade , so i don't really trust anything i write. i find it easier to start unit testing from the bottom up , so breaking down any big functions into smaller parts and verifying those . EOA 
 newbie in neural prog : which os and hw to choose ? : machinelearning no , you don't need a cuda enabled card . tensorflow is modularized , and works on cpu too . it would work better with gpu , as it intelligently delegates operations like blas , and matrix operations on gpu , but that isn't strictly necessary . also, a linux system would always be recommended , as most libraries including tensorflow will not work on windows . ideally native performace is better than virtualized , but since you do not want to use native , you could use a vm . docker is just a container , so i am not sure if that would work at all . what you need is a full os in a vm , with a real kernel . if you did have a gpu , it would be necessary to have installed the right graphic card drivers too . if that were the case , i would recommend ubuntu for relatively having best nvidia support in the linux ecosystem . EOQ also , it should be noted that ( afaik ) there's no way to virtualize gpu , which means linux has to be a host os in order to use graphic cards . EOA 
 newbie in neural prog : which os and hw to choose ? : machinelearning no , you don't need a cuda enabled card . tensorflow is modularized , and works on cpu too . it would work better with gpu , as it intelligently delegates operations like blas , and matrix operations on gpu , but that isn't strictly necessary . also, a linux system would always be recommended , as most libraries including tensorflow will not work on windows . ideally native performace is better than virtualized , but since you do not want to use native , you could use a vm . docker is just a container , so i am not sure if that would work at all . what you need is a full os in a vm , with a real kernel . if you did have a gpu , it would be necessary to have installed the right graphic card drivers too . if that were the case , i would recommend ubuntu for relatively having best nvidia support in the linux ecosystem . EOQ i agree , linux is the most suitable environment for machine learning but it's not necessary to learn the basics of ml and nn . before you go and dual boot or emulate i would recommend taking a look at a number of options available for windows . these include matlab and octave , matlab has its own neural network library which is great for learning . python will work on windows , enthought canopy is also a very good package i would also recommend looking at , try installing scikit learn ... EOA 
 newbie in neural prog : which os and hw to choose ? : machinelearning no , you don't need a cuda enabled card . tensorflow is modularized , and works on cpu too . it would work better with gpu , as it intelligently delegates operations like blas , and matrix operations on gpu , but that isn't strictly necessary . also, a linux system would always be recommended , as most libraries including tensorflow will not work on windows . ideally native performace is better than virtualized , but since you do not want to use native , you could use a vm . docker is just a container , so i am not sure if that would work at all . what you need is a full os in a vm , with a real kernel . if you did have a gpu , it would be necessary to have installed the right graphic card drivers too . if that were the case , i would recommend ubuntu for relatively having best nvidia support in the linux ecosystem . EOQ thanks for your advice , i will try it ! EOA 
 newbie in neural prog : which os and hw to choose ? : machinelearning no , you don't need a cuda enabled card . tensorflow is modularized , and works on cpu too . it would work better with gpu , as it intelligently delegates operations like blas , and matrix operations on gpu , but that isn't strictly necessary . also, a linux system would always be recommended , as most libraries including tensorflow will not work on windows . ideally native performace is better than virtualized , but since you do not want to use native , you could use a vm . docker is just a container , so i am not sure if that would work at all . what you need is a full os in a vm , with a real kernel . if you did have a gpu , it would be necessary to have installed the right graphic card drivers too . if that were the case , i would recommend ubuntu for relatively having best nvidia support in the linux ecosystem . EOQ your welcome . let me know if you get stuck . when i started i found ironing but the issues hard than actually the implementation of the machine learning code . EOA 
 newbie in neural prog : which os and hw to choose ? : machinelearning no , you don't need a cuda enabled card . tensorflow is modularized , and works on cpu too . it would work better with gpu , as it intelligently delegates operations like blas , and matrix operations on gpu , but that isn't strictly necessary . also, a linux system would always be recommended , as most libraries including tensorflow will not work on windows . ideally native performace is better than virtualized , but since you do not want to use native , you could use a vm . docker is just a container , so i am not sure if that would work at all . what you need is a full os in a vm , with a real kernel . if you did have a gpu , it would be necessary to have installed the right graphic card drivers too . if that were the case , i would recommend ubuntu for relatively having best nvidia support in the linux ecosystem . EOQ well , i'll be appreciate if you just give me an advice regarding docs : which book do you recommend for newbie ? i know there are so many good book about neural networks also subj is progressing so rapidly so i'm afraid to waste of time reading old/obsolete concepts . in other words : i need some good materials to deep dive into this topic ( like deep dive into python book ) . ps . at this moment i found a couple books : building machine learning systems with python harrington machine.learning.in.action NUM y natural language processing with python a brief introduction to neural networks by david kriesel EOA 
 solomonoff's induction in machine learning : machinelearning li and vitanyi are the references for ait , you can look at this page for applications and i guess the best is to read their book : an introduction to kolmogorov complexity and its applications . i don't really see how it could be used for lda . EOQ lda uses prior distributions for topics-documents and topics-vocabulary . won't solomonoff induction help in estimating those priors ? EOA 
 solomonoff's induction in machine learning : machinelearning li and vitanyi are the references for ait , you can look at this page for applications and i guess the best is to read their book : an introduction to kolmogorov complexity and its applications . i don't really see how it could be used for lda . EOQ i think that the universal prior for a finite object s is basically the NUM /2-k(s) where k(s) is the kolmogorov complexity of an object . it's supposed to be a very generic kind of prior that applies to all finite sequences . if you already know that s is text , then you can easily build a prior that is very close to the data in comparison . EOA 
 solomonoff's induction in machine learning : machinelearning li and vitanyi are the references for ait , you can look at this page for applications and i guess the best is to read their book : an introduction to kolmogorov complexity and its applications . i don't really see how it could be used for lda . EOQ there was a paper on practical implementation of approximation of kolmogorov prior for universal compressed sensing-recovery of undersampled signal assuming minimum complexity . unlike normal compressed sensing it doesn't assume signal is sparse . URL EOA 
 solomonoff's induction in machine learning : machinelearning li and vitanyi are the references for ait , you can look at this page for applications and i guess the best is to read their book : an introduction to kolmogorov complexity and its applications . i don't really see how it could be used for lda . EOQ i'm pretty sure that one cannot actually perform solomonoff induction , but i believe approximations to it exist . some useful keywords for finding papers on such approximations : monte carlo aixi EOA 
 solomonoff's induction in machine learning : machinelearning li and vitanyi are the references for ait , you can look at this page for applications and i guess the best is to read their book : an introduction to kolmogorov complexity and its applications . i don't really see how it could be used for lda . EOQ indeed the true solomonoff prior is non computable . you can try to approximate it but i don't think it's very practical ( at least at the moment ) . EOA 
 online machine learning course study group ? : machinelearning i am definitely down ! would be exciting to see other people's viewpoints as going through the material . EOQ i would like to join too . i even got the companion book for this course . EOA 
 online machine learning course study group ? : machinelearning i am definitely down ! would be exciting to see other people's viewpoints as going through the material . EOQ awesome ! i'm thinking about picking the book as well this week . it got pretty good reviews on amazon . made a new subreddit /r/mlstudygroup/ for the course and machine learning learners in general if you'd like to join EOA 
 online machine learning course study group ? : machinelearning i am definitely down ! would be exciting to see other people's viewpoints as going through the material . EOQ definitely joining as well ! EOA 
 online machine learning course study group ? : machinelearning i am definitely down ! would be exciting to see other people's viewpoints as going through the material . EOQ i would very much like to!!thank you !! EOA 
 online machine learning course study group ? : machinelearning i am definitely down ! would be exciting to see other people's viewpoints as going through the material . EOQ this sounds like something i am interested in . i will join . EOA 
 online machine learning course study group ? : machinelearning i am definitely down ! would be exciting to see other people's viewpoints as going through the material . EOQ for anyone looking to join late i just asked to join and got added to the group so don't hesitate to request an add ! EOA 
 [ama request] richard s . sutton : machinelearning regarding reinforcement learning , what do you think that could be the single most important accomplishment over the next few years ? EOQ URL EOA 
 poisson in lda : machinelearning  is probably just a hyper parameter . as for why poisson , who knows . i suppose you could model the length of a document as a point process where words arrive at rate  . EOQ i'm still trying to grasp lda , so pardon my stupidity . why do the words have to arrive at a rate ? are they not finite and predetermined ? regardless , doesn't the generative process operate sequentially ? for each document , then each word within that document ? can't n just be set to some fixed natural number ? EOA 
 poisson in lda : machinelearning  is probably just a hyper parameter . as for why poisson , who knows . i suppose you could model the length of a document as a point process where words arrive at rate  . EOQ i'm not saying that the words must arrive with some rate , but i'm only saying that you could think of that as one motivation for using a poisson distribution . also , this describes the forward process-that is , it answers the question how does the model suppose that the data was generated ?. when you actually receive data , yes, it's very easy to count the number of words in each document . so in that sense , they are finite and predetermined once you have the data . i'm not sure what you're asking in your third question because the way i understand it , your process doesn't actually differ from the proposed process unless you meant that one should think documents are infinite lists of words ( which has its own issues ) . ( and your fourth question was already answered. ) EOA 
 poisson in lda : machinelearning  is probably just a hyper parameter . as for why poisson , who knows . i suppose you could model the length of a document as a point process where words arrive at rate  . EOQ finally , the poisson assumption is not critical to anything that follows and more realistic document length distributions can be used as needed . EOA 
 what are the most cutting edge things possible with deep learning ? : machinelearning i think this recent demonstration of work headed by yann lecun shows the most impressive cutting edge applications : URL it hints at actually smart ai assistants perhaps not being far off , which to me seems very exciting-tackling not just perception but actual reasoning and common sense . EOQ thanks ..!! EOA 
 what are the most cutting edge things possible with deep learning ? : machinelearning i think this recent demonstration of work headed by yann lecun shows the most impressive cutting edge applications : URL it hints at actually smart ai assistants perhaps not being far off , which to me seems very exciting-tackling not just perception but actual reasoning and common sense . EOQ cutting edge things EOA 
 what are the most cutting edge things possible with deep learning ? : machinelearning i think this recent demonstration of work headed by yann lecun shows the most impressive cutting edge applications : URL it hints at actually smart ai assistants perhaps not being far off , which to me seems very exciting-tackling not just perception but actual reasoning and common sense . EOQ will the deep learning based behaviours of physical robots be more impressive than leaps in computer vision , speech and nlp ? EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ i really do not know . it is probably an issue with the code i wrote , but i thought that it would iterate until k-NUM or it reached the max number of iterations . however, it does not terminate the algorithm . EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ have you tried looking into other implementations of k-means ( e.g. the one from scikit-learn ) ? EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ not yet , but i will now that i know . thank you ! EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ i am no where near an expert in this field , and actually just found this subreddit today (already learning ;)). it seems to me that a neural network architecture or deep learning algorithm could be well suited for this . i know this is a broad answer , but hopefully someone can confirm and extend my answer . good luck ! EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ yea i was just looking into neural networks , but i am concerned about the time complexity of the algorithm . i want to this algorithm to run in a relatively low amount of time , but if i have to sacrifice time for better clustering/classification i would . EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ are you worried about classification runtime or training runtime ? EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ classification runtime EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ then a neural net is fine. so long as the input vector is easy to compute ( or already exists ) , a neural net takes very little time to classify a single sample . training a net is the time expensive part . EOA 
 choosing a learning algorithm for instance clustering/classification : machinelearning out of curiosity , how does a k-means algorithm end up with only NUM cluster if you set k-3 ? EOQ thank you ! EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ another lazier-but-easier option would to train the model with cross-entropy but choose the model with the best auroc cross-validated performance . EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ i wonder if there is any theoretical work on the relationship of auroc ( and also auprc ) with binary cross entropy . i have made a ton of experiments and i have always observed that auroc monotonically increases with the decrease of binary cross entropy . but maybe this is not always the case ... ? EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ auc roc only is only effected by the order/ranking of the samples induced by the predicted probabilities . for any auc score you have a range of cross entropy scores because cross entropy considers the actual values . you can maintain an order while changing probabilities ( e.g. by multiplying or adding ) . as an extreme case , a perfect auc score of NUM may have a larger than zero cross entropy . EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ you can't calculate the auc for a single example , only for a larger test set . so it's going to be pretty difficult to optimize using sgd . EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ training is done in minibatches ( NUM-512 examples ) . i do recognize though that this will increase the auroc variance , potentially too much . EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ you might be interested in the following paper : zhao , p. et al .-online auc maximization-icml NUM URL the main difficulty is that the auc is not differentiable , and thus you have to find a suitable approximation for performing gradient-based maximization . edit-at the moment i'm using the auc-pr for evaluating my models , but i'm using a margin-based ranking loss as a proxy : implementing something for directly maximizing the auc is right on my todo list . EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ i was considering doing something that would be implemented similarly to this . i'm quite interested in how your method will work . you should try writing an aucroc objective symbolic implementation . you'll need to do it symbolically ( in their abstracted theano-esque functions ) as it will take the gradient of this . you may also have to modify/work around weighted.objective as it will expect the output of loss to be per example . you could potentially just spread the auroc across each example , as i'm not sure how weighted.objective is used vs the above loss function . URL URL EOA 
 has anyone successfully implemented auroc as a loss function for theano/lasagne/keras ? : machinelearning i wrote a theano n-dimensional auroc implementation here , but think of it as more of a rough draft . there are tests though . URL EOQ i've had the same interest and built a custom loss function that attempts to include rank information alongside cross entropy . it would be fairly easy for you to try it in keras : URL the idea is that any positive outcome with a prediction less than the maximum prediction of all the negative outcomes is contributing to a loss in the auroc . you may want to try it with larger mini-batch sizes so that it more closely simulates the validation data set . EOA 
 nvidia px NUM and drivenet : machinelearning any info on how their marketing turned NUM fp32 tflops into NUM tdlops ? EOQ i was wondering about this as well . in earlier presentations on maxwell , nvidia revealed a mixed precision architecture that could do NUM-bit half precision flops at NUM x rate . that's now official in the cuda docs for NUM architectures ( presumably for jetson and low power maxwell ) . NUM x makes sense as you can fit NUM in one NUM-bit register slot . so the NUM tdlops suggests a NUM x half precision fp throughput for pascal , which is weird but not unprecedented . for example on kepler NUM-bit fp math is slightly faster than any other NUM-bit instruction . the other possibility is support for even lower bit precision ops , like a NUM x NUM bit op . that's seems unlikely . hopefully it's not something they spent too much on . even with NUM terrabyte/s mem bandwidth , the NUM-bit flop/bandwidth byte ratio is still the same as maxwell at NUM :1. a ratio of NUM :1 is pretty high-hard to utilize . EOA 
 nvidia px NUM and drivenet : machinelearning any info on how their marketing turned NUM fp32 tflops into NUM tdlops ? EOQ didn't jen-hsun mention special instructions as part of the chip as well ? i'm wondering if they modified the silicon or something to optimize for certain calculations . EOA 
 nvidia px NUM and drivenet : machinelearning any info on how their marketing turned NUM fp32 tflops into NUM tdlops ? EOQ the gpu's will use as much power as the engine . EOA 
 nvidia px NUM and drivenet : machinelearning any info on how their marketing turned NUM fp32 tflops into NUM tdlops ? EOQ a nuke bomb in every car ! EOA 
 nvidia px NUM and drivenet : machinelearning any info on how their marketing turned NUM fp32 tflops into NUM tdlops ? EOQ free heating :) your heater doesn't compute anything useful yet ? that's soo NUM th century EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ see for example here : URL i'm pretty sure at most places if you don't register they'll just drop you , whether or not it's explicitly stated . EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ in your example , they say that the fee includes NUM paper , but they don't say that it will be published in the proceedings even if you fail to present ( or am i missing something? ) EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ they don't say that the presentation itself is optional , just that the registration is mandatory . EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ no one is going to check if you actually give a presentation . or you can just email and say you got sick . EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ the one ( and only ) conference i've submitted to required so . but i suppose a sample of one is not much representative EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ same for me . however, in my case , it was just someone which was needed to make the presentation , but not necessary one of the author . EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ most conferences people from our research group submit to require you to give a presentation if your paper gets accepted . during the final panel discussion at aamas last year this came up as well and everyone was pretty clear about the need to present your stuff . i mean that's kind of the purpose of a conference . if you don't want to give a presentation submit to a journal . whether someone actually checks that you gave your presentation is a different question ( there have been prerecorded presentations if i recall correctly ) . what you can always do however is to ask someone else to give your presentation . it sucks a bit for the audience though since they can't talk to the author and ask in-depth questions afterwards . EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ i think that in deep learning , journals are not on the same level as nips , iclr and icml . besides, sometimes you just don't know ahead of time if you'll be able to attend . EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ i think conferences being more prestigious ( among computer scientists ) or having a higher impact generally holds for computer science . most journals are just too slow for the field . conference dates are announced ages before and notification about acceptance is usually still a while before the actual conference . so under normal circumstances it should be fairly easy to plan for the conference and keep the dates free . in the end you'll have to decide yourself whether the little extra prestige or impact of the conference is worth the hassle of keeping the dates clear/finding a replacement presenter . also keep in mind that the impact of the conference doesn't necessarily influence the impact of your paper . if your research is sound and good it'll get loads of citations even in less prestigious journals or conferences . EOA 
 askml: if your paper is accepted to a conference , do you have to attend it for the paper to be published ? : machinelearning i've never seen this explicitly stated in the information for authors . does this vary from one conference to another ? EOQ usually , and you have to pay for travel . it's why conferences are essentially academics only . EOA 
 creating an a.i bot from facebook conversations and progressive learning . : machinelearning python is a fine language , choosing python shouldn't be the reason you fail/succeed . the biggest issue i think you will face is that you don't have much training data , for example google's neural chatbot used two datasets consisting of NUM m tokens and NUM m tokens . EOQ [ deleted ] EOA 
 creating an a.i bot from facebook conversations and progressive learning . : machinelearning python is a fine language , choosing python shouldn't be the reason you fail/succeed . the biggest issue i think you will face is that you don't have much training data , for example google's neural chatbot used two datasets consisting of NUM m tokens and NUM m tokens . EOQ correct , m-million, the workhorse dataset for google was almost a billion words . you can read there paper i'm not sure what else you could try , you may have to do some googling to see what can be done in the python ecosystem . EOA 
 creating an a.i bot from facebook conversations and progressive learning . : machinelearning python is a fine language , choosing python shouldn't be the reason you fail/succeed . the biggest issue i think you will face is that you don't have much training data , for example google's neural chatbot used two datasets consisting of NUM m tokens and NUM m tokens . EOQ okay , thank you very much for your help :) i just want to make myself an assistant , that would be pretty cool :) EOA 
 creating an a.i bot from facebook conversations and progressive learning . : machinelearning python is a fine language , choosing python shouldn't be the reason you fail/succeed . the biggest issue i think you will face is that you don't have much training data , for example google's neural chatbot used two datasets consisting of NUM m tokens and NUM m tokens . EOQ are you ? EOA 
 creating an a.i bot from facebook conversations and progressive learning . : machinelearning python is a fine language , choosing python shouldn't be the reason you fail/succeed . the biggest issue i think you will face is that you don't have much training data , for example google's neural chatbot used two datasets consisting of NUM m tokens and NUM m tokens . EOQ nltk is a library for natural language processing(nlp) in python you also need scikit learn for ml which you could use to do a sentiment analysis for example to check if a new received message has a positive or negative mood and respond accordingly EOA 
 creating an a.i bot from facebook conversations and progressive learning . : machinelearning python is a fine language , choosing python shouldn't be the reason you fail/succeed . the biggest issue i think you will face is that you don't have much training data , for example google's neural chatbot used two datasets consisting of NUM m tokens and NUM m tokens . EOQ that's a pretty big jump back from building strong artificial intelligence system tho . EOA 
 creating an a.i bot from facebook conversations and progressive learning . : machinelearning python is a fine language , choosing python shouldn't be the reason you fail/succeed . the biggest issue i think you will face is that you don't have much training data , for example google's neural chatbot used two datasets consisting of NUM m tokens and NUM m tokens . EOQ check out grokitbot . the main framework is there , just feed it data . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ hum , yes, i omitted it because then it would be too long . tagging data consists of not boolean but integers-number of tags the song gets . i convert it to topics to reduce dimension and remove synonyms . the topic data is then float in [0,1]. i'm also thinking of converting it as a topic classification problem by quantisation . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ is there any particular reason why you want to predict the counts ? i think that's an extremely noisy signal and a difficult task . if you do want to do this , it might be easier to predict the logarithm of the counts instead . ( or log(1-x ) to avoid issues with NUM ) . or i guess the 'correct' way to do this is to optimize the log likelihood of a binomial or poisson distribution instead . it's probably easier to just say tagged or not tagged and then predict NUM s and NUM s , which means you can use binary cross-entropy for each individual tag . that's how i've always treated this problem . you could even consider crafting some way to turn the counts into probabilities , and then predict those ( again using binary cross-entropy as the loss ) . i don't think it necessarily makes sense to use NUM x3 filters on time-frequency representations , by the way . the time and frequency axes are completely different so there is no good reason to use square filters . in my experience it's beneficial to have only limited frequency invariance ( so make the filters span almost the entire frequency axis ) , or even none at all ( then you get NUM d convolutions which can be a lot faster ) . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ i think he is doing a topic model over the tag counts and not regressing the counts themselves ( judging by the fact that he says the topic scores are bounded [0,1 ] i would guess lda/plsa) so its seems like he's regressing p(topic) EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ /u/lepotan exactly guessed what i'm doing . yes, i used lda to make song-topic matrix , which is what i'd like to learn . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ hi , good to hear from you ! as /u/lepotan said i'm putting topic vector , not the tag counts . i can think of some heuristic way of quantise it as a binomial variable-thresholding with some probability like NUM or something , but the nature of topic-that a song may be equally tagged with two different tags as they are not always exclusive-makes me hesitate to do so . and that's true , well, last time when i used NUM x3 filters on spectrogram i could find it captures not only harmonic structures but also some rhythmic structures , which might help differentiating strong-beat music with the others or classical and the others . however i'm not sure it's really worth to do considering the computation . probably there could be some other way that can take both time-axis and frequency-axis structures into account by combining NUM d convolutions in different axis per layer . this structure is more like a initial one to check it would really make sense to attack this task as i'm questioning on defining the problem , too, as you also noted , and that's the reason i'm with one that i'm familiar with . hopefully i would settle down something better ! EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ that doesn't matter for regression but there is popularity bias , so in a topic , let's say 'sad' , larger number in a song than the other song could mean it is more sad , or it's just more popular . if this is the only problem than a normalisation by norm(topics) would resolve it easily but not sure if wouldn't cause another problem-that a song can have many topics equally , and that's the reason i'm normalising it by max(topics) for each song . and therefore i don't think it is good idea to allocate only one topics for each topic . instead i can make it as a multi-class classification problem-i think what you mentioned includes this idea-, probably this is one think i should try if regression keep failing . mse is kind of working now ( after removing all dropouts ) but the performance is quite poor . probably because of the distribution . i think small filters on frequency axis would give frequency invariance of the feature , and also the features can capture some harmonic structures , which i'm not sure it is essential for this work though , yet. if there's some features that only can be captured by convolution in frequency axis , should it be okay to use it ? because anyway all the features are supposed to be activated in certain local area . by the way , are you still working on music-thingy ? i have a work extended from my deconvolution and auralisation of cnns , would be fun to talk about the intermediate results , of course it's asking you a favour :) EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ in general , binary crossentropy seems to be a much stronger signal than regression via mse-and though it is normally for classification , as long as you normalize each output target between NUM-1 you can use it for regression as well . i have even used it for framewise regression of spectrograms with little issue . the trick ( from g . hinton i think ? though he probably wasn't the first ) is to treat something like pixel values ( NUM-255 , NUM-255, NUM-255 ) for rgb data as probabilities then plot the mean prediction e.g. first take each pixel and divide by NUM so now every pixel is in ( NUM-1 , NUM-1, NUM-1 ) . then use a sigmoid output layer ( or a bernoulli rbm ) to predict the value of every pixel and its rgb value ( so the output dimension is NUM x number of pixels ) , followed by a binary crossentropy cost . at prediction/generation time , just use the value from the sigmoid directly ( call it p ) , rather than sampling from a bernoulli with probability p as you normally do-this technically ( iirc ) represents the expectation of the pixel value over infinite samples so it is a valid thing to do , though it feels kinda hacky the results look like they should . it seems super likely you could do the same approach here , and have much better luck . in general for mse prediction to even kinda work ( things like vae with real valued reconstruction error , for example ) i have always had to transform the input with pca first and work in pca space-that means no convolution ! EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ NUM d convolution along time-axis seems still available if pca is done for each frame , doesn't it ? i had a colleague who was doing that for a classification task . what's more curious for me is the reason you had to do so . how is use of pca related to mse prediction ? i think i'm not getting what you're suggesting exactly , so let me summarise it . in the example , pixel values are what we would like to predict . train it with a network which has a sigmoid layer at the end with binary crossentropy as a loss function . predict a new value , p ( prob for a bernoulli distribution for each dimension ) , and use it . please let me know if it's correct . if it's correct , then it looks like all i had to do is just replace rmse with binary cross-entropy . and thanks for the comments :-) EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ yes-you got it ! as for the pca thing , i have talked to a few people and it seems related to gaussian the input-but pca still worked better than mean/std per feature normalization , or any of the other typical input normalization tricks . it is something that just helped a lot , and i haven't really seen it anywhere besides kingma et . al. 's paper-and even then it is only in the code , and not mentioned anywhere in the experiments iirc . for a NUM d convolution you should be ok if operating on per frame , though it may make more sense to use dct , or cepstrum representations due to existing literature . they should all kinda do the same thing i think-energy compaction that is effectively reweighting the mse . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ binary crossentropy rocks ! with few other modifications-leakyrelu/parametricrelu , now finally i'm seeing the gradient is flowing back , though it needs to be better . thank you so much . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ there was a good set of slides by harold steck at recsys this year on dealing with ranking metrics using neural networks-i have been looking for a place to plug it in my code but maybe it is useful here ? the paper is also available here and a summary by an attendee here . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ there are still many zeros in the topic vectors , but yes , it can be useful here , i will take a look for details . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ from my experience , dropout only hurts in regression tasks . i have no idea why EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ thanks ! did it consist of fully-connected layers only ? EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ did what ? EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ i'm wondering if your networks were based on convnet or dnn ( w/ fully-connected layers ) . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ just feed-forward , fully-connected EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ okay , thanks! EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ in my experience , maxout was helpful to deal with real-value regression . ( dqn, drqn problems ) i had replaced relus with maxout(k-5) . EOA 
 regression task with convolution neural networks : machinelearning why are music tags an mse cost ? instead of bernoulli crossentropy using sigmoid output ? maybe i am missing the point of your tags-can you give more details ? EOQ will look into that as well . thanks! EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ it's being trialled in pharma , to reduce the costs of screening . i'm interested to see any other developments . EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ can you elaborate on how it is being trialled in pharma ? EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ to label a molecule as biologically active or inactive , it has to be synthesized and tested , which is quite expensive and might take days . active learning is used to select the most promising compounds or to select the compounds feature space locations where your model is not confident . EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ fascinating . is this technology publically available , or is it an in house research tool being developed ? if the latter do you know how widespread the approach is ? EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ check this review : URL EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ remember reddit's recommended tab that they bragged about so much around NUM to NUM ? too bad it turned out to be not much more than a publicity stunt . despite active learning stunts like that , seems active learning is still heavily used and continues to grow . i put an active learning annotation system into nearly every supervised ml system i build . much of the ml industry still consists of independent consultants though . i could see why they would have incentive to force their clients to return to them for training data upgrades , instead of delivering an active learning system for the clients to use themselves . for the future of e.g. reading comprehension tasks , more sophisticated methods active learning seem immensely important . for something so difficult , it may be best to just copy how humans learn to do it . as far as i know , active learning has proved immensely important in teaching babies their first language ... EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ do you have any examples of the applications you've worked on ? papers or public code ? EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ hi , active learning is still used heavily in a lot of places . in fact , my company just had a paper published on a new active sample selection algorithm for text classification . i work in the legal industry where it is used very , very heavily to reduce labeling costs . typically a senior attorney does the training , and good active learning can decrease the amount of training by thousands of documents , which translates to thousands of dollars in attorney fees . EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ link to paper ? EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ pm sent . EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ why keep it private ? EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ because it contains personally identifiable information that i would rather not be linked to my reddit account due to other subreddits i'm involved in . EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ may i also have the paper ? EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ me too ! EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ cool , can i get a pm ? EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ i'm also interested . EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ me too , please. EOA 
 what happened to active learning ? : machinelearning i can't remember exactly what they talked about , but the talking machines did a podcast about active learning which gave a lay of the land i think URL EOQ thanks for the great info . if you don't mind i would also love to see that paper . thanks so much . EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ these links are great , introductory material . op, please consider . EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ roc doesn't exactly validate classifiers itself . it's more of a tool to gauge how the precision and recall vary as you adjust the decision threshold . it helps you make the choice about where to out the threshold based on what sort of performance you're after . auc is just the integral of the roc curve . it's a simple way of condensing the entire curve down to a single number to make it easy to compare classifiers . auc may not be ideal if you only care about performance at a particular point or section on the curve . EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ here's my attempt at doing this , as if you were actually a NUM year old : let's say a bunch of your friends from kindergarten are playing a game of sharks and pirates , outside. in case you forgot how the game is played , some kids are sharks ( their job is to eat the pirates ) and some kids are pirates (their job is to jump over and avoid the sharks-at all costs!). let's pretend you hurt your foot , so you're not allowed to play , you can only watch . worse yet , you weren't watching when mrs . teacherlady decided who's a shark and who's a pirate , so you need to guess . since a player can only be a shark or a pirate , you decide to play a game with yourself : guess who's a pirate . somehow or another , you are able to assign each player a number ( maybe the number is based on whether it seems like the player is running away from other players ) . you decide to make the game a bit more interesting for yourself : you want to make a simple rule that takes the number associated to a player and makes the guess pirate or not . the rule you chose is : if playernumber > ; NUM { guess pirate } else { guess shark } you assemble your list of pirates and sharks , and give it to mrs . teacherlady to see if you're right . she tells you , not quite , /u/gay.hat.on.nun: you have a true positive rate ( tpr ) of NUM /14 and a false positive rate ( fpr ) of NUM /14 . you're confused by the jargon , but she quickly explains : a true positive rate ( or true pirate rate , to be cute ) is ratio ( you're an advanced NUM year old ) of people that you guessed as pirates that are actually pirates , to the total number of pirates . similarly, the false pirate rate , is the ratio of guessed sharks to the total number of sharks ... mrs. teacherlady isn't trying to be surreptitious , she split the class of NUM students NUM-50 into sharks and pirates . this gets you thinking : what if you had chose NUM in your rule , instead of NUM ? you do so , and hand your new list back to mrs . teacherlady. she gives now tells you : all your sharks are correct but you've guessed that over half the class are pirates ! your fpr is NUM /14 , while your tpr is NUM . you reflect on this , and realise that something feels a bit fishy here . to get a handle on the situation you shift your rule to NUM , and the results are not good : honey ... all your pirates are right , but now you've guessed over half the class as sharks !? your tpr is NUM /14 , while your fpr is now NUM . as you can see , your magic number seems to have some real effect on who gets classified as a pirate or not ; moreover, you can see that the magic number is tied to two other numbers , ( fpr, tpr ) . you know that there's a high chance you'll get made fun of ( or beaten up ) if you start spitting a table of numbers at your fellow kindergarteners , when all you really want to do is tell them about how you were able to assign them numbers , while they were playing , and from those numbers you could guess whether they were a shark or pirate . so what do you do ? you goto mrs . teacherlady, who's just come back from her cigarette break looking pretty disheveled ( you don't really notice , because she's usually pretty unkempt and overworked ) . [ note : this is where we have to get a bit technical ] she explains to you that if you kept coming back to her with different lists ( each for a new rule ) , you'd have a whole curve of (fpr , tpr)'s and this curve has to sit inside a square ( whose sides are of length NUM ) . so, one way to reduce this list of (fpr , tpr)'s to a single number , would be to figure out how much space lies below our ( fpr , tpr ) curve . if the curve cut our square in half ( and thus had area of NUM /2 ) , we'd say that whatever your did to assign numbers to your classmates looks pretty random . maybe you gave jimmy a value of '7' because that's his favourite number , and you gave jane a value of '2' because you don't like her ( despite both of them being pirates ) ? if your curve takes up almost all of the square , it would seem as if the method you chose to give people numbers does a really good job separating the pirates from the sharks . conclusion : the roc curve is how we visually represent the effect of thresholding a continuous value , so as to yield binary predictions . however, the curve , by itself , isn't typically used to compare classification procedures , since this would require comparing curves ( something that gets tricky once you have curves that intersect each other at multiple points ) . instead, we compare the auc of procedures . while this may seem a bit coarse-there's a great deal of info in the roc curve-it leads to good decisions most of the time , and in the standard sanity checks . i.e., when curve1 is above curve2 for all values of fpr , then auc(curve1) > ; auc(curve2) , which means an auc-based selection method would select the procedure associated to curve1 which is what any sane human would do , by looking at the curves . EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ this is brilliant . thank you so much ! EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ you're plotting something different than usual if your curves aren't monotonic . EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ i was plotting over the thresholding function . it was a median filter-constant where the constant was what was being incremented . i did multiple plots various median filter lengths , scaling factor and constant . EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ i think your fp rate is what's different . usually that axis is precision on the whole dataset , while yours seems to be (true pos)/(predicted pos) . EOA 
 eli5 receiver operating characteristic ? : machinelearning URL with URL EOQ explain like i'm brainless ( elib ) EOA 
 feedback for sklearn neural network ? : machinelearning or this this version EOQ sklearn doesn't have gpu support , also in faq they mention how sklearn is not suitable for large neural networks . EOA 
 feedback for sklearn neural network ? : machinelearning or this this version EOQ sklearn's neural network implementation is very new : only added recently and not even in the stable release yet . it isn't as fast or full-featured as lasagne/theano or keras/theano so it really only fills the need for basic networks . EOA 
 simultaneously fitting two functions : machinelearning your approach looks what i would do . any optimization can only seek to optimize on one goal or objective function . you can include multiple goals in two ways : by combining them in some way , as you did , with weights representing different importance ; or by serial optimization , like optimize for first goal , and then optimize for second without deteriorating first goal too much so that it becomes constraint in next round . EOQ why do you want to fit the two functions simultaneously ? EOA 
 simultaneously fitting two functions : machinelearning your approach looks what i would do . any optimization can only seek to optimize on one goal or objective function . you can include multiple goals in two ways : by combining them in some way , as you did , with weights representing different importance ; or by serial optimization , like optimize for first goal , and then optimize for second without deteriorating first goal too much so that it becomes constraint in next round . EOQ there are sometimes good reasons to fit two functions in concert . the most important is probably feature importances / feature learning . if you believe the problems to be related , it can be good to regularize the problem jointly . a good way to do this is to learn a low rank embedding simultaneously from both problems . you can get a similar effect by simply using l2 regularization on all coefficients . if you don't include regularization , the answer will be identical to what you'd get by solving the problems independently . another good way to create joint regularization is to use partial least squares . in fact , i'd likely just go straight for partial least squares . EOA 
 simultaneously fitting two functions : machinelearning your approach looks what i would do . any optimization can only seek to optimize on one goal or objective function . you can include multiple goals in two ways : by combining them in some way , as you did , with weights representing different importance ; or by serial optimization , like optimize for first goal , and then optimize for second without deteriorating first goal too much so that it becomes constraint in next round . EOQ you definitely need to scale both of the target variables . if one is drastically larger than the other , it's easily possible that it will dominate the optimization and the algorithm will learn nothing for the smaller of the two . EOA 
 to what extent are kaggle competitions determined by variance ? : machinelearning variance does play a significant role , but there's an interesting phenomenon in the score distribution for most competitions . there's a plateau as you near the top of the lb , but the top NUM to NUM or so entries tend to break through the plateau in a way that really sets them apart and makes it obvious that they're operating at a higher level than the crowd forming the majority of the plateau i.e. they appear to outliers . EOQ for some competitions the plateau of what is possible for a certain data set is reached way before the competition ends . this happens increasingly so , because competitor skill is going up , tools/hardware and modeling approaches become more powerful , and a larger number of contestants enter . to combat this , kaggle hosts more competitions and is very careful with longer competitions ( netflix-style multiple years would be out of the question these days ) . anthony goldbloom admits [ NUM ] that sometimes the difference between NUM-5 is statistically insignificant , meaning it takes an amount of luck to win . much less variance than with poker , but still a necessary amount . i guess this is why they pay out prize money sweepstakes-style . [ NUM ] URL EOA 
 to what extent are kaggle competitions determined by variance ? : machinelearning variance does play a significant role , but there's an interesting phenomenon in the score distribution for most competitions . there's a plateau as you near the top of the lb , but the top NUM to NUM or so entries tend to break through the plateau in a way that really sets them apart and makes it obvious that they're operating at a higher level than the crowd forming the majority of the plateau i.e. they appear to outliers . EOQ believe it or not there are people that believe the entire leaderboard is simply variance . i've run into two of them on linked in . EOA 
 to what extent are kaggle competitions determined by variance ? : machinelearning variance does play a significant role , but there's an interesting phenomenon in the score distribution for most competitions . there's a plateau as you near the top of the lb , but the top NUM to NUM or so entries tend to break through the plateau in a way that really sets them apart and makes it obvious that they're operating at a higher level than the crowd forming the majority of the plateau i.e. they appear to outliers . EOQ the obvious way to fix this is to use a bigger test set . EOA 
 to what extent are kaggle competitions determined by variance ? : machinelearning variance does play a significant role , but there's an interesting phenomenon in the score distribution for most competitions . there's a plateau as you near the top of the lb , but the top NUM to NUM or so entries tend to break through the plateau in a way that really sets them apart and makes it obvious that they're operating at a higher level than the crowd forming the majority of the plateau i.e. they appear to outliers . EOQ it's ironic that supposed experts on this topic nonetheless take it for granted when scoring their own competitions . EOA 
 to what extent are kaggle competitions determined by variance ? : machinelearning variance does play a significant role , but there's an interesting phenomenon in the score distribution for most competitions . there's a plateau as you near the top of the lb , but the top NUM to NUM or so entries tend to break through the plateau in a way that really sets them apart and makes it obvious that they're operating at a higher level than the crowd forming the majority of the plateau i.e. they appear to outliers . EOQ the data is available , run the analysis yourself . EOA 
 anyone interested in fantasy football ( soccer ) ? qa tool : machinelearning hi , thanks for the kind words about our website. sure thing , what would you like to know more about ? EOQ the interface , and the tools you offer are great , but as a machine learning guy , i guess i'm most curious about which algorithms you are using , especially in terms of points predictions . i'm assuming that between optasports and the site itself , you have access to plenty of data , but if are willing to share any more of your inside secrets i'd be very interested in hearing the process you use . EOA 
 does anybody know of a human chat dataset ? : machinelearning there are plenty of irc logs . though mining these without anonimization can be a bit controversial . for instance the #ubuntu channel on freenode goes back years and has millions of messages . EOQ the problem with most of those is that they're very tech/web centric , so the language model built from those probably won't generalize well to other domains . EOA 
 does anybody know of a human chat dataset ? : machinelearning there are plenty of irc logs . though mining these without anonimization can be a bit controversial . for instance the #ubuntu channel on freenode goes back years and has millions of messages . EOQ some people use opensubtitles database in papers EOA 
 does anybody know of a human chat dataset ? : machinelearning there are plenty of irc logs . though mining these without anonimization can be a bit controversial . for instance the #ubuntu channel on freenode goes back years and has millions of messages . EOQ nltk has a few . EOA 
 does anybody know of a human chat dataset ? : machinelearning there are plenty of irc logs . though mining these without anonimization can be a bit controversial . for instance the #ubuntu channel on freenode goes back years and has millions of messages . EOQ URL EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ i'm applying this network to varied data sets so we'll see if it's over trained ... at the moment i'm keeping the hidden layer of my nn small ( at NUM ) . i'm not seeing any improvement past that size . i'm not familiar with binary logistic loss functions ... EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ see here under first half : URL if you are training a net with a single hidden layer that has only NUM units , then it sounds like unless something is going massively wrong , that your data is just not very complicated . i'd try some other models , like basic categorical regression , boosting, etc ., and see what they're able to do . EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ yeah . it's multiple channels of emg and some other sensor data . the net is classifying the emg data ( with the other data ) EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ use a well known dataset , such as mnist , cifar, and compare your accuracy with other peoples reported accuracy . EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ this is novel research . there is no well known data set . i'm classifying data gathered during experiments . EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ how do i tell what the overall performance is ( ie , how good the network is ) ? this is a regression problem . start with networks that you know are good or bad ( perhaps using a score from NUM through NUM ) and then train another neural network to predict that score as a function of the neural network's parameter vector . is comparing r values ( from the regression between outputs and targets ) the method to do this analysis ? why use a fixed evaluation metric when you could learn a metric ? EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ you dropped this : /s EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ he has been working hard on his trolling recently so this time it can be excused . EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ here is the official guidebook for the toolbox . you might have already gone through it , but if not , it will hopefully prove useful : URL they actually explain in more detail what each function does and they provide some intuition as well . EOA 
 advice about analyzing neural network performance : machinelearning karpathy has excellent and totally practical advice for monitoring the performance of an nn in his class notes here : URL but in terms of evaluating a net , everyone does it by looking at charts comparing training to validation loss . it sounds like your model is working really great . except , neural nets are notorious for overfitting . if you never see overfitting with more training , that would make me suspicious that something's gone wrong . if you're looking for a metric for measuring the loss-well , that's what loss functions are for . if your classifications are binary , you're probably best off using a binary logistic loss function and supplementing that with measures of recall , precision, f1 , etc. etc . EOQ thanks . i've gone through the online documentation but i'll read this in more depth . EOA 
 suggestions for online training image recognition ? : machinelearning this is what lecun uses : URL literally training on the fly . EOQ do you know where the demo is hosted ? the video mentions the code is available but doesn't mention where . also , from the video it looks like there are a fixed number of labels that he is training . do you know if the total number of labels was set in advance ? thanks ! EOA 
 suggestions for online training image recognition ? : machinelearning this is what lecun uses : URL literally training on the fly . EOQ he mention that model uses parzen window classifier on top of imagenet pretrained convnet . there is no such thing as fixed number of classes , but as you add more performance might suffer i guess . as for code etc-try to google it . EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ how do you think transfer learning fits in or relates to one shot learning that has come up recently ? EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ that's very interesting too :) i haven't read the gory details of one-shot learning , but it seems to model an approach where the agent focuses attention on a special experience in particular . forgive that my understanding could be superficial , but maybe this means that a child's brain decides to learn a lot about the few cats it sees because of the nature of the stimuli ( moving , organic creature with a face , making novel sounds , or whatever ) . it picks out very detailed features of these stimuli , helping it recognise the next cat better than other things less worthy of focus . transfer learning would relate to this if by learning to recognise a cat , the child then senses a familiar experience when it reads puss n boots . in that case , it has applied the learning of a visual recognition task to a fairly distant activity with different goals . i think our ability to do this is related to our ability to understand metaphors-a cat is like a small dog ( a simple metaphor ) , or finding new love is like a bright , sunny day in spring (a pulp , but more complex , metaphor). the most important common features of two unrelated experiences are brought to the fore , helping to make broader generalisations . all great things to think about . EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ well the brain generalizes concepts all the time ; we would be unable to go through our days without it . if we had to relearn every time how a chair works , how much strength to apply to a glass to hold it , etc. we would never get anywhere . the brain does this on just about any concepts . my impression is that you're describing the generalization process as metaphors . my personal intuition is that it isn't necessarily that humans learn/generalize much faster than computers do ( i.e. computers need thousands of cat pictures to understand cats ) , rather it's that we humans have the advantage of pre-conceptions that computers don't . we already learned what a wall looks like , if you show me a picture of a cat , i have a vast array of knowledge of all the backgrounds possible that are already established leading me to the possibly of easily distilling what parts of a picture makes a cat . this greatly reduces the problem space of associating/learning the cat from the pictures . moreover, we have contextual information before we get to meet the cat . we have well established that the background is background before we got to see the cat that moved there . and then , maybe ( i would assume ) , learning about the cat is much more efficient for a human because once we excluded the irrelevant data out of the learning process , we can focus(/greatly simply the search space) on just the tiny differences of how the cat moves over time . i see it a bit like , for instance word embeddings ( word2vec ) . it essentially shows us that a lot of word meanings can be deduced based on the context ( sentences/words around ) in which they are used . maybe my analogy/intuition makes some kind of sense . EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ i haven't heard of any application of word2vec on metaphors yet . i suspect that's because of the nature of metaphors-the vectors can give you information about coocurrence and maybe semantic distance between words , they don't encode other information essential for metaphor processing like synonymy or hypernymy very well . also , have you looked at the literature on metaphors in general ? burke's four master tropes comes to mind . going back to word2vec , a better application for that would probably be to look at metonymy classification . however that would require you to already have a corpus of examples prepared . it may be simpler to start looking at schemes . as figures of syntax ( vs . tropes, which can be thought of as figures of semantics ) i think that you'd be able to leverage the vectors better . there's a few groups that i know of that are doing active work on rhetorical figures in a machine learning context . if you're interested in following up on the subject i can pm you the contact info of some of them . EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ thanks for the links . i know that word2vec only encodes the semantic space of all instances of the word within a corpus , making it hard to disambiguate metaphoric from non-metaphoric uses . i have a vague idea about comparing this baseline to specific instances in situ might provide some insight in spotting metaphor . obviously still a half baked idea , but something i was wondering if others had worked on previously . EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ there is a workshop on metaphor . you can find the proceedings here . jonathon gordon at usc has had some good stuff lately . he works with hobbs and they've gotten some weighted abduction work that's fantastic ( and open source ) . EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ excellent resource , thank you . EOA 
 metaphor with word2vec : machinelearning this is very interesting . in my opinion , decoding metaphors is less to do with the various word2vec models and more to do with transfer learning . the essence of metaphors is that you use an experience from one context to describe an idea from another , seemingly unrelated one . i would say that one greatest weakness of the current machine learning paradigms is their inability to carry out transfer learning . this is why the big neural nets need to look at thousands of cats instead of one or two . in my opinion , when a child sees a cat and recognises it , it effectively creates a metaphor for the previous cats it saw . machine learning algorithms just see a completely new experience with a small handful of similar features . transfer learning is so exciting because it is as of yet still very poorly understood from a machine learning point of view . in my view , when machine learning algorithms can recognise a cat with previous experience of just NUM-4 data points , they will be on track to understand and create knowledge metaphors . EOQ see this side-project : URL rather than looking at some stablished metaphors ( and analogies ) , he also looked at new ones . e.g. stock market ? thermometer . EOA 
 is admissions into ms statistics programs getting increasingly difficult due to the popularization of data science ? : machinelearning there's still less people applying than you'd expect . admission to statistics departments is still much less competitive than math or computer science . EOQ yes-it might be getting harder , but on an absolute level getting into a stats masters program is pretty easy as long as you aren't totally unqualified . p.s. sick posting history op EOA 
 is admissions into ms statistics programs getting increasingly difficult due to the popularization of data science ? : machinelearning there's still less people applying than you'd expect . admission to statistics departments is still much less competitive than math or computer science . EOQ thanks ! EOA 
 is admissions into ms statistics programs getting increasingly difficult due to the popularization of data science ? : machinelearning there's still less people applying than you'd expect . admission to statistics departments is still much less competitive than math or computer science . EOQ yes EOA 
 is admissions into ms statistics programs getting increasingly difficult due to the popularization of data science ? : machinelearning there's still less people applying than you'd expect . admission to statistics departments is still much less competitive than math or computer science . EOQ great ......... EOA 
 is admissions into ms statistics programs getting increasingly difficult due to the popularization of data science ? : machinelearning there's still less people applying than you'd expect . admission to statistics departments is still much less competitive than math or computer science . EOQ how come ? EOA 
 is admissions into ms statistics programs getting increasingly difficult due to the popularization of data science ? : machinelearning there's still less people applying than you'd expect . admission to statistics departments is still much less competitive than math or computer science . EOQ i think it's not as difficult as getting into ms data science kind of programs . EOA 
 using pre-trained models : machinelearning it depends on what you want to do . caffe is famous for their model zoo[0] which has lots of pre-trained architectures . many other frameworks have something like this to a lesser degree . for example , tensorflow has a version of google's inception-v3 : URL ( search for wget ) . [ NUM ] URL edit : grammar EOQ thank you , i'm still really new to the topic , for now i mainly use theano , not sure if there is something for that EOA 
 using pre-trained models : machinelearning it depends on what you want to do . caffe is famous for their model zoo[0] which has lots of pre-trained architectures . many other frameworks have something like this to a lesser degree . for example , tensorflow has a version of google's inception-v3 : URL ( search for wget ) . [ NUM ] URL edit : grammar EOQ lasagne has a modest collection of pre-trained models which can be used in combination with custom theano code : URL a nice example of this type of usage is ryan kiros's neural-storyteller , where a pre-trained lasagne model is used only for extracting image features ( everything else is custom theano code ) : URL EOA 
 using pre-trained models : machinelearning it depends on what you want to do . caffe is famous for their model zoo[0] which has lots of pre-trained architectures . many other frameworks have something like this to a lesser degree . for example , tensorflow has a version of google's inception-v3 : URL ( search for wget ) . [ NUM ] URL edit : grammar EOQ thank you ! i will give it a look ! i'm still unsure on which framework i should learn , there are a lot out there and each one has it's interesting features . ( i saw the link on github comparing some of the major ones but it feels a bit inconclusive ) EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ what are your expectations after completing the nanodegree ? better/more interesting job and/or higher salary ? what salaries can one expect after becoming an expert in machine learning ? if it's not any different than the $100k a year i make , then i have trouble justifying spending free time on it ? although i understand the job market is projected to be very strong over the next few years ( although it seems having a phd may/is required??? ) EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ i am actually making a career change . i currently work in the semiconductor industry doing microprocessor design , and i want to move into an industry with higher growth potential ( semiconductor industry is very mature , and has seen massive consolidation in NUM ) . the ideal career goal for me is to start my own company in ml/ai . i am giving the ml nanodegree a shot , to build a solid foundation in ml for me to develop further expertise in the field . i also strongly considered applying to ga tech's omscs program , but in the end i decided to pursue udacity's program because i wanted to develop the expertise with an industry-focus , rather than academic focus . from my experience in university , i found traditional academia-based education is extremely thorough theoretically ( which can be a good thing , depending on what you want ) , but i wanted an education that can get me up-and-running as soon as possible . EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ would you mind elaborating a little more on the programming language requirements for the omscs courses , please? i'm fluent in r and python , and plan on picking up java soon . but being fluent in c ? i see that as a whole different ball game ! ( am also considering the ml specialization there. ) EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ gt says the following : in general , we expect students who enter the program to be very comfortable working with multiple programming languages such as c , java, and python ( there is no provision within the program for make up any deficiencies ) and to have taken several more advanced topics , such as advanced os , networking, theory , and/or algorithms . EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ thanks ! any idea where i can see the syllabi of the particular courses ? the website seems to only present the whole curriculum . EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ all the courses available via the omscs have course descriptions and prerequisites here : URL EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ i'm doing it , i've spent about NUM hours on it and i'm about halfway through the content , but i already have some experience working in the field . EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ NUM hours ? the program is supposed to take about NUM hours . did you find the nanodegree to be shallow or is it actually any good ? and are you doing it for free , or paying for it . EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ i put the lectures on NUM x , and the projects aren't particularly hard , just long and kind of tedious . if you have no prior experience i would recomend doing andrew ng's machine learning course and auditing the udacity courses . its basically an aggregate of a bunch of different machine learning courses offered on udacity , and projects that are custom made for the course . EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ hmmm ??do you think that doing andrew ng's machine learning course & the udacity courses would make one employable ? i'm curious to know if you think the material is in-depth enough to actually learn practical skills rather than just abstract ideas that are on good on paper . also , even if you put the lectures on double speed , that's still NUM hours ? they say it's supposed to say NUM ?500 hours ??are they just grossly exaggerating ? EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ i think doing those courses then competing in a couple kaggle competitions while building a portfolio of projects would make one employable . with your math background you could probably do URL which is the full stanford andrew ng course , which is much more advanced than the coursera course . EOA 
 anyone do or doing udacity's machine learning nanodegree ? : machinelearning if you're about to start ga tech's omscs , maybe you can just specialize in machine learning ? wouldn't udacity's ml nanodegree be redundant ? i am currently enrolled in udacity's ml nanodegree . i completed andrew ng's machine learning course on coursera prior to starting the ml nanodegree , so some of the material was already familiar to me . i started the ml nanodegree in early november , and i completed project NUM a couple weeks ago , so i guess i am ahead of schedule . i don't know how many hours i have put into the nanodegree , but i do have a full-time job and i am concurrently doing udacity's front-end nanodegree . honestly i thought project NUM and NUM were pretty easy . the tough part was writing the report to analyze your results , and justify the approaches you have taken . from what i see project NUM should be very exciting and challenging . also, udacity plans to add a deep learning w/ tensorflow course , so i am very excited about that as well as the project to go along with it . EOQ they schedule it such that you should finish the NUM rd project by june NUM th , but i went through all of the content for the first two projects and i'm done with the second project , and according to my pomodoros i've spent no more than NUM-30 hours . i think its a gross exaggeration , with enough focus you could finish this program in a month or two and safe yourself some money . EOA 
 is there active research going on in theoretical machine learning ? : machinelearning conference on learning theory EOQ kleinberg recently produced an impossibility theorem for clustering that's been influential ( less than a year old but close to NUM citations ) . EOA 
 image detection : from pictures to cartoons : machinelearning URL URL and to promote my own work , yolo has shown good results generalizing from images to artwork : URL EOQ yolo looks great , will give it a go ! i would like to train it on my own dataset ( aerial images ) that is very different from the usual trained models . do you recommend training from scratch or use transfer learning ? what size data set would be required for accurate results ? would NUM-1000 images be enough ? EOA 
 image detection : from pictures to cartoons : machinelearning URL URL and to promote my own work , yolo has shown good results generalizing from images to artwork : URL EOQ thanks for the links ! EOA 
 image detection : from pictures to cartoons : machinelearning URL URL and to promote my own work , yolo has shown good results generalizing from images to artwork : URL EOQ i tried this a few months back with caffe default model and clarifai online demo-doesn't work , at all . would have been surprising if it does , since cartoon cats have ( often ) very little in common on a pixel level with real images of cats . if you trained a model on a bunch of cartoons , then i think the results would be better . though there is much more variety in drawing styles than in real images EOA 
 image detection : from pictures to cartoons : machinelearning URL URL and to promote my own work , yolo has shown good results generalizing from images to artwork : URL EOQ wrote about related things here : URL EOA 
 image detection : from pictures to cartoons : machinelearning URL URL and to promote my own work , yolo has shown good results generalizing from images to artwork : URL EOQ that's super interesting , but it was trained on drawings right ? is the training set available for use ? it would be interesting to run it through e.g. google's image recognition and see how well it performs on that . also vice versa , testing the system trained in that article on real-world examples . EOA 
 image detection : from pictures to cartoons : machinelearning URL URL and to promote my own work , yolo has shown good results generalizing from images to artwork : URL EOQ data set is public & linked in my post ;-) EOA 
 image detection : from pictures to cartoons : machinelearning URL URL and to promote my own work , yolo has shown good results generalizing from images to artwork : URL EOQ i totally didn't see that was a link , my bad . thanks :) EOA 
 is there a publicly available corpus of children's books for nlp research ? ( this would provide a simplified nlp r & d arena ) : machinelearning i recently seen a paper from facebook ai research team where they introduced children's book dataset taken from project gutenberg. you can download it from here ( the children's book test section ) fb.ai/babi EOQ do you have a link or name for that paper ? i'm interested to see what they did with those kid's books . EOA 
 is there a publicly available corpus of children's books for nlp research ? ( this would provide a simplified nlp r & d arena ) : machinelearning i recently seen a paper from facebook ai research team where they introduced children's book dataset taken from project gutenberg. you can download it from here ( the children's book test section ) fb.ai/babi EOQ they just experimented with question/answering models for children's books . link : URL EOA 
 is there a publicly available corpus of children's books for nlp research ? ( this would provide a simplified nlp r & d arena ) : machinelearning i recently seen a paper from facebook ai research team where they introduced children's book dataset taken from project gutenberg. you can download it from here ( the children's book test section ) fb.ai/babi EOQ how would children's books improve over using , say, ? EOA 
 is there a publicly available corpus of children's books for nlp research ? ( this would provide a simplified nlp r & d arena ) : machinelearning i recently seen a paper from facebook ai research team where they introduced children's book dataset taken from project gutenberg. you can download it from here ( the children's book test section ) fb.ai/babi EOQ in wikipedia there is no conversations , for example . depending on what you want this could be important or not . EOA 
 is there a publicly available corpus of children's books for nlp research ? ( this would provide a simplified nlp r & d arena ) : machinelearning i recently seen a paper from facebook ai research team where they introduced children's book dataset taken from project gutenberg. you can download it from here ( the children's book test section ) fb.ai/babi EOQ it's not about improvement per say , but rather about having a simple/tractable data set on which to quickly iterate on , gain insight , etc. which you can then follow up with wikipedia . edit : ahh, you said simple wikipedia ; that actually looks pretty good ! is there a convenient downloadable form ? EOA 
 is there a publicly available corpus of children's books for nlp research ? ( this would provide a simplified nlp r & d arena ) : machinelearning i recently seen a paper from facebook ai research team where they introduced children's book dataset taken from project gutenberg. you can download it from here ( the children's book test section ) fb.ai/babi EOQ usually . EOA 
 is there a publicly available corpus of children's books for nlp research ? ( this would provide a simplified nlp r & d arena ) : machinelearning i recently seen a paper from facebook ai research team where they introduced children's book dataset taken from project gutenberg. you can download it from here ( the children's book test section ) fb.ai/babi EOQ one option is to go through project gutenberg to find children's books , then stripping out the project gutenberg headers ( which is sadly nontrivial ) . they have a lot of public domain works already transcribed in .txt form . EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ nips and icml are probably equally prestigious from an academic standpoint , but nips's historical roots in connectionism ( e.g., neural is part of the name ) have meant that it's gotten much more attention as part of the deep learning boom , especially from industry participants . EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ a belated review post : URL EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ thanks for this , very useful for those of us who did not attend to get a feel for the flavor of the event ! EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ hey , thanks for linking to my blog posts . :) EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ why is this removed ? EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ what do you mean ? EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ it came back ! previously it said removed...some sort of glitch of reddit ? EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ yeah , maybe some kind of cap theorem in action . for me , though, i always was here . EOA 
 nips NUM overviews collection : machinelearning is nips the premier ml conference ? EOQ one more : cinjon.com/nips-2015 EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ i can't make much sense of that question , sorry. sigmoid y-mx-b functions ? those are two different , practically opposite , things in the same noun phrase . basically though , without nonlinear elements , you can't do anything worthwhile at all . with nonlinear elements , you can theoretically compute pretty much anything computable , and it doesn't really matter what the nonlinearity is . some functions are better than others in terms of ease of training and ease of computation , but in some sense anything will work . even relu , which is just f(x)-x if x > ; NUM , NUM otherwise . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ if you represent x as a vector of bits it becomes a formidable problem . only lstms and recent advanced neural gpu models can solve it when formulated this way . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ papers ? EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ URL URL EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ it depends on how you represent the data . if you don't expect the model to generalize to longer sequences ( which is reasonable as you can always zero pad ) , then you can solve this problem with a few fully connected layers and no recurrence pretty easily ( i just tried it yesterday ) . it's a much harder problem to build a model that is trained to multiply NUM bit numbers and expect it to know how to multiply NUM bit numbers . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ i agree with your point , though i'm quite sure that a simple multilayer dense nn can't solve multiplication for n bigger than some small number . bitwise addition and especially multiplication are hard problems due to global non-smooth dependencies between variables ( NUM -NUM -NUM ; NUM -NUM -NUM ) . dense nns don't have enough generalization power to infer these dependencies if they are not shown every possible pair of numbers during training , which becomes prohibitive very fast . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ i'm having a hard time understanding why the more basic architecture of neural nets can't solve a vector of bits represented by y-x2 . are there any papers or basic explanations ? this might be something that has been known/proven for many years or decades , but i can't find papers/explanations . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ why would you need lstm for such a problem if there is no explicit long time dependence ? and i don't know what you mean by neural gpu models but i doubt you need gpu-level performance to learn y-x2 . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ i suppose he meant a temporal stream of bits rather than a vector of bits ? EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ sure , but as long as the temporal correlation is not so strong and with a lack of dependencies on long time windows , lstm is probably overkilling the problem . ( x2 is nonlinear but surely not that hard to learn with much simpler methods , probably even if we were to describe it as a simple ode x.dot-x2 ) . would lstm and other proposed methods ( gpu? ) work ? sure, but i wouldn't go that far when a mlp with a handful of nodes can probably do the same in a really short time . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ lstms are good at modeling sequential algorithms that have state and iterate on it , it's not just time series . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ dumbass it's a paper EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ neural gpu model ? i don't see how the gpu vs cpu implementation detail has anything to do with this question . gpu's are great for speed and if you have lots of data , but a cpu will get there in the end too . lstm ? what aspect of learning a simple function like y-x-x is related to time series data or remembering temporal/spatial lags ? EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ read the paper on neural gpu's ...they are not actually physical gpus' EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ dumbass it's a paper . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ note that one of the more common implementation of a neural network is through the use of sigmoid(wx-b) function in each one of its nodes . so essentially you are telling it to approximate a polynomial function by recursive application of those sigmoid functions . EOA 
 when a neural net solves for y-x-x , does it require learning how multiplication and polynomials work , or are multiplication and polynomials pre-trained via the structure of a neural network ? : machinelearning from a practical perspective , can a neural net solve for y-x2 without the use of sigmoid y-mx-b functions being [ pre-learned and ] used at some or all of nodes ? EOQ right-my speculation is that without the wx-b sigmoid , or other node level function , or what i'm calling learned data , neural nets can't solve y-x2 , even with range and precision levels simple enough to be needed for practical applications . EOA 
